[
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/370",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "fdagostino",
            "datetime": "Aug 2, 2021",
            "body": "When capturing a View with two children, a gl-react Surface rendering a photo taken with the camera and a Image PNG mask on top of it, and saving it to the media library, the saved image contains only the image rendered in the Surface and is not including the PNG mask/overlay.Please, let me know if you need further info.react-native@0.63.2  invalid\n└── react-native-view-shot@3.1.2 Android - ExpoWhen capturing a View it should capture all the visible children.When capturing a View it is capturing only one child.You can find a repro here: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GitKat",
            "datetime": "Sep 5, 2021",
            "body": "Ok I ve been working on it recently. and found a way to achieve this.\nFor some reasons when you Capture directly with camera opened it will always gets on top.\nI solve this issue first taking the picture from camera\nThen sets the Image uri to rasterize other elements  on top of uri came from the camera",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GitKat",
            "datetime": "Sep 5, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fdagostino",
            "datetime": "Sep 5, 2021",
            "body": "Hey , can you provide me with a code example?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GitKat",
            "datetime": "Sep 6, 2021",
            "body": "Ok. my code is not so professional I'm just learning...\nfirst: Im using RNCamera to open camera,\nI dont use RNCamera to take pictures. Im using CaptureRef/CaptureScreen (upto you what you use )inside captureHandle function , this is where I  get the capture from RNCamera, and then setting the URI to a hook and passing\nit to image component and take an auto screenshot() after 600 msHere in second step,  im using ImageBackground Component to set my image as background.now that takeScreenShot() function capture the screen with text on it, im using date for my use case...Now the last step if you want further more actions its upto you. im just uploading at this point.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GitKat",
            "datetime": "Sep 11, 2021",
            "body": "u can close this issue now !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 28, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/377",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "alenmestrov",
            "datetime": "Sep 7, 2021",
            "body": "Hi,Is it possible to use react-native-view-shot with expo-camera in iOS?For example, like this:\n<ViewShot style={{ flex: 1, backgroundColor: \"transparent\" }}\nonLayout={(res) => { this.setVariables(res.nativeEvent.layout); }}\nref={ref => { this.imageContainer = ref; }}>\n<Camera style={{ flex: 1}}\nratio={this.state.ratio}\npictureSize={this.state.pictureSize}\ntype={this.state.cameraType}\nref={ref => { this.state.camera = ref; }}\nonCameraReady={ () => { if(this.state.pictureSize == null) { this.setupPictureSize(); }}}>\n\nThank you very much in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 28, 2022",
            "body": "It's probably not supported at the moment and it would be wiser to have the \"expo-camera\" itself providing a way to \"capture\" on the camera component itself, because snapshotting the View that is produced by the camera won't be the best quality & can easily be broken if the rendering implementation changes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 28, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/327",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "aytacabay",
            "datetime": "Oct 27, 2020",
            "body": "I have a question ?How can I solve this problem?I am making software as a hobby. Sorry if this is logically wrong.\nHow do you think I should do this ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aytacabay",
            "datetime": "Oct 28, 2020",
            "body": "import React, { useRef, useCallback, useState, useEffect } from 'react';\nimport { View, Text, TouchableOpacity, Image, ScrollView } from 'react-native';import { RNCamera } from 'react-native-camera';\nimport ViewShot from 'react-native-view-shot';\nimport Draggable from 'react-native-draggable';export default function CameraFuncScreenShot() {const [base64ScreenShot, setbase64ScreenShot] = useState(null)\nconst [cameraIsOpen, setCameraIsOpen] = useState(true);let viewRef = useRef();let takeAPicture = async () => {\nif (cameraIsOpen) {\nawait viewRef.current.capture().then(base64 => {\nsetbase64ScreenShot(base64)\n})\n}\nawait setCameraIsOpen(false)\n}\nuseEffect(() => { }, [cameraIsOpen])return (\n<ViewShot\nref={viewRef}\noptions={{ format: 'jpg', quality: 0.9, result: 'base64' }}\nstyle={{ width: '100%', height: '100%', flexDirection: 'column', justifyContent: 'flex-start', alignItems: 'center' }}\n>\n<Draggable\nimageSource={require('../assets/imgIcons/1.png')}\nrenderSize={80}\nx={200}\ny={300}\n/>\n{\ncameraIsOpen\n?\n<RNCamera\noptions={{ format: 'jpg', quality: 0.9, result: 'base64' }}\nstyle={{ width: '100%', height: '100%', flexDirection: 'column', justifyContent: 'flex-end', alignItems: 'center', backgroundColor: 'red', }}\ntype={RNCamera.Constants.Type.back}\nflashMode={RNCamera.Constants.FlashMode.on}\nzoom={0}\nmaxZoom={0}\nandroidCameraPermissionOptions={{\ntitle: 'Permission to use camera',\nmessage: 'We need your permission to use your camera',\nbuttonPositive: 'Ok',\nbuttonNegative: 'Cancel',\n}}\nandroidRecordAudioPermissionOptions={{\ntitle: 'Permission to use audio recording',\nmessage: 'We need your permission to use your audio',\nbuttonPositive: 'Ok',\nbuttonNegative: 'Cancel',\n}}\n>\n{\n({ camera, status, recordAudioPermissionStatus }) => {\nif (status !== 'READY') {\nreturn (\n<View\nstyle={{\nwidth: '100%',\nheight: '100%',\nbackgroundColor: 'lightgreen',\njustifyContent: 'center',\nalignItems: 'center',\n}}\n>\nWaiting\n\n)\n} else {)\n}",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "serbanstef",
            "datetime": "Nov 26, 2020",
            "body": "same with react-native-maps. any progress?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Nov 27, 2020",
            "body": "The main readme have a support table to explain what is technically possible and what is not.\nThere is likely no chance we can fix something in this library if some frame don't allow to get pixels from.\nFor camera I would recommend to look at possible ways offer by camera lib themselves to get an image.For reactnativemap a workaround is also mentioned in the main readme",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mobeendev",
            "datetime": "Mar 3, 2021",
            "body": " did you found any solution or other library.\nI am also trying to do something like this, but when a screen-shot is taken it only shows the camera image and the nested/overlayed images is not displayed.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aytacabay",
            "datetime": "Mar 3, 2021",
            "body": "unfortunately no.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "impu1",
            "datetime": "Aug 11, 2022",
            "body": "In case anyone in the future stumbles upon something like this - this might help.\nSo I had the same issue where I had an overlaying component (position: 'absolute') that did not get included in the screenshot. I basically wrapped the component in a view with style={{ flex:1 }} and it started working.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Nov 27, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/395",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "CanRau",
            "datetime": "Feb 27, 2022",
            "body": "I've seen  but still can't get it working. In another screen I pasted  which works just fine, so I've no idea what I'm doing wrong.\nSo far only tested on Samsung Note 8 running Android 9Successfully taking snapshot without failure",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CanRau",
            "datetime": "Mar 30, 2022",
            "body": "Haha, after fixing it myself the last time (forgot about this issues) I had to re-start my React Native project, installing all dependencies etc, I again failed to get it working until I realized that I had set  &  in the  options without realizing that they were (way) bigger then the actual view  I'm not sure how I fixed it the last time, but wanted to share that this might help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CanRau",
            "datetime": "Mar 30, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/258",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "daheeahn",
            "datetime": "Sep 20, 2019",
            "body": "Hi! My component contains both the view and the camera screen (RNCamera). However, when I do 'captureScreen' using view-shot, only the camera's screen is captured and my view is not included in the result. How can I capture the entire screen?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Sep 20, 2019",
            "body": "I'm not sure to understand what you meant by \"only the camera's screen is captured\".\nI think it captures everything except the \"system UIs\" like the status bar, and i'm not sure we can technically do it in Android/iOS.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jan 19, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/313",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "michaelVictoriaDev",
            "datetime": "Aug 4, 2020",
            "body": "I have an error sayingthe captureRef code :\nmy code looks like this",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ViraliVasa",
            "datetime": "Jan 13, 2022",
            "body": " Any solution?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mellertson",
            "datetime": "Apr 25, 2022",
            "body": "Did you find a solution to this?  I am getting the same exact error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mellertson",
            "datetime": "Apr 25, 2022",
            "body": "I got it working.  Here is what worked for me.The bit that got it working for me was to wrap ViewShot inside a SafeAreaView and then inside a ScrollView.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Carry3",
            "datetime": "Aug 15, 2022",
            "body": "because of 'snapshotContentContainer: true' so you should use ref in scrollView maybe waht I say is wrong but when  I delete 'snapshotContentContainer: true' it works",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 28, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/425",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "dfrl-dev",
            "datetime": "Jul 11, 2022",
            "body": "Am I misunderstanting, or is it impossible to capture a custom/complex view?I've implmented a native component for a piece of camera hardware, and I'd like to capture a thumbnail, but can only seem to get blank white images.Is the interoperability table the hard and fast list of supported views?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jul 15, 2022",
            "body": "it may or may not working depending on if the underlying technology allows to be captured with the current approach we have.\nrespectively:Then, there are two possibilities to solve a view that wouldn't be captured:Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jul 15, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/photoprism/photoprism/issues/1362",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "citadella",
            "datetime": "Jun 7, 2021",
            "body": "I was receiving the following errors in my logs when scanning these Fuji RAW files:time=\"2021-06-07T00:34:44+02:00\" level=error msg=\"location: record not found (find cell s2:14cab798d68c)\" time=\"2021-06-07T00:34:45+02:00\" level=error msg=\"location: Error 1366: Incorrect string value: '\\xC5\\x9Fikta...' for column photoprism.places.place_city at row 1 (create place s2:14cab7b3667c)\" time=\"2021-06-07T00:34:45+02:00\" level=error msg=\"location: Error 1366: Incorrect string value: '\\xC4\\x9F\\xC4\\xB1' for column photoprism.cells.cell_name at row 1 (create cell s2:14cab7b3667c)\"time=\"2021-06-07T01:54:24+02:00\" level=error msg=\"thumbs: thumbnail not cached\" time=\"2021-06-07T01:54:24+02:00\" level=error msg=\"thumbs: thumbnail not cached\"After chasing this down a bit it seemed it may have been a permissions issue, but Michael suggested turning off Darktable and it seems RawTherapee does indeed recognize these files:2021-06-07 12:00:57 DEBU camera: cache hit for fujifilm-x100s\n2021-06-07 12:00:57 INFO media: 11 thumbnails created for FujiRAWlg-02073 [1.361047004s]\n2021-06-07 12:00:56 DEBU metadata: a05be2c98993488a46e5fc1e045So it seems to be an issue with Darktable. Darktable was converting the files into .jpgs and I could download the .jpgs from within PP, but they would not display as thumbnails anywhere.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "citadella",
            "datetime": "Jun 7, 2021",
            "body": "Have confirmed it's working within RawTherapee, so it appears to be an issue exclusively with Darktable:2021-06-07 12:51:51 DEBU index: updated yaml file FujiRAWlg-00877.yml\n2021-06-07 12:51:51 INFO converting Unsorted Recovered Pics/Fuji RAW LARGE/FujiRAWlg-00880.RAF to jpg\n2021-06-07 12:51:51 DEBU lens: cache hit for zz\n2021-06-07 12:51:51 DEBU camera: cache hit for fujifilm-x100s\n2021-06-07 12:51:51 DEBU index: image classification took 148.85892ms\n2021-06-07 12:51:51 DEBU index: FujiRAWlg-00878.RAF.jpg was modified (new size 3921779, old size 6234504, new timestamp 1623063110, old timestamp 1622999034)\n2021-06-07 12:51:51 INFO index: updated main raw file “Unsorted Recovered Pics/Fuji RAW LARGE/FujiRAWlg-00878.RAF”\n2021-06-07 12:51:51 DEBU photo: using label Fujirawlg to create title for pquai2j2f0xirhgk\n2021-06-07 12:51:51 INFO index: updated related jpg file FujiRAWlg-00876.RAF.jpg\n2021-06-07 12:51:51 DEBU media: no new thumbnails created for FujiRAWlg-00876 [52.859µs]\n2021-06-07 12:51:51 DEBU index: updated yaml file FujiRAWlg-00876.yml",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mtoupsUNO",
            "datetime": "Jul 31, 2021",
            "body": "I have mixed feelings about this. I have tried both darktable and rawtherapee with my Fujifilm RAF files and they both have problems.Darktable only recently added support for my camera, so previously I needed to disable it. Rawtherapee had support first, but it is buggy (see ) so it is generating bad jpgs.I would prefer to have the ability to choose between them, using the \"Disable Darktable\" and \"Disable Rawtherapee\" checkboxes in Advanced Settings. It looks like  would always disable darktable for RAF even if I decided to disable Rawtherapee also.It may be sensible to default to Rawtherapee instead of Darktable, but please allow the user to override this default using Advanced Settings. Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 31, 2021",
            "body": "Does it work with the version of Darktable that comes with our latest Docker image?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mtoupsUNO",
            "datetime": "Aug 2, 2021",
            "body": "No it does not, when using docker image  which includes darktable 3.4.1.My Fujifilm camera is supported in darktable starting with version 3.6.0 (released about 1 month ago). I verified support for these RAF files with the standalone darktable package.I thought I could test out the new darktable pretty easily by upgrading it within the photoprism docker image. But as it stands, this line If I could override this with a config option (without recompiling photoprism, building a new docker image, etc), that would make it much easier for me to test. For now, I was able to work around this by going back to  which predates Using the older photoprism, I updated darktable to 3.6.0 and then verified that photoprism  work correctly with the new darktable on my Fujifilm RAF files. This confirms that darktable is definitely preferable to RawTherapee in my case. This is why I ask for you to let me choose which converter to use (which was true prior to ). Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Aug 2, 2021",
            "body": "It should work out of the box without additional configuration, so just removing this or asking the user to configure it first won't do. We'll find a solution for our stable release. Note you can change the CLI command names (not the parameters though) to implement more complex logic eg in a  wrapper script.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": "We've added comma-separated file extension blacklists for both converters so that you can configure what to use for which RAW format:Does this work for you?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "citadella",
            "datetime": "Oct 1, 2021",
            "body": "Aye, should do!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 8, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 8, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 8, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 8, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Aug 4, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 21, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 24, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Sep 26, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Sep 26, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Sep 26, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Sep 29, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Nov 11, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/281",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "KaeganClark",
            "datetime": "Jan 3, 2020",
            "body": "Bug report\nI am using captureScreen like so:It worked the first few times I used it and now I keep getting this error:Version & PlatformAndroidExpected behavior\nTake a screenShot.Actual behavior\nError and fail to screenShot.Help is greatly appreciated!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "abdulbasit1248",
            "datetime": "Jan 5, 2020",
            "body": "For android the stable package is 2.5.0 please install this one.Then\nin MainApplication.Java you have to import like this\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KaeganClark",
            "datetime": "Jan 6, 2020",
            "body": "\nSteps I followed:Same error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KaeganClark",
            "datetime": "Jan 13, 2020",
            "body": " \nI have found that if the react native camera is authorized then the captureScreen() fails but if it is unauthorized the screenshot works. Ideas?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KaeganClark",
            "datetime": "Jan 14, 2020",
            "body": "I solved this by removing this for loop  from viewshot.java.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KaeganClark",
            "datetime": "Jan 14, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/photoprism/photoprism/issues/1388",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "zaharcelac",
            "datetime": "Jun 24, 2021",
            "body": "When I import video files (mp4) from my phone they got assigned wrong time. Photos made at the same moment are treated properly.:A photo and a video were shot at 14:51 EDT. Imported photo has correct 14:51 EDT timestamp. Imported video got 18:51 EDT.\nIf I check source video file with  tool, it shows 18:51 UTC.It looks like import ignores timezone metadata from video.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 24, 2021",
            "body": "Cab you provide us with example files for testing?Time zones are not really standardized in metadata. We calculate it based on the GPS data if any.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zaharcelac",
            "datetime": "Jun 24, 2021",
            "body": "Here is the full output of video metadata. These discrepancies happen with both my Samsung phones.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jun 24, 2021",
            "body": "I can confirm the same issue with videos I took with my OnePlus device. I think I noticed this issue also with videos I took with an older Canon camera.\nI'd be happy to provide you with a video & photo which I took with the same device on - where should I sent those to?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Jun 25, 2021",
            "body": "   Testfiles can be send to  :)@zahaecelac we will have a closer look next week",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 30, 2021",
            "body": "Thanks for providing those samples!While the file names show about the same create time (15:33 vs 15:40), the  fields have a few hours difference:When there is no time zone, this typically means it's the local time since there is no convention (which is a shortcoming of the standard). This makes sense as not all photos / videos have GPS data. If it would be UTC by default, you wouldn't know the local time later.The JPEG additionally contains a , which should always be UTC. The file with the \"wrong\" time is the video however, so this doesn't help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 5, 2021",
            "body": "I'll go ahead and close this as we've received no more feedback.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zaharcelac",
            "datetime": "Jul 5, 2021",
            "body": "Hi! Should we expect a fix for videos where dates contain explicit timezone?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 5, 2021",
            "body": "It's something that should be fixed in your phone or camera as its use of the created date field seems inconsistent, see above. Probably best we can do is release the batch edit feature so that you can manually fix it - or have the money to manually maintain a list of problematic devices to fix their metadata. You may also look into existing Exif tools to update your video metadata and then reindex.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 6, 2021",
            "body": " I've checked with iPhone X, Samsung S7, Samsung A5, and OnePlus 7T - all with the same timezone issue. So it's not an obscured bug.What about using exiftool's  flag?See  for the documentation.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 6, 2021",
            "body": "Interesting, does this add a timezone automatically? Need to be careful, can't change the behavior with every release or depending on the Exiftool version.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 6, 2021",
            "body": "From what I know, it's an Android specialty and may also depends on the OS version.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 6, 2021",
            "body": "I'm not an exiftool expert, but I've checked also with a video taken with iPhone X iOS 14.6:Same with Samsung A5 Android 8.0.I came across some old forum threads about this topic:\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 6, 2021",
            "body": "Found this on  So basically as expected. May then be fixed for you, and broken for others. Need a camera model lookup table to get it right for everyone. Worst we can do is changing the behavior with every release.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 6, 2021",
            "body": "If it's an issue of \"fix for some, broken to others\" - shouldn't PhotoPrism aim to have it fixed for the majority of users? I'm not saying that  case is the same as for most users, but the quote you pasted states that UTC is the Quicktime standard in this case.BTW, what about cross checking the timezone using GPS tags when available?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 6, 2021",
            "body": "If you knew how often we changed things back and forth already Impossible for us to know at development time who the majority is. We'll work something out, but after having the time to think about it and perform testing.Thank you very much for looking this up in the Exiftool docs / forum! ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": "Added the Exiftool flag as requested, and started a new preview build:Let us know if this works for you!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 12, 2021",
            "body": "Had to tweak Exiftool parameters to make it work, and started a new preview build for testing:The new Docker image should be available within the next hour, unless the build fails and needs to be restarted (check link above).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 13, 2021",
            "body": "As it turns out, the  parameter converts  to local time using the server's time zone - which doesn't help as it will still be treated as local time by PhotoPrism instead of UTC.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 13, 2021",
            "body": "Was just about to try the preview build and test this.So if I took a video in timezone  and another video in timezone  and my server's timezone is set to  -  will treat all videos as if they were taken in timezone  as the local time?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 13, 2021",
            "body": "With this fix, MP4 and Quicktime video create dates will be explicitly stored as UTC if their metadata doesn't contain a specific time zone. Other video metadata needs to include an explicit time offset or will be assumed to be in local time. I'm starting a new preview build which should be available within the next one or two hours. Check our build server for this:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 13, 2021",
            "body": "Development preview and  have been updated. Feedback welcome!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 16, 2021",
            "body": "Hey , sorry for the late reply - I struggled setting up a preview image alongside a latest image, I ended up using SQLite for that. Plus, I wanted to gather some relevant an irrelevant videos to test this.Anyway - this seems to be working great!\nI did noticed an issue of a mp4 not being recognized as a video but with a \"live\" tag (HTML class  instead of ), and so this fix wasn't applied to it - and so the timezone wasn't detected.\nWhat are \"live\" files as opposed to \"video\" files? Is it an issue of file size and/or video length?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": "Excellent, thanks for testing! Another user tested for us as well, so we thought this is working and I closed the issue.Live photos are somewhat special in that there are \"official\" Apple LivePhotos and short videos we just classify as \"live\" so that you can hover with the mouse to watch them. The last case shouldn't result in other metadata as it's handled directly in the Exiftool JSON parser before the classification as \"live\" happens.Can you provide us with live photos to debug the issue? Either attached to this issue, or send them to  if private.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Jul 16, 2021",
            "body": "My bad - the timezone seems to be correct. I guess I was too fast to jump into conclusion.\nThanks again!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Jun 25, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jun 30, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 5, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 8, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Jul 12, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 12, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 13, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 13, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 16, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lastzero",
            "datetime": "Jul 27, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#1447"
        },
        {
            "user_name": "graciousgrey",
            "datetime": "Sep 26, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "dror3go",
            "datetime": "Oct 25, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#1668"
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/411",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "This is failing the build and started happen after the new release of the package.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "UNIDY2002",
            "datetime": "May 25, 2022",
            "body": "Hi, I've also encountered a similar error:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Looks like this is only supported in updated Java / React-Native versions. What version are you guys using?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": " react native version 0.63.4",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Looks like a problem with Java, you need at least Java 8 for lambdas to work. I think RN changed Java to Java 11 starting 0.65, but I can't find anywhere before that what's the default Java version.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": " is there any other way to solve this with my current react version. As this started happen only from today.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Not without upgrading Java or changing the code to not use lambdas.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Can you share part of your build.gradle files to see whether or not you are setting the java/jdk version to 7?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "// Top-level build file where you can add configuration options common to all sub-projects/modules.buildscript {\next {\nbuildToolsVersion = \"30.0.3\"\nminSdkVersion = 21\ncompileSdkVersion = 30\ntargetSdkVersion = 30\nrenderscriptVersion = 21\nndkVersion = \"20.1.5948944\"\ngooglePlayServicesAuthVersion = \"16.0.1\"\nfirebaseMessagingVersion=\"21.1.0\"\n}\nrepositories {\ngoogle()\njcenter()\n}\ndependencies {\nclasspath(\"com.android.tools.build:gradle:4.1.0\")\nclasspath 'com.google.gms:google-services:4.3.5'\nclasspath \"com.bugsnag:bugsnag-android-gradle-plugin:5.+\"\n//classpath(\"com.android.tools.build:gradle:3.5.3\")\n// NOTE: Do not place your application dependencies here; they belong\n// in the individual module build.gradle files\n}\n}allprojects {\nrepositories {\nmavenCentral()\nmavenLocal()\ngoogle()\nmaven {\n// All of React Native (JS, Obj-C sources, Android binaries) is installed from npm\nurl(\"$rootDir/../node_modules/react-native/android\")\n}\nmaven {\n// Android JSC is installed from npm\nurl(\"$rootDir/../node_modules/jsc-android/dist\")\n}}",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "What's your Java version? Looks like it's not in the configuration file. Try  and  to see what it points to.From RN 0.63, the environment setup clearly states that Java 8 is needed, so I'm not sure why you would be using anything older than that: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "I have installed jdk11 while doing the setup. ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Please also share the  file. The RN docs clearly define Java 1.8 () for RN 0.63. Any chance java 1.6 is defined there?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "There's something your build stack that's telling the compiler to use Java 7, but I can't really find where that would be defined.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "\ncompileSdkVersion 30\nbuildToolsVersion '30.0.3'\ncompileOptions {\nsourceCompatibility JavaVersion.VERSION_1_8\ntargetCompatibility JavaVersion.VERSION_1_8\n}",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Maybe android studio is pointing to java 7? From your error above, the compiler is using java 7 instead of 8.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "If i remove this package, everything seems to be working fine.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cristianoccazinsp",
            "datetime": "May 25, 2022",
            "body": "Yes, but you're still compiling with Java 7, so any new package you add may complain as everything requires at least Java 8.The easiest option is to send another PR to replace the lambda with a regular  call, but the same issue will happen with future libraries. It would be good to find the root cause of why you are using Java 7 instead of 8.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": "Much Thanks, will try it ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 25, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#387"
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 25, 2022",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "EashSundar",
            "datetime": "May 26, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/250",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "viralS-tuanlv",
            "datetime": "Aug 30, 2019",
            "body": "How can i take a shot with rncamera on ios, i try on android work perfect, but ios just a black screen when i take a shot. Have any PR to fix it? thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Taym95",
            "datetime": "Aug 30, 2019",
            "body": "I'm just curious, why you would like to take a snapshot of a camera and not returning picture from rncamera directly?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "viralS-tuanlv",
            "datetime": "Aug 30, 2019",
            "body": "Because i am using rncamera for recoginze face, and if i am using take picture of rncame ra it very slow and heavy size, on android, i used to view shot and very fast and good, so i want to use take screenshot instead take picture of rncamera",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wcandillon",
            "datetime": "Oct 18, 2019",
            "body": " I'm having a similar issue/use case. Is the solution to pipe the camera input into a GLView? Something similar to  ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "minuitagency",
            "datetime": "Nov 16, 2019",
            "body": "Hi, did anyone found a solution for this ?Thanks,Théo",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jan 19, 2020",
            "body": "I think if react-native-view-shot can't capture the pixels (and renders black instead) which happens inconsistently between iOS and Android, there is a low chance we can solve it from this library and I would look more at what the initial library (react-native-camera) offers in term of capturing the camera content.It seems there is a method  which is probably what you need to use for now.If the usecase to use react-native-view-shot was because of compositing/rendering some effects on top of it, I would suggest to look either at react-native-webgl (if you can write it in GL) or simply have an intermediary  and snapshot that, or even writing custom native code to do what you need.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Taym95",
            "datetime": "Sep 11, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#254"
        },
        {
            "user_name": "gre",
            "datetime": "Jan 19, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/245",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "yigerendehaotianqi",
            "datetime": "Aug 18, 2019",
            "body": " iOS? Android?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 18, 2019",
            "body": "I don't think you need this library to take a camera picture, directly use   function: (we probably should remove that mention from our README)thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 18, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/235",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "jordangarvey",
            "datetime": "Jul 23, 2019",
            "body": "I’m trying to render a basic View with a border radius inside the ViewShot component (), but the saved image in the camera roll has a white background. Is it currently possible to save an image with transparency?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": "I don't think this issue is on  side. probably the cameraroll / lib you are using is adding the white color and does not support transparency?Proof that it's not a problem on this lib side:this example implement screenshotting a rounded rectangle and putting it back into an  on top of a background color. It properly works with defaults (which uses PNG)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/227",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "klawdyo",
            "datetime": "Jul 9, 2019",
            "body": "Warning: componentWillReceiveProps is deprecated and will be removed in the next major version. Use static getDerivedStateFromProps instead.view-shot: 2.6.0\nreact: 16.8.6\nreact-native: 0.60.0 Androidget print screen  then save in camera roll",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": "this is now fixed",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/225",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "skuttenkuler",
            "datetime": "Jun 27, 2019",
            "body": "import ViewShot from \"react-native-view-shot\";\ncomponentDidMount () {\nthis.refs.viewShot.capture().then(uri => {\nconsole.log(\"success\")\nCameraRoll.saveToCameraRoll(viewShot, ['photo']);this.state = {\nimageURI : 'file:///sdcard/screenshot.jpg',\n}<ViewShot ref=\"viewShot\" options={{ format: \"jpg\", quality: 0.9 }}>\nAndroidTo take View Snap shot then save snapshot to Android Camera Roll",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sergiosrax",
            "datetime": "Jul 4, 2019",
            "body": "Is happening for me too",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": "please check with latest release (3.0.2) and prefer the use of React.Ref (via React.createRef or useRef)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/214",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "gaminilesh",
            "datetime": "May 15, 2019",
            "body": "Hey I'm having this issue. Please help!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Taym95",
            "datetime": "May 15, 2019",
            "body": "The issue is in import, use it like this:If it works close this issue, please ;)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gaminilesh",
            "datetime": "May 15, 2019",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "davychhouk",
            "datetime": "Jun 3, 2019",
            "body": " I encountered the same problem which was because of improper linking in iOS project.\nYou have to manually link this package since  does not really link the package for you in the iOS project.I was able to get away from this issue following this manual linking: If it works, please consider closing the issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/193",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "nkaczor",
            "datetime": "Jan 13, 2019",
            "body": "I am trying to use react-native-camera along with react-native-view-shot\nI have following code:And I can see this Text component in live, but unfortunately it isn't contained in view shot result image. This also concerns other components as well.\nI've found this  which describes my problem, but without any working solutionreact-native@0.57.8\n└── react-native-view-shot@2.5.0 AndroidElements which are drawn on top of react native camera preview are visible in snapshotElements which are drawn on top of react native camera preview aren't visible in snapshot",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "OleksandrKucherenko",
            "datetime": "Jan 15, 2019",
            "body": "Camera is a difficult thing... its a direct buffer to GPU video memory. Capturing of the screen with GPU video buffer is not so trivial.Camera, OpenGL - those things require special code for making possible capturing and not always its possible.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "OleksandrKucherenko",
            "datetime": "Jan 29, 2019",
            "body": "hint: deep tech details: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jan 29, 2019",
            "body": "Yes, I don't think it will be easy to offer a cross-library dependencies unless we have a way to interop between react native libraries but it's pretty tricky. I think what you are looking for is a way to get a frame from the Camera natively\nif doing effect over it, then you can use in second step the camera snapshot and draw something on top and do a second shot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jan 29, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/181",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "BipinAle",
            "datetime": "Dec 5, 2018",
            "body": "How to get image from uri?captureRef(this.refs.viewShot, {\nformat: \"jpg\",\nquality: 0.8,\nwidth: size, height: size,\nresult:\"tmpfile\"\n})\n.then(\nuri => console.warn(\"Image saved to\", uri),<== this uri is pointing towards android cache file and when I open there is nothing.\nerror => console.warn(\"Oops, snapshot failed\", error)\n);androidI want to the change uri to base64.There is no data in cache folder.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ZMChoo",
            "datetime": "Dec 19, 2018",
            "body": "Hi  , I also faced the same issue here, the cache file is empty, how would you fixed it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BipinAle",
            "datetime": "Dec 20, 2018",
            "body": "@mun5865 cache file is never accessible. In my case, i converted that uri in to base64 and got the actual image.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Piyush132000",
            "datetime": "Dec 9, 2020",
            "body": "hlo friends, i am here with solution of this problem , For solving this problem you have to go on your App permission on your real mobile and allow for camera storage then you can easily save your ViewShot on Your Mobile.go to App Permisssion in your App.info\nallow Camera accesss storage",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BipinAle",
            "datetime": "Dec 5, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/163",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "alz10",
            "datetime": "Jul 20, 2018",
            "body": "how can i change the file destination when i capture() it? the  puts it in a folder where it is hidden and cannot be seen in my phone file manager.For example i wanted to save it in  folder..",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alz10",
            "datetime": "Jul 20, 2018",
            "body": "solved: Using ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "omorhefere",
            "datetime": "Jul 31, 2018",
            "body": "How did you avoid this error: \"Error: open failed: EACCES (Permission denied)\"?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HeinXtet",
            "datetime": "Sep 14, 2020",
            "body": "before save to file request storge permission of both",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Piyush132000",
            "datetime": "Dec 6, 2020",
            "body": "hii bro i am also facing same problem if you find answer of this problem please help me",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Piyush132000",
            "datetime": "Dec 9, 2020",
            "body": "hlo friends, i am here with solution of this problem , For solving this problem you have to go on your App permission on your real mobile and allow for camera storage then you  can easily save your ViewShot on Your Mobile.go to App Permisssion  in your App.info\nallow Camera accesss storage",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alz10",
            "datetime": "Jul 20, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/159",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "personapp",
            "datetime": "Jul 6, 2018",
            "body": "readFile() method of both \"react-native-fs\" and \"rn-fetch-blob\" works well for static asset files or the output of \"react-native-camera\" library. But the readFile() method can not read the file that captured via \"react-native-view-shot\" as tmpfile.react-native@0.52.2\nreact-native-view-shot@2.3.0\nreact-native-fetch-blob@0.10.8\nrn-fetch-blob@0.10.11\nreact-native-fs@2.10.14Am I missing something ?\nThanks in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Jul 6, 2018",
            "body": "how do you check if it worked? is console.warn logging something?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "personapp",
            "datetime": "Jul 6, 2018",
            "body": "This is not the full code. I reduced it to highlight the issue.console.warn logs that RNFetchBlob failed to read file. This only happens when the filePath comes from react-native-view-shot",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "personapp",
            "datetime": "Jul 6, 2018",
            "body": "I also tried to read file via ImageStore.getBase64ForTag(). It throwed \"ERCTERRORDOMAIN0\" error.\nI should also note that, Image component can load the captured image file.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "personapp",
            "datetime": "Jul 10, 2018",
            "body": "Any solution please.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jgreen210",
            "datetime": "Sep 6, 2018",
            "body": ", are you seeing this for android, iOS or both?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jgreen210",
            "datetime": "Sep 7, 2018",
            "body": "If this problem you are having is just for android, there's some chance  might help.  Although, I suspect you have some other problem. That's since we didn't get any failures due to the issue fixed by this PR in our tests (which compare pngs saved by this library with checked-in reference images) and since you're getting errors for base64 images too, and there's no buffering involved in that case.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "serhiipalash",
            "datetime": "Nov 13, 2018",
            "body": "Hi  !\nI think this bug is still active. has this bug in Expo 31 compare to Expo 30 (iOS tested). In Expo 31  was updated to use  v2.5.0, and now  result \"tmpfile\" uri is not compossible with  or  any more.\nThat is because \"tmpfile\" uri path is in  and  and  can only work with paths that are in public system scope . When you try to save taken view snapshot to user photos or temporary dir, you will see an error \"Couldn't read ...\".As I remember in Expo 30 version of  was something around v1.1.0.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 11, 2019",
            "body": "I think this is fixed. I was properly able to get the data from file and render it back in works on iOS and Android.Please report back if you are still having an issue (writing an example to reproduce the bug is the best)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 11, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/148",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "hungdt-ibl",
            "datetime": "May 17, 2018",
            "body": "  Android?Build release android complete",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "OleksandrKucherenko",
            "datetime": "Oct 16, 2018",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 10, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/91",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "herel",
            "datetime": "Aug 28, 2017",
            "body": "** render device **\n\nAny suggestion?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "herel",
            "datetime": "Aug 29, 2017",
            "body": "Fixed  :Dchange in RNViewShotModule.javaFile externalCacheDir = context.getExternalCacheDir();\nFile internalCacheDir = context.getCacheDir();toFile externalCacheDir = context.getExternalCacheDir();\nFile internalCacheDir = context.getExternalCacheDir();Switch to an accessible directory or\nHe seems to be running out of memory",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 29, 2017",
            "body": "Hi, thanks for the issue,that createTempFile code was borrowed from react-native's it looks like the current logic to pick the cache folder is:so i'm not sure it's about running out of space problem because it should always pick the bigger space.. however maybe there are some weird other issue?would you mind trying to investigate more, like what is the exception exactly (if any?)also try to log value of  and ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Sep 22, 2017",
            "body": "not sure what we can do better than current approach (which should chose the best available space)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Sep 22, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/85",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Friendly-Robot",
            "datetime": "Aug 22, 2017",
            "body": "I'm currently using a derivative of React-Native-Camera that doesn't have a capture functionality. So in order to \"capture\" the image of the camera, I'm attempting to utilize React-Native-View-Shot. However, despite successfully returning the promise with the uri, the saved view shot renders a black image. Is this behavior normal for camera views?I've tried taking the view shot from  and from the  to no avail. Does anyone have another suggestion which I may attempt?Unfortunately, using a different camera library which does support capture is not an option. :(",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Friendly-Robot",
            "datetime": "Aug 23, 2017",
            "body": "I ended up switching between the non-capturing camera view and React-Native-Camera using state in order to get the shot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 23, 2017",
            "body": "Yeah I think camera as well as videoviews and glkviews is going to produce a black result (and not sure if this can be solved).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Friendly-Robot",
            "datetime": "Aug 23, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "n1ru4l",
            "datetime": "Jan 8, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "react-native-camera/react-native-camera#2035"
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/79",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "fxfactorial",
            "datetime": "Aug 13, 2017",
            "body": "Hi,I am trying to take a snap shot of an image that is created by react-native-camera along with an image inside of that image, to create a watermark like effect. What I'm getting though is a blank image, I feel like the code is correct and that the zIndexing might be part of the part?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 25, 2017",
            "body": "Hi  , are you still experiencing this issue, have you find a workaround? I have not tested snapshotting zIndexing a lot yet, maybe we should test with a simpler example see if things would work (like snapshotting a container with 2 absolute rectangle overlapped zIndex)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Aug 25, 2017",
            "body": "Also I don't think  as a background is longer supported in React Native, they seems to have introduced an ImageBackground tho (  )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fxfactorial",
            "datetime": "Aug 25, 2017",
            "body": " Yes, still have this issue. Correct, Image this way is deprecated but this is on 0.45 when it was okay.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonasmorthorst",
            "datetime": "Feb 2, 2018",
            "body": "Hi  - are you still facing this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fxfactorial",
            "datetime": "Feb 2, 2018",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Mar 29, 2018",
            "body": "please try with  , probably will fix it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mobeendev",
            "datetime": "Mar 3, 2021",
            "body": " I put an image on top of camera and took screenshot, but image is not displaying when screen shot is taken!\ncan you suggest what should I do.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gauravroyzz",
            "datetime": "Aug 5, 2021",
            "body": "any update on this? i have a mapView with 2 views absolute positioned , I cant see them on the screenshot",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fxfactorial",
            "datetime": "Aug 13, 2017",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Mar 29, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/61",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "i mean if you save image into path: DCIM/Camera ,\nthe image file could find in [file manager],but not showed in system photo application",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Apr 30, 2017",
            "body": "have you tried to use  ?according to some answers on  to save into gallery should be android's  which is exposed by the lib in dirs.PictureDir with this code: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "i am  very gratefull to recieve your reply,  i have allready tried use dirs.PicturesDir , and DcimDir ，if you use origin-android-system , image in those Dir is abled to be finded in gallery 。 but if you  use customization-android-system  ,such as xiaomi,oppo,huawei,lenovo ,it is can't , it lead to if user  use other app such as wechat or facebook ....,they can't find the image they have captured.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Apr 30, 2017",
            "body": "I'm not sure to know the answer :'(\nmaybe the customization-android-system use another path?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "i checked those picture,it is in the same path。but not showed，it is seems customization-android-system have traced the Image url  when take a photo or take a screenshot or save a picture 。and only show picture is traced",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "\nhere is a sample , when i use wechat, wechat could display the image i have save from browser or screenshot i have taked just now。but, not showed if i use  react-native-view-shot,  and they are in the same path.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Apr 30, 2017",
            "body": "maybe they rely on the filename? (like the DCIM number) or maybe they rely on a given extension?\n(have you tried JPG vs PNG ?)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": ",yeah,i have tried, i tried different filename and quality, even the same file name but still not show。i  search the reason from net,seems when save a image,it is neet to notice system ,to let system know gallery is updated.code like this\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Apr 30, 2017",
            "body": "do you want to try that and provide a PR if this works?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "i don't know the real reason,i am not a android developer,i just search from the net. it is a difficult work for me ,but i think i will spend time to figure it out  ...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "Apr 30, 2017",
            "body": "now it 3:00 clock beijing time ,i need to go sleep.i will try it tomorrow . i think this is the solution ,it is in stackoverflow,the link you give me",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "May 7, 2017",
            "body": "hey,dear  , i have tried and suceed by add this,work perfectly\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 7, 2017",
            "body": "oh! cool :) would you like to create a PR ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aqnaruto",
            "datetime": "May 7, 2017",
            "body": "thank you ,but ,what is pr?  public project??pull request? i am allready create a pull request",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 7, 2017",
            "body": "sorry, a Pull Request, on Github",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 23, 2017",
            "body": "see PR in short: a better solution is to use CameraRoll.saveToCameraRoll and we will deprecate use of path in this library",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "May 23, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/gre/react-native-view-shot/issues/58",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "valentinancy",
            "datetime": "Apr 26, 2017",
            "body": "I was tried the example code and i got a uri like this:\nfile:///data/user/0/com.last/cache/ReactNative-snapshot-image1446502249.png just like:\n\"/storage/emulated/0/Pictures/ReactNative-snapshot-image799473449.png\"so i could use the path for the CameraRoll library from react native like:\nCameraRoll.saveToCameraRoll(path,'photo')Thanks and please help hehe",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gre",
            "datetime": "Apr 26, 2017",
            "body": "have you tried using the  option ? ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "valentinancy",
            "datetime": "Apr 26, 2017",
            "body": "Thanks! sorry i miss reading your documentation!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhawnaparasher",
            "datetime": "Apr 23, 2019",
            "body": "How did you solve this?\nI am facing the same issue, please help!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "valentinancy",
            "datetime": "Apr 26, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/77",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "sffranke",
            "datetime": "Dec 13, 2019",
            "body": "Hi all,\nthe docker image crashes after trying to set up a wakeword and refuses tu start up again.\nSo I tried the virtual environment version. It always crashes when restarting after saving changes.\n./run-venv.sh: line 28:  2303 Segmentation fault      python3 app.py \"$@\"Any hints how to run this amazing software under raspbian buster?\nTIA\nSteffen",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Dec 13, 2019",
            "body": "I'm getting a lot of bug reports lately about the latest Rhasspy on Rpi, so I'm guessing there's something out of whack with Python 3.7 and one of the dependencies. I'll take a look ASAP and push a new version soon with (hopefully) some fixes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sffranke",
            "datetime": "Dec 13, 2019",
            "body": "At the moment I just restart it after each save 'n crash. I am very impressed of your work, thanks so much for sharing!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KiboOst",
            "datetime": "Dec 13, 2019",
            "body": "Hi,I had exact same behavior/error.Here is my config:\nConfig:But curiously I don't have it anymore ! The only change I can remember of is settings a custom snowboy hotword. Not sure if realated of course.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "frkos",
            "datetime": "Dec 23, 2019",
            "body": "Just switched from ALSA to pyAudio and had the same issue...\nI've tried to change microphone device in settings and found that when I select  the issue disappears (I'm using PS3eye camera as a mic)So in profile I have:Please try to do the same, maybe it will help you too\nI'm in the middle of testing but the result is promising",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NullEnt1ty",
            "datetime": "Jan 26, 2020",
            "body": "In my case this was fixed by switching from  to . I'm using the  which seems to act odd with PyAudio.This is the content of my  file if someone wants to debug:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "moritzschaefer",
            "datetime": "Mar 22, 2020",
            "body": "I still face this exact same issue (Pi 3B). I pulled the most recent git repository and each time I save+restart from within the webapp, I get the segfault.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Dec 13, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Jan 15, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/2277",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "mbrine555",
            "datetime": "May 1, 2018",
            "body": "When using the dependency parser, it seems like there can be a lot of ambiguity when trying to assign negation, unlike with something like Stanford's parser.For example:returnsThis output seems to imply that the negation could refer to either , which is not the case. A Stanford output for the same sentence looks something like:Is there any way to clear up this ambiguity currently?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "May 1, 2018",
            "body": " Which Stanford dependency scheme is that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mbrine555",
            "datetime": "May 1, 2018",
            "body": " Sorry, I should've been more clear. It's the Universal Dependency scheme used in Stanford's CoreNLP 3.9.1.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dxiao2003",
            "datetime": "Jun 15, 2018",
            "body": "If you run the visualizer you can see that the \"not\" refers specifically to the first \"is\" token.  Not exactly the same parse as Stanford but at least it's not ambiguous as to which \"is\" it's referring to.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Dec 14, 2018",
            "body": "Merging this with . We've now added a master thread for incorrect predictions and related reports – see the issue for more details.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Jan 13, 2019",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "May 1, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jul 6, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Aug 15, 2018",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Dec 14, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Jan 13, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/2",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "dmshimself",
            "datetime": "Dec 31, 2018",
            "body": "Hi - I installed a fresh copy of hass.io on a Pi3 with a ps3 USB camera plus audio USB device plugged in and managed to get to the web interface just fine.  As a test, I selected that (2: USB Camera-B4.09.24.1: Audio (hw:1,0). and press Hold to Record and held the mouse down, but when I let go, the Release to stop remained red.  I left it for a while but nothing came back.  In the add on log I get the following.  Any thoughts appreciated and I'm happy to run any other initial tests recommended.  The main hassio system log showed nothing unusual that I could see, just saying the add was started. version 1.13",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Feb 18, 2019",
            "body": "Everything in the log looks OK to me. Can you try updating to the latest version and, if that doesn't work, try switching the microphone system from PyAudio to ARecord in the Settings page? Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Mar 21, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/okfn-brasil/serenata-de-amor/issues/69",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Irio",
            "datetime": "Sep 16, 2016",
            "body": "No legalese allowed in this document. It is expected to be written just in Portuguese, since the data journalists are all fluent in the language.Check \"Texto Atualizado\" at  for the original text.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Sep 21, 2016",
            "body": "Artigo 2 - Atende as seguintes despesas:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Sep 21, 2016",
            "body": "Artigo 1 - Pode utilizar mais R$ 1353,04 reais, o deputado que for:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Sep 21, 2016",
            "body": "Artigo 3 - A cota pode ser usada da seguinte forma:Artigo 4 - Reembolso é efetuado mediante requerimento padrão, assinado pelo parlamentar, que assume inteira responsabilidade pela despesa, atestando que:4, 5 e 6: Estão no item 2.Sobre o item de uso da cota para ursos, palestras, seminários, simpósios, congressos ou eventos do mesmo gênero:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Sep 21, 2016",
            "body": "Artigo 5. Sobre transporte aéreo e de serviços e produtos postais podem ser feitos desde que sejam comprovados com Requisição de Passagem Aérea (RPA) e Requisição de Serviços Postais (RSP)Artigo 6. As empresas de transporte aéreo credenciadas devem informar, quando solicitado, informações detalhadas dos bilhetes emitidosArtigo 7. Se o bilhete emitido é contra as normas, é descontado automaticamente em folha de pagamentoArtigo 8. Despesas com telefone só são reembolsadas quando comprovadas que são de responsabilidade do deputado.Artigo 9. Não é admitido aluguel de imóvel que pertença ao próprio deputado ou qualquer entidade que ele tenha participaçãoArtigo 10. Artigo 11. A cota é calculada proporcionalmente ao período de efetivo exercício no mês.Artigo 12., desde que não haja convocação de suplenteArtigo 13. Artigo 14. A cota não pode ser adiantada, transferida de um beneficiário para outro.Artigo 15. ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 3, 2016",
            "body": "Great work! Let's publish it? Where? We can convert it do  and add to the repo, or as a linked Gist, whatever…",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Irio",
            "datetime": "Sep 16, 2016",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Oct 19, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#91"
        },
        {
            "user_name": "cuducos",
            "datetime": "Oct 20, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Nov 4, 2016",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 5, 2016",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "anaschwendler",
            "datetime": "Nov 5, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 8, 2016",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "wisner23",
            "datetime": "Nov 13, 2016",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "Irio",
            "datetime": "Feb 27, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Feb 28, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/7744",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "mitchellspryn",
            "datetime": "Apr 11, 2021",
            "body": "When attempting to install spacy via pip (both CUDA and non-CUDA) on ubuntu 18.04, there are lots of compilation errors:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "Apr 12, 2021",
            "body": "Hi, you probably just need to upgrade pip and related tools before installing spacy:After upgrading pip, it should install binary wheels on x86_64, so you probably won't need to compile anything. And even if you do, the newer pip will be able to compile the package correctly.We would also strongly recommend using a virtual env. You can see general install instructions here, click \"virtual env\" to add the additional instructions for that:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "Apr 12, 2021",
            "body": "Let me move this to installation section of the new discussion board...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "Apr 12, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "explosion",
            "datetime": "Apr 12, 2021",
            "body": [],
            "type": "locked and limited conversation to collaborators",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/1826",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "sanjeeku",
            "datetime": "Jan 11, 2018",
            "body": "I just installed Spacy 2.0.5 in Python 3.6.4 (that Anaconda).\nI also installed the default model ('en')\nSpacy is giving seg fault when I try to load my text file (it is about 2MB in size).Here's the code the reproduces it:\nPython 3.6.4 |Anaconda, Inc.| (default, Dec 21 2017, 21:42:08)\n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.I tried with another text file (slightly larger though) with the same result.Is there any other information I can provide to troubleshoot this seg fault?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sanjeeku",
            "datetime": "Jan 11, 2018",
            "body": "An update:  I tried on a new/clean aws instance where I installed Spacy differently (using conda forge). I still got the same seg fault.Here's the environment info:\n(py3) ubuntu@ip-172-31-16-211:~$ python -m spacy info --markdown",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sanjeeku",
            "datetime": "Jan 11, 2018",
            "body": "Further update:\nI had an old conda env with spacy 1.9.0 installed.\nBoth text files were parsed perfectly.\nSo the SegFault issue is only with Spacy 2.0 or later (I have tested with 2.0.4 and 2.0.5)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "godelstheory",
            "datetime": "Jan 30, 2018",
            "body": "I am experiencing a similar issue, though it occurs when using the English language model  method. The problem occurs < 1% of the time in a corpus of 250K documents, but I have yet to determine its root cause. An example paragraph is shown below.Similarly, the problem occurs in 2.0.5, but is not present in 1.9.0. I have reproduced this across multiple machines.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sanjeeku",
            "datetime": "Feb 5, 2018",
            "body": "Confirming that this bug continues to exists in Spacy v2.0.7\n - I am happy to privately send you the text files on which it is bombing. Please let me know where to send.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Feb 17, 2018",
            "body": " Thanks, could you mail to  ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sanjeeku",
            "datetime": "Mar 5, 2018",
            "body": " -- Just emailed the text file.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 29, 2018",
            "body": " Thanks for the text, finally got to this.Your issue is simply that the text is too long. This is rather frustrating --- I wish we used less temporary memory per word than the neural networks currently do. However, I don't see a way around this without significantly impacting performance.I've added an error message and added an option on the  class to note the problem. In your case, the solution is very simple: just process each newline individually. Your problem is different. I think the problem occurs from parsing the text twice. This shouldn't cause a segfault, but as a workaround, you can avoid doing that for now? You can verify that the double-parsing is the problem by changing the first line to .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Jan 7, 2019",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Jan 12, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 29, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 29, 2018",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "idc9",
            "datetime": "Apr 9, 2018",
            "body": [],
            "type": "issue",
            "related_issue": "chartbeat-labs/textacy#154"
        },
        {
            "user_name": "ines",
            "datetime": "Dec 8, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Jan 7, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/okfn-brasil/serenata-de-amor/issues/95",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "pmargreff",
            "datetime": "Oct 26, 2016",
            "body": "Hi guys, how about data visualization, do you already chat about? It could be a good way to show to people who haven't any technical skills or are from different field how the project would save a lot of money. I started develop some charts using D3. I'm curious to know about the kind of features you think is relevant to show (?). At this time I'm trying the money by person, by state and by subquota, what do u think about it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Oct 27, 2016",
            "body": "That's a great topic — thanks !I'm aware  was working on something visual, and  might be interested in this material for communication purposes. this is a topic to be discussed in the  in the near future — when we consolidate a structure for a website focused on communication this repo here tends to narrow its focus to data science, while the other one will embrace many communication related topics I guess.Filipe and Pedro — what do you think?Pablo would you mind sharing something you already have? Maybe some screenshots would be nice to give more substance to what can be done from it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "filipelinhares",
            "datetime": "Oct 27, 2016",
            "body": "Hey !I'm studying data vis and starting to dive in D3 and other tools. As  said, when we consolidate a structure for the new website we can use visualizations to improves the experience and interaction with the data.Data visualization is an awesome field to explore in this project .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Oct 28, 2016",
            "body": "We think on three basic kind of visualizations to startup.: Compare total values by states monthly, we do that dynamically using .: A heatmap, to show companies who have received most piece of money, the heatmap show the value, company name, cnpj and ranking position on mouse hover event. We use  because we have found only D3 heatmap using days and months to index the values (calendar heatmaps), maybe in near future we can adapt  to accept a simple matrix and use that.: A  to each congress person. The radar contain 18 axes with subquota categories and the money spent in the each category. Maybe in a second moment we can join 2 or more persons in the same radar to compare (). We have tried use , but the problem is, it was build in D3 V3 and it isn't compatible with D3 V4, would be awesome if someone help or refactor this model to version 4, because this Radar looks better in compare with all others.After finish an reasonable model and documenting the basic (maybe on the next week) I can open the repo to anyone get it and add new visualizations.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Oct 29, 2016",
            "body": "Another thing, maybe show the values by person in this , where each color is a sub-quota group and inside we can set the companies and correspondent value in the proportion. What do you think about that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Oct 31, 2016",
            "body": " Those pieces of dataviz are awesome! I think we should fit it somewhere, sure thing.  do you think the new website has an section for that?  any thoughts on that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Nov 3, 2016",
            "body": "Hey,  a (temporary) first version, you can check the  with a basic documentation to start up the project. Feel free to suggestions or ideas.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 3, 2016",
            "body": "Wow! That's awesome. Thanks for that, !I definitively believe this could help people understand the importance of this quota.  can take it into account while planning our website and  while planning our communication.Just one minor detail (I'm not criticizing, just trying to make data more meaningful for people): I think the view by state has a lot of bias: , and the allowance also differs from state to state (some pages there are returning  — we can check that later). How difficult is it to ponder the total by state according to:Once more, many thanks, mate!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Nov 3, 2016",
            "body": " I understand, it could point to the wrong way if u don't have all details, but I don't think it's hard to fix.For the first suggestion is possible  from this state, it will show the mean, and show the number of congress person and total value in the tooltip.I'm only a little confuse with the second one, you say the possible total  or the ? And the suggestion is about generate a number equalizing this two metrics in the same one or divide in two different views/charts?About the  - it's probably because it isn't a server, in the really is something like by backup computer and it isn't properly prepare to maintain a website stable. I have another problem with the size of  from the third view, I thinking how compact or make this file smaller (1.3 MB on this moment), but I haven't any good idea yet.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 3, 2016",
            "body": "About the second one, I suggest (it's merely a suggestion, I haven't put a lot of thought on it) dividing:On a second stage dataviz could show who (within this given state) pushes the mean up or down…About the  it was com camera.gov.br (not on your server). I was trying to link the max allowance by state for you ; )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Nov 3, 2016",
            "body": "Yes, I get it and make some sense, but I don't know if will have some impact if the people don't have an idea from the value itself. Maybe a line or something marking the max limit could representing almost the same.I really like about the suggestion to see the outliers. I'll think in something and exec when I have some time.I updated to mean value, and it really equalize much better, but some weird things happen' like in the lasts months of year (2014/2015) we can see the value bit the max. I'll try to find why and when it's happen to try explain this point.Another observation, I was checking the last behavior and the  value isn't consistent for all occurrences, you can .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 3, 2016",
            "body": "Good. I couldn't spend some time on that today, I'm sorry about it.We're debating this on , but we haven't reached a decision yet. Hold on a while longer ; )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ronybarbosa",
            "datetime": "Nov 4, 2016",
            "body": "Why don't you use tools like kibana for data visualization ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Nov 6, 2016",
            "body": " About the net_value bit the roof, I did found any reason, but I send a request to like you suggest on .@ronydj Hello, I never use that, the only thing I know: the people use that with Elastic Search (and it doesn't mean nothing to me). If you know, you don't care about teach, you have free time, contact me on: pmargreff at gmail dot com. I added two new charts on site, monthly value and average by party(really like this one).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 6, 2016",
            "body": " Looking forward to check what they're gonna say ; )There's a small  (Montly instead of Montly in one of the titles). But overall it's very good ; )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kassimorra",
            "datetime": "Nov 10, 2016",
            "body": "Hi Guys,\nFriend of mine told my about this great project. I got interested on this subject.Is there anywhere that can I see what you want to show ?I read this topic but didn't found the storytelling or the analysis that need to be done.Kassim",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pmargreff",
            "datetime": "Nov 10, 2016",
            "body": "I get a answer about the net_value bit the max value, the complete answer was:Translate:And the  say that. I believe it could impact on some other metrics too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Nov 11, 2016",
            "body": "Great,  — many thanks for sharing their response.Welcome . Sorry about not getting back to you sooner.We were discussing that these days and probably  will be in touch — he's focused on communication and probably you both could better discuss what would be interesting in terms of .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Mar 24, 2017",
            "body": "Closing this as dataviz is more relevant at  repo now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Oct 31, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "cuducos",
            "datetime": "Mar 24, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "Irio",
            "datetime": "Feb 27, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "thaisvergani",
            "datetime": "Feb 2, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#454"
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/2470",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "prashant334",
            "datetime": "Jun 21, 2018",
            "body": " \nbelow is a code that I have used as u mentioned in my previous question.\nafter execution getting error likeTraceback (most recent call last):\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 118, in \nmain('en')\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 86, in main\ntrain_ner(nlp, train_data, output_directory)\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 32, in train_ner\nfor batch in minibatch(get_gold_parses(nlp.make_doc, train_data), size=3):\nFile \"/home/prashant/.local/lib/python3.6/site-packages/spacy/util.py\", line 393, in minibatch\nbatch = list(cytoolz.take(int(batch_size), items))\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 19, in get_gold_parses\ngold = GoldParse(doc, entities=entity_offsets)\nFile \"gold.pyx\", line 418, in spacy.gold.GoldParse.\nKeyError: 0",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 22, 2018",
            "body": " ANY UPDATE HOW TO RESOLVE THIS?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SandeepNaidu",
            "datetime": "Jun 22, 2018",
            "body": "Prashant,\ntrain_data is empty? Did you not paste that code? Can you paste some sample?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 22, 2018",
            "body": "I intentionally keep it empty because they are too big.  But here I am attaching\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SandeepNaidu",
            "datetime": "Jun 22, 2018",
            "body": "Try changing this line\ngold = GoldParse(doc, entities=entity_offsets)\nto\ngold = GoldParse(doc, entities=entity_offsets['entities'])Later after fixing this,\nAlso the label strings you are adding to the ner component are different from the sample text you have given. Make sure they match.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 25, 2018",
            "body": " yes labels I have changed. Please review below code.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 25, 2018",
            "body": "\ngold.pyx key error is not now showing. But ner results are zero. Here I am attaching my test data.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 25, 2018",
            "body": "   What could be the reason for gold.pyx error?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jun 25, 2018",
            "body": "As  mentioned above, the labels in your code are not consistent. In some places, you're using , and in others, it says . If your results are bad, this can also have other reasons: for example, the task might just be difficult to learn, especially with just a handful of examples.Crime locations and victims aren't very well-defined, independent categories. They're mostly defined by the  and  within the text. For example, if \"California\" refers to the US state, it's always a  entity. But is it a crime location? That really depends. Sometimes you can find that information in the same sentence. Sometimes not. That makes those labels very difficult to predict from machine learning point of view. A better strategy for training a model could be to focus on improving the predictions of , ,  etc. and then use the dependency parse (or a custom parser) to extract whether they refer to a crime location or a victim.By the way, for general usage questions like this that are specific to your code, you might want to  instead. This will reach more people and you'll often get quicker replies. On the issue tracker, we mostly try to focus on bug reports, performance issues and feature requests. It's also always helpful if you can provide a small code snippet that illustrates the problem, instead of dumping an entire script. This makes it easier for us and others to reproduce the problem and help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "prashant334",
            "datetime": "Jun 25, 2018",
            "body": " I have used CRIME_LOCATION only in nlp.meta['name']. Another question is how to assign 2 entities for meta.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Jul 25, 2018",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jun 21, 2018",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jun 25, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jun 25, 2018",
            "body": [],
            "type": "issue",
            "related_issue": "#2479"
        },
        {
            "user_name": "lock",
            "datetime": "Jul 25, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/711",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "dexterbeng",
            "datetime": "Jan 1, 2018",
            "body": "First of all, sorry for I am a beginner, and thanks to those people who read this topic!I have a small request is that once my PIR sensor detects an object, it will trigger my Picamera to streamed real-time to preview the video feed on the mirror display for around 5 seconds.How to accomplish it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "Jan 4, 2018",
            "body": "to accomplish this, you will have to write a plugin to extend the mirror functionality.i have extended the autosleep function to use the camera of my webcam for motion detection.\n(see my smart-mirror/motion pull request to look at the code. mostly in the motion.js detection side)but I use an external program for the motion detection..  see    it can also stream the video..  you would have to write the code that opens the window and streams the data to it.. (and close the window etc)..",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dexterbeng",
            "datetime": "Jan 21, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/702",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "dexterbeng",
            "datetime": "Dec 18, 2017",
            "body": "First of all, I added up a pi camera with my mirror, and I want to display the capturing video in my mirror display, how do I set up with my repository?The expectation result as shown below:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dexterbeng",
            "datetime": "Jan 1, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/690",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "transmit-ter",
            "datetime": "Oct 19, 2017",
            "body": "Hi there!\nMy Smart mirror does not respond to microphone\nThere are no errors in all sources when i run\npi@raspberrypi:~/smart-mirror $ npm startI did follow step by step process on docs.smart-mirror.io\nI tried rec test.wav with PS eye like post , the bars moved but It did not work on smart mirrors.pi@raspberrypi:~/smart-mirror $ aplay -l\n**** List of PLAYBACK Hardware Devices ****\ncard 0: ALSA [bcm2835 ALSA], device 0: bcm2835 ALSA [bcm2835 ALSA]\nSubdevices: 8/8\nSubdevice #0: subdevice #0\nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \ncard 0: ALSA [bcm2835 ALSA], device 1: bcm2835 ALSA [bcm2835 IEC958/HDMI]\nSubdevices: 1/1\nSubdevice #0: subdevice #0pi@raspberrypi:~/smart-mirror $ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: CameraB409241 [USB Camera-B4.09.24.1], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0pi@raspberrypi:~/smart-mirror $ nano ~/.asoundrc\npcm.!default {\ntype asym\nplayback.pcm {\ntype plug\nslave.pcm \"hw:0,0\"\n}\ncapture.pcm {\ntype plug\n# This is your input device (it may be different from what is seen here)\nslave.pcm \"hw:1,0\"\n}\n}My Environment\nEnvironment name and version: Raspberry Pi 3, node.js v6.11.4\nOperating System and version: Raspbian",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Oct 19, 2017",
            "body": "I  I know how to fix this :) From the smart mirror folder run:Then go into the remote config app and ensure that you have the correct input device selected and save.Give that a shot and let me know if you run into any issues.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "transmit-ter",
            "datetime": "Oct 20, 2017",
            "body": "well, i tried\ngit pull\ngit checkout dev\nnpm install sonus@next\nand config my input device (USB Camera-B4.09.24.1)\n\n, but it did respond once after i said \"what can i say\". After that nothing happened.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ghost",
            "datetime": "Dec 12, 2017",
            "body": "i have the same problem did u find solution?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Dec 13, 2017",
            "body": "You may also want to take a look at ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "transmit-ter",
            "datetime": "Dec 26, 2017",
            "body": "Yeah thanks a lot for your help, it worked",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 24, 2017",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "transmit-ter",
            "datetime": "Jan 1, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/654",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "poetic420",
            "datetime": "Jun 12, 2017",
            "body": "i closed the magic mirror window..\ni try to open it again and cant..pi@raspberrypi:~ $ npm start\nnpm ERR! Linux 4.9.24-v7+\nnpm ERR! argv \"/usr/bin/nodejs\" \"/usr/bin/npm\" \"start\"\nnpm ERR! node v6.11.0\nnpm ERR! npm  v3.10.10\nnpm ERR! path /home/pi/package.json\nnpm ERR! code ENOENT\nnpm ERR! errno -2\nnpm ERR! syscall opennpm ERR! enoent ENOENT: no such file or directory, open '/home/pi/package.json'\nnpm ERR! enoent ENOENT: no such file or directory, open '/home/pi/package.json'\nnpm ERR! enoent This is most likely not a problem with npm itself\nnpm ERR! enoent and is related to npm not being able to find a file.\nnpm ERR! enoentnpm ERR! Please include the following file with any support request:\nnpm ERR!     /home/pi/npm-debug.log",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 12, 2017",
            "body": "I ran the comman in directiry\nstill a problempi@raspberrypi:~/smart-mirror $ npm startRemote listening on \nERROR s:641\nreturn binding.open(pathModule._makeLong(path), stringToFlags(flags), mode);\n^Error: ENOENT: no such file or directory, open '/home/pi/smart-mirror/keyfile.json'\nat Error (native)\nat Object.fs.openSync (fs.js:641:18)\nat Object.fs.readFileSync (fs.js:509:33)\nat Object. (/home/pi/smart-mirror/sonus.js:17:29)\nat Module._compile (module.js:570:32)\nat Object.Module._extensions..js (module.js:579:10)\nat Module.load (module.js:487:32)\nat tryModuleLoad (module.js:446:12)\nat Function.Module._load (module.js:438:3)\nat Module.runMain (module.js:604:10)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 12, 2017",
            "body": "Looks like you're missing your Google Cloud Speech keyfile: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 13, 2017",
            "body": "hey thanks for replay!\nthis is my first project and im new to all of this so sorry if i have stupid questions..i have an issue with the Train module keyword (snow boy)\ni downloaded the public module, but i cant seem to understand where to save it\"\" Once trained, download the model and save it to the root of the .\"\"HOW do i save it in root of the smart mirror dirctory????\ncan you guys explain where do i need to save it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 13, 2017",
            "body": "the root of your smart mirror is . Here is an overview of some basic naviation and file manupulation in linux:\nYou can also use the Pi's GUI file system. Just download your model and then use the file system to move it (click and drag, nothing complex).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Jun 15, 2017",
            "body": " is this now resolved?If not please update us and let us know how we can help. If it is please comment what solved the issue and then close the issue. thank you.Also, we're available on  to help assist you in real time.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 19, 2017",
            "body": "well, i tried everything you said...\nI run my smart mirror and no mic recognition or any reaction to \"smart mirror\" command..\nwhen run \"npm start\" i get the following error:pi@raspberrypi:~/smart-mirror $ npm startRemote listening on \nERROR le.js:471\nthrow err;\n^Error: Cannot find module '/home/pi/smart-mirror/node_modules/grpc/src/node/extension_binary/grpc_node.node'\nat Function.Module._resolveFilename (module.js:469:15)\nat Function.Module._load (module.js:417:25)\nat Module.require (module.js:497:17)\nat require (internal/module.js:20:19)\nat Object. (/home/pi/smart-mirror/node_modules/grpc/src/node/src/grpc_extension.js:38:15)\nat Module._compile (module.js:570:32)\nat Object.Module._extensions..js (module.js:579:10)\nat Module.load (module.js:487:32)\nat tryModuleLoad (module.js:446:12)\nat Function.Module._load (module.js:438:3)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 19, 2017",
            "body": "-also:$ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: Camera [Vimicro USB2.0 UVC Camera], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0\"And get only errors:\npi@raspberrypi:~ $ $ arecord -l\nbash: $: command not found\npi@raspberrypi:~ $ **** List of CAPTURE Hardware Devices ****\nbash: #HOME SECOND FLOOR WALL 2.py: command not found\npi@raspberrypi:~ $ card 1: Camera [Vimicro USB2.0 UVC Camera], device 0: USB Audio [USB Audio]\nbash: card: command not found\npi@raspberrypi:~ $   Subdevices: 1/1\nbash: Subdevices:: command not found\npi@raspberrypi:~ $   Subdevice #0: subdevice #0Current behavior: smart mirror opens but no reaction to commands at allthanks for your reply!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 20, 2017",
            "body": "all works!!!\nafter new ras jessy installed and a little help from the guys at discord chat (they are the best!)\nthank you everyone for the help!\nthe smart-mirror is amazing!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "poetic420",
            "datetime": "Jun 20, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/627",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "jh13626",
            "datetime": "Mar 30, 2017",
            "body": "Voice recognition is only recognized when the voice recognition is clicked on the remote page when performing Speech Recognition.Next voice recognition must be pressed again when performing the following voice recognition :\nIs there a way to allow continuous recognition of voice recognition?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Mar 30, 2017",
            "body": "ok i need to update this to be more clear... voice recognition on the remote has nothing to do with voice recognition on the smart-mirror... please refer to the docs on how to configure voice recognition.first you'll have to configure your capture device then you'll have to configure the voice recognition...\nthere is no continuous voice recognition as that would kill your API requests quota and would cost you a lot of money not to mention send all nearby voice conversations to google...Also, we're available on  to help assist you in real time.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jh13626",
            "datetime": "Apr 2, 2017",
            "body": "\nThank you. Let me ask you a little more.\nDoes remote page speech recognition work without sound configuration and voice configuration?\nI think I have completed all the configurations for speech recognition. However, if I do not press the Speak button on the remote control page, the Smart Mirror will not respond. Continuous speech recognition, as I say, is speech recognition without touching the Speak button with the mouse.And let me know if you have the server code for the discord chat.\nThank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Apr 2, 2017",
            "body": "What do you mean server code for discord chat?Yes the remote does not require sound or voice configuration it is done completely different than the smart mirror...When you configure the mirror properly you will say the hotword as in \"smart-mirror\" and then there will be a white bar at the bottom of the screen and then you will say the command... so it goes like this...\"Smart-mirror\" white bar appears \"show me how to tie a bow tie\" and then YouTube loads with the video...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jh13626",
            "datetime": "Apr 15, 2017",
            "body": "\nThanks to you, speech recognition is well solved. But I have additional questions. Is it possible to run other applications on the Smart Mirror program? For example, can I run a motion recognition program or run a camera application to operate with voice?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 15, 2017",
            "body": " Absolutely. Check out the existing motion plugin (or really any plugin for that matter).\nDev docs: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jh13626",
            "datetime": "May 5, 2017",
            "body": "\nHi There is one more thing to ask.\nI want to implement the following subway service.()\nHowever, I do not know what path to write the code because the folder structure is different.\nFirst, I created a subway folder in the plugins folder. Help me...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aishaamila",
            "datetime": "Apr 30, 2018",
            "body": "I cannot use the voice recognition. When i click on voice button on my phone, It says that unsupported on ios/safari but i am using android. Is there anything i need to change ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Mar 30, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Mar 30, 2017",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/536",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "evancohen",
            "datetime": "Jan 12, 2017",
            "body": "As an alternative motion input source, we should track camera movement (so people don't need additional hardware on top of what they've already got)As a part of the config, you should be able to specify if you are using a timer, IR motion sensors, or camera.\nNo idea what the performance implications would be for monitoring your webcam like this.We're currently limited to a timer and motion sensor (that requires additional libraries). It'd be nice to explore options to give us the same functionality but don't require that additional hardware.I've seen some other people do interesting things like this (looking at you )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Jan 12, 2017",
            "body": "booo.... privacy concerns.... boooo..... bathroom creeper alert.... lol",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rabidaudio",
            "datetime": "Jan 13, 2017",
            "body": "I've got a little proof-of-concept going on . It's currently just a short python script that emulates 's console messages. It should be pretty trivial to convert it to javascript, but at the time I wasn't aware there were OpenCV bindings for javascript.I'm using a webcam because I have a spare and also because it has a microphone, but I imagine most people would want to use the Raspberry Pi camera module. I suspect OpenCV will treat these the same, but I haven't played with the Pi's camera module before.Running the script chews up a bunch of CPU cycles, but it does run, at least on a Pi 3 model B. Currently I'm opening a video stream and processing every frame, but it should be possible to take a picture periodically (e.g. every second) and then sleep. I'll play with this tonight and see how it goes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Jan 13, 2017",
            "body": "you'll probably need to rebase your branch from the upstream dev... but if you can port the python to js and incorporate it into motion.js I can set up a drop down box in the configUI to use either pir or webcam",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jan 13, 2017",
            "body": " scales down the image to reduce comparison times (and as you mentioned, we wouldn't necessarily have to compare every frame. You could do < 10 FPS and it'd still be effective.I doubt we'd need the full power of OpenCV to get this to work. That said, if it's performant enough we could do some pretty cool things with it :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "Feb 22, 2017",
            "body": "I just submitted a pull request that supports this, uses  for the detection,\nand a small enhancement to the autosleep plugin.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "May 3, 2020",
            "body": "add 0.26",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jan 12, 2017",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "Feb 22, 2017",
            "body": [],
            "type": "pull",
            "related_issue": "#591"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#598"
        },
        {
            "user_name": "sdetweil",
            "datetime": "Sep 26, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "Apr 29, 2020",
            "body": [],
            "type": "removed",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "May 3, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/418",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "HumzahF",
            "datetime": "Oct 25, 2016",
            "body": "Hi everybody,\nFor some reason the app is not picking up my microphone. I set up the keyfile and everything in the config file.speech : {\nprojectId: 'magicmirror-147422',\nkeyFilename: './home/pi/smart-mirror/MagicMirror-9d1fd3963297.json',\nkeyword : \"Smart Mirror\",\nmodel : \"smart_mirror.pmdl\", // The name of your model\nsensitivity : 0.5, // Keyword getting too many false positives or not detecting? Change this.\ncontinuous: false // After a keyword is detected keep listening until speech is not heard\n}I also tried opening the app in dev mode to see if I have any errors but\nnothing happens when I say smart mirror, no errors and no activity comes on the console.\nI also ran 'npm run microphone-debug' and my microphone is not there in the dropdown menu.  I also ran 'arecord -l' and here is my result:$ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: Camera [Logitech Webcam], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0and this is in my /.asoundr:\npcm.!default {\ntype asym\nplayback.pcm {\ntype plug\n# This is your output device (In this case AUX out on the Pi)\nslave.pcm \"hw:0,0\"\n}\ncapture.pcm {\ntype plug\n# This is your input device (it may be different from what is seen here)\nslave.pcm \"hw:1,0\"\n}\n}this is all that I have doneIm on a Raspberry Pi 3any help would be appreciated",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 7, 2016",
            "body": "I'm pretty sure that we sorted you out on Gitter, but for anyone else with this problem:\nCheck out the .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 7, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/421",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "filimikr",
            "datetime": "Nov 4, 2016",
            "body": "Hey,I have an issue with my usb (creative) camera for the microphone.\nRaspberry recognizes the device and in the sound config file(asoundrc)  I set it as default recording device.\nMicrophone-debug recognizes the device as well, and I can record and play my recording.\nAlthough when I run the Smart-Mirror the microphone is not working. I think that my config.js file is properly edited.Any help would be appreciated. Thank you!(Raspberry 3/raspbian upgraded/smart-mirror updated)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kurtdb",
            "datetime": "Nov 4, 2016",
            "body": "If you start the mirror using , do you see any logging concerning the speech?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "filimikr",
            "datetime": "Nov 4, 2016",
            "body": "No, i have only this folowing error (and one more for the rss), but nothing for speech.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 4, 2016",
            "body": "You might want to give the troubleshooting steps a shot and just run the keyword spotter and speech recognition to see what is up: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "filimikr",
            "datetime": "Nov 6, 2016",
            "body": "Actually I re-installed everything from the scratch(raspbian-smart-mirror etc), and now things are better. The app recognizes the usbCamera normally, and after I trained my own model, the keyword seems like it's ok(it shows this white line across at the bottom.(sonus shows me that it recognizes the keyword as well)The problem is that after the white effect , it does nothing. It doesn't listen to any command, like \"What can i say or sth, and The white effect stays there until I close the appps. I ran it in dev mode and no errors appeared.\n:/ thanks I imagine that It's something with the APIs or the json key, but everything seems correct , So I really don't know where the problem is.. Errors I see after the npm start dev Propably the error was for the Google Speech keys, but I Fixed it. It was working properly for some minutes and then I received the error:Any thoughts about that?\nthanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 7, 2016",
            "body": "You might want to review the steps that you used to get your key. If it doesn't have valid permissions then hotword detection will work but streaming will fail: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 7, 2016",
            "body": "also the project name in the  file must match the project name in  file",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 14, 2016",
            "body": "Sounds like you haven't enabled billing. Can you double check that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": " is this now resolved?If not please update us and let us know how we can help. If it is please comment what solved the issue and then close the issue. thank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 2, 2016",
            "body": "haven't heard a response in 10 days. I'm closing this issue...  if you're still having issues please comment on this issue and we can reopen..",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 2, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 2, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/326",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "justbill2020",
            "datetime": "Jun 30, 2016",
            "body": "Merging Motion detection and Auto-Sleep / Auto-Wake Functionality.  I'll start working on this today. should have something for it later on today or tomorrow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Aug 12, 2016",
            "body": "Currently looking into implementing johnny-five.io for motion detection...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Aug 19, 2016",
            "body": "Have you made any progress? Not sure what camera you are using, but I've come across this to work with my PS Eye:  re:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Aug 19, 2016",
            "body": "@glcohen I have not further pursued this at this time as i've spent a lot of time troubleshooting on gitter getting the mirror up and running and focusing on the documentation. However, the motion detection we were looking at implementing was using a PIR rather than the camera... as a camera in a bathroom is kinda creepy in my humble opinion... My suspicion would be a bathroom is the likely place people would be putting it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Aug 19, 2016",
            "body": "Fair point; however I wouldn't rule the option out. It's likely possible to distort the image in such a way that the camera can pick up motion but see nothing valuable. You can also set the config to save no images.It's a simpler alternative for those who have a single unit camera/mic like the PS Eye. I'll do some thorough experimenting and report back.Of course people could always choose to be creepy, we can't stop that!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Aug 19, 2016",
            "body": "to each their own I guess... adding that functionality isn't a bad thing... if you want to fork the project and work on that to see what you can come up with that would be cool....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "Sep 30, 2016",
            "body": "it should be configurable, and device extendable... you want to use PIR, then do this, you want to use camera, then do that (new cam type, extend this). In my case the mirror will be out in a hallway, and I am using webcam for voice, so have cam for motion detection.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GobleSt",
            "datetime": "Oct 10, 2016",
            "body": "I agree with  .  Mine is next to the front door.  I think we should have both options for what different people want to do.  The webcam option can continue in issue  I suppose.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 7, 2016",
            "body": "the issue with using the webcam is limited resources on the PI... as stated in . since most are using a Rasp Pi adding webcam support for motion detection might have to wait until they release the raspberry pi 8... which should have the performance needed for this whenever they get around to that version of coarse  lol",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": "removed suggestion status, moved this into feathub and added the in progress and help wanted labels...added to Please refer to feathub for all future comments on this suggestion. use this issue for additional development support or details...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": " i started working on this again using johnny-five... the main issue is that it requires root permissions to use the raspi-io node module and access the GPIO pins on the pi... i don't like that... also it seems like the IPC renderer doesn't actually trigger the commands as it should... all of this is frustrating as hell... however the  script I added to the  works perfect... if you want to take a look here's the branch link... ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": "holy crap nevermind... i figured it out!!!!!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": "completed johnny-five functionality in PR  tested this extensively and should be ready to go...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GobleSt",
            "datetime": "Dec 6, 2016",
            "body": "did a git pull today and see that the auto time out works great!.... the wake up not so much.  How do I disable it until I am ready to do a complete rebuild?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": "blank out the wake_cmd and sleep_cmd...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GobleSt",
            "datetime": "Dec 6, 2016",
            "body": "...in the config.js .  Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 7, 2016",
            "body": [],
            "type": "modified the milestone:",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#64"
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 6, 2016",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 15, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 15, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/307",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "geman220",
            "datetime": "Jun 25, 2016",
            "body": "As far as I can tell PulseAudio is crashing causing audio playback to fail on the mirror.  Opening Pauvcontrol and muting then unmuting the output sometimes will enable playback for a short period (5-10 seconds).  You can keep muting / unmuting to resume playback for short bursts of time.  The same behavior seems to happen just by turning the volume up/down.  It's almost as if changing any setting \"wakes up\" pulse.  Because of this I am fairly certain the issue exists within PulseAudio.  Unfortunately I am unable to get my microphone to work with the mirror if pulseaudio is not running.Running  I noticed the following pop up whenever the audio drops out:After more fiddling, I am not sure what combination of things has made the biggest impact but here is where I'm at.I am seeing far less of the errors from pulseaudio in regard to   Sometimes I can play a whole YouTube or SoundCloud, it's hit or miss.  In general I am getting longer play times before a crash.  Once it does crash it will not recover until I close the mirror and re-launch.    I am using the original 2.5A wall wart but I may buy a new one just to be sure.I disabled and removed bluetooth from my pi.  I'm sure it wasn't using much power, but it was using SOME power so it's gone.  I've made more completely arbitrary changes in my daemon.conf, (specifically increasing  and   and I'm now up to 15 minutes (and counting) of continual playback in Soundcloud.  My advice is to play around with those values until you get good playback.  Like I said I've also made more changes but I'll have to check them all to give a more detailed explanation.  Looks like the issue is 100% related to Pulseaudio, but more specifically it's resource usage.  If it's possible, my suggestion would be to move away from Pulseaudio.Wanted to update the changes I made to the best of my memory.I believe that's all the changes I made, but it was a long process of trial and error so I can't guarantee those were the only changes I made, but hopefully it helps get someone at least 90% of the way there.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 26, 2016",
            "body": "Thank you  for this super well documented issue! Seems to share the same root cause as . When streaming recorded audio and simultaneously playing, PulseAudio (more specifically the ALSA sync) hangs on a rewind.I'll be looking at two potential solutions here: fixing the PulseAudio issue, and removing PulseAudio as a dependency for the smart mirror.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "geman220",
            "datetime": "Jun 26, 2016",
            "body": "No problem, happy to help however I can.  Hopefully if someone else has this issue they can at least try this workaround.  I appreciate your help!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 2, 2016",
            "body": "After some investigation I have found that it is actually fairly straightforward to get this working without PulseAudio (which should solve this issue).First, you'll want to determine your playback and recording devices:Here the playback device is card 0, device 0, or hw:0,0 (hw:0,1 is HDMI audio out).Then you'll want to determine the recording device:Here the recording device is card 1, device 0, or hw1:0. I've modified this configuration file to work for every microphone I could test with, it should be general enough to work for everyone.And finally you'll want to use these to fill in your   file:Reboot!\n allows several applications to record from the same device simultaneously. This is essentially what what PulseAudio was doing for us before, only it's a huge dependency and was crashing. Luckily  comes as a part of ALSA on recent distributions of the Raspbian so this is an easy fix.tl;dr Audio on Linux is ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "7h30n3",
            "datetime": "Jul 3, 2016",
            "body": "Still doesn't work for me.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 3, 2016",
            "body": " not a lot to go on there... Can you please include the make and model of your webcam, the output of , and any terminal errors you get after activating the keyword?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Keopss",
            "datetime": "Jul 7, 2016",
            "body": "My keyword is not recognizer :( i tried with annyang.service() in console and microphone start to work. But any answer with keyword",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GobleSt",
            "datetime": "Jul 8, 2016",
            "body": "After following the \"remove pulseaudio\" instructions, no voice is detected at all for me.\nThe first thing I noticed is the my aplay -l looked slightly off:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`\nOnly 7/8...weird but w/e.\nPrior to reboot the arecord -l looked similar with 0/1 as opposed to 1/1.  After reboot it shows 1/1.\n\n\n\nFirst thing I attempted was:\n\n\n\nNo change.I checked npm start dev and saw no recognition of voice.  3 errors related to traffic which I haven't implemented but nothing else.I checked audio via browser and OMG it is horrendous.  I will try to troubleshoot this evening because I am running out of time this A.M.....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "7h30n3",
            "datetime": "Jul 9, 2016",
            "body": " I finally found an easy solution for voice recognition (Hotword Detection always worked) and I'm still using PulseAudio. I just removed the \"Volume Control (ALSA)\" from the taskbar as described here:Maybe you can verify that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GobleSt",
            "datetime": "Jul 9, 2016",
            "body": "I reinstalled pulseaudio etc and removed the volume control as per above....commands are responsive again but playback (soundcloud, youtube) still has issues.  I haven't been able to work on this because working longer hours lately....will try to devote sometime soon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Keopss",
            "datetime": "Jul 12, 2016",
            "body": "I have done this steps:1º - \n2º - Configured \n3º - Setup Audio Sound Output 4º If i follow steps of \"Update 3\" i get sound some seconds and later silence a any sound is playedKeyword works perfectly. but no sound :(",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "franklinam1",
            "datetime": "Jul 22, 2016",
            "body": "Hey all, so I had this same problem and my setup is as follows:Symptom - USB MIC does not work, tried all troubleshooting steps in guide and in bugs but no luckI found a post on the RP site that had similar symptoms so I changed the following and it seemed to fix all of my issues\"where there is a file called aliases.conf in /lib/modprobe.d which contains the line options snd-usb-audio index=-2 and overrides the etc/modprobe.d/ files, so you need to change that one.Comment out with a # the line “options snd-usb-audio index=-2”\"In /usr/share/alsa/alsa.conf I un-commented “load card-specific configuration files (on request)”I'm not a programmer or dev at all, but your mirror inspired me to code. Heres a link and credit to the person who resolved this issue for me. Thanks all!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 22, 2016",
            "body": "Closing this out because this is no longer an issue :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "geman220",
            "datetime": "Jun 26, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#284"
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 28, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 2, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "alexa-pi/AlexaPiDEPRECATED#82"
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 5, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#333"
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 7, 2016",
            "body": [],
            "type": "modified the milestone:",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 7, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#335"
        },
        {
            "user_name": "evancohen",
            "datetime": "Aug 3, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#368"
        },
        {
            "user_name": "evancohen",
            "datetime": "Aug 31, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#361"
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 22, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Sep 22, 2016",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Sep 22, 2016",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 29, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#403"
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Oct 8, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#386"
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Jan 25, 2017",
            "body": [],
            "type": null,
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/266",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "sata73",
            "datetime": "May 31, 2016",
            "body": "It would be great to show different calendar or even RSS feeds per user via voice command, eg. \"Show Evans calendar\" or log in as a user via voice command to show specific content and log out to show default content.\nIt could probably be done by adding users to modules in the config and define if it is for this or for that user or for all.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 31, 2016",
            "body": "I think the best approach here would be to use the webcam and facial recognition here, as suggested in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sata73",
            "datetime": "Jun 1, 2016",
            "body": "Thanks for your fast reply!\nI thought about voice control, because not everyone is fine with having a cam spying into the room. And since voice control is already enabled, it might be worth a thought, using the available technology.\nThe command home shows the default layout. Probably different layouts could be defined which can be called by custom commands like Evans home? Is that possible?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jallen1227",
            "datetime": "Sep 5, 2016",
            "body": "Agreed on the privacy relative to cameras, what about switching based upon use of Pi's NFC module.  That way, a phone or tablet could be used to switch between users.  Certainly not as convenient as voice but opens up options and validation / security of content shown.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": "Migrated to Issue will be closed and tracked on feathub moving forward. Please refer to feathub for all future comments on this suggestion.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 31, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/170",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "mojosoeun",
            "datetime": "Mar 20, 2016",
            "body": "Hello.\nFrom 9am to 4pm KST, voice recognition didn't work. But after that time it began working again. I'm trying to figure out why but I can't solve it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Mar 21, 2016",
            "body": "This is a known issue. My current theory is that because of the high volume of mirrors we are collectively using up Electron's Google speech key.I am currently investigating solutions (and am open to suggestions)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shekit",
            "datetime": "Apr 12, 2016",
            "body": "Any updated theory on this? I'm facing a very random issue where my code was working earlier in the day and suddenly stopped working, no voice detection.. nada..Here's another issue I opened on annyang that details all my attempts ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 18, 2016",
            "body": "You are correct in you assumption that the issue is with key utilization (there have been may discussions about this on the gitter chat, which I suggest you check out).I'm looking at alternatives (BlueMix, Microsoft, etc) as well as investigating offline  to reduce quota usage.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 20, 2016",
            "body": "Another update: I've got keyword spotting functioning in the  branch. There are a number of issues that exist with this implementation, namely poor performance and some comparability issues with certain microphone setups.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shrimp69",
            "datetime": "Apr 26, 2016",
            "body": "I just set it up today and it seems I already made too many requests. I have my own Google API keys but in a matter of 2-3 minutes I made over 500 requests and now it seems to be down for me.How do I add this branch to my existing git folder? ( on the raspberry )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skydazz",
            "datetime": "Apr 26, 2016",
            "body": "I am still getting \"Google speach recognizer is down :(\" when I plug in any sort of microphone in to it. I have my Own Speech Keys. May I suggest that its a driver issue. Is it set to only be compatible with a list of mics? (PS: It was \"Say \"What Can I Say\" to see a list of commands\" when I unplug the microphone.)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skydazz",
            "datetime": "Apr 26, 2016",
            "body": "Also I get the \"[1444:0426/103700:ERROR:logging.h(813)] Failed to call method: org.freedesktop.NetworkManager.GetDevices: object_path= /org/freedesktop/NetworkManager: org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.NetworkManager was not provided by any .service files\" Message in the Terminal When I start it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skydazz",
            "datetime": "Apr 27, 2016",
            "body": "I have started from scratch. I downgraded from Jessie to wheezy and I am following the documentation exactly as it is printed (except config.js). I am using a USB camera as a mic as it states in the documentation also. Will post results soon",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skydazz",
            "datetime": "Apr 27, 2016",
            "body": "Ok, my issue now, still with sound, is getting the usb camera mic as the mic being used, I can use any USB sound device for anything. I have tried turtle beach px22 controller, USB sound card, and a USB camera",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 28, 2016",
            "body": " have you tried following the directions in the troubleshooting section of the documentation? You may also want to look at  (which was an old thread on the issue that may help you find an alternative solution)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Sachin1968",
            "datetime": "Apr 29, 2016",
            "body": " Were you able to resolve the issue you had with \" Failed to call method: org.freedesktop.NetworkManager.GetDevices: object_path= /org/freedesktop/NetworkManager: org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.NetworkManager was not provided by any .service files\"I have the same issue and can't figure out how to resolve it. Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 29, 2016",
            "body": " that error is unrelated to this thread and is harmless - you can be safely ignore it.\nYou can find more info on .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 2, 2016",
            "body": "So, another update for you all :)\nKeyword spotting officially works in the in the  branch. Unfortunately the Pi is not quite powerful enough to process everything in real time. Because of that I've added a clap detector to that same branch, all you have to do is clap (a configurable number of times) and the mirror will start listening to you.\nYou'll have to install  (it's a dependency for clap detection)You will also have to run  after switching to this branch because of the new dependencies. Make sure you update your  file to reflect the new properties in !Since this is all very new stuff I haven't had the chance to test it extensively. I already anticipate there being issues with the clap detection microphone configuration, luckily this is totally something that you can set up. In your config you can use the clap  object to change the following settings for clap detection:As always, if you have any questions you can post them here or ask on gitter.\nSince commands are intermittent in the  branch I've added a shim to Annyang to \"simulate\" a request. This can be done in the dev console with the following:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 13, 2016",
            "body": " we'll get you sorted out in gitter :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "joerod",
            "datetime": "May 13, 2016",
            "body": "I had the same problem with overusing my 50 speech API calls in about 10 minutes of use so I'm happy to test the new \"clap\" feature.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Keopss",
            "datetime": "May 18, 2016",
            "body": "Hi  ! i don´t know what happen with my rabs :(I have installed smart-mirror-master and works fine.Then i install sox and smart-smirror with keyword then edit config.js and add overrides options but no clap and speech detection.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 18, 2016",
            "body": "The geniuses over at  have created an offline keyword spotter that should work. In order to find out I need your help to train the keyword \"smart mirror\". Just follow these steps:I'll continue to keep this thread updated with my progress and should hopefully have a working prototype this weekend!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Keopss",
            "datetime": "May 23, 2016",
            "body": "Hi! how gone? fine?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 23, 2016",
            "body": "I have a working implementation of keyword spotting, I'm currently trying to fix an issue on the Pi 3 that causes recognition to fail because of a native PulseAudio issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "trenkert",
            "datetime": "May 23, 2016",
            "body": "Hello, I've succesfully installed smart mirror and I am very impressed!However, I also get \"Google Speech Recognizer is down\"...How long will it take for you to implement the new solution into the main branch?Just an idea:\nCould you use jasper with pocketsphinx () to train a number of keywords than then either activate Google STT or Amazon Alexa? Or is Kitt.ai definitely better for keyword recognition?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 24, 2016",
            "body": "I tried Jasper... it's quite resource intensive and is painful to use as a dependency (building projects on top of it is great, building integration into an existing project not so much).I also tried PulseAudio (same native recognition engine that Jasper uses) and it wasn't quick enough to recognize keywords without significant lag.Snowboy (from the folks at kitt.ai) is super lightweight and very fast. Sure it requires a wrapper for their Python library, but that's not too difficult.I actually have a working prototype with snowboy on the  branch (using the OSX binaries). The only problem on the Pi now is with PulseAudeo, which is having issues with the Pi 3. Once I sort that out (and I think I have a fix) we'll be good to go.It's been a long journey, with lots of painful dead ends, but I'm feeling really close!tl;dr Snowboy is great, I should have something working really soon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "trenkert",
            "datetime": "May 24, 2016",
            "body": "cool! What's the problem with pulseaudio?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 24, 2016",
            "body": " it's an issue with Bluetooth that causes PulseAudio to crap out. Even after disabling it within the config the issue persists (which makes me think there may be another root cause). I'm worried that the real cause is a conflict between dependencies of the mirror and keyword spotter (but I haven't confirmed this yet, and I don't  it's the cause).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chenguoguo",
            "datetime": "May 25, 2016",
            "body": "Sorry for coming into this late. Snowboy is a C++ library and doesn't have much dependency. It will work as long as you can feed it linear PCM data sampled at 16K, with bits per sample 16, and number of channels 1. PyAudio is only used for demo purpose. If it turns out that PyAudio is the problem, we can turn to other alternatives for audio capturing.In the  we are trying to add examples of using Snowboy in different programing languages. So far the examples are using PortAudio or PyAudio, but if you look at the code (e.g., the C++ demo code ), you can see that switching the audio capturing tool should be easy., let me know if it turns out that PyAudio is the problem. We can look into other alternatives for audio capturing.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 25, 2016",
            "body": "Hey  thanks for dropping in! Awesome to see you all so committed to your (super awesome) project. I managed to write a pretty hacky IPC between Node and your pre-packaged Snowboy binaries/Python wrapper. It's  not the ideal way to use Snowboy with Node, but I just wanted to see if I could get something that would work.I don't think it would be too challenging to  so it could be easily consumed via Node. I'll take a look at it this weekend if I get the chance For everyone else: I managed to coax PulseAudio into cooperating on my Pi, and everything seems to work super well! You can test it out by doing the following:As always let me know if you have any issues over on .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chenguoguo",
            "datetime": "May 25, 2016",
            "body": "That's great !  is also helping us working on the NodeJS module, see the issue . He'll likely get something soon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "trenkert",
            "datetime": "May 26, 2016",
            "body": " I've experienced a similar issue. I would guess it has to do with pulseaudio-bluetooth. It works for me when I start pulseaudio manually once again after login.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ojrivera381",
            "datetime": "Jul 18, 2016",
            "body": " Thanks. I rebuilt it all seemed to be fine on my lab monitor in my office however when I moved it to its perm location speech stopped working. Also how do i exit it and get to the main desktop with menus. right now if I alt+f4 is closes the window but I can't see any menus to go through pi settings or launch terminal etc.. Thanks again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 18, 2016",
            "body": " is the Pi still connected to the same WiFi network? Have you exceeded your 50 query/day quota? The menu is missing because you have  installed.\nYou can probubly also press the windows key on your keyboard, (which opens the Raspbian equivalent of the start menu). You can also get to the terminal via the recycling bin on the desktop (hacky, I know).If those two things look good, I would follow the instructions for troubleshooting in the docs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Mar 23, 2016",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Mar 24, 2016",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Mar 24, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 22, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#228"
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 29, 2016",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 1, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 25, 2016",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "May 27, 2016",
            "body": [],
            "type": "pull",
            "related_issue": "#261"
        },
        {
            "user_name": "ikucukkaya",
            "datetime": "Jun 10, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#282"
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 11, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#287"
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 11, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jun 11, 2016",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 22, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Jan 25, 2017",
            "body": [],
            "type": null,
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/59",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Shubham0209",
            "datetime": "Feb 6, 2016",
            "body": "Wouldn't it be nice that instead of showing a single comment all the time, mirror may show different comments and greet us based on the detected face.Can you suggest me any method to so.Moreover i would also like to \"HOW TO USE PUSHBULLET API\". Basically being a newbie i don't know how and where to add these api code.please can you tell me about this in detail.THANKS!!!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Feb 7, 2016",
            "body": "I've experimented with face detection in the past, but I haven't pushed it out publicly because there are a number of issues with it. As for PushBullet I haven't completed this integration yet. I'm experimenting with Android devices this week and will probably have PushBullet done in the next couple weeks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "andrewda",
            "datetime": "Feb 7, 2016",
            "body": " You could possibly make a branch with the face detection code implemented. I would love to take a look and see what I can do about contributing!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Feb 7, 2016",
            "body": "I'll clean up what I have and do that :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kurtdb",
            "datetime": "Feb 8, 2016",
            "body": "What framework did you use? I've been reading up on this and it seems like OpenCV is the framework to use for this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Shubham0209",
            "datetime": "Feb 9, 2016",
            "body": " how can we integrate webcam to take our picture on command like \"click my picture\"?By doing this we can integrate the feature of selfie mirror.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "therealgambo",
            "datetime": "Mar 5, 2016",
            "body": ", Do you know how ironic that sounds? I can understand the want and need from a technical point of view, but do you really need your picture taken only for it to be displayed in a screen behind a mirror. It kind of defeats the whole purpose of a mirror.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Shubham0209",
            "datetime": "Mar 5, 2016",
            "body": "@missionrulz actually it was only step 1.What i was actually thinking was to maybe post those pictures on fb etc or send them to the phone. so the basic idea was to click the picture and share it. I know it sounds weird.lol",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tomasvr",
            "datetime": "Mar 6, 2016",
            "body": "Thats a pretty neat idea! The point is that you would be able to see the full picture you're about to take of yourself through a big mirror. Only placing the camera at the right position seems kind of tricky because it should be at eye height, where the screen is.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Shubham0209",
            "datetime": "Mar 6, 2016",
            "body": "yeah  you feel me right. lol",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "paviro",
            "datetime": "Mar 11, 2016",
            "body": "Not all of it is published yet but I started adding facial recognition to another mirror project you could probably use the code as it is right now to get something up and running:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kurtdb",
            "datetime": "Jul 4, 2016",
            "body": " how fast is the facial recognition? I don't think the rpi (even v3) is fast enough to do this in a user-friendly way. (e.g. look at the video at )A user won't wait for 5 seconds to see his/her information, they want it now. I've been looking into alternatives, but it seems that the way to go here might be either a cloud hosted service or an expensive camera that can do the facial recognition for you. (although I didn't find any camera's that had an API from which you could stream the information)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "paviro",
            "datetime": "Jul 4, 2016",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "cybertza",
            "datetime": "Jul 22, 2016",
            "body": "there is a really great idea on the selfie mirror is the 1 picture a day stream, if you captured 1 picture a day of yourself for ever, it would be rather cool, and with an overlay you may be able to calibrate it so that your face is always in the same place, but yea, just interesting.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Jul 22, 2016",
            "body": " I like the idea! Want to take a shot at it and send a PR?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "paviro",
            "datetime": "Jul 22, 2016",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "7h30n3",
            "datetime": "Aug 28, 2016",
            "body": "What's the current status of the face recognition feature?Maybe we could use the face detection (Just \"face detected\" and \"no face detected\", no certain faces) as a substitute for the hotword detection.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "paviro",
            "datetime": "Aug 28, 2016",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": "Migrated to Issue will be closed and tracked on feathub moving forward. Please refer to feathub for all future comments on this suggestion.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Feb 7, 2016",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Feb 9, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#63"
        },
        {
            "user_name": "evancohen",
            "datetime": "Apr 6, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#215"
        },
        {
            "user_name": "evancohen",
            "datetime": "May 31, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#266"
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "sdetweil",
            "datetime": "May 10, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/204",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ryansaam",
            "datetime": "Nov 4, 2021",
            "body": "\nUnable to run project\nI followed the \"2.2 With Bundler\" install steps \nTo have the project run\nReact.js, Node v16.13.0Error:Error:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 4, 2021",
            "body": " is delivered as ES2020 module, but amazingly even latest  creates an app setup to use  old  7.0 which is not compatible with ES2020\n(ES2020 support was introduced in Babel 7.8 which was released in January 2020, I don't know why FB uses such really old versions in )You can either update your environment or update  to latest one from  (2.5) as I've just posted an update that includes polyfils for ES2018Note that  2.5 is not yet released on NPM (likely next week), but you can install it using\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ryansaam",
            "datetime": "Nov 4, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 4, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/158",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Krammig",
            "datetime": "Sep 1, 2021",
            "body": "Would love to try the demo but all that is displaying is the rotating progress circles and a message saying Starting Detection.\nAppreciate if you could let me know if there is something I need to do at my end.Chrome\nFirefox\ni5 16GB",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Sep 1, 2021",
            "body": "I just tried with Chrome and Firefox and it works fine here.Btw, do you have a GPU or integrated graphics?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Sep 2, 2021",
            "body": "any updates?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Krammig",
            "datetime": "Sep 4, 2021",
            "body": "Sorry just saw your reply.\nDedicated GPU\nI had tried with both Chrome and Firefox.\nWill check the inspector in the coming hour and let you know.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Sep 8, 2021",
            "body": "closing due to idle time, will reopen if information is provided.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Krammig",
            "datetime": "Sep 1, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Sep 1, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "Krammig",
            "datetime": "Sep 4, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "Krammig",
            "datetime": "Sep 4, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Sep 8, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/243",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "avestaHarsh",
            "datetime": "Jan 21, 2022",
            "body": "I am using the face comparison feature in my electron app. On the laptop (with default camera) it's working fine but when I try to run this app in the system(PC) (which does not have a default camera), also I have tried setting external webcam in the system, the app is not starting even not getting any errors/warnings.Human library version: \nTensorFlow/JS version : \nNodeJS and version : \nOS : \nElectron version : \nRAM : \nProcessor : Below is my configuration for Human.Note: I have also checked with the updated latest version of the Human Library ( npm package), TensorFlow/JS ( npm package), and checked in i5 processor CPU but still not working.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 21, 2022",
            "body": "there have to be some errors/warnings, did you open browser inspector window (it exists even in electron) and see what's written there?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avestaHarsh",
            "datetime": "Jan 21, 2022",
            "body": "The app does not start itself. so the browser inspector window will not be prompt.Here Is the terminal screen.\nAll I came to know after spending a day is, the app is crashed while I am creating an instance of Human class.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 21, 2022",
            "body": "So the EXE you've built works on your notebook, but completely fails to start on your desktop system?What you've just posted is build log (and PLEASE, post such items as text, not as image screenshots) but that should not be relevant since EXE is already built?Enable  logging (see  and post what happensWithout actual error, nothing I can do.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avestaHarsh",
            "datetime": "Jan 21, 2022",
            "body": "Yes, whatever I have developed/built is working on my laptop but not working on a desktop.I got this error in the console.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 21, 2022",
            "body": "this error is an issue with your electronjs logging - it cannot open a log file and it aborts with unhandled error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avestaHarsh",
            "datetime": "Jan 24, 2022",
            "body": "To overcome the above error I have tried with close electronjs logging, and the error was gone but still unable to start the app.The issue with below line, something is wrong with the default config value.\nIf I am commenting above line then the app starts smoothely.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 24, 2022",
            "body": "I'd love to help, but I need an actual error to be able to do anythingCheck  documentation on how to display browser console log and post entire log here\nI don't use  at the moment, but there are plenty of hints in this thread:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avestaHarsh",
            "datetime": "Jan 24, 2022",
            "body": "At the time of , something goes wrong at below linethat's why I have debugged today in the library (more specifically in Human class, namely human.ts (under src folder) / human.esm.js(under dist folder)) and tried to console logs but nothing consoled anything.I am curious to know, Does Human class looking for a default camera by default and my desktop does not have a default one and my laptop does have so it works well?Currently, I have set  , ,   for face comparison, do I make any mistake here? Do I need to change the backend for the electron app?(Also I have tried with add an additional webcam to the desktop before compiling my app).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 24, 2022",
            "body": "Human does not look for input at all at the time of creating an instance. It only looks at input once you call \nEven then, it doesn't care about WebCam - it uses HMTLElement you pass - Image, Video, Canvas, etc.Re: WebGL - I don't see why that would matter in Electron.But you can simplify startup by saying  so it just creates a class instance without triggering any work at the startup time.Again, don't try to  whats going on - why don't you enable proper logging and see whats actually going on?\nHuman logs everything to console and its very informative - any error will be immediately visible",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 31, 2022",
            "body": "closing the issue as no actual information or logs were provided so far (its been over 10 days).\nonce logs are available, issue can be reopened.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 21, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 21, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 31, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/205",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Safari throws the following exception:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": "Strange that  is undefined on error in Safari, but ok, I'll patch TFJS to check for that condition. However, that happens inside error handler - there is no working around the fact that GL context got destroyed.Can you copy & paste browser log instead of screenshots? I need them to submit patch to TFJS as that is where error happens (I could create a workaround in , but its better to fix at the source)?Anyhow,  backend doesnt exist in browser, so if you select it Human will automatically use  insteadSimilar for  unless  is enabled in experimental flags in browser ()Can you try couple of things?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Of course!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": "actually, can you paste the log using  ?\nit should be more verbose since exception handler there is disabled by default, so it should show stack trace",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Here you go:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Enabling  doesn't make a difference (Human will still use the  backend). ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Sorry, forgot to mention: It does work fine in the latest Chrome and Firefox.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": " runs several checks to see if browser supports  correctly before allowing its usage ( is still experimental), check output of  to see details.I cannot believe that Safari still hides WebGL v2 behind a flag and uses v1 by default as v2 was added in Safari 12 which is over 3 years ago!Yup, I though that might work. Chrome and Firefox have WebGL v2 enabled by defaultAnyhow, I'll create a workaround for WebGL v1 (which is basically to disable some internal features if GL v1 is detected so performance will be slightly slower)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Yeah, well... I've stopped being surprised by this kind of BS from Safari a long time ago. Honestly I was surprised Safari is supported as well as it is  The only thing worse than Safari right now (not counting IE) is Mobile Safari. I'll be testing there as well once you've published a fix.Btw, I've also tested with v2.3.2 and no difference - in case that helps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": "Btw, I appreciate the fast and helpful answer! I've only started looking into this project but so far I'm beyond impressed! Is there any way to donate or support you in some other way?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": "Didn't think it would, but wanted to check - human v2.5.1 is first release that uses my custom built  and wanted to make sure that is not the cause in any way by default uses  backend which my custom variation of  with some tweaks, but it requires  2.0 (main difference is in startup time - its about 2.5x faster, but that makes it about 0.1x slower in inference which in my book is a good tradeoff) basically disables  and forces fallback to default New code is on github, still using same 2.5.1 tag (i'll wait few days to see if there are more issues with 2.5.1 before publishing 2.5.2)Also added a trivial fix to TFJS itself: thanks!no donations needed\nsupport? i welcome any suggestions and contributions!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": "quick update: my patch for  is approved & merged to master branch which triggered new build of  and  - new ci pipeline reduces this process from months waiting for a release to hours!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 9, 2021",
            "body": "Awesome. Thanks for the update!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaKleSoft",
            "datetime": "Nov 8, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 8, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/278",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Nekronik",
            "datetime": "Jul 5, 2022",
            "body": "We have also received reports of different errors, that appear to be internal, where we don't know how to fix them nor what they mean.\nAny help would be very appreciated.Sadly I don't have more code context for this one.\nWe are also getting this message sometimes \nWe have not identified what produces them, nor found a common pattern.\nNo errors",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 5, 2022",
            "body": "there is a lot going on here, can you provide info what is your platform - where are you running human? which browser and on what hardware?most of the errors point to either browser/hardware incompatibility or extremely underpowered device.for example:All 3 of those sound like they have same root cause - .The only one that's left is:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Jul 5, 2022",
            "body": "For the related-together errors, human always runs on:I have not yet found a way to determine which devices are not going to be able to run human. Do you have any ideas on that regard?For the ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 6, 2022",
            "body": "I haven't tested using less popular browsers such as Samsung Internet or Mi Browser and don't know their capabilities - and unfortunately, browser capabilities vary drastically, so it could be that some don't support WebGL v2.Regarding which devices are not going to be able to run human - well, it depends. Key factors are:Regarding shape error, it happens here: `but  are results of a model execution, so only way this results in error is if model did not execute correctly - again most likely due to low memory issues. in any case, i'll add some additional checks in that part of the code in the future as well.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Jul 6, 2022",
            "body": "I have checked which devices produced the errors, and for all the errors the devices where indeed old (2012 - 2018), but the error regarding the shape was appearing on newer devices:So the theory about that this error was due to a low memory is probably not correct.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 6, 2022",
            "body": "Yes, that reshape error was suspicious from start, but only way that happens is if something within model went wrong - I'll have to dig deeper into that.If you have any insights on what was being processed at the time of the error?Anyhow, I'll add some error handling code to capture error as well as attempt to treat it as non-critical. Likely target end of next week as I'm traveling at the moment.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Jul 6, 2022",
            "body": "The source was a video stream of the front-facing camera with 1080p resolution.\nThis is how we are currently consuming human:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 6, 2022",
            "body": "Nothing strange in either input or in config.\nI'll need to dig into this when I'm back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 7, 2022",
            "body": "i figured out a  issue, but that is already fixed a while back, you just need to update  on affected clients.old code waswhich caused a random bug when model was updated as there is no guarantee that order of resulting tensor will be exact - so following code was sometimes trying to analyze wrong result.code fix identified each tensor based on its expected shape instead of relying on hardcoded order of variables, so it cannot be anything else:let me know if there is anything else remaining here that should be looked at?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Jul 7, 2022",
            "body": "I have indeed not received any report for the reshape error on version , sadly I can't update past  since the next released version introduced the IndexedDB.\nOnce we can update we will double-check it. I will open a new issue if I spot it again.\nFeel free to close the issue or leave it open if you have some work related to it pending.\nThanks :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 7, 2022",
            "body": "i'll close this issue and keep the webview one one - target for new release is end of next week.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jul 7, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/273",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "a1782680475",
            "datetime": "Jun 9, 2022",
            "body": "When using the front camera, the content of the camera is opposite to the reality. How can I mirror it like a mobile camera?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2022",
            "body": "you can either set a CSS transform on the output canvas yourself like this:\nor you can use build-in methods in  by simply enabling\n\n(note that  must also be set to true - which is the default, just don't disable it)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 10, 2022",
            "body": "First of all, thank you for your answer.\nI can do this by using scaleX (-1), but some face detection boxes will be flipped so that they cannot be read. See the following for specific effects.\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 10, 2022",
            "body": "Boxes seem fine, what's reflected is text - and you don't have to use built-in methods, you can draw it yourself.And what about second method I've mentioned?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 10, 2022",
            "body": "\nThe second method seems to have some serious problems... I'm not sure if there is something wrong with my code. I before used face-api.js,At that time, I could make some changes by modifying the source code, but the human is obviously more complex. I hope my demand can be supported from the project, because obviously, if the left and right are reversed, it is difficult for users to center their faces in the picture. I hope to be able to do the same as a self timer camera.Thank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 10, 2022",
            "body": "Second method should not produce results you're seeing, I'll take a look, but may be a slow response as I'm leaving for a trip tomorrow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 10, 2022",
            "body": "Thank you very much. Do you need my code?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 10, 2022",
            "body": "It's all according to your time. I'm not in a hurry. I can wait until you come back from your trip.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 10, 2022",
            "body": "Yes, having your code would help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 10, 2022",
            "body": "i just a quick look before my flight and i don't think there is an issue, its just a question of which canvas you're drawing.if you have 'config.filter.flip = trueinputVideo` as input, it will be correctly flipped before processing. but then the question is what is displayed on screen? if you display original video (or copy it to output canvas), it will not be flipped so detected results will look exactly opopsite of image.instead, you can draw processed image (so after it has been flipped) - it is part of the result output.for example:i've just updated  and you can see how its handled there - you can simply enable/disable flip and it works fine.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 13, 2022",
            "body": "Think u.\nAccording to your method, the problem has been solved.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "a1782680475",
            "datetime": "Jun 9, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/154",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Utopiah",
            "datetime": "Aug 12, 2021",
            "body": " Can't get the node webcam demo to run Return some prediction from the connected working webcam node/CLI",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 12, 2021",
            "body": "can you try to explicitly install  and verify there are no errors during installationmy best guess is that there is something wrong with tfjs-node to tensorflow.so node binary bindings and that is RPI specific (I cannot reproduce on non-RPI platform)you might also want to try with  v14 as it uses binary bindings API v7\nand what is the output of  on your RPI? It's likely  architecture which means 32-bit kernel. not sure if latest tensorflow supports 32bit kernels, so you might need to use older tensorflow (which means older version of  since it has tensorflow.so pre-packaged).the stack trace you've posted points to error during kernel op registration which happens first time any backend-specific kernel op is called (which in this case happens to be )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 12, 2021",
            "body": "Thanks. I rolled back to node 14 but initially got a segfault (no error message) trying to run . I then ran  which gave me no error and  (so I assume TFjs working). I then tried again and got :Details on RPiAlso FWIW I did  resulted inwhich is about correct. Ironically enough with(switched back with nvm) so I assume it might be related to the webcam part on the RPi rather than tfjs-node.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 12, 2021",
            "body": "Ah,  14 and 16 binary bindings (NAPI v7 vs v8) are not compatible, so any modules that rely on binary bindings must be reinstalled (as bindings are established during installation). In your case, that would apply to both  and  as they rely on external binaries.This last stack trace shows that  did not provide a valid image, so later  complained about valid image type.But yes, I'd agree that it looks like  is now working (it definitely wasn't in the first stack-trace) and the remaining issue is  not binding to  correctly.I know this is a pain, but can you try with both Node 16 and 14, but each time make sure that both  and  are freshly installed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 13, 2021",
            "body": "Tried but same result. What I did was move ~/human to ~/human_node16 then switch to node14 using nvm.I then pulled human again from git then ran npm install there. Again node worked but not node-webcam :",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 13, 2021",
            "body": "you've mentioned that  alone can extract image screenshow from your webcam\nany chance you can try test project with  alone?\nexamples are at i really don't know how to troubleshoot that library and that's why i included the note in the example that this example  is unsupported",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 13, 2021",
            "body": "Just tried cloning their repo then  and npm with their example and both worked.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 13, 2021",
            "body": "i had a hard time reproducing on my current hardware, but then remembered that i could setup  for emulation...anyhow, updated  is uploaded, it should work now:issue was in trying to promisify output of \nit used to work, but somewhere it broke - at the end, i just used callbacks instead",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 14, 2021",
            "body": "Indeed, just pulled and tried, works now!Thanks a lot, closing the issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 12, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 12, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "Utopiah",
            "datetime": "Aug 14, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/275",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 20, 2022",
            "body": "\nHumanGl throws error: Requested texture size [4739x4739] greater than WebGL maximum on this browser / GPU [4096x4096].\nModels I have activated are face tracking and emotion detection.\n2048 or 4096 seems to be reasonable limits. At least as of 2020 it looks like .Human v2.8\nSamsung SM-G991B\nChrome Mobile Webview 102.0.5005\nAndroid 12",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 20, 2022",
            "body": "can you describe what is your workflow? what is the actual input device and/or resolution?\nnote that if  is not manually disabled,  will automatically resize any input larger than QHD (looking at width > 3840), but you can also change  and  to any value - can you try that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 21, 2022",
            "body": "Thank you for your work and quick reply.The error was thrown by a Samsung SM-G991B, Chrome Mobile Webview 102.0.5005 with Android 12. This device seems to have a 4k front camera, so that is where perhaps the error is coming from.I disabled  because I was worried about performance degradation, but it was probably unnecessary. The workflow we have is:\nCreate video stream, and every 500ms run human detection on that stream to get the emotions. ()So I see two options:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 21, 2022",
            "body": "Both options are valid:If only thing you're interested is emotion, you might want to experiment with  value in  (i didn't expose it as configurable value, but i probably should) - its basically how tight is crop around the face going to be before passing it to other models such as emotion - and it does have quite an impact.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 21, 2022",
            "body": "Wow especially the last part  sounds very interesting ;)  If you could implement a config that would be awesome! Its better than forking.I just deleted  and will await the results. Do you think it would be wise to set  or  to lets say a resolution of 480p to further improve performance?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 21, 2022",
            "body": "i totally forgot, it is exposed in config, but as hidden optional parameter: , default value is 1.4setting low resolution only really improves performance when you're running in web worker as transferring pixel data between threads is not cheap. otherwise, i'd suggest to run with 720p minimum. reason is that after face detection, you still want cropped face box to be ideally 384px (its resized up or down to that resolution). now, if your face takes 90% of frame, then running at 480p is ok. but if face is smaller, then having larger input resolution helps.btw, i've just noticed that you've disabled face mesh model (because you don't need it). but...face detector is really quick, but resulting boxes are a) not really precise, b) there are some false positivesface mesh when runs also stabilizes face boxes by removing false positives and rewrite face box coordinates with precise ones. so depending on your use case, you may want to try with mesh enabled even if you don't care about mesh detection",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 22, 2022",
            "body": "Found the config you are referring to here: My config with scale factor set to 1.1 and mesh not disabled looks now like this:Let's see how those results will perform. Thank you very much! Do you know of any other performance tweaks for my specific usecase?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 22, 2022",
            "body": "Depending how static the input is, you may want to play with caching setting.Btw, I'm closing this issue as resolved as original item is no longer a problem - but feel free to post on this thread if you have any further questions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 23, 2022",
            "body": "Ok thank you so far!To further explain my usecase: I am taking an MediaInput stream from mobile devices front camera and analyze every 500ms the emotions. On low level to medium level devices this causes a lot of performance issues, as we also run video playback and media recorder. So I am thinking on handling the emotion detection on server site. Or do you know of any suitable optimizations to keep human running on the client?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 24, 2022",
            "body": "In that case, I'd go for a web worker implementation - its not faster than running  in the main thread (its actually a touch slower as there is extra step or copying pixel data to worker thread which takes couple of ms), but it completely frees up main thread to do anything else without locking up every 500ms when detection runs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ButzYung",
            "datetime": "Jun 24, 2022",
            "body": "A bit off topic, but recently I discovered that it's actually faster to pass  instead of pixels data as () in web worker. The reason is that  stays in GPU the whole time, while for  it needs to be extracted from the main thread and put it back to GPU in web worker, and these processes take CPU time. The difference can be quite significant, in some cases up to 25% of fps increase. This is tested on  models running on web worker, but I suppose the improvemnet applies to  as well.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "maxcodefaster",
            "datetime": "Jun 20, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 20, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 22, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/116",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 3, 2021",
            "body": "Hello Vladimir,I want to read streams from a camera not webcam?\nHow can I do it?\nI just simply changes all getVideoTracks()[0] to getVideoTracks()[1] in the index.js and face3d.js, but I got the 'Exception Error' on the screen.\nActually after allowing the camera to capture, the camera's lights get on (like what happen with webcam) and it seems camera is ready but no stream!Thx",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 3, 2021",
            "body": "It was connected without changing anything. I just changed it to the original getVideoTracks()[0] and run again and it worked with my external camera.\nHere I want to know what if I have several cameras and webcams connected to my laptop, what is the priority or how can I select a specific one to get streams from?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 3, 2021",
            "body": "before call to  there is a call to  which returns all streams that fit given constraints - and if you want to select a specific camera, you select contraints that match that camera.check out specs at ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 25, 2021",
            "body": "Actually there are two cameras (s and ca), both of them are recognized by the algorithm and they appear in the permission pop-up. One of them works but the other one (ca) doesn't.\nI face different situation when I change usb ports. For the ca camera sometimes I receive error of 'CAMERA ERROR: STARTING VIDEO FAILED' and sometimes even I select it trough pop-up permission but the webcam get active (I mean I select camera ca but the webcam starts to give stream). what is the reason? How can I fix it? both cameras are very similar in characteristics and same brand.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 26, 2021",
            "body": "How are you accessing second camera?Also, can you open browser inspector, there should be a more detailed error noted there.\n(human library logs to browser console in more details)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 26, 2021",
            "body": "I do nothing, as soon as I connect the S camera via usb to the laptop it will be recognized by the algorithm and will be in the list of devices it the pop-up permission. I select it and the algorithm starts getting streams from the camera. but it is not true for the other camera (ca), I don't know why?\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 26, 2021",
            "body": "it's not  that picks up multiple cameras, it's the browser.\ndemo simply asks browser 'give me camera that fits constraints' and since default constraints is basically anything, browser gives it full list.\nyou picking camera in the browser pulldown list doesn't mean you're selecting it, demo will still try to use default (first one) if there is no more specific constraints.\nso you  second camera and that means nothing, it will still try to access first.what you should do is list all cameras programmatically and then choose which one to use.for example, this will list all cameras:and then in  modify camera constraints to explicitly choose which one to use:i might be able to do it with a dynamic selector, but i don't have multiple webcams available, so cannot test anything.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 26, 2021",
            "body": "i've added some additional messages to function  in , new code is on  main branch:it should log something like this to browser console on startup:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 3, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 3, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 24, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/152",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "websocketing",
            "datetime": "Aug 10, 2021",
            "body": "I'm glad to see this project. I wonder if this project supports anthropometry? I need to measure the human parts of the generated manikin. I hope I can get your reply and information.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "websocketing",
            "datetime": "Aug 10, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 10, 2021",
            "body": "no, it doesn't support  - but that is a really interesting topic - i'll do some research on it\nand if you have a working example, let me know",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 10, 2021",
            "body": "from the top of my head...given that we don't know distance or reference scale to start with, we need to start with either:if focal length is well-known, its a decent startif relying on a refence object, there is issue with 3d depth:\nis reference object closer or further away from measured?\ncould partially be solved with 3d reconstruction models, but that is pretty low precision\nonly \"real\" way to do it would be using spectroscopic cameras so 3d reconstruction is precisewithout that, we simply don't know if we're looking at a doll in a model house or a giant on a hilland in both cases, we still have issues withso all-in-all, i don't think anthropometry would be any good without having a well-defined camera parameters to start with\nwhich fortunately in some cases can be extracted from the image exif properties for images, but it's missing for video feedsbut not all cameras with same params have same level of distorsion - that depends on the actual glass elements\nso most distorsion correction software works with a built-in lens distorsion database and still only corrects barrel distorsions\nwhile perspective distorsions are hit-or-miss (just look at photoshop or lightroom)all-in-all, i could add some for-fun functions that do different measurements, but only if\na) user provides camera parameters in the config\nb) there are no significant perspective distorsions (meaning camera angle is straight-on)let me know your thoughts",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 13, 2021",
            "body": "closing as it cannot be implemented without user providing a lot of information manually",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "websocketing",
            "datetime": "Aug 10, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 10, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 12, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Aug 13, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/115",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 25, 2021",
            "body": "\nHi Vladimir,I followed the structure you wrote me before, but I still get error and can not install.\nBut I was able to install it on my friend's laptop (I think there is sth wrong with my pc), now I wanted to run it as I was previously doing but it is not working anymore.\nI just want to see the demo as before. In vs code, activating the 'Go live' and selecting the demo. page opens, camera activated but no stream/video, no output.I updated and install it again but did not work.\nAny idea?thx",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 25, 2021",
            "body": " is not a VSCode feature, it's a feature of some plugin installed in VSCode that creates a live web server - and I have no idea how that plugin works since it could be any number of them.Can you use dev web server that ships with  instead? It's fully documented.And if there are issues with browser, would be good to get a log from browser inspector, not just a screenshot.My best guess is that a \"live server\" that a plugin created is not a fully functional one or it references a wrong root path so page fails on loading models, but since there is no log, I really can't tell.At a minimum \"go live\" must be selected on the project root and then manually navigate to demo in your browser - it's the only way path to models can be reached as they are below project root, not inside demo folder.But none of this has to do with library issues...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 27, 2021",
            "body": "closing as no user feedback and issue is not library related.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 27, 2021",
            "body": "I just restart the system and then it worked.\nI think there were several files processing in the queue and the browser cache was engaged, that is why it was not working and after restarting the problem was resolved.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 25, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 25, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 27, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 27, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/125",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jun 9, 2021",
            "body": "Hello Vladimir,There are several good head pose models available out there but when I test them in different conditions their performance drops.\nI checked this  to learn more details about head pose estimation (FACE: FACING CENTER/LEFT/RIGHT ...) of Human but I didn't find what I am looking for.\nHow the head pose task works? What are the inputs? what is the reference parameters to calculate it? Do you use MediaPipe facemesh to calculate the 3D face and position?\n do you use a pre-trained model for head pose estimation? if yes would you please share the link.Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2021",
            "body": "entire gesture analysis is done in  and is based on calculations performed as a last step of  method and takes  as input.specifically, for face gesture, its method  that looks at z-coordinate of the points of edges of the eyes:\n(i choose edges of the eyes instead of edges of the face as edge face is more likely to get occluded with higher angles)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jun 9, 2021",
            "body": "Thank you\nok so you use the eyes outer corners and nose tip (or some where there) to calculate the distance and then the head pose.\nThen what about the pitch yaw and roll of head? where they come from?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2021",
            "body": "pitch/yaw/roll also come from face mesh point coordinates, but math is very different (and more complex)\nsee  method , code is annotated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Feb 7, 2022",
            "body": "Hello ,Hope you are fine.\nI am struggling with head pose extracted from mediapipe facemesh. when I changed the camera position the head pose estimation gives wrong estimations, which I expected it actually.\nFor example when I change the camera position from the center at the front to center at the bottom (there is not a huge difference in camera position), and change the thresholds for up, down, right and left but it doesn't work well correctly.\nDo you have any idea?Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Feb 7, 2022",
            "body": "are you having difficulty implementing your own algorithm or having an issue with algorithm in human?\nmy algorithm for calculating face rotation angles should be agnostic to camera position.\n(some other things may depend on camera position such as eye gaze estimation, but not face itself).take a look at:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Feb 7, 2022",
            "body": "I first checked the Mediapipe then tried the Human and I faced same problem. see the different for same  but different camera positions in images below.\nFirst image: the camera is exactly at the , and my head is center as well, the estimation is correct.\nSecond image: the camera position is at the , and my head is again center exactly the position in previous image, but the estimation is wrong and shows 'Head Up'. Of course from point of view of the camera my head is up but I need world actual coordinate not the head pose w.r.t. the camera' coordinate.\nI changed the thresholds but it still doesn't give correct estimation. can  or  work for Human?\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Feb 7, 2022",
            "body": "i don't see an issue - there is no way that anyone (actual person or AI algorithm) can determine if person is looking up or camera is placed on a low mount unless there is some reference point.And if you have a reference point, then it's easy to calculate angle and simply subtract it from a detected one. Of if you already know angle of the camera, just subtract it from detected angle.Trying to modify thresholds or looking how OpenCV does it is IMO non-necessary and overly complicated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jun 9, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jun 9, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/102",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Nekronik",
            "datetime": "Apr 13, 2021",
            "body": "After using the library I got some questions:. Is the  method return signature correct? I'd expect it to not contain a .. Is it possible to only draw the face mesh and not the box?. Is it possible to customize drawing styles? Are they global or per draw method request?Also, since I had to write better types for the gestures and it is one of your goals, this is what I got. Maybe this can help you.The  is not complete since I did not know all the  possible values.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": " returns  which i use in automated tests, otherwise  return values should be ignored.\nNote that  internally is just a  call with different preprocessing and using embedded test images.Sure, by setting Yes, you can customize it via  objectEntire  class is documented in Currently they are global. But its a good idea to have a local override, I'll add that today and update here when done.Why do you need such strong typing for gestures?\nI didn't create fixed typings for it because gestures are considered expandable - idea is that user can add additional gesture detections (although that code to provide user-level functions is not yet published)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Apr 13, 2021",
            "body": "Thanks, I was not aware of the typedoc site.To ensure I don't misspell any of the provided gestures.I was no aware of it. Depending on the implementation of this feature the gestures could be strongly typed for the pre-defined ones and just a matter of providing a generic with the type of the user-defined gestures.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": "how about a compromise - strongly type gesture.part, but leave gesture itself as generic.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Apr 13, 2021",
            "body": "This would work, up to you, you know the library roadmap better than me :)\nI will keep it strongly typed on my end.But this would be an improvement anyway since right now I have to dobecause  has asignature, instead of",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": "btw, i've just implemented global/local draw options. is global (will bump version to 1.5 as this renames existing ``human.draw.drawOptionshuman.draw.*` method accepts optional parameter `drawOptions` which is used to override global options for that draw method only.and  are now defined as strongly typed interface :)new version will be published later today.i think that was the last part of this issue, so i'll close it for now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nekronik",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/99",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "MuriloRS",
            "datetime": "Apr 13, 2021",
            "body": "\nNo face is found in this image:\n\nfile://src/models/facemesh.jsonfile://src/models/blazeface-back.json`, // can be 'front' or 'back'.\n// 'front' is optimized for large faces\n// such as front-facing camera and\n// 'back' is optimized for distanct faces.\ninputSize: 256, // fixed value: 128 for front and 256 for 'back'\nrotation: false, // use best-guess rotated face image or just box with rotation as-is\nmaxFaces: 1, // maximum number of faces detected in the input\n// should be set to the minimum number for performance\nskipFrames: 11, // how many frames to go without re-running the face bounding box detector\n// only used for video inputs\n// e.g., if model is running st 25 FPS, we can re-use existing bounding\n// box for updated face analysis as the head probably hasn't moved much\n// in short time (10 * 1/25 = 0.25 sec)\nminConfidence: 0.3, // threshold for discarding a prediction\niouThreshold: 0.2, // threshold for deciding whether boxes overlap too much in\n// non-maximum suppression (0.1 means drop if overlap 10%)\nscoreThreshold: 0.5 // threshold for deciding when to remove boxes based on score\n// in non-maximum suppression,\n// this is applied on detection objects only and before minConfidence\n},face: {\nenabled: true,\ndetector: { modelPath: 'file://src/models/blazeface-front.json', enabled: true, rotation: true, return: true },\nmesh: { modelPath: 'file://src/models/facemesh.json', enabled: true },\niris: { modelPath: 'file://src/models/iris.json', enabled: false },\ndescription: { modelPath: 'file://src/models/faceres.json', enabled: true },\nemotion: { modelPath: 'file://src/models/emotion.json', enabled: false },\nage: { modelPath: 'file://src/models/age.json', enabled: false },\ngender: { modelPath: 'file://src/models/gender.json', enabled: false },\nembedding: { modelPath: 'file://src/models/mobileface.json', enabled: true },\n},\n// body: { modelPath: 'file://models/blazepose.json', enabled: true },\nbody: { modelPath: 'file://src/models/posenet.json', enabled: false },\nhand: {\nenabled: false,\ndetector: { modelPath: 'file://src/models/handdetect.json' },\nskeleton: { modelPath: 'file://src/models/handskeleton.json' },\n},\nobject: { modelPath: 'file://src/models/nanodet.json', enabled: false },\n};`The result:Why is not finding an face in that image?**Environment",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": "using  model it doesn't detect a person as that model is trained for center-aligned faces,\nthis photo is too far to the edge. and in general  model is good only for webcam input, not for photos.use  model (which is default for a good reason) and it works fine:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MuriloRS",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/118",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "okeoke85",
            "datetime": "May 11, 2021",
            "body": "Actually this is a question,How can i get frames or images from uploaded video to send detection with the second of frame in healty and performanced way, it is not directly related to human api if you went through same paths and if you can help me, it would be a peach. Thanks in advance.lets say i have 5 min. video and i need to get an image every five seconds, desired result will be likeInput : video url, i dont have any video on machine as file, i just have url,Results = [\n{ sourceVideo : videoUrl, time:takenTime, detections:[] },\n{ sourceVideo : videoUrl, time:takenTime, detections:[] }\n]",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mayankagarwals",
            "datetime": "May 11, 2021",
            "body": "I am looking to do something similar. Except using a camera streamFor your problem, you can probably use ffmpeg to read frames from a video file (found an example ) and for each of the images, run the detector. Keep agressive caching enabled as that will give better results (how is described here : )I am also trying to find a tool to read frames from camera stream and its surprisingly hard to find one that fits the bill.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 11, 2021",
            "body": "you can use this, i already did with video streaming; i have video url as input and i want to do it server side with node.js, you can use this function on client;",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 11, 2021",
            "body": "your question is about triggering detection of a video from url at fixed intervals, here's a quick example:your question is about how to get and decode video stream in a nodejs environment (and additionally, how to read webcam from nodejs environment), i've just answered that in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 11, 2021",
            "body": "But i'm on node.js side, will this work?const video = document.getElementById('video');I mean i dont have a htmlVideoelement i just have the url of  the video like this;",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 11, 2021",
            "body": "Ah, didn't know that. And no, definitely not going to work.\nNodeJS has no either image or video decoding functionality. Image is simple enough to do on CPU using Node's v8 engine, but decoding video in JavaScript is waaaay too slow, so you really do need an optimized external decoder, most commonly .\nAnd to use it from nodejs, I'd look at .(you might go to a lower level and do actual bindings from NodeJS to  or whatever decoding library, but that gets really messy really fast, I'd stay on  level)If your video (from URL or from file, doesn't matter) is fixed-length, then it's easy to generate screenshots every x seconds and save them as list of files:And once you have list of images, it's easy to process them using .\n(I'd still use  as target for that to avoid actual disk writes)If your video is a stream, then it's a bit trickier, but should still be doable with a  followed by  loop",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 11, 2021",
            "body": "FYI, A bit off-topic, but maybe it helps to clarify few things...Modern browser components:Out of all that, NodeJS shares  language engine with Chrome, the rest is browser-specific.\nEven simple  function doesn't exist in NodeJS and requires a 3rd party library as it's implemented in Browser's network layer.But NodeJS does add some functionality on top of V8 in form of built-in libraries as well, such as  for filesystem access or  used to create web server which doesn't exist in Browser.And for desktop apps, there is  which is basically NodeJS V8 + Blink engine, so a lot more functionality.So when looking what is doable in NodeJS, best to look what is done in V8 inside Browser.\nOr use  which is a complete headless Chrome browser that can be triggered and controlled from NodeJS.Modern browsers are by far the most complex software today other than OS itself.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 11, 2021",
            "body": "Such valuable infos, thank you Vladamir.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 15, 2021",
            "body": "i'm closing this issue as it was a general question. please open a new issue if there are specific implementation problems.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 15, 2021",
            "body": "Sure, thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 17, 2021",
            "body": "i just did a quick prototype using  to decode input video (demo is using a file, but same applies to stream or webcam, just change input params) and pass data to  for processing. everything is done via pipes, so there are no ugly temp files.main trick was to use motion jpeg as output format which is then easily parsed for frame start/end markers which gives jpeg per each frame.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 17, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "okeoke85",
            "datetime": "May 11, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 11, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 15, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/105",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": "Hello,Regarding the talk we had I attached 4 photos. It think there is sth wrong because I don't get even a single frame correct gaze direction.\n\n\n\nand I have another question:\nwhere can I find the FACE related prints on the left up corner, first line? I just want to simply change the 'FACE: FACING CAMERA' to sth els, e.g. FACE: FACING CENTER. just changing a word. I check index.js, node.js files but did not find where you print those words.Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 18, 2021",
            "body": "Those are good examples - and yes, my simplified math is just too simple - comparing area sizes of irises is insufficient when you're facing camera, but looking away from cameraI'll update here when I have a new solution readyinside  it just calls built-in helper functions indside  to draw\nand the one you're looking for is \nbut again, they just print what they get from   object - words are defined in so you can either:for example, really silly but simple way of converting entire result to string, replacing strings and returning back as result\n(better way would be to walk the object and replace values as needed)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": "These are what I have in the Human folder.\nI don't have demo and src folders.\n\n\nis it ok to just download the missing folders and putting them into my Human folder?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 18, 2021",
            "body": "this does not look like  official npm package or git clone.\nthis looks like someone manually copied  folder and placed  and  folders inside  and that's what you're using.you're missing all of the sources and documentation.where does this copy come from and how was it installed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": "A friend of mine prepared it and gave it to me. I just got the file and extracted it.\nhow should I fix it or can I continue with it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 18, 2021",
            "body": "you can continue using it, but then you wont be able to update easily as i rollout any fixes or changes (your friend would have to prepare it again)documentation you can see online here on github, so that's not an issue\nbut you're missing actual sources, you only see the demo (things in '/dist' are compiled and minimized, so not readable) - that's why you cannot find things when you're searching for stringsyou could also download everything from here on github, but then you also wouldn't be able to update automatically\nso it would be best to either use  to install NPM package or  to clone repositoryanyhow, for purpose of this issue, to rename  to  you don't need any of that, you can still do that by doing either:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 18, 2021",
            "body": "ok, i've just made some changes to :i've tried also 'looking up' and 'looking down', but human eye just doesn't have height compared to iris height to be able to run math precise enough - it's ok for close zooms of eye, but otherwise not precise enough so i won't enable it.new code is already on  and will be published on  later this week (waiting for some fixes from tfjs team to be able to bundle new tfjs 3.4.0)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 19, 2021",
            "body": "Hello Vladimir,you changed the text but, than you, but I still need to find where you print those stuffs at the left up corner.\nthe only thing I found from the folder that I have is the below screenshot.\nI don't know maybe js is completely different from what I know (python). I want to see the lines of code that you print those strings, Especially the first line (FACE: .....)here by commenting the line 165, all the texts at left-up corner will disappear. but I want to access each line of that corner text separately.\nİt is very straight forward when I go through the github folders -> src -> gesture -> gesture.ts -> line 42 to 65. I don't have the src file and I can not find the source of last explanation you made (simple object -> string -> replace -> object conversion - walk the object and replace values as needed).\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 19, 2021",
            "body": "Yes, it can be done and I've tried it - the problem is reliability of results:There is no source, that is just how it can be done on your side\nAnd those are two different methods, not two steps in one method:simple object -> string -> replace -> object conversion:walk the object and replace values as needed):or the same with a map and regex function:if you do that before  is called, it will replace values as you want and then draw will draw your new results.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 19, 2021",
            "body": "ok, i've changed it to include up/down:as suspected, precision is not perfect, but why not",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 19, 2021",
            "body": "let me know if there are any remaining questions regarding gaze detection or this issue can be closed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 20, 2021",
            "body": "Currently, I don't have but I will have as soon as I get able to install the library (Human). As you said it is better to have the right package but I have difficulty to install it. I clone the repository and do the npm i but I get this error. I tried some solutions but it didn't resolve the error.\nHope you can help :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 20, 2021",
            "body": "to install human, you don't need to clone it and then install it's dependencies\ninstead, just run  to install  package from  and that's itthen to update  at any time, just run  to update to latest minor version or  to update to latest major versionor alternatively copy  from git main branch using \nin that case, to update to latest changes, you'd use but again, no need to run  inside human in either caseinstalling dependencies inside  is only needed if you plan to make changes to the library as it installs \nand in that case, best procedure would be to have a separate tree for :and such local fork can be used in your actual project by installing it from local path",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 21, 2021",
            "body": "i'm closing this issue as related to gaze detection.\nif there are any other questions, let me know.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 27, 2021",
            "body": "Hello,\nI am able now to play with prints at the left corner, and changing them the way I want,\nI need to find two more things:Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 28, 2021",
            "body": "best is to take a look at how drawing is done in  and copy those functions to your code and use them instead of built-in one. for example, in , inside function , it calls  - remove that and replace with call to your function. and you can use  as template to build that function. same for gestures.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 24, 2021",
            "body": "Hello Vladimir,I have tested the gaze direction in several different situations and the performance is not robust, as soon as I changed the position of the camera or light condition the accuracy drops. Actually I expected it because gaze direction function is a rule-based (mathematics based) and the ratios will change with both position of camera and the person.\nI thought it will be a good idea to integrate a robust DL based gaze estimation model into Human. There is an open source python pre-trained model that I am using for my study.\nI am supposed to change the algorithm to JS and then integrate it to Human, or is there any way to skip this conversion part?What is your idea? any solution to make the gaze direction finding part work more accurate?The Iris distance is working reverse, When I get more close to the webcam it shows higher distance and we I get more far from the camera it shows lesser distance.Thx",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 24, 2021",
            "body": "Can you share the model so I can take a look?Iris was calculating iris size, not actual distance. I've corrected that, latest code is on git.Values returned now are distance from camera in cm corrected for a typical webcam field of view of 88 degrees\n(for example, when i'm sitting in front of notebook, iris distance will be ~30cm).Note that there is no way to determine camera field of view programatically, so for more correct measurements user should adjust this value accordingly.Btw, this iris distance was an actual issue - can you in the future open a separate issue for such items so it can be tracked and closed correctly?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "May 25, 2021",
            "body": "I just shared the link via email.\nSure I will open separate issue for iris distance.\nThanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "May 25, 2021",
            "body": "No need to open separate issue for Iris distance anymore since it's already fixed in main branch as of few days ago\nJust please do so for any new issues you find in the futureAnyhow, I've just created a new discussion item and tried to answer most of the questions there: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Feb 7, 2022",
            "body": "Hi Vladimir,I checked the iris detection of mediapipe and I faced problem regarding the directions correctly and while searching for similar issues I found . It can give the iris position inside the eyes correctly when it is at right and left but for up and down it gives wrong estimation since the eyelid landmarks move as iris landmarks move and by the result the position of iris inside the sclera (eye) remains constant. see the video in the link I shared.\nI think you have this problem for Human as well. Do you have any idea?Thx",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Feb 7, 2022",
            "body": "exactly the same problem and i don't see an easy way out. but its also a low priority for me given everything else.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 21, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/57",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "simon-lanf",
            "datetime": "Jan 7, 2021",
            "body": "I know that right now I can know if the user's eyes are facing the camera by knowing they are both at the same distance from the camera. The problem is that if someone is in the far left for example, this person would have to turn it's head in an angle from the camera's point of view and now this person is not facing the camera.I wonder if there is a way to detect these cases. We could triangulate where the person is looking from the 3d coordinates, or try to guess if the person is looking by adding parameters X and Z in the equation, for example if you are in the far left, you should be facing slightly the right.Let me know if you have any idea.edit: I'd like to add that I'm not looking for exact gaze or where the iris points. just if the head if turned to the camera.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 8, 2021",
            "body": "one way that comes to mind is to compare the size of the iris for left and right eye - if a person is looking towards camera, they should be within small margin. and that should hold regardless if person is in the center of the frame or at the edge and looking at an angle.for example:but...iris is a small object, so it could be unreliable in borderline cases where face is small or iris size cannot be detected accurately.another way would be to look at z-axis of points on left and right edge of the face.\nlike it's currently done in :but instead of currently fixed 10% threshold, project both points on a semi-circle with radius being distance from the camera and adjust z value before calculating difference. and use iris-based calculation for distance (included in the default result set) to calculate radius of the semi-circle .it's a bit more approximations, but since points are much more spread out i'd guest it could end up being more reliable.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 11, 2021",
            "body": "i've added gesture detection for iris-based  and published a new version, so i'm closing this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "simon-lanf",
            "datetime": "Jan 7, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 8, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 11, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/55",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ost12666",
            "datetime": "Jan 5, 2021",
            "body": " says emotions are in the result root but it is actually inside the face object.Also I am not sure human.defaults work, at least sometimes it returned undefined",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 5, 2021",
            "body": "also there is a type 'discust' should be 'disgust'",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 5, 2021",
            "body": "and 'surpise' should be 'surprise'not sure if those are doc errors or also errors in the code",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 5, 2021",
            "body": "I guess code const annotations = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surpise', 'neutral'];",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 5, 2021",
            "body": "you're correct. and the code was ok (in , but docs were not).\nfixed via ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 6, 2021",
            "body": "Code is still wrong in surprise:const annotations = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surpise', 'neutral'];While doc is corrected://  'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 6, 2021",
            "body": "it was missing a git push - sorry :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 6, 2021",
            "body": "The library is awesome! thanksI really like the gestures, very useful",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 6, 2021",
            "body": "Thanks!Feel free to suggest additional gestures, current ones are just few samples.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 6, 2021",
            "body": "I am now trying to implement head nods detection using head down and head up gestures so its more time based than static gestures. I implemented raise hand by looking at all gestures that has finder, hand or finger name in it. I am also using facing camera and face neutral for detecting user is looking at the camera.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ost12666",
            "datetime": "Jan 5, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 5, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 5, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/42",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lenonMax",
            "datetime": "Dec 8, 2020",
            "body": "\nWhen I open the mesh and iris models on my laptop(i5,2.3GHz), the first time I open the web page, there will be 4-5 seconds of lag. During this time, animation and other things cannot be rendered.\nps: After the execution of human.detect(videoElement,config), the web page will freeze, and the video will not freeze if it is only opened.\n1.Enable the mesh and iris models in file of config.\n2.Preload the JSON and bin files.\n3.Open a new page and call the camera to enable face detection\n**Environmentthanks a lot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 8, 2020",
            "body": "stall is due to initial model warmup.issue was that some models cannot be pre-warmed-up using empty image (e.g., face returns no results for empty image, so models that rely on detected face cannot be warmed up).added embedded sample image for purpose of warmup.fixed via: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Dec 8, 2020",
            "body": "Oh,I see.So cool!Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Dec 9, 2020",
            "body": "After my test, I found that the sample photos must contain detectable faces in order to trigger the face detection, otherwise it will not be effective. Could I pass in any picture?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 9, 2020",
            "body": "built-in  function uses embedded photo that cant be changed.\nbut warmup is just an early call to detect, nothing else.so you can skip  and do explict call to  using your sample picture, only thing is you want to disable caching of frame results and use picture only for warmup.something like",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Dec 8, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 8, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 8, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/41",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "djthegr8",
            "datetime": "Nov 27, 2020",
            "body": "\nCamera access not supported in the browser, tried in Chrome and Edge.\n\nOpen demo\n\nWorks as required\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 27, 2020",
            "body": "Never mind, this seems a solved problem ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 27, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 27, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/63",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 27, 2021",
            "body": "Hello,\nI have checked the facemesh demo of Mediapipe () and face landmark detection (). The second one really works better and more robust even when I change my head pose extremely and quickly.Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 27, 2021",
            "body": "it's a tradeoff between precision and performance and it's fully tunable in configuration.\nhow it works is that library tries to re-use previously detected face bounding box without running it again and just runs the mesh and iris detector instead. so if the face moves suddenly, face will disappear and bounding box gets invalidated so it re-runs on the next frame. again, behavior is fully tunable.also disabled by default for performance reasons is face angle calculation so if head tilts more than ~20 degrees, it will get invalidated. also can be enabled via configuration.yes, it does, you just have to set configuration parameter  to a value higher than 1. again, it's about performance vs precision.check out configuration documentation for list of all configurable parameters: Pre-trained network, trained on imdb dataset.pre-trained network, not just simple math.\nmath is used to calculate gestures such as .list of all models used are in credits page: \nlist of pre-defined math-based gestures that are included (others can be added by user) is in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 27, 2021",
            "body": "Thank you very much for the detailed answer.\nI still have lots of questions about mediapipe also. I don't want to bother you, so is there any forum for asking these questions? for example a forum that I can ask the library's developer directly my questions or sth like that? (not the tensorflow github forum)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 27, 2021",
            "body": "for mediapipe, authors state that general questions should be posted via ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": "closing the issue as the original question is answered.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 28, 2021",
            "body": "Hello Vladimir,I am sorry for asking alot, I am amateur but very curious about this library.What do you mean by performance? precision is also a concept of performance. you mean if I make the model to act more accurate, the performance will decrease? but what is performance here? is it the speed of algorithm?mediapipe (the new version) also does the face detection/bounding box/mesh/iris jobs as well but why it doesn't have lag, shaking and disappearing even when I rotate my head more that 30 or 40 degrees? in its demo I don't tune anything and it works very well.For the age prediction have you done any fine tuning over the pre-trained network or any other changes?For emotion detection, why you didn't use simple math (calculating difference in distances between eyebrow etc.) what will be its deficiency or problem?If simple math won't work for emotion detection correctly, would it work for mouth opening and gaze direction?!!Thx\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": "yes, if you make model to act more accurately it will decrease performanse. this can be due to many things.\nfor example (and there are other examples as well):new version of mediapipe is written in C++ and compiled to WASM. so entire pre and post processing that i've mentioned above is much faster than in JavaScript.also, JavaScript is single threaded, so any calculation will block processing - thus more lag. mediapipe can perform asynchronous interpolations between model runs. meaning, it runs the model prediction, but also calculates averages between each run so it can draw much smoother frames.i have added async processing in  library as well, but due to overal nature of JavaScript, it doesn't help much.in reality, mediapipe is a superior way of doing it, but it's proprietary - you can only do what they allow you and not much else, not room for user modifications/improvements/tunings.also, mediapipe models are pre-compiled to tflite format and cannot be easily re-used elsewhere.no additional fine tuning.it's possible, but emotions are complex - it's easy to differentiate between angry vs happy, but not so easy to mathematically describe surprise vs digust. i just found using model to be easier and more reliable.it does work for anything you can clearly describe mathematically. so things like 'mouth open' or 'looking left' or 'leaning down' are easy.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 28, 2021",
            "body": "The other reason of lower performance can be merging several models together in human, is it correct? since mediapipe has separated all functions (e.g. hand tracking and facemesh are separated and work solely, maybe that is why it has better accuracy and performance?)Do have plan to publish the python version of your work?The demo that I checked was for facemesh and iris tracking which is provided for js! not C++ I think\n\n\nIf I'm not mistaken the older version has Iris and mesh for C++,Since I just focus on eyes and head, is it better to use mediapipe not Human? since I need both high accuracy and performance at the same time.I previously wrote a script which was using dlib and tensorflow facial landmarks, from those landmarks I calculated for head pose and gaze direction (a rule based/mathematical method, not using any DL) but since the landmarks were not accurate, my model had low accuracy. since this facemesh is very precise about coordinate of eyes/Iris landmarks, I think it would solve my problem.\nI saw this solution from this video in linkedin which is similar to your package: In above you mentioned I can easily mathematical calculate for looking left or down, are they just examples and is it true for other directions such as up and right?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": "All models in  are separate and each can be enabled/disabled individually in configuration.\nThere are some dependencies such as:No.MediaPipe JS solution uses JS just to load WASM module which was written in C++. Actual processing is done in WASM, not JS.\nJust like their Python solution is not actually written in Python, it just exposes a loadable Python library.Your choice. With  you get full configurability - you can tweak it as much as you want to get equal or even higher precision than MediaPipe. On the other hand, using MediaPipe out-of-the-box is pretty good.Any such rules are simple (and yes, up and down are already included). Complex rules that cannot be done reliably with just math and where pre-trained models are better would be ones that vary from face to face - for example, your facial expression for surprise might not be mathematically same as for me.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 28, 2021",
            "body": "ok, thank youIs this library (Human) available for both academic and industry use? I know it is open-source but is it for both purpose freely?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": "Re: licenseLibrary is released under MIT license.\nSome models are originally under Apache 2.0 license.Both are permissive - so in short, yes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 29, 2021",
            "body": "I really appreciate your detailed answer.In the Human library is there any black box module? can I use/change/modify all the functions and modules?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ButzYung",
            "datetime": "Jan 29, 2021",
            "body": "In terms of functionality, /TFJS solution is obviously better than the current MediaPipe JS soltuion. The current MediaPipe JS solution is quite buggy. It doesn't work on web worker. It is not well documented and sometimes you have to do trial-n-error just to make things work. Its output is bugged. For example, MediaPipe JS facemesh output doesn't give the z coordinates for some unknown reasons.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 29, 2021",
            "body": " other than the fact that included models are pre-trained, there is nothing you cannot change/modify.\nand if you want to add additional modules or swap out existing ones, that is also pretty easy as design is modular.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": "Hello Vladimir,Hope you are well.Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 18, 2021",
            "body": "Yes, thank you! :-)Btw, can you open a new issue for this? Both questions are valid, but unrelated to this issue that is already closed.I just tried using webcam and it works for me.\nCan you send some sample images where its misdetected along with details of your configuration?Note that mathematically  is defined as difference between area size of left and right iris and if they are within 25%, then  will return .This threshold can be changed from 25% to anything (see , but it's not exposed as user-configurable value as I don't want to have too complex of configuration file (it's already pretty complex), so I just use some value that looked ok.Face can be up/center/down and left/center/right at the same time, so gestures returns two separate results.\nBut gestures return object is just that - an object - and If you want to combine them, it's just a question of printing them together instead separately.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Apr 18, 2021",
            "body": "Ok, I will open a new issue",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lghasemzadeh",
            "datetime": "Jan 27, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 27, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 28, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Jan 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/40",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "djthegr8",
            "datetime": "Nov 26, 2020",
            "body": "\ni am getting very low fps for WASM in my hand-tracking, so just one model that too without landmarks. This would give me 3FPS with WASM SIMD Threaded, which is very very less\n\nSimply check my \n\n10FPS, at least?The source code is ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 26, 2020",
            "body": "when filing a performance issue, try to provide as much data as possible:anyhow, i did some digging and seems there is a quite a bug in  backend which causes this model to loose hand tracking so it has to be redone almost every frame - so all the caching optimizations are not used. this needs to be fixed in tfjs.see  for details.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 26, 2020",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 26, 2020",
            "body": "chrome  - can you be precise?\nthere are prod, beta, dev and canary channels and i cannot guess which one you use. and i don't know if your chrome was updated immediately or not (e.g., chrome 87 came out few days ago and had several breaking changes).resolution - no, it should be reported in the issue.\nplus the link you sent is just a link to readme and inside it link to demo goes back to my page. so i don't know where your demo is.webgl - anytime there is a performance issue, there should be something to compare it with. reference model, different backend, etc. just saying \"it's slow\" doesn't help me to investigate.anyhow, seems there is a bug in wasm backend as i wrote before, i'll wait for a feedback from tfjs team.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 26, 2020",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 26, 2020",
            "body": "i checked your demo and execution on my notebook with i7-8750 is 9+ fps, that is well within expectations.\nlooking at stats, image processing is ~10ms and model inference is ~100ms, so on my system resolution could be 4x and it would only drop to 8fps.i'm afraid that until there is more optimized wasm backend published by tfjs team, there is nothing i can do here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "djthegr8",
            "datetime": "Nov 26, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 26, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 26, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 24, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/48",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "vladmandic",
            "datetime": "Dec 15, 2020",
            "body": "I tried to use the MediaPipe API in my project, but unfortunately it doesn't seem to support web worker (a must in my case, since there are some intensive 3D animations, and there is little room in the mani UI thread for other CPU-intensive task). So at the end I tried your   library instead, but I encountered some issues.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 15, 2020",
            "body": "I've converted this conversation to an issue as it's better fitted and I can track enhancements.Sure, that is ok.\nAlthough I'm curious what are benefits you're seeing with loading via importScript instead of importing ESM module?Ahhh, the magic of missing items in workers hits again - I'll try to find an alternative.\n is used only to load embedded JPEG data that is used for warmup as all browsers have a built-in decoder.\nI could embed ImageData instead, but as that is uncompressed it would increase library size.\nOr I could use a 3rd party JPEG parser, but I try to limit additional dependencies.\nWill figure out something.PoseNet model has only some optimizations that should not impact it's accuracy - did you try changing default parameters in config?That should not be a problem. Only reason why I avoided it because it's too big to embed. I'll run some tests tomorrow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ButzYung",
            "datetime": "Dec 15, 2020",
            "body": "I want to have the option to switch between human and the conventional TFJS (loaded via importScripts), as it is not possible to use both import and importScripts in worker at the same time. Maybe I can load TFJS via import as well, but module support in web worker is still fairly new (Chrome 80+, no Firefox), so browser support is a concern.Yeah my config is customized. I have disabled all face-related models, leaving only body and hand. For body I only need to detect one person so I set maxDetections to 1. But that doesn't seem to be the cause of the problem. Even if I leave the body config untouched, the accuracy is still the same. In fact if I don't lower scoreThreshold to something below 0.5, most of the time the body is not detected at all. Even if it is detected this way, the scores of some body parts are low and the arms are \"jumping\" here and there (my app is focusing on uppper body detction, not the full body).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 15, 2020",
            "body": "True. And it's even worse for mobile platform - Chrome still doesn't support modules there.\nFor my apps I prefer to use imports as usual to avoid unnecessary complications with importScript, but then create a bundle at the end and load that bundle instead. If you look at , that is exactly what it does on each source file change.Strange - I'll investigate.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": " i've spend too much time trying to work with a broken tfjs 2.8.0 release, just downgraded back to tfjs 2.7.0 and re-implemented  so it should work with web workers.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": " regarding your comment on  - note that body score is just average of scores for each keypoint. so if you're looking at just upper body and lower body is hidden, then average score is going to be low although score for upper body parts is high. if looking at upper body only, set  to low value such as  and check for each  in your app instead.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": ": i've implemented special case for single body detection (i didn't have that special case for body, only for hand), but i really don't like how it behaves - keypoints are accurate, but it cannot determine left vs right so every few frames points from left hand get switched to right hand and vice versa.Difference is performance and not precision - single pose just uses  to determine most likely keypoint out of each possible ones. multi pose actually traverses the tree to find most likely neighbor.Try it out, but most likely I'd remove this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": " Work on Body MobileNet modelI've switched default in  to  instead of  and I like it much more.\nI have to keep it small and performant, but you can try other variations. E.g., MobileNet with with 8 strides is so much slower than with 16 strides - which makes sense since it analyzes 4 times bigger matrix (each area that is analyzed is image size divided by stride vertically and horizontally).You can try different MobileNet models like this:\n\n\n\n\n\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": " Work on Body ResNet modelOk, this was a bit messier than I wanted since ResNet and MobileNet models return results in different order (?!), but finally  is compatible with both.You can enable ResNet models like this:\n\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 16, 2020",
            "body": " I've figured out what is the difference in MediaPipe new compiled demo.Anyhow, model returns point cloud for each keypoint (e.g. there are multiple possible points for ). JS code finds one with highest score and that's it - but difference between them can be 0.0001% and switching back and forth, so result is \"jumpy\".Compiled one finds average between all points with high scores (as they are all ) - thus the result is much smoother output and confidence score is still high.It would be doable to do in JS as well, but it's not high on my list right now.Anyhow, that's all from me for now on this thread - major work and 6 updates.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ButzYung",
            "datetime": "Dec 17, 2020",
            "body": "Been testing the new version of body detection. Unfortunately it's still not that accurate and \"jumpy\". Changing models don't seem to help much. ResNet is better, but not much. What's even stranger is that when maxDetections is left as default or bigger than 1, sometimes it would detect more than 1 body (the max I noticed was 4), even though I am the only one sitting in front of the camera LOL. When maxDection is 1, behavior is somewhat different like you mentioned as it will mess up left and right hands sometimes (something I already noticed when using the TFJS version in the past, though it was less serious). But besides the hands problem, maxDections=1 still looks better than >1 (which is even more jumpy) in general.I notice that the body detection of your live demo looks better than how I use it in my app in general. I have tried various config combo and can't really figure out the reason. Maybe it's because I am running my app on  but not a native browser? Web worker (BTW your demo doesn't seem to work with web worker option ON)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 17, 2020",
            "body": "Ok, I'll leave it in.Strange, I find it much better up to a point of really good except for some jitter due to lack of smoothing (see my notes on keypoint cloud).\nBtw, I did notice that body models in general are very sensitive to lighting conditions and tend to be quite jumpy in darker areas.I'll take a look. It was working, but I probably broke it unintentionally.Electron vs browser shouldn't matter.\nWhich backend are you using? I believe there is a rounding issue in WASM backend at the moment which can cause some precision errors (I have several issues on that open with TFJS team), better try WebGL if that is an option.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ButzYung",
            "datetime": "Dec 17, 2020",
            "body": "WebGL. Checking the console but couldn't see anything wrong in the config or any config difference between your demo and my app. The only \"difference\" is the reports on tf flags, in which your demo seems to show more details, but for properties that exist on both sides, they return the same value.On a side note, I can have both  (without body, just hand detection) and TFJS PoseNet (loaded in the conventional way) running at the same time. Yeah it's clumsy, but at least it works lol",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 17, 2020",
            "body": "Can't explain the difference since I haven't seen your app.Regarding  demo and web workers - I just tried it in Chrome & Edge and it works. Even better if you enable  as then UI refresh is completely detached from processing.Which browser? Firefox is missing several features and I've decided not to support it for web workers currently. It would be possible, but a major pain - I'd rather wait for Firefox team to finally implement things like offscreenCanvas.Update: Ahhh,  that hosts live demo resolves relative paths differently than my local environment, so  was not even loading (error 404).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 24, 2020",
            "body": "i'm closing this issue as there is a lot of things worked on here, but feel free to open a new one to track further work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 4, 2021",
            "body": "FYI, I've managed to successfully convert and implement MediaPipe's  model as alternative to .\nPoseNet is still the default, but can be switched via configuration options. If interested, check out model notes as it's performance is quite difference.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 15, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 15, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 17, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Dec 24, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/84",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lenonMax",
            "datetime": "Mar 8, 2021",
            "body": "\nPart of face even no real face can be recognize as humanbeings.\n\n1.Open  face detector only.\n2.Fist to the camera.\n3.Function of human.detect returns value which has face and the face.length>0.\nThe fucntion reutrns value no face if there is no face in the detection area.**EnvironmentLooking forward to ur reply.\nTks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 8, 2021",
            "body": "how it works is that detector prepares list of candidates for face boxes and then mesh actually discards invalid entries and also stabilizes facial bounding boxes\nif mesh is disabled, output of detector only is not reliablecan you retry with mesh enabled,\nbut instead of just looking at , check the values fori'll update docs to reflect thisbtw, version  is a bit old, there have been several improvements since.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "Thank you for reply.\nThe resaon I set the mesh disabled is in order to save some flow.\nI 'll try it now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "So,I found my fist scored 0.8330078125 and sometimes my face scores below that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "And I found the type of human in node_modules@vladmandic\\human\\dist change to typescript.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "That's the  value? Because  (when mesh is enabled) should be far lower for the fist :)Yes, I've switched the project source to TS. But the stuff in /dist is always compiled JS.Let me know if you have any other questions...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "That's just result.face.confidence then disable the mesh. And I also got the faceConfidence and boxConfidence which is high score when I use my fist to the camera.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": " is just a pointer to  if mesh is disabled or to  if mesh is enabled.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "I found a way around this problem by double-checking the returned data, but it doesn't work on Firefox and will report an error.\nthe error is :",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "my code,photo is the base64 of current photo.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "Yes. but none face is still get high score.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "Can you post values for  AND ?That's a separate issue and it's due to fact that image is not ready at the time when you call .\nBasically, you assign  and then call  immediately after, but browser may take some time to actually process the src you just assignedWhat you need is to wait for image to actually get loaded before calling  - something like:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "or if you have image data in base64 already, you can skip image DOM element completely, something like:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "Oh.I see. That's my fault.I didn't consider that.lol. Thank you so much.\nThe value of faceConfidence maybe around 0.7.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "And does it drop after a while?I just noticed that you have  in your config meaning actual face detection is skipped for some frames and instead it returns interpolated values (for performance reasons)When testing, it's best to force full detection each frame by setting ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "I hope to recognize faces more accurately even if the mesh is disabled.The reason why I disable it is that it spends more time to load. The user experience is not particularly good",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "And the browser will caton before runing the scripts.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "Sorry i didn't notice it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "if you only need face detection and nothing else, you can try alternative model\ndefault model is BlazeFace, but that only works when combined with FaceMesh\nbut there is also FaceBoxes which is a standalone detector - exactly for the reason that BlazeFace is not that good when running as standalone modelI'm still looking for a single good algorithm for face detection that can replace both.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "I will do as u say.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "Regarding load time - is it actual  or  that is not good for user experience?\nYou can completely skip initialization time if you set  (instead of default  as  takes time to initialize GL shaders, but WASM has no such thing). Once initialized,  is faster than , but for just face detection that wouldn't matter.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "To summarize,",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 9, 2021",
            "body": "I will do it. It's so cool to discuss with you. I like the repo which is wonderful.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 9, 2021",
            "body": "I just published an update:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 10, 2021",
            "body": "Thanks a lot.I will try it now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 10, 2021",
            "body": "You can skip testing faceboxes, I've decided to remove it and focus on improving primary model - blazeface. Everything else stands.Also note that if you have a face in the frame with high score (e.g. 0.9) and you cover the face with the hand, but corner of the face is still visible, you may get one false positive with low score (e.g. 0.1) for the hand, but also another positive hit with mid score for partial face (e.g. 0.5).that is actually desired behavior, so when reporting false positives you need to check actual visualization what is it reported on.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 10, 2021",
            "body": "Okay.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 10, 2021",
            "body": "\nMy fist was recognized as a face.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 10, 2021",
            "body": "With mesh disabled?\nWhat's the boxConfidence?How about with mesh enabled? And what's the faceConfidence then?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 10, 2021",
            "body": "It works well when mesh is enable But I disabled the mesh and the value of confidence is 0.5xxx.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 10, 2021",
            "body": "Glad it works with mesh.And is the performance with mesh when using WASM backend acceptable?And even without mesh, the score drops, so setting minimum score to something like 0.75 would solve both cases.Anyhow, I'll take another look to tweak scoring of model without mesh as it does seem a bit high.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 10, 2021",
            "body": "It is faster than before when i used WASM and with mesh enabled.I guess u have solved my problem already.\nThanks！(:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MuriloRS",
            "datetime": "Apr 13, 2021",
            "body": "Where can I find this faceboxes model?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Apr 13, 2021",
            "body": "I wrote a month ago:If you really want to check it out, you can clone the repository and go back to version 0.9.x ( model was removed with 1.0 release). alternative, old versions are archived on , so you can just install a 0.9.x version of  using .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lenonMax",
            "datetime": "Mar 8, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 8, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Mar 10, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/vladmandic/human/issues/38",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Jimmysh",
            "datetime": "Nov 23, 2020",
            "body": "When multiple people's face gesture. The information can't be matched to real one.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 23, 2020",
            "body": "can you elaborate - why not? each face returns separate face embedding feature vector, so it's just a question of having a loop to go through all detected faces.i've updated embedding notes: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jimmysh",
            "datetime": "Nov 24, 2020",
            "body": "result.gesture.face data can't match with face.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 24, 2020",
            "body": "ahh, my bad, i was talking about embedding.\nyes, gestures currently mashes everything together - and it's not only about face.i need to redesign that part.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 24, 2020",
            "body": "Implemented via Gesture is now returning a different format, it's an array of objects which also includes index of a person that gesture belongs to.Documentation and demo have also been updated.Example output:Please confirm before this case can be closed.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 25, 2020",
            "body": "closing as resolved.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jimmysh",
            "datetime": "Nov 23, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 23, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 24, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 24, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 24, 2020",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "vladmandic",
            "datetime": "Nov 25, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/312",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Sivajstme",
            "datetime": "Apr 18, 2018",
            "body": "can we add a google cardboard view camera like shown in the below image to the google vr view using three.js\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Sivajstme",
            "datetime": "Apr 18, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/272",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Emanouel41",
            "datetime": "Nov 8, 2017",
            "body": "I know this one has been posted before but none of it helped me and none of it seemed to solve my issues so i would love to have your help.So here is the case:I am trying to run a vr video throught html file\n\nthe file is the classic congo video but i changed it and added another injected file that was shot with a 360 camera.Since i do not have my own domain i try to run it with Xamp (not sure if CORES properly added but i am having a hard time fixing it).*\nSorry its a png but i am not sure how to post the code here since parts of it are being \"deleted\" for some reasonError:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emanouel41",
            "datetime": "Nov 9, 2017",
            "body": "never mind i solved it my self",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bmobio",
            "datetime": "Sep 24, 2018",
            "body": "Hello,\nI'm facing the exactly the same problem. Can you tell me how you fixed it ? Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lincolnfrog",
            "datetime": "Nov 20, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/233",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 20, 2017",
            "body": "I tried adjusting the color properties of hotspots in hotspot-renderer.js, but my project still displays with the default hex values.  I saw on a closed 2016 issue that hotspots were not customizable, but I wanted to check in and confirm that this was still the case.Any assistance is appreciated.  Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aerialglasgow",
            "datetime": "Jul 20, 2017",
            "body": "Hi BrendanI was able to alter the hotspot colors here// Constants for the focus/blur animation.\nvar NORMAL_SCALE = new THREE.Vector3(1, 1, 1); // AERIAL\nvar FOCUS_SCALE = new THREE.Vector3(1.5, 1.5, 1.5);\nvar FOCUS_DURATION = 1000;// Constants for the active/inactive animation.\nvar INACTIVE_COLOR = new THREE.Color(0x00ABCE); // AERIAL ORIG (1, 1, 1)\nvar HOTSPOT_COLOR_NORMAL = 0x00ABCE;\nvar HOTSPOT_COLOR_ARROW = 0x54A719;\nvar HOTSPOT_COLOR_OUTER = 0xffffff;\nvar ACTIVE_COLOR = new THREE.Color(0.8, 0, 0); // AERIAL ORIG (0.8, 0, 0);\nvar ACTIVE_DURATION = 100;\nvar FOCUSON_ACTIVE_DURATION = 1000;\nvar FOCUSOFF_ACTIVE_DURATION = 200;// AERIAL Amendments\nvar HOTSPOT_INNER_RADIUS = 32;\nvar HOTSPOT_INNER_RADIUS_FACTOR = 0.6;\nvar STANDARD_DISTANCE_FROM_CAMERA = 4;// Constants for opacity.\nvar MAX_INNER_OPACITY = 0.8;\nvar MAX_OUTER_OPACITY = 0.6;\nvar FADE_START_ANGLE_DEG = 35;\nvar FADE_END_ANGLE_DEG = 60;You should be able to search the code library for these entries. Hope that helps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 20, 2017",
            "body": "Thanks, Aerial.  That's where I'm making my adjustments, but I'm not seeing it update in my project.  I tested again this morning and it still didn't work.  Additionally, I adjusted the color properties on  and  within  but that also didn't work.After adjusting the source code on hotspot-render.js, do I need to recompile the API using build?  I assumed having the entire project directory on the server would be sufficient to reflect those updates when changes were made within the src folder.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aerialglasgow",
            "datetime": "Jul 20, 2017",
            "body": "Ah yeah, I think you would have to recompile the API, your changes were probably overwritten",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 21, 2017",
            "body": "UPDATE:  It's working now.  I'm not sure why it wasn't before, but I'm good.  Thanks again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 21, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 21, 2017",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "BrendanCarlin",
            "datetime": "Jul 24, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/167",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 3, 2017",
            "body": "Default yaw works great on the first scene I load, but when the user clicks a hot spot and loads a new scene via vrView.setContent(), the default yaw seems to load relative to where the user the was looking when they pressed the hot spot.So if the user is looking really far left and loads a new vr View. The new image's default yaw will load too far to the left. I'd like the new image to load in the same direction no matter where the user is looking when they press the hot spot.Anyone else having this issue? Sorry if this is unclear",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 4, 2017",
            "body": "When a new scene is set up it does not take into account the current rotation of the camera.You can subtract the current camera yaw ( y rotation ) here:...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 4, 2017",
            "body": "Thanks for the response. Unfortunately I don't think it's working for me.I changed line 202 in world-renderer.js to\nMaybe it's an order of operations thing? Do I need some parenthesis?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 4, 2017",
            "body": "are you building it? ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 4, 2017",
            "body": "Nope, sorry I'm kind of a newbie. I don't usually use npm or any package manager.Can I not just edit the js file in my src folder?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 4, 2017",
            "body": "For a quick fix you can edit embed.js in /build\nsearch for ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 4, 2017",
            "body": "You sir, are a saint.Thank you very much. Works perfectly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 4, 2017",
            "body": "you're welcome thanks :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 4, 2017",
            "body": "For mobile this needs to be changed.  For now use this to cancel it on mobile",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gol4er2219",
            "datetime": "Jun 27, 2017",
            "body": "Were you able to find a fix for mobile? I would really like it to be consistent between desktop and mobile.\nThanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 27, 2017",
            "body": "No fix on mobile yet. I don't think it takes into account the gyroscope. But  is the mastermind, he might have some ideas.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gol4er2219",
            "datetime": "Jun 28, 2017",
            "body": "OK thank you! I'll keep looking into it and see if I find anything I can use.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jun 29, 2017",
            "body": " lol thanks.  I'll see what i can do...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jul 1, 2017",
            "body": "Try this out:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gol4er2219",
            "datetime": "Jul 1, 2017",
            "body": "You're so awesome  !! It looks like those changes worked! Thank you  as well. Thank you both.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lincolnfrog",
            "datetime": "Jul 11, 2017",
            "body": "Tommytee added this fix to master @ ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "georgedumontier",
            "datetime": "Jun 4, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Jul 1, 2017",
            "body": [],
            "type": "pull",
            "related_issue": "#208"
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/145",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "transpirman",
            "datetime": "Apr 20, 2017",
            "body": "Hi,I tried the hotspot example in VR mode (Google Chrome on S7 Edge)When I gaze at any hotspot, the white circle grows but I do not see any recticle.I would also like to activate a short timer so gazing at hotspot would trigger a click after a few second (like a \"fuse button\" as described here: ).\nIs there a way to do that ?Do someone have code examples of how to capture these events like Hotspot onFocus, etc...Thanks !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "weddingdj",
            "datetime": "Apr 20, 2017",
            "body": "I had the same issue using Google Chrome 57 on a Nexus 5. Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "willian-sisinfo",
            "datetime": "Apr 25, 2017",
            "body": "Same thing using latest version on iOS. The docs are not clear how we should proceed when VR mode is true. Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 26, 2017",
            "body": " its easy, you have function HotspotRenderer.prototype.focus_ ,   there you can put some setTimeout and add logic if it focus more than 1 sec or whatever do this  if someone focus and than blur just remove it. Should work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "transpirman",
            "datetime": "Apr 26, 2017",
            "body": "Thank you for the code.Actually I started building on that already, but encountered a problem for which I found a workaround. I will try to post code asap.The problem : when hotspots are created, the 'update' function in renderer emits a focus event, and no blur event.\nThe workaround: initialize a FocusEnabled status to false, and delay a true assignment",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 26, 2017",
            "body": " it will help to you this.hotspots[id].show check it before settimeout in focus",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "transpirman",
            "datetime": "Apr 26, 2017",
            "body": " Oh thank you for the tip ! I did not think of it.What I did so far in hotspot_renderer:\n\nvar HoverTimer; // \nvar HoverReadyTimer; // \nvar HoverReady = false; //  = function(pitch, yaw, radius, distance, id, image, is_stereo) {\nclearTimeout(HoverReadyTimer); HoverReady = false;\nHoverReadyTimer = setTimeout(function() { HoverReady = true; }, 500);\t// ready for real blur event\n...\n = function(camera)\nif (isIntersected && !this.selectedHotspots[id] && ) {\nthis.emit('focus', id); this.focus_(id);\n}\n = function(id) {\nvar hotspot = this.hotspots[id];// Tween scale of hotspot.\nthis.tween = new TWEEN.Tween(hotspot.scale).to(FOCUS_SCALE, FOCUS_DURATION)\n.easing(TWEEN.Easing.Quadratic.InOut)\n.start();\n// color change\nvar inner = hotspot.getObjectByName('inner');\nthis.tween = new TWEEN.Tween(inner.material.color).to(ACTIVE_COLOR, ACTIVE_DURATION)\n.start();// Virtual Click (todo: real fuse button)\nif (HoverReady == true) { var that = this; HoverTimer = setTimeout(function() { that.emit('click', id); that.up_(id); }, 2000); }\n};I will try to create a proper 'pull request' once I get how to fully use GitHub :-)Now, any idea about the reticle not showing up ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 26, 2017",
            "body": " i found the reason, just need change this in package.json and rebuild",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "May 8, 2017",
            "body": "for the reticle, uncomment ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "May 9, 2017",
            "body": "The false calling of focus that happens when a hotspot is added can be stopped by moving this line  two lines down so it is under \"this.effect.render...\"This fix is also needed: Then, adding to the code from fix2015: ( hotspot-renderer.js )These changes (and the reticle enabled) are in the branch named \"gaze\" here. (will do a pr soon)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 29, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#153"
        },
        {
            "user_name": "tommytee",
            "datetime": "May 19, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#135"
        },
        {
            "user_name": "tommytee",
            "datetime": "Jul 1, 2017",
            "body": [],
            "type": "pull",
            "related_issue": "#210"
        },
        {
            "user_name": "tommytee",
            "datetime": "Jul 11, 2017",
            "body": [],
            "type": "pull",
            "related_issue": "#223"
        },
        {
            "user_name": "lincolnfrog",
            "datetime": "Jul 12, 2017",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/8",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "PoC22",
            "datetime": "Apr 23, 2016",
            "body": "May be just me - but when I use the  variable it has different results depending on whether I'm viewing on mobile (iPhone 6 Plus, Safari) or desktop (iMac 2015, Safari) - both latest versions as of writing.I can also confirm this is the case with the latest version of Google Chrome on iPhone. - initial view as expected when using the  variable, e.g. =90 - does what it says on the tin.\n - ignores  completely.Images used taken with  camera.Cheers.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Dec 1, 2016",
            "body": "This should be fixed, please try again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Dec 1, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/131",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "fix2015",
            "datetime": "Mar 15, 2017",
            "body": "Hi guys, i found bug, when i try change rotation it not change, it work on with thiscamera.parent.\nhere what i do\n\nbut if i do that\n\nbut in this way center will be change and than i can't find correct camera.rotation(xyz)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tommytee",
            "datetime": "Mar 27, 2017",
            "body": "The camera rotation is controlled by device orientation (gyroscope, accelerometer) or a mouse (on desktop) using the webvr-polyfill or a webvr capable browser.The camera parent ( cameraDummy ) is what you probably want to rotate.If you really want to control the camera directly (and correctly know where it is positioned when using the webvr-polyfill on the desktop) you will need to update camera.quaternion ( or camera.rotation ), vrDisplay.phi_ and vrDisplay.theta_.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lincolnfrog",
            "datetime": "Jun 15, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/2",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "wahengchang",
            "datetime": "Apr 14, 2016",
            "body": "Is it possible to read 360 (not 720) images taken by cardboard APPIt look like only both mono and stereo Spherical panorama(not cubic format) images are supported for new , is 360 (horizontal  only panorama) planed to support ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gbentley",
            "datetime": "May 1, 2016",
            "body": "I'd also like to see this integrated as a query string parameter, so that standard Cardboard Camera photos can be displayed correctly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marklundin",
            "datetime": "Jun 11, 2016",
            "body": "What's the format of the Cardboard Camera photos? Are they equirectangular?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Jun 12, 2016",
            "body": "Yes. Also see this Cardboard Camera => over-under converter, which we recently released: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marklundin",
            "datetime": "Jun 12, 2016",
            "body": "Awesome!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gbentley",
            "datetime": "Jun 12, 2016",
            "body": "There is an issue with the convertor though - the resulting image on all my\nconversions ends with a black space on the right of the top image,\ncompressing the top image, and making the stereoscopy a little useless.I was delighted when it was released, but this issue hasn't been fixed (and\nvery few channels to provide feedback on it).On Mon, 13 Jun 2016 5:03 am Boris Smus  wrote:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Jun 13, 2016",
            "body": "Please attach an example of the cardboard camera input and converter output\nto this thread.On Sun, Jun 12, 2016 at 12:26 PM Geoff Bentley \nwrote:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marklundin",
            "datetime": "Jun 13, 2016",
            "body": "Any change this could be opensourced ? The OdsConverter looks like it would be really handy",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gbentley",
            "datetime": "Jun 13, 2016",
            "body": "Images below.  What's really strange is that the converter gives back a PNG (check the file format), but calls it a JPG.\n There is a JS-based convertor you can use at  - it doesn't blur it, but it works.  Also see  . All discussed at Here's the result from the JS-based convertor:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Dec 1, 2016",
            "body": "Ok, if there are issues with the cardboard camera converter, please file a new issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Dec 1, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "zdrawku",
            "datetime": "Feb 24, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#110"
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/120",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "bobvanluijt",
            "datetime": "Feb 7, 2017",
            "body": "Any advice on how to convert a regular image into an equirectangular image? I've created a 3d image that I would like to convert into an equirectangular image for vrview.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yvan-sraka",
            "datetime": "Feb 25, 2017",
            "body": "Hi, what is the format of your image?\nPerhaps the the  could do the trick !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "Jun 26, 2017",
            "body": "These \"how to\" questions are better posted on stack overflow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bobvanluijt",
            "datetime": "Feb 7, 2017",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "Jun 26, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/839",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "stolarek",
            "datetime": "Sep 24, 2021",
            "body": "Hello everybody,\nI use the nice library in my system with multiple cameras. It works just fine. The system takes pictures once a day via the connected cameras. Now to my problem: I want to display the live stream of individual cameras on my settings page and the user should be able to switch between the images of the cameras and align the cameras. I use currently for every webcam one \"WebcamStreamer\" class with port 8080 + n and switch the livestream with them. After a few minutes of inactivity, the cameras should be switched off. I use WebcamStreamer stop(). Is it possibele to restart an existing Streamer or schould a new object be created?\nI tried different combinations but at some point I can't see the livestream.\nWhat is the best way? Is it possible to switch different cameras with just one \"WebcamStreamer\"?\nI would be happy if there are suggestions here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": "Hi  ,so sorry to tell you that restart webcam streamer, webcamera API must reallocate stream reading postion again. but this is not implemented, please see comments:/**",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/39",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "03difoha",
            "datetime": "Aug 11, 2016",
            "body": "I have tried to convert the example 'coral' image and a panoramic image I took on the cardboard camera app on the cardboard camera converter site and everytime it gives me the error",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Aug 11, 2016",
            "body": "Please attach the image you're trying to convert.On Thu, Aug 11, 2016 at 7:45 AM 03difoha  wrote:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "03difoha",
            "datetime": "Aug 11, 2016",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Aug 11, 2016",
            "body": "Can't reproduce. Mine converted without a hitch.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Dec 1, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/71",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "libbybaldwin",
            "datetime": "Nov 2, 2016",
            "body": "When following the link to the converter tool and instructions on the page below, I can't convert a file to use in an embedded VR View. The text states \"To use these images with VR view, download the image, and then use our conversion tool to create a stereo 360 image that meets our image specifications.\"I have tried both dragging and manually opening my downloaded Cardboard Camera app-captured file to the converter page. The page/app appears to run for about 15 minutes (converts locally or in cloud?), hijacks my Chrome/Ubuntu, then stops and nothing happens. I do not get directed to the converted file or an error message.The image I used works perfectly in the Cardboard app with a Cardboard viewer. Captured and viewed on Nexus 5X, Android 7.0. Downloaded to desktop computer and used in converter page. After failed conversion, found error in browser: \"Uncaught TypeError: canvas.toBlob is not a function\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Nov 2, 2016",
            "body": "Thanks Libby, which browser are you using? Could you paste your user agent from ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "libbybaldwin",
            "datetime": "Nov 2, 2016",
            "body": "Hi Boris.Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Nov 3, 2016",
            "body": "Well, looks like your browser doesn't have canvas.toBlob, but I think Chrome has had this for a while.Just to clarify, this is Chrome on Linux, right? Is it Chromium? Are you sure that you're up to date? M48 is quite old.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "libbybaldwin",
            "datetime": "Nov 3, 2016",
            "body": "Yes, Google Chrome. It is supposed to inform me to update or just auto update, but I noticed it doesn't or hasn't. Update bars are white. Ubuntu 14.04 - yes I am due to update OS, Chrome and also hardware in fact (need new laptop). Didn't think it was all SO old as to fail at something like this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "borismus",
            "datetime": "Nov 3, 2016",
            "body": "Yeah, sorry. We rely on that function call to make image conversion more\nperformant. Try upgrading and let me know if it works?On Thu, Nov 3, 2016 at 11:30 AM Libby Baldwin \nwrote:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "libbybaldwin",
            "datetime": "Nov 3, 2016",
            "body": "Yes, I will try upgrading, starting with Chrome only.Under Image specifications the page says \"Mono images should be 2:1 aspect ratio (e.g. 4096 x 2048)\".\nUnder Real World Capture, Cardboard Camera App section it implies (as I read it) using default image from Cardboard Camera App (anyways no options or settings besides audio on/off) as input to converter.\nTwo of my .jpg images from the app are 9308x1641 and 9174x1604. What am I missing? Should I scale the images to nearest power of two, or should the converter accept any size? (sorry if this is OT, I can move to another issue, perhaps relating to documentation)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "Nov 3, 2016",
            "body": "Once you run the CC images through the converter they should meet the VR\nView spec.On Thu, Nov 3, 2016 at 11:46 AM, Libby Baldwin \nwrote:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "libbybaldwin",
            "datetime": "Nov 3, 2016",
            "body": "Strictly concerning this Issue: Upgraded Google Chrome  48 and I am able to convert. Fix is to upgrade browser. I am running version 54 now.In my case, I was running the 32bit version (for unknown reasons) of Google Chrome version 48, which is no longer supported. Updates stopped coming earlier this year with no warning from browser itself. As of today I am running Ubuntu 14.04 on 64 bit hw. I followed instructions to update Google Chrome AND force [arch=amd64] for this and future Google Chrome updates. Scroll down to the answer from  from Mar 6 (2016) to see update instructions: Thank you! I have many holes to fill in my knowledge of VR, 360, OU, etc terminology, which explains my wonky question. Working on it.. :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "Nov 3, 2016",
            "body": "Glad you got this fixed!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "Nov 3, 2016",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/googlearchive/vrview/issues/140",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "theunreal",
            "datetime": "Mar 31, 2017",
            "body": "Currently, the click event returns an  which is always  when the user didn't click on an hotpost.If the click event would return the  and  of the clicked point, it could be really helpful.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Mar 31, 2017",
            "body": "You can write function that will return worldRenderer.camera.rotation and than do that -\nYaw: worldRenderer.camera.rotation.y * 180 / Math.PI,\nPitch: worldRenderer.camera.rotation.x * 180 / Math.PI,",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "theunreal",
            "datetime": "Apr 1, 2017",
            "body": "Hey  , How I can actually access the worldRenderer?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 3, 2017",
            "body": "try merge this commit  , and you will have methold  vrView.getPosition(); that return to you YAW and PITCH",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 3, 2017",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "theunreal",
            "datetime": "Apr 7, 2017",
            "body": "Looking awesome  !\nBut I can't figure out how to import your unminified version in my Angular 2 project and test it.\nPreviously I Just included the following js file in my index.html and declared the VRView object:\nI tried to include your vrview.js but I got  error in the iframe",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 7, 2017",
            "body": "you need clone project and write command\nnpm run watch - its for  unminified version",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "willian-sisinfo",
            "datetime": "Apr 25, 2017",
            "body": "Hi . This seems awesome... just spent the weekend calculating and transforming the panorama pixels into coordinates and now I see your branch.I've tried what you told (cloning and running npm), but I'm not being able to see the hotspot I created... Is there any change how they are created or displayed?Thank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 26, 2017",
            "body": " is shouldn't hide your hotspot, maybe its something else, can you show your script maybe i will find pb",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "willian-sisinfo",
            "datetime": "Apr 26, 2017",
            "body": "Sure. Actually is pretty much the aquarium example provided by google, only difference is JSON structure.`tp = {\nstartPreview: function (tourId) {\n$.ajax({\nurl: '/dashboard/tour-virtual/preview',\ntype: 'POST',\ndata: {\n'tour' : tourId\n},\nbeforeSend: function () {\n$.LoadingOverlay('show');\n},\n}).success(function (data) {\n$('#modalTour').modal('show');//            vrView.on('getposition', function(e) {\n//                   console.info(e);\n//            });}`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fix2015",
            "datetime": "Apr 26, 2017",
            "body": " i fixed, now try this  or just clone my repo  , and there you will see hotspot and on /examples/hotspots/index.html you will find, how get ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "willian-sisinfo",
            "datetime": "Apr 26, 2017",
            "body": " That's great! I'll test it later on. Thank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "thriskel",
            "datetime": "May 22, 2017",
            "body": "apparently it is getting the center of the view pitch and yaw, but how do I make it so it gives the pitch and yaw where the click was located?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nathanmartz",
            "datetime": "May 5, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/837",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "GivouDev",
            "datetime": "Sep 6, 2021",
            "body": "Hello Guys,the API doesnt detect any Webcams on my Linux Machine.The Code i use is not that important, because...its not doing really something but here:\n`package org.givou.ai;import com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;import javax.swing.*;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;public class Main {\nstatic JFrame ui;}\n`I have installed all Libraries and slf4j-simple extra, because i got this StaticLoggerBinder error.\nNow when i try to fire up my project it shows me this:\n[main] INFO com.github.sarxos.webcam.Webcam - WebcamDefaultDriver capture driver will be used\n[main] WARN com.github.sarxos.webcam.Webcam - No webcam has been detected!And Webcam.getDiscoveryService().getWebcams() is also showing 0 webcams.\nhwinfo --usb finds my webcam aswell, and i can access it with mplayer.\nThanks to anybody who tries to help me :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": "hi ,\nthere indeed some cases camera can not be detect. The root cause is that default built-in camera driver depends on bridj which is a JNA solution from java to native code. Unfornaturely, bridj community stopped update for a long time so that can not work for new OS.\nWhat you can do is change other driver.Alex",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/834",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "cameronboddie",
            "datetime": "Aug 13, 2021",
            "body": "Hello I have a demo where I need to use the webcam to capture a QR. Simultaneously, I will be on a zoom call with my PM. Is there a way to make this work? It works while not on Zoom but doesn't work even if my camera is off. I haven't been able to properly troubleshoot the issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Aug 13, 2021",
            "body": " underlay camera hardware is an exclusive resource, once you are in Zoom meeting, the camera is taken by Zoom. You can not open it again from another process. at JVM high level, it is hard to debug.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cameronboddie",
            "datetime": "Aug 15, 2021",
            "body": "Thank you for the quick response. I appreciate it, I'll look into some work arounds for the demo.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cameronboddie",
            "datetime": "Aug 15, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/833",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "brianmichalk1",
            "datetime": "Aug 9, 2021",
            "body": "Sometimes my USB glitches while in a getImage() call, which hangs my program.  I've put this into a separate thread and use a semaphore timeout to indicate when getImage() takes too long.  Killing the thread sometimes works but doesn't seem to be the correct approach.  Should I use Webcam.shutdown?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Aug 13, 2021",
            "body": "Hi  ,Java is working at high level, for webcam-capture, it is a lighweight of wrapping of camera accessing via JNA. so device glitch is not handled well well though we introduced WebcamDiscoveryListener.\nWhen you open camera without reset new Driver explicitly, default is:\n\nit is thread unsafe and blocking getImage().My suggestion is:\nyou can attach a WebcamDiscoveryListener to watch camera devices though it is not fully guaranteed.\nmanage the getImage() thread gracefully with timeout and WebcamDiscoveryListener",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/832",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "chottu33",
            "datetime": "Jul 12, 2021",
            "body": "Hellowhile the recording is on, webcam is not getting connected again, if the camera is disconnected and connected back.\nObservation: Discovery Service of the camera is not running\nOS: Ubuntu 20, Centos 8\njars version - webcam-capture-0.3.13-20200330.202351-7.jar, bridj-0.7-20140918-2.jar\njava version: 1.8.0",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Jul 24, 2021",
            "body": "could u please provider hardwaew info?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/829",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "kashis7870",
            "datetime": "Jun 12, 2021",
            "body": "Please help me to fix this problem, i am getting a black screen at the given location and also getting few errors.\nPlease find attached.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": "please check your camera, probably VGA is not proper for your hardware",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/830",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "shichiye",
            "datetime": "Jun 15, 2021",
            "body": "At present, I know that the camera can be obtained by a given name, but I cannot confirm which camera it is. I want to know whether the corresponding USB camera can be obtained by the USB camera similar to the hardware id. I don’t seem to find the corresponding api.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 15, 2021",
            "body": "There is no such API in the Webcam Capture project. What we have here is a very basic abstraction over the UVC device. There is no API to get the USB information. Even if we go into the lower level and dive deep into the native C++ code, there is no such thing as well. The operating system abstracts the device and completely decouples it from the USB transport.One could in theory use some 3rd party library like  to scan the ports and correlate UVC data from Webcam Capture with the hardware device class present in the USB port, but I never tried this and cannot even tell if such thing is possible. This is just a brief idea that came into my mind.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eduramiba",
            "datetime": "Jun 16, 2021",
            "body": "I'm working on publishing as open source a native driver capable of providing actual device ids and very good performance. For the moment it will work on Windows only. Maybe tomorrow it's ready, I will reply back.We use the driver in a production app and works really well.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eduramiba",
            "datetime": "Jun 17, 2021",
            "body": "Hi,I just published the driver here: Please let me know if you try it successfully or you have any problem.To get the device id you will need to cast  to ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shichiye",
            "datetime": "Jun 18, 2021",
            "body": "  Thank you both for your answers.\nI will try it, and then I will reply to you ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pacioc193",
            "datetime": "Dec 10, 2021",
            "body": "Could be possible to know the id device of the USB with the relative istance ?USB\\VID_0C45&PID_6366&MI_00\\6&183AF011&0&0000",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eduramiba",
            "datetime": "Dec 10, 2021",
            "body": "Yes, you can with  and casting  to ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pacioc193",
            "datetime": "Dec 10, 2021",
            "body": "Could you help me with an example in pure Java... I tried to follow the tutorial but after loading the dll no output of which webcam is connected to the computer. Without loading dll everything is working well. i tried to contact you on twitter also.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pacioc193",
            "datetime": "Dec 10, 2021",
            "body": "Also dll are loaded in the project Netbeans as in photo.\n\nI also tried to put in same place where opencv read it's own dll. OpenCv could work well without any issue but this driver won't work....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eduramiba",
            "datetime": "Dec 10, 2021",
            "body": " This library tries to load its DLLs from the natives folder relative to where the program is executed. Adding to netbeans or as jar won't do anything.Please make sure to run your program in a folder where natives is located. Also I recommend setting up a slf4j backend such as logback for viewing logging messages.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pacioc193",
            "datetime": "Dec 10, 2021",
            "body": "It works. Was a matter of libraries... I'm not using maven project... Standalone application developing here.\nThanks for the support",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eduramiba",
            "datetime": "Dec 10, 2021",
            "body": " Great! ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pacioc193",
            "datetime": "Dec 10, 2021",
            "body": "I think the best ways to solve this issue in the future... release a zip file with all the dependency ahah",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 15, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "shichiye",
            "datetime": "May 19, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "shichiye",
            "datetime": "May 19, 2022",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "shichiye",
            "datetime": "May 19, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/828",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Maruti-Nandan",
            "datetime": "Jun 12, 2021",
            "body": "I tried to run this codeAnd  got this error:Please help me to fix this error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": "The SLF4J logging framework JAR is missing in your classpath (with other required dependencies as well, e.g. BridJ). See  wiki page for more details on how you can fix it. Please also check the  section on the main page for the ZIP. In short - you need to have Webcam Capture JAR in the classpath, but also SLF4J and BridJ (or JARs for other capture framework if you are using something non-default, e.g. MJPEG).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maruti-Nandan",
            "datetime": "Jun 12, 2021",
            "body": "Thanks for replying",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maruti-Nandan",
            "datetime": "Jun 12, 2021",
            "body": "I added that but now the picture that is captured is totally black.\nSo, How should I fix that?\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/827",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "mayurb90",
            "datetime": "Jun 7, 2021",
            "body": "Hi,\nI am looking for a way to get HardwareId for the webcam using Saroxs library. I can get the camera name but looking for VIDPID of it.\nAny help would be much appreciated .Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": "Hi ,Sorry to disappoint you. Unfortunately, this is not possible. The Webcam Capture API get the webcam name and the webcam ID (nothing fancy, 0 for the 1st device, 1 for the 2nd device, 2 for the 3rd device, etc) from the operating system. These are the only parameters the underlying driver can access from Java. On Linux there is no ID at all, only the camera name, and not always. Sometimes it's only the device file from the  directory.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mayurb90",
            "datetime": "Jun 12, 2021",
            "body": "Hi ,Thanks for the information but do you have any suggestion on what could be done in the scenario where system is connected with multiple webcams but we just want to use one for the feed. Also, when you said webcam name from the OS, do we know how OS gets this information, does it have something to do with driver?Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jun 12, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/821",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "LuisHenriqueFA14",
            "datetime": "May 26, 2021",
            "body": "I don't use java a lot, so i don't know almost nothing about java (what that i need to compile or what is a .class file), but i'm trying to make a camera app to me, but i can't use it...I was having some troubles with package, i solved it, and when i was done, it was an error like:./com/github/sarxos/webcam/Webcam.java:20: error: package org.slf4j does not exist\nimport org.slf4j.Logger;So i started my search to solve it. I found a lot of issues about that, and i found something about put the  dir into the classpath (idk where are the classpath lol)So, i need some help with the libs, like, where to put it ?\nI need to put the  dir to my root path ?\nI need to put the  dir to my root path or the   ?\n(Please, be so specific bc i'm dumb)\nThank you, and sorry about my english ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jrbgarcia29",
            "datetime": "May 28, 2021",
            "body": "you need to build your project by running \"mvn install\" in your project base dir so that dependencies will be downloaded and missing jars will be available to your local repository",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LuisHenriqueFA14",
            "datetime": "May 28, 2021",
            "body": "When i run  it returns an error talking .\nHow can i install it ?\nIt has another way to build my project ?\nThank you again!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "May 31, 2021",
            "body": "This is not issue, please find java maven guileline by yourself",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "May 31, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/818",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "hqwl159",
            "datetime": "May 6, 2021",
            "body": "When the device number is greater than 15, the device information cannot be obtained.In Centos7\nI can read the camera info on the path of \"/dev\"\nI use the webcam-capture codeThe output  is \"1\".But I can read  all the  camera device info when the device name ranges from \"video0\" to \"video15\".So I guess the code cannot get device information after “video15”.Is my code error ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "flx5",
            "datetime": "May 7, 2021",
            "body": "That is an issue with the default driver OpenIMAJ. It only scans the device files 0 to 15. .\nYou might want to try the JavaCV driver. In that implementation every device in /dev/video* should be recognized.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hqwl159",
            "datetime": "May 7, 2021",
            "body": "Thank you most sincerely!!!\nThe webcam-capture can work without camera driver.  Does the webcam-capture or Linux OS  use the  OpenIMAJ as default driver?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "flx5",
            "datetime": "May 7, 2021",
            "body": "If you don't specify which driver should be used with setDriver, webcam-capture uses the default builtin driver which is OpenIMAJ regardless of the operation system.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hqwl159",
            "datetime": "May 7, 2021",
            "body": "Thanks a lot!!!\nI find  it in README.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hqwl159",
            "datetime": "May 7, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/807",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "zy5651768",
            "datetime": "Feb 23, 2021",
            "body": "I use default driver ,fps is low on ubuntu (  3-5 fps ) ,maybe change driver will good?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Feb 23, 2021",
            "body": "Hi,\nBecause pure Java preview panel and decoding is not high performance as native c, if your camera is 1080p, base one your PC performance, 3-5 fps make sense.Regarding other driver, maybe you can try JNI based driver like opencv or vlcj which are litter faster but not significant. and also you can set System properties in your launch arguments:OpenGL or XRender is enabled to get low latency, something like\nexport _JAVA_OPTIONS=-Dsun.java2d.opengl=True\nexport _JAVA_OPTIONS=-Dsun.java2d.xrender=Truebase on your hardware, maybe higher fps can be there.\nAlex",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Feb 27, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/801",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "schdai",
            "datetime": "Jan 5, 2021",
            "body": "Hi, I'm using video capture card to get image from some devices. There are more than one input, including  S-Video, Composite and etc.  Can webCam select input channel？ How？TKS。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexmao86",
            "datetime": "Feb 2, 2021",
            "body": "use WebcamCompositeDriver",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Feb 2, 2021",
            "body": "The drivers from WebcamCapture project supports only UVC devices and so will not be able to detect cameras connected via S-Video or Composite Cinch, but in general, if you implement (create) custom driver for every input you desire, then you can bundle these drivers together using the  as  suggested.Just for your information. I never found a Java project to stream video from S-Video or Composite Cinch. I guess you will need some kind of TV tuner for this. Not sure, I never worked with analog video data.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Feb 2, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/794",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "hatemibr",
            "datetime": "Nov 22, 2020",
            "body": "Hello there,I have been trying to find solution in all over the internet. I deeply investigated every ticket about this error but i have lost my day doing that. I am lost I NEED your help:I ' am using the last stable version of webcam (webcam-capture-0.3.12) - and I have successfully added the jar files to my program and i just tried to add this codebut I am getting a very strange Exception :any ideas what's going on here ?\nI am using Windows 10 64bit",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hatemibr",
            "datetime": "Nov 22, 2020",
            "body": " can I get your attention on this bug please ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Moimus",
            "datetime": "Nov 23, 2020",
            "body": "+1 I have nearly the same issue. I'm using 64 bit ubuntu on a raspberry pi 3b+ with Java 8 (64 bit too) though. While everything works fine on win 10 it just won't run on ubuntu",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hatemibr",
            "datetime": "Nov 23, 2020",
            "body": "You said it works fine on win10 64bit?can you please provide the steps you followed in order to let it works on win10 ?\nStep by step",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DanielMartensson",
            "datetime": "Dec 25, 2020",
            "body": "I have the same issue with Raspberry Pi 4B. I can't use the camera. I have open an issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hatemibr",
            "datetime": "Mar 3, 2021",
            "body": "This issue has been fixed by moving  library to C:\\Windows\\System32.This library can be found into the documents which the author developed. (src\\com\\github\\sarxos\\webcam\\ds\\buildin\\lib\\win64)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hatemibr",
            "datetime": "Mar 3, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/790",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ktrocki",
            "datetime": "Oct 22, 2020",
            "body": "Hi  ,\nI am trying to capture information about which ports my external cameras are connected to. I have a physical 6 camera array that I would like to match to my 6 camera live streamed GUI array. All 6 cameras are identical so the following code produces identical names other than the last index.  I believe they are discovered in an arbitrary order as the display order changes run to run. Is there a way to get more information about the webcam (COM Port, Physical Port etc?) so that I can match physical camera location to display location? Thanks!Output:\nUSB2.0 UVC PC Camera 0\nUSB2.0 UVC PC Camera 1\nUSB2.0 UVC PC Camera 2\nUSB2.0 UVC PC Camera 3\nUSB2.0 UVC PC Camera 4\nUSB2.0 UVC PC Camera 5",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Feb 2, 2021",
            "body": "This is not possible. The abstraction layer presented to us by operating system prevents us from checking to which port the UVC device is connected to.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Feb 2, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/771",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "geekeritcom",
            "datetime": "Apr 24, 2020",
            "body": "When I use the application to open the camera for the first time, it always throws an exception, but it is normal to open the program for the second time and the camera afterwards.It is very strange that this exception always appears when I run the program for the first time after restarting the server, and it is always good when running afterwards, but I cannot ask the client to always start the program twice. After restarting the server, I tried The following method does not workLooking forward to your answers urgently, thanks~",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "geekeritcom",
            "datetime": "Apr 27, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/768",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "gwynncausing",
            "datetime": "Apr 14, 2020",
            "body": "Recently when I used Sarxos Camera with 'slf4j-api' but there was an error like thisthen it was stated here  to change it into slf4j-simple and then the next error is this'java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory'I am currently doing a personal project in java swing with a camera API",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jan 29, 2021",
            "body": "See ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jan 29, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/764",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "geekeritcom",
            "datetime": "Mar 23, 2020",
            "body": "How to perform QR code analysis while displaying camera pictures，Now I can only display pictures or QR code, but they cannot work at the same time",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Mar 23, 2020",
            "body": "Hi ,This is not true. You can display both image from webcam, and QR code. The problem is how you implement this :)You can for example open camera in async mode so it does not block when you call , pass it to , and use the very same  instance in a separate  to read QR code from image. Both image display and analysis will be done here in parallel.Another example would be have a custom  which reads the code from image before it's returned from , but this solution will most likely drop effective FPS since QR code analysis time will be added to the  execution time. QR code analysis and image display will be done here one after the other (and for every image)Please take a look at this basic example. Here image from webcam id displayed in parallel to the QR code analysis:This example is the embodiment of the the first idea I described above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "geekeritcom",
            "datetime": "Mar 24, 2020",
            "body": "Thank you very much for responding so fast. Yesterday I also found a solution by referring to the image of the network camera. I saw your reply code today. Thank you very much. In any case, thank you for your reply and such a great tool.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "geekeritcom",
            "datetime": "Mar 26, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/744",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "oper2000",
            "datetime": "Oct 15, 2019",
            "body": "Works on previous Mojave MacOS",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "oper2000",
            "datetime": "Oct 15, 2019",
            "body": "duplicate ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "oper2000",
            "datetime": "Oct 15, 2019",
            "body": "duplicate ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "oper2000",
            "datetime": "Oct 15, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/742",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 4, 2019",
            "body": "Hi,I'm facing a problem with sarxos api. I'm using windows 10 and java 8, but the camera is black, like is not working, not capturing image.When I use windows 8 or linux, the application works fine.There is something to do?My code is below:`\npublic class App {}\n`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "noorball",
            "datetime": "Oct 7, 2019",
            "body": "This code works just fine for me (Windows 10, Java 8).\nYou can try to check permissions in Camera privacy settings in default Windows settings and in Vendor settings app (e.g. Lenovo Vantage etc).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 8, 2019",
            "body": "It's so weird..\nI am using desktop pc and I've already checked and setup the Camera privacy in Windows.\nFor me is showing a black panel.\nLooks like the application can communicate with the camera, but could not show the image.\nI don't know if it's a driver problem or hardware problem.\nMy camera is C922 Pro Stream Webcam.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 8, 2019",
            "body": "In others application from Windows the image is showing.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 8, 2019",
            "body": "The debug console:`\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/C:/Users/raphael.nascimento/Downloads/logback-classic-1.0.9.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/C:/Users/raphael.nascimento/.m2/repository/org/slf4j/slf4j-log4j12/1.7.26/slf4j-log4j12-1.7.26.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See  for an explanation.\nSLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]\n09:46:07.748 [main] INFO  com.github.sarxos.webcam.Webcam - WebcamDefaultDriver capture driver will be used\n09:46:07.755 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:08.275 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:08.275 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:08.279 [main] DEBUG com.github.sarxos.webcam.Webcam - Setting new resolution 640x480\n09:46:08.362 [main] DEBUG com.github.sarxos.webcam.WebcamPanel - Starting panel rendering and trying to open attached webcam\n09:46:08.369 [SwingWorker-pool-2-thread-1] DEBUG com.github.sarxos.webcam.WebcamLock - Lock Webcam C922 Pro Stream Webcam 0\n09:46:08.372 [atomic-processor-1] INFO  c.g.s.webcam.ds.cgt.WebcamOpenTask - Opening webcam C922 Pro Stream Webcam 0\n09:46:08.372 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Opening webcam device C922 Pro Stream Webcam 0\n09:46:08.373 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device 0 starting session, size java.awt.Dimension[width=640,height=480]\n09:46:08.692 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device session started\n09:46:08.694 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Clear memory buffer\n09:46:11.280 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:35.854 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device com.github.sarxos.webcam.ds.buildin.WebcamDefaultDevice@7548be95 is now open\n09:46:35.855 [SwingWorker-pool-2-thread-1] DEBUG com.github.sarxos.webcam.Webcam - Webcam is now open C922 Pro Stream Webcam 0\n09:46:35.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:35.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:38.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:41.326 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:41.332 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:41.333 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:44.334 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:46.751 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:46.758 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:46.758 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:49.759 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:52.168 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:52.174 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:52.175 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:55.175 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:57.605 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:57.612 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:57.612 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:47:00.613 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:47:03.028 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:47:03.041 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:47:03.041 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 8, 2019",
            "body": "I passed the parameter -client when run by command line e its works fine.\nLike this:\n\"java -jar -client class.jar\"Why I need to pass this parameter?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 9, 2019",
            "body": "Problem solved..\nI was using Spring Tools Suite, its a IDE based on Eclipse, but the camera doesn't work in that.\nNow I am using Eclipse IDE and the camera is working fine.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "raphaelbn",
            "datetime": "Oct 9, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/741",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "kumarmayank503",
            "datetime": "Sep 24, 2019",
            "body": "Hi Sarxos,I have used your api to display webcam and scan barcode .Though when i deployed it on server machine or if i am trying to open my local computer url from someone else machine,when I click camera icon in their machine ,it is opening webcam panel and camera in my machine.I am using jar -webcam-capture0.3.12.jar and bridj 0.7.0.jar.\nPlease help me why it is happening and what is the solution.Below is my code which I am using.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jan 29, 2021",
            "body": "The code above is using cameras from the computer where the code is running. It is impossible to open video feed from the other computers using code attached above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Jan 29, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/739",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "kumarmayank503",
            "datetime": "Sep 16, 2019",
            "body": "Hi Sarxos,I am using your above code ,did some modification according to our requirement but issue is that when result is coming null and at that time if I am closing the webcam panel then still camera keep getting on and going to infintie loop.Below is my code:package com.fourcs.clm.iwarranty.eclaims.controllers;import java.awt.Dimension;\nimport java.awt.FlowLayout;\nimport java.awt.image.BufferedImage;import javax.swing.JFrame;import com.fourcs.clm.iwarranty.eclaims.domain.ClaimHelper;\nimport com.fourcs.clm.sh.core.domain.Product;\nimport com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;\nimport com.google.zxing.BinaryBitmap;\nimport com.google.zxing.LuminanceSource;\nimport com.google.zxing.MultiFormatReader;\nimport com.google.zxing.NotFoundException;\nimport com.google.zxing.Result;\nimport com.google.zxing.client.j2se.BufferedImageLuminanceSource;\nimport com.google.zxing.common.HybridBinarizer;public class WebcamQrCodeTesting extends JFrame  {}Can you advice if result is null at that time if I am closing webcam panel then how should I know that webcam panel is closed or open so that I can put that condition and close the camera on that particular condition.Please advice as it is going on infinite loop.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kumarmayank503",
            "datetime": "Sep 16, 2019",
            "body": "It is working after adding this line..setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);But this line start giving another issue that is closing the server forcefully.So after running this functionality on my project I can not run any other functionality.Soi i dont want to use this line.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kumarmayank503",
            "datetime": "Sep 16, 2019",
            "body": "Its start working fine after adding a listener and having a  flag as cancelled inside that like below:this.addWindowListener(new WindowAdapter() {\npublic void windowClosing(WindowEvent evt) {\ncancelled = true;\nsetVisible(false);\n}\n});",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kumarmayank503",
            "datetime": "Sep 16, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/714",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "SoftPathTechnologies",
            "datetime": "May 9, 2019",
            "body": "Hi Sarxos, i am working on my Final Year Project, Surveillance System Based on Motion Detection and i am Using WebCam Capture in Java. Can you tell me what image processing algorithm you used in its implementation please? So I can discuss that in my literature review.Thanks as i anticipate your favourable response.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "May 10, 2019",
            "body": "Hi @lilblazekonceptzThe motion detection algorithm I used is very simple. The individual parts may be described somewhere in a literature, but to be honest I never checked and implemented it based solely on some very pragmatic assumptions which are described later.The fundamental steps are the following:This is done to reduce noise. We use a filter which performs a box blur on an image. Box blur is based on the moving window in which we calculate average values for every channel in RGB space. The blur filter I'm using is derived from JHLabs filters library by Jerry Huxtable. I cannot describe in details how it works, but you can check the code to understand more, but believe me - there is no magic there.In general this is implementation of This is done so we can work on luminance data. It is much easier to do conversion from RGB to luminance because if we stay within RGB we will need to perform motion detection for every channel separately and then average the output, which is generally more complex.The conversion is done by changing RGB data from every pixel to NTSC luminance (aka brightness). The formulation used for this is based on old NTSC spec (from 1953). It's deprecated, but we don't care, because this is not used to display anything on screen and we only need it for calculation. If we would need to display resultant luma on screen, then I would use newer spec.The newest formula used to calculate luma, defined in sRGB spec, is :And the 1953 formula used to convert RGB to luma is:This is done in code  (the variable names in code are different, but please don't be mislead by this).The values 77, 151 and 28 are equivalents of 0.299, 0.587 and 0.114 which are coefficients resulting from CIE color matching functions and standard chromaticities of RGB channels. RGB values used in algorithm are in range from 0 to 255 and luma coefficients are in range of 0 to 1, so:Luma is later converted back to RGB in such a way that resultant pixel is gray. Each channel value (R, G, and B) has the same value which is equal to luma. There is also alpha channel used in formulation, but this does not matter in our case because we use JPG data which has no alpha channel present.This is done by by comparing all pixels with the previous values (from previous image) to calculate how many of these pixels are different. Pixel intensity difference must be greater than a given threshold (configurable value). If this is true, increase number of different pixels by one. At the end calculate percentage value of modified area by using this simple formulation:Where:After we have percentage value of modified area we can then compare this value with a configured value. If percentage area is higher than a given value, then motion detection is fired and motion detector is engaged. If percentage area is lower than a configured value, then there is no motion and motion detector is disengaged.When you take a look into the code:You may see that there are other things happening. For example a center of gravity is calculated, a threshold points are stored, but these have no connection to the motion detection algorithm itself. It's just so you can get these values from the object and visualize on your image (e.g. mark different pixels with a different colours, mark motion centre of gravity, etc).The code may look a little bit crappy so please do not use it as a reference on how one should implement such algorithms. It was implemented mainly to serve a very basic purpose and was much shorter, but later was modified by a community to add no-engage zones, support for pluggable algorithms, etc, and today I would implement it differently.Take care and I wish you a best luck with your Final Year Project! Computer engineering is awesome :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "May 10, 2019",
            "body": "Hi @lilblazekonceptz,One more thought. The default driver is derived from  so it would be very nice if you reference it in your literature:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SoftPathTechnologies",
            "datetime": "May 11, 2019",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "shinobisoft",
            "datetime": "Mar 14, 2020",
            "body": " Sorry to reopen this, but is the method mentioned above also suitable for light detection? I have an old Android phone that I use as a security camera but it doesn't have or support night vision. I've written a DVR for this camera but I'd like to figure a way to detect day light changes to enable or disable recording at certain times of day. If the above method isn't suitable do you have any ideas how to implement what I'm looking to do?Thanks for your time!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "May 12, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/713",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "msgilligan",
            "datetime": "May 3, 2019",
            "body": "I'm having trouble getting camera input on macOS Mojave due to a requirement for .I'm not sure what combination of changes I need to make so that the OS knows to prompt the user for permission. It may require code-signing. If anyone has experience with this, please let me know. If not, I'll share information here when I figure it out.If changes to  are required we should make them. If not, we should at least document what needs to be done to make it work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "msgilligan",
            "datetime": "May 3, 2019",
            "body": "So, it looks like all you have to do to get it working is add the following to your :",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arman-sydikov",
            "datetime": "Jul 28, 2019",
            "body": "\nI have put those lines into , but asking for camera permission dialog does not pop up.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "msgilligan",
            "datetime": "May 3, 2019",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "msgilligan",
            "datetime": "May 7, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/sarxos/webcam-capture/issues/660",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "sachinrke",
            "datetime": "Aug 31, 2018",
            "body": "hello sir,\nfirst of all, i would like to congratulate you for this wonderful project (Webcam-capture API). you have done grate work. now my issue is that i want to zoom in/out, panning and increase/decrease brightness in IP camera using this (Webcam-Capture API). please help me out to achieve these functionality for the camera...\nthank you in advance....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Sep 9, 2018",
            "body": "Hi , this unfortunately is not possible with existing Webcam Capture API. If your IP camera expose some kind of REST API which can be used to zoom/pan/tilt then you can try using e.g.  to communicate with your IP camera.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Sep 9, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "sarxos",
            "datetime": "Sep 9, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/smellslikeml/ActionAI/issues/56",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "shamine5",
            "datetime": "Jul 6, 2022",
            "body": "I would like to do activity recognition on a live cctv camera. In the Read.md section it is mentioned that the deployment file has reference codes to do it, where can I find this file.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4659",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Gloriabhsfer",
            "datetime": "Aug 9, 2022",
            "body": "Hi all:\nI have follow the steps form the manual for UE4 and AirSim for Linux to install and build the project. However, I encounter with question while I am trying to open the blocks Uproject. My system is Ubuntu 20.04 and Unreal_Engine 2.47.2. I try convert the project when I am open the projectAny ideas?Best,\nGloria",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 9, 2022",
            "body": " have you updated your UE version?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Fergus-MW",
            "datetime": "Sep 14, 2022",
            "body": "Hi Guys, I've got the exact same issue. Was a solution found? I've tried reinstalling Airsim and Unreal to no avail.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 12, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4648",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "EvelynM7",
            "datetime": "Jul 29, 2022",
            "body": "Originally, this problem showed up in computer vision mode. I was trying to modify camera parameters in the settings.json, but when that was not working due to cv mode, I found the client.simSetFocalLength function. However, this gave the following error:  I thought this might be an issue with cv mode, so I also tried it with multirotor settings and with simGetFocalLength. Both of these are giving similar argument errors in the debug and terminal as well as in the client.py.I tried looking up solutions to this problem and the only reason I could find was one person saying that it might be due to client and server versions, but I checked that both of mine are up to date. (There seems to be hard coded 1s for the versions in client.py?) Any suggestions or solutions would be greatly appreciated!\n\n\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 12, 2022",
            "body": ", which python client version are you using?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EvelynM7",
            "datetime": "Aug 16, 2022",
            "body": ", I'm using Python 3.10.6 64 bit in Visual Studio Code.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 16, 2022",
            "body": ", what number do you get running this:\nimport airsim\nairsim.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EvelynM7",
            "datetime": "Aug 16, 2022",
            "body": "I'm now getting .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 16, 2022",
            "body": ", sorry it is ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EvelynM7",
            "datetime": "Aug 16, 2022",
            "body": ", It says '1.7.0'.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 16, 2022",
            "body": "Sorry, now I see you are running from source. Are you running the simulation from a binary? This is typically a version mismatch between client and server.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 12, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4614",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "dskuma",
            "datetime": "Jul 8, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jul 11, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4589",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JiancongWang",
            "datetime": "Jun 23, 2022",
            "body": "I am running 5 virtual cameras with fov 90 degree in +-xyz (omit the -y for now since it points to the ground) direction to form a cube map and I want to stich them together to form an epirectangular image. But I found that the exposure of the images are drastically different with each other even when the image are collected by python script simultaneouly (so there should not be any effect of light direction change). This is from the landscapeMountain scene.  I suppose this is due to auto exposure. So I want to ask if there is a way to set manual exposure level here?Here is how I added the virtual camera in the settings.json.\n\"Vehicles\": {\n\"PhysXCar\": {\n\"VehicleType\": \"ComputerVision\",\n\"DefaultVehicleState\": \"\",\n\"AutoCreate\": true,\n\"PawnPath\": \"\",\n\"EnableCollisionPassthrogh\": false,\n\"EnableCollisions\": true,\n\"RC\": {\n\"RemoteControlID\": -1\n},\n\"Cameras\": {\n\"MyCamera0\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": -90\n},\n\"MyCamera1\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0\n},\n\"MyCamera2\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 90\n},\n\"MyCamera3\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 180\n},\n\"MyCamera4\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 90, \"Roll\": 0, \"Yaw\": 0\n}\n}\n}\n}The collection script (modded from fov_change.py so the rest of it is the same):requests = [airsim.ImageRequest(\"MyCamera0\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera1\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera2\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera3\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera4\", airsim.ImageType.Scene)\n]\nclient.simSetCameraFov(\"MyCamera0\", 90)\nclient.simSetCameraFov(\"MyCamera1\", 90)\nclient.simSetCameraFov(\"MyCamera2\", 90)\nclient.simSetCameraFov(\"MyCamera3\", 90)\nclient.simSetCameraFov(\"MyCamera4\", 90)responses = client.simGetImages(requests)\nsave_images(responses, \"new_fov_90\")This is running UR 4.27.2 with the latest airsim from github.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 24, 2022",
            "body": "Hi ! This may be due to missing image postprocessing. Can you try applying the solution given in  thread?\nI like that image. How did you generate it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JiancongWang",
            "datetime": "Jun 24, 2022",
            "body": "I think that post processing is not the problem here. In real camera terms, I want to make sure the camera ISO/aperature/exposure time are all the same for the image so that their exposure is as closed as possible. I think now in airsim the image is auto adjusted to fit a certain target. I want to turn that off and instead manually fixed the picture intensity.\nI tried histogram match all the image to the middle one. It certainly looks better but the seam between stiching is still very bad.The stiching is done by the py360 convert package. So once you have the 5 fov90 views by the 5 cameras in the settings.json, you can treat it as the cube map faces and convert that to equirectangular using this.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 24, 2022",
            "body": "Nice package, thanks! changed the default unreal camera to the cinematographic one. I think what you are searching for is  method added by that PR",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JiancongWang",
            "datetime": "Jun 27, 2022",
            "body": "Thank you for sending me this! I will give it a try.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 24, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4610",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "DonMaEn",
            "datetime": "Jul 7, 2022",
            "body": "When using the orthographic camera, holes appear in the depth map images saved to disk when using the client.startRecording() function. These holes only appear on the ground mesh, not on buildings or trees. I have tested this using the pre-compiled AirSimNH scene and the LandscapeMountains scene. In the below images you can see in yellow the hole, and you can see how the trees are not affected by this bug. The RGB image is from the exact same area as the depthmap image, and you can see there is no visible hole in the ground mesh.Also, see the following screen shot of a point cloud I created from the depth maps and RGB images that shows where the holes appear in the AirSimNH scene:\nN/ABecause I am using centos stream 8 I have not been able to compile Airsim from source, and therefore could not test on the master branch.I tried using the latest version 1.8 pre-compiled binaries, however when running my collection script airsim crashes. I'm guessing this is because I am still using version 1.7 of the airsim python API. 1.8 is not yet available on pypi so I cannot test this theory.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jul 8, 2022",
            "body": "Thanks for the report ! The holes always appear in the same places or this is a stochastic behavior?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DonMaEn",
            "datetime": "Jul 8, 2022",
            "body": ", good question. The holes do not always appear in the same place. The following image is a point cloud I created using the same method, but of a smaller area. You can still see the holes, but they are in different locations.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jul 8, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4573",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JiancongWang",
            "datetime": "Jun 15, 2022",
            "body": "Hi guys,\nI am trying to add custom cameras to a car when using computer vision mode. The settings.json I use is as follows{\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"ComputerVision\",\n\"Vehicles\": {\n\"PhysXCar\": {\n\"VehicleType\": \"PhysXCar\",\n\"DefaultVehicleState\": \"\",\n\"AutoCreate\": true,\n\"PawnPath\": \"\",\n\"EnableCollisionPassthrogh\": false,\n\"EnableCollisions\": true,\n\"RC\": {\n\"RemoteControlID\": -1\n},\n\"Cameras\": {\n\"MyCamera0\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": -90\n},\n\"MyCamera1\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0\n},\n\"MyCamera2\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 90\n},\n\"MyCamera3\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 180\n},\n\"MyCamera4\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 90, \"Roll\": 0, \"Yaw\": 0\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"ImageType\": 0, \"CameraName\": \"MyCamera4\", \"Visible\": true},\n{\"WindowID\": 1, \"ImageType\": 0, \"CameraName\": \"MyCamera1\", \"Visible\": true},\n{\"WindowID\": 2, \"ImageType\": 0, \"CameraName\": \"MyCamera2\", \"Visible\": true}\n],\n\"Recording\": {\n\"RecordOnMove\": false,\n\"RecordInterval\": 0.05,\n\"Cameras\": [\n{ \"CameraName\": \"MyCamera0\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true },\n{ \"CameraName\": \"MyCamera1\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true },\n{ \"CameraName\": \"MyCamera2\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true }\n]\n}\n}This runs fine when I set the sim mode to \"Car\". But when I switch the sim mode  to \"ComputerVision\", it gives me an error of \"there were no compatible vehicles created for current simmode\". So I am writing to ask how to resolve this issue?I am running this on Ubuntu 20.04, unreal4 4.27.2.I tried to add the \"PawnPath\" and add a drone to the vehicle setting and still no luck.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Jun 16, 2022",
            "body": "For Computer vision simmode, the vehicle type must also be ComputerVision",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JiancongWang",
            "datetime": "Jun 16, 2022",
            "body": "Thanks for the tip! In that case can I just replace the vehicle type of \"PhysXCar\" to \"ComputerVision\"?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 20, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4554",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "skarful",
            "datetime": "May 31, 2022",
            "body": "I am simulating a drone with a lidar and a camera sensor. I wish to obtain images every 100ms and hence have set the parameter in the airsim_node launch file as below:\nHowever, I see a lot of variance in the time between receiving two images. In the screenshot below, I have printed the time (in milliseconds) between receiving two images. The range goes from around 21ms to 99ms.Is there a way I can get a fixed frame rate? () I tried changing the  to 0.2 (a slower rate) but the variance in obtaining the image still exists. This time it ranges from around 140 to 190msThe settings.json file is attached below.I tried looking up the code and see that image callbacks are handled using an  in . Could this be an issue? Is there an alternative I can use to maintain a fixed frame rate? Any help will be appreciated, ThanksFrom airsim_ros_wrapper.cpp:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 13, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4543",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "TorsteinOtterlei",
            "datetime": "May 25, 2022",
            "body": "How do you make the snow texture that is shown when pressing F4 appear when taking pictures using AirSim (scene camera) ?Is there any way to programmatically enable this snow texture with code ?When in ComputerVision mode, pressing F4 gives you an overlay on the map that puts snow on all the textures. However, this snow texture does not show in the live camera-view or when taking pictures using AirSim. The issue is shown in the image below (downward facing camera).I have tried all the simSetWeatherParameter options, but these only show a \"filter\" over the camera with showfall etc.Airsim version (pip package): 1.6.0\nOS: Windows 11 Home 21H2 1000.22000.675.0\nPython version: 3.6.13\nUnreal version: 4.27.2settings.json below",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 22, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4525",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "godhj93",
            "datetime": "May 16, 2022",
            "body": "I want to get depth images in realtime. I have tried to solve it but failed. I tried following methods:But the problem is that It autimatically normalize the depth value from 0~100m*, Its value is almost 0 or 255 because It sees sky(depth is infinity) so it becomes useless value.So my question is that Is there any method to change maximum value for normalization like 100m*.\nI'm happy to get any solution to get depth value in realtime.I want to get depth images in realtime.OS: Ubuntu20.04\nPython: 3.9\nUnreal: 4.25.4I described it above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "May 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4522",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "nabil-mokhtar",
            "datetime": "May 13, 2022",
            "body": "I'm using airsim for simulating drone to acheive 3d mapping using RTABMAP algorithm , so using \nwhich create ROS node publishes this topics:my problem is depth topic hz is   = each frame takes  seconds , copmared to rgb topic hz which is ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SabryTarek",
            "datetime": "May 14, 2022",
            "body": "I have the same issue with Depth-Image\nDepth-Image FPS: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "May 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4578",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "lukas-utopiacompression",
            "datetime": "Jun 16, 2022",
            "body": "When multiple vehicles (see settings.json below) specify a camera with the same name (e.g. \"img0\") ROS1's tf frame_ids become ambiguous i.e. only one of the drone has a transformation from itself to  or . However, all drones should have their own distinct transformation from their respective img0 frame to their NED frame.Related PR  fixes the same issue but for ROS2.\nRelated PR  fixes this partially.Screenshot of rqt_tf_tree, when camera frame_ids are ambiguous.\nScreenshot of rqt_tf_tree, when camera frame_ids are distinct (e.g. by adding the vehicle_name to the frame_id):\nn/a",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 20, 2022",
            "body": "Hi ! Thanks for submitting the PR. I'd like to know why you didn't upgrade to ROS2 when you ran into this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lukas-utopiacompression",
            "datetime": "Jun 20, 2022",
            "body": "Good morning ,Happy to help!We haven't transitioned for two reasons:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 20, 2022",
            "body": "Thanks for the reply, . Keep in mind that you can use the Ros1-Ros2 Bridge to connect your project with ROS2 nodes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lukas-utopiacompression",
            "datetime": "Jun 16, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#4579"
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 20, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4576",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "HarrySoteriou",
            "datetime": "Jun 16, 2022",
            "body": "I have searched through all issues both open and closed that highlight low FPS of simGetImages such as , ,  and .How close are you to resolving the msgpack bottleneck and what are the most up to date tricks that can significantly improve FPS?I think I saw something about compiling with specific binaries but I had a hard time compiling AirSim before so I don't want to risk it if it's not worth it. If at least 30fps are not attainable then AirSim is not a viable RL simulator.I am training an RL script that takes as input low resolution RGB images (256x256x3) and then takes action. I started with 1-2FPS and started trying everything I could to increase my FPS because training 1 million timesteps took a whooping 5d7h and I can't run experiments that take so long.I use the following function to make image calls but I have to ensure I am not getting an empty string binary or else training stops (hence the while loop and the .shape==0 check)Specs: Windows 10.0.19044\nUE: 4.27.2\nAirSim: 1.6\nGraphics Card: NVIDIA GeForce GTX 1080 Ti\nRAM: 4GB\nPython: 3.9.10\nmsgpack-python 0.5.6\nmsgpack-rpc-python 0.4.1I have tried the following:Steps 4 and 5 cause instabilities (when evaluating a trained example that used asynchronous steps or increased wallclockspeed, it's behaviour was worse than random. An example trained for 1million time steps without using 4 and 5 converged to the expected behaviour). With everything I tried I am running training with 5-7 FPS.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "misakabribri",
            "datetime": "Jul 27, 2022",
            "body": "Here are testing results on my machine:Devices: Intel i7-9700K+NVIDIA GeForce 3070\nImage size: 1270*720\nAverage rendering time: 40msI also need an approach to increase the FPS to 30.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mgrova",
            "datetime": "Sep 20, 2022",
            "body": "Hi ,\nI am also trying to get a valid FPS for visual odometry tests (30 fps would be perfect), but I can't get more than 15 fps approx.Have you made any progress in this time?Best regards and thanks in advance!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "misakabribri",
            "datetime": "Sep 21, 2022",
            "body": "Hi \nMaybe you can package your UE4 project as a released app and then try again. By my observation, if the UE4 project's window is not active, it will also cause low fps. Also, I use multi threads in C++ code to reduce total time of the image capture loop.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mgrova",
            "datetime": "Sep 21, 2022",
            "body": "Hello !I'm working with the binary of my own enviroment with \"NoDisplay\" as \"ViewMode\" in settings.json file. In addition, I use the C++ wrapper and ROS to grab the images. I'm going to try to generate the binaries in release mode, thanks!Using multithreathing have you achieved 30 fps when grabbing the images in C++ in real-time?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "misakabribri",
            "datetime": "Sep 21, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "mgrova",
            "datetime": "Sep 22, 2022",
            "body": "Im taking the images (one depth and one scene) at 640x480. Simulating only these two cameras and using the ROS1 wrapper, I don't get more than 8-10 FPS in my workstarion with RTX 2080Ti.Do you have any repository or code with which you can get that frequency when obtaining the images?Sorry for the inconvenience and thank you very much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "misakabribri",
            "datetime": "Sep 23, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 22, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4494",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JuanshuB2",
            "datetime": "Apr 23, 2022",
            "body": "Is it possible to get the LiDAR debug points as an 2D image directly for AirSim? Like the one above but only the green points. whitout the objects?I want the 2D image from the LiDAR pointcloud not to do it manually.The image could be get from the 3D point cloud transformation to 2D, but if AirSim already give us that projection, it can be useful to be able to delete the background in the image. I have figured out about subtracting an image with the green points and another without them, but as the points are visible in the world, I have no access to a image as the second one.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "May 2, 2022",
            "body": " that is not currently possible. Why do you need the 2D image of the points reached by the lidar?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JuanshuB2",
            "datetime": "May 4, 2022",
            "body": "Thanks for your reply . I need the 2D image because we want to do the overlapping camera-lidar. Now we are transforming the 3D pointcloud to 2D image but the overlap sometimes is not easy, so a ground truth like that would be great. However, I guess we can work with that image, even if we can not have both. Maybe some kind of image processing can be done to delete the green points.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "May 2, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4488",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "BBBBBBob",
            "datetime": "Apr 19, 2022",
            "body": "In the OpticalFlow setting, the optical flow cannot be returned when the camera moves in a certain direction.  Here is a video example :\n\nThe problem could be that the velocity is mapped between the -1 and 1, but the Unreal Engine Material only returns the black color when the velocity is non-positive, even though  is enabled.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Apr 29, 2022",
            "body": "  optical flow is not intended for visualization. Please try with OpticalFlowViz (which corresponds to ImageType 9 instead of 8 )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BBBBBBob",
            "datetime": "Apr 30, 2022",
            "body": "  Hi, thanks for your reply! Indeed, I would like to retrieve the optical flow velocity from the image, so what is the right way to get the velocity information?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "COATZ",
            "datetime": "Jul 5, 2022",
            "body": "Hi , were you able to find a solution to get the optical flow from AirSim?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Apr 29, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4473",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "StardustLu",
            "datetime": "Apr 9, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "OPyshkin",
            "datetime": "Apr 15, 2022",
            "body": "Have you rectified the input images?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "StardustLu",
            "datetime": "Apr 21, 2022",
            "body": "Thanks for your reply :)\nI did nothing with the RGBD image\nI just edit  and record them, then I calculate the correspondence by RGBD image and extrinsic parameters in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Apr 12, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4435",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "SaundersJE97",
            "datetime": "Mar 25, 2022",
            "body": "I am using a reinforcement learning algorithm for motion planning and frequently when the drone collides with an object the drone teleports to the wrong start location.  I have noticed this happens more often when the drone is closer to the obstacle.  The behaviour has not been consistent, sometimes the drone will not take any movement commands, alternatively the drone will gain a huge speed boost and will consequently be thrusted in the forward direction.Please see the following YouTube video for what I have been experiencing: I am using the following client command to fly the drone through the environment.  Where action is a variable in the range of [-1, 1].The teleportation code is as following.(Please see code above)(No errors experienced from python on UE)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 28, 2022",
            "body": " This is a known limitation for doing RL quickly with our flight controller. You are teleporting the drone but the state of the controller remains the same. We would need to have an API method for saving the state of the controller and another one for loading it.\nAnother way that was asked is to increase the clock frequency to go fast again to that location.\nYou can spawn a new drone on that point and set its kinematic state, but you are not going to have the same controller state that when it collided.\nFor now the only solution is to re run the simulation till that moment.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaundersJE97",
            "datetime": "Mar 28, 2022",
            "body": "Thank you for your reply, this clears it up.  Ideally I would benefit from a vehicle specific 'reset(vehicle_name)' api call.  Is there a function within the SimpleFlight Controller that would reset the controller state?\nFor example would resetting the firmware under 'SimpleFlightApi.hpp' would provide this?Thanks,\nJack",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 29, 2022",
            "body": "@SaundersJE9, In that case, you can call  before \nI think you would not need to cancel the last task.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaundersJE97",
            "datetime": "Mar 30, 2022",
            "body": " That works for a single drone, however I was considering doing multi-agent reinforcement learning and, although still useful, using  resets all drones within the environment.  I understand it is currently a feature request to add , I would be happy to investigate how to add this to the api since I will need it in my project.Taking a look at AirLib and more specifically RpcLibServerBase.cpp, the command either  or 'getVehicleApi(\"\")->reset()` is called.  I tried modifying this to add a specific vehicle reset function within the api:From my understanding this only resets the 'simulation' drone and not the AirLib instance of the drone.  I am missing the controller reset function.  I had some issues with this and had to run it on the game thread.  Unfortunately, this still didn't fix the issue I outlined in the beginning of this thread.Looking into  the object VehicleApiBase inherits from UpdatableObject which when reset is called will revert back to it's initalized state.  I added  to the reset(vehicle_name) api call but I still get the rebounding effect when hitting.  My current thought is I need to reset the controller.  However, within the SimpleFlightApi.hpp the firmware is reset when calling .So if the controller is also being reset, I am not sure what could be causing it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 30, 2022",
            "body": " I would need to dig into this. Another idea is to use  but we don't have a  so I don't know how your RAM usage will end. Can you give it a try?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaundersJE97",
            "datetime": "Mar 31, 2022",
            "body": "I hadn't considered that function, I'll give it a try and report back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 28, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4428",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "mgodmere",
            "datetime": "Mar 22, 2022",
            "body": "The Airsim ROS2 package produces jerky drone movement every time an image is captured while using any movement command such as the  subscriber.  I verified this by slowing the capture rate by setting the roslaunch  to 1Hz.  When flying with a constant velocity the drone stops moving for a split second at 1Hz.  When using the default rate of 20Hz the drone struggles to move at all since its movement is interrupted by frequent image captures.  This jerky movement gets worse the more cameras or higher resolution images are used.  I'm guessing this is due to a threading or client issue with the package although the settings make it seem like they run in separate callback threads.  The standard airsim client obviously doesn't produce this behavior.  I will also mention that the ROS2 package runs just fine with image capture in realtime when using an RC controller.This is a show stopper for using the new ROS2 interface and I want to avoid rewriting/forking it.  I was hoping to run several vehicles with multiple cameras, but this seems like a currently unscalable proposition.  Am I using it incorrectly?  Can someone verify that this is a current issue?  It seem strange to me that this is a bug/issue when moving and capturing is one of the core use cases of airsim.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nikola-j",
            "datetime": "Jun 6, 2022",
            "body": "I have the same issue, did you maybe figure out a way to fix this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hoangvietdo",
            "datetime": "Jun 24, 2022",
            "body": " I did a quick test with your PR, even when the vehicle stand still, similar behavior of this issue still existed (It seems like the camera processing also affects the timestamp of other topics). You can refer to ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nikola-j",
            "datetime": "Jul 14, 2022",
            "body": "Hi , do you mean similar behavior to the current issue (jerky movement) or to your issue (timestamps)? Was there a difference in timestamps without my changes? Did you build ros2 airsim in Debug or Release?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hoangvietdo",
            "datetime": "Jul 14, 2022",
            "body": "Hi , what I mean is that both of these two issues share the same problem which involves running a vehicle and capturing images. This setup causes both time-delay in the timestamp as well as the problem in this issue.I actually don't know about Debug or Release, I just simply copy and paste your , build, run the simulation and check the timestamp.The time difference when using a stereo camera has been detected and discussed for a long time in ros1, still, no PR has been proposed to solve it. I have not checked the timestamp before your PR in ros2.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nikola-j",
            "datetime": "Jul 20, 2022",
            "body": "Yes, but this PR isn't meant to fix the issue with the timestamps being out of sync, but to make the ros2 implementation usable. Without this fix, the drone won't move properly, and the timestamps would be even more out of sync.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 23, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "nikola-j",
            "datetime": "Jun 7, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#4559"
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4414",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "COATZ",
            "datetime": "Mar 15, 2022",
            "body": "Hi,Thanks a lot for adding the OpticalFlow feature to AirSim but I am having some issue getting it.What are the correct options of the  to get the 2-channels ground truth  from my front drone camera (not the OpticalFlowVis) using the Python API.My camera parameters are width = 100, height = 100 and I am currently using:\n.\nBut the output is a single channel image instead of the two expected:\n  (and not 20000 as expected)I tried with the option \"pixel_as_float=False\" and got a 3-channels output of . But the raw ground truth optical flow should be float so I am guessing that is not the right option...Thank you!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zimmy87",
            "datetime": "Mar 22, 2022",
            "body": "Since simGetImages just returns the buffer of a SceneCaptureComponent2D instance, I would not expect to get output with multiple entries for each channel. Looking at how OpticalFlow's post process material is defined , the 2 channels (vx & vy) are mapped to the emissive color of the material, meaning that these channels will be encoded in the pixel color of the output of simGetImages. I don't know exactly how Unreal's material editor maps a float2 to a color value, so you may need to play around with the output, but it looks like you need to parse each pixel's color for the values of both channels. If you find out how this conversion works, please post that in this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BBBBBBob",
            "datetime": "Apr 1, 2022",
            "body": "Hi, I have met the same problem, have you found a solution to fix it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BBBBBBob",
            "datetime": "Apr 11, 2022",
            "body": "In the material,  x-velocity is encoded in the red channel and y-velocity in the green channel. If we observe the optical flow image, there are only red and green colors in the OpticalFlow setting. However, Unreal Engine only returns the black color to the material when the speed is non-positive. It is correct that the optical flow image is dark when the vehicle is at a standstill, but it does not return any optical flow when the camera moves to the right side or upwards.  Here is an example: \nThis problem only exists in the OpticalFlow setting.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wangpinzhi",
            "datetime": "Aug 30, 2022",
            "body": "Hello, have you found a solution? I met the same problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Mar 22, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4412",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "zimmy87",
            "datetime": "Mar 15, 2022",
            "body": "Currently, UDetectionComponent::calcBoundingFromViewInfo() calculates the location and size of the 2D box around detected objects using a 2D projection of the output of . As can be seen in , due to the size and shape of colliders as well as the possibility of an actor containing invisible components, AActor::GetActorBounds may or may not reflect the bounds of the visible meshes attached to an actor. Since AActor::GetActorBounds also returns a rectangular box, it's possible a 2D projection extends beyond the visible meshes depending upon how the target actor is rotated relative to the camera. One possible solution to this would be to iterate through an actor's components and use the output of  for each visible component as the basis for the 2D box. However, this solution may result in a performance impact due to the additional iteration over each detected actor's components. More research should be done to determine if this is feasible or if there is a better solution.This would address concerns raised in  that the detection APIs do not accurately represent the bounds of detected objects on a 2D display.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bozcani",
            "datetime": "Jun 9, 2022",
            "body": "Is there any progress on this issue? I am not a expert of Unreal programming expert but what do you think about using segmentation information inside the Airsim to refine object bounding boxes?Currently, we can record segmentation images and refine bounding boxes in an offline manner. Can we make this refinement inside Airsim as a built-in feature?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dellhuyongcai",
            "datetime": "Jul 19, 2022",
            "body": "Is there any progress on this issue?I just know the box is axis aligned bounding box,has anyone else tried the original bounding box ? it seems that ue4 does not have original bounding box function.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zimmy87",
            "datetime": "Mar 15, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#4367"
        },
        {
            "user_name": "zimmy87",
            "datetime": "Mar 15, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4353",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "AmitNativ1984",
            "datetime": "Feb 15, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Feb 24, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4345",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "pdx97",
            "datetime": "Feb 8, 2022",
            "body": "Hi i am unable to view my subwindow in the Unreal engine for my drone even though the camera name in the code and the setting.json are same, i have read various issues and even followed them but nothing is working out .\n{\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true, \"External\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\":true}\n]}",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Feb 10, 2022",
            "body": "You have External set for the first subwindow but it's not an external camera. Try removing this param",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pdx97",
            "datetime": "Feb 10, 2022",
            "body": " i tried that too still the subwindow doesn't appear.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pdx97",
            "datetime": "Feb 10, 2022",
            "body": "{\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\": true}\n]}even with these settings it doesn't appear",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Feb 12, 2022",
            "body": "Wonder why AirSim is even starting, you don't have  set in the settings, add that and it should work",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pdx97",
            "datetime": "Feb 12, 2022",
            "body": "No even after adding SimMode it doesn't work . {\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"Multirotor\",\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\":true}\n]}",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Feb 13, 2022",
            "body": "The above settings are working fine for me, using the 1.7.0 release Blocks binary on Linux. Details of which platform, version, etc haven't been added in the issue description, please do so. A more minimal settings which should work as well -Also make sure that the correct settings.json is getting picked up, try removing everything and just having SettingsVersion, and Car simmode",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Feb 23, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4344",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "HarrySoteriou",
            "datetime": "Feb 8, 2022",
            "body": "I was tweaking the settings file, trying to get the front rotors out of the simGetImage() field of view (when using a custom front camera the two front rotors would be part of the captured image no matter what I changed, i.e. pitch: -90.0, x : 3, z: 5--> NOTHING CHANGED WHAT MY simGetImage was returning )...I tried using external camera= True, in my script without modifying the settings file... this has caused a crash and even when reverting the change AirSim will not load.... After trying to fix the issue without success I have tried building everything from scratch again on 2 separate computers using the same version UE=4.27. I have a Block solution (Block.sln), that matches the UE4 version, AirSim Plugins are placed in the project but I don't get prompted to use AirSim as soon as I load the environment.Steps:Expected behaviour:\nLoad in the environment and be able to Play AirSimObserved behaviour:\nLoad in the environment and although the plugin is listed I do not have the option to run AirSim",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HarrySoteriou",
            "datetime": "Feb 8, 2022",
            "body": "Manage to overcome this issue by ensuring that I was using the right version of VS when building the Blocks environment and returning my settings.json to the original one (On that computer I have 2 versions of VS: 2019 and 2022.. )Then I ran the simulation using my configured settings.json/ not the default one and got the same error as Edit: Resolved this issue by return settings.json to it's default value",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HarrySoteriou",
            "datetime": "Feb 9, 2022",
            "body": "For the love of god make a clear tutorial on how to build the blocks environment and common issues that are faced, this is the 3rd day of me trying to return to the point I was before editing my settings file.\nThe documentation at different point says to do different things:I dont have the option to PLAY AIRSIM, when I enter my blocks environment. I have ran the  DebugGame_Editor X64 and then build using \"Develop Editor\" and  x64",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HarrySoteriou",
            "datetime": "Feb 9, 2022",
            "body": "followed this tutorial:  and solved my issues, if airsim crashes soon after running a script containing simGetImages ensure that the settings.json file doesnt have any invalid settings",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Feb 22, 2022",
            "body": "Hi !\nThanks for keeping the issue up to date.1_ The developer command prompt is only necessary to run build.cmd\n2_ With binaries, it is not required to clone the repo\n3_ update_from_git.bat or GenerateProjectFiles.bat are not called in the case of binaries\n4_ You already solved it.Sorry about the state of the documentation. You can submit a pull request to improve it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Feb 22, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4322",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "MartinRossel-67",
            "datetime": "Jan 26, 2022",
            "body": "Hey, I want to use AirSim to record my one simulation.\nI need to run a \"normal\" et a \"segmentation\" record as the same time frame a camera I choose.\nI would like to call the record from Blueprint.\nI prefer not to use AirSim game mode, I got my own.The thing is: I don't know C++ and yet I didn't figure it out where I can run a \"segmentation\" record.\nIf you have any hint or any tutorial about what a would like to do, I would appreciate cause haven't figured it out on my own yet.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Feb 3, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4312",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "mmuetert",
            "datetime": "Jan 24, 2022",
            "body": "Hello,I am trying to run the segmentation.py  example from the PythonClient folder in airsim. When I try to run the script, it runs fine until the line#get segmentation image in various formats\nresponses = client.simGetImages([\nairsim.ImageRequest(\"0\", airsim.ImageType.Segmentation, True), #depth in perspective projection\nairsim.ImageRequest(\"0\", airsim.ImageType.Segmentation, False, False)])  #scene vision image in uncompressed RGBA array\nprint('Retrieved images: %d', len(responses))where I get the error\"msgpackrpc.error.RPCError: rpclib: function 'simGetImages' (called with 3 arg(s)) threw an exception. The exception is not derived from std::exception. No further information available.\"I had the same error in a custom script I was writing so I decided to test the issue with the provided python script. I test the c++ version of HelloDrone and it runs simGetImages just fine.I am running python3.6 on Ubuntu 20.04. My python packages are up to date and my Airsim install should be as well. I very recently upgraded from Ubuntu 18.04 to Ubuntu 20.04, but I never ran the python versions on 18.04, so I can't say if they worked before the upgrade.Any help would be appreciated!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sudo-hello-world-2",
            "datetime": "Jan 25, 2022",
            "body": "Hi .\nThe Code above works fine fine for me. Are you sure \"0\" is your camera name?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zimmy87",
            "datetime": "Jan 31, 2022",
            "body": "This might be related to , can you apply the fix in  and see if that fixes the issue for you?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 31, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4210",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "EatingEdu",
            "datetime": "Dec 10, 2021",
            "body": "{\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"Multirotor\",\n\"PawnPaths\":{\n\"DefaultQuadrotor\": {\"PawnBP\": \"Class'/AirSim/DJI450/BP_FlyingPawn.BP_FlyingPawn_C'\"}\n},\n\"Vehicles\": {\n\"PX4\": {\n\"VehicleType\": \"PX4Multirotor\",\n\"QgcHostIp\": \"127.0.0.1\",\n\"QgcPort\": 14550,\n\"LocalHostIp\": \"192.168.43.64\",\n\"DefaultSensors\": {\n\"Barometer\": {\n\"SensorType\": 1,\n\"Enabled\" : true,\n\"PressureFactorSigma\": 0.001825,\n\"PressureFactorTau\": 3600,\n\"UncorrelatedNoiseSigma\": 2.7,\n\"UpdateLatency\": 0,\n\"UpdateFrequency\": 50,\n\"StartupDelay\": 0},\n\"#SubWindows\": [\n{\"WindowID\": 1, \"ImageType\": 0, \"CameraName\": \"l\", \"Visible\": true}\n]\n}I want test my control algorithm through the px4 and airsim ;\nwhen i use api moveByMotorPWMsAsync() send the pwm to  airsim,  i find the airsim env has not response.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EatingEdu",
            "datetime": "Dec 11, 2021",
            "body": "in the log file ,it is a record like this:\n[2021.12.11-07.33.12:331][354]LogTemp: Not Implemented: commandMotorPWMs",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 13, 2021",
            "body": " moveByMotorPWMsAsync() is a low-level API. I don't see why you would use it with PX4. What are you trying to do?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EatingEdu",
            "datetime": "Dec 14, 2021",
            "body": " I trained a model by RL, the print of the net is the pwm number, so I should use this API to run. I used it in software , but when i change the settings.json for px4 ,this api does not work. I do not know what is wrong.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 14, 2021",
            "body": "If you select PX4 then the pwm signals come from the controller... Setting the pwm signals only works with SimpleFlight as \"flight controller\".\nWhy do you need PX4?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EatingEdu",
            "datetime": "Dec 14, 2021",
            "body": "I need PX4 to get the mavros message to do the HIL.  So I think px4 and pwm api can not use at the same time in the airsim , is it true?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 14, 2021",
            "body": "That is correct. You would need to implement  for the mavlinkmultirotorsimapi class",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 13, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5626",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Dhruva678",
            "datetime": "Jul 28, 2022",
            "body": "import glob\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport cv2try:\nsys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\nsys.version_info.major,\nsys.version_info.minor,\n'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\nexcept IndexError:\npassimport carlaIM_WIDTH = 640\nIM_HEIGHT = 480def image(image):\nmatrix_representational_data = np.array(image.raw_data)\nreshape_of_image = matrix_representational_data.reshape((IM_HEIGHT, IM_WIDTH, 4))\nlive_feed_from_camera = reshape_of_image[:, :, :3]\nimage = cv2.cvtColor(live_feed_from_camera, cv2.COLOR_BGR2GRAY)\ncv2.imshow(\"\", image)\ncv2.waitKey(1)\nreturndef camera(get_blueprint_of_world):\ncamera_sensor = get_blueprint_of_world.find('sensor.camera.depth')\ncamera_sensor.set_attribute('image_size_x', f'{IM_WIDTH}')\ncamera_sensor.set_attribute('image_size_y', f'{IM_HEIGHT}')\ncamera_sensor.set_attribute('fov', '70')\nreturn camera_sensoractor_list = []\ntry:finally:\nprint('destroying actors')\nfor actor in actor_list:\nactor.destroy()\nprint('done.')",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Dhruva678",
            "datetime": "Jul 28, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5591",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yeapzexin",
            "datetime": "Jul 16, 2022",
            "body": "Hi there,I am currently using CARLA 0.9.12 to do the Autonomous Driving Software Project. I am trying to find the default maximum acceleration and deceleration of the spawn vehicle I am controlling. The config.json file that I used to spawn vehicle with via this command (roslaunch carla_ros_bridge carla_ros_bridge_with_example_ego_vehicle.launch) is shown as below{\n\"objects\":\n[\n{\n\"type\": \"sensor.pseudo.traffic_lights\",\n\"id\": \"traffic_lights\"\n},\n{\n\"type\": \"sensor.pseudo.objects\",\n\"id\": \"objects\"\n},\n{\n\"type\": \"sensor.pseudo.actor_list\",\n\"id\": \"actor_list\"\n},\n{\n\"type\": \"sensor.pseudo.markers\",\n\"id\": \"markers\"\n},\n{\n\"type\": \"sensor.pseudo.opendrive_map\",\n\"id\": \"map\"\n},\n{\n\"type\": \"vehicle.tesla.model3\",\n\"id\": \"ego_vehicle\",\n\"spawn_point\": {\"x\": -77.9, \"y\": -17.59, \"z\": 0.2, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 90.0},\n\"sensors\":\n[\n{\n\"type\": \"sensor.lidar.ray_cast\",\n\"id\": \"vlp16_1\",\n\"spawn_point\": {\"x\": -0.5, \"y\": 0.0, \"z\": 1.7, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"range\": 100,\n\"channels\": 16,\n\"points_per_second\": 300000,\n\"upper_fov\": 15,\n\"lower_fov\": -15,\n\"rotation_frequency\": 20,\n\"noise_stddev\": 0.0\n},\n{\n\"type\": \"sensor.camera.depth\",\n\"id\": \"depth_middle\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"fov\": 90.0,\n\"image_size_x\": 400,\n\"image_size_y\": 70\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_front\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_view\",\n\"spawn_point\": {\"x\": -4.5, \"y\": 0.0, \"z\": 2.8, \"roll\": 0.0, \"pitch\": 20.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0,\n\"attached_objects\":\n[\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_top\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 6.0, \"roll\": 0.0, \"pitch\": 90.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0,\n\"attached_objects\":\n[\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n},\n{\n\"type\": \"sensor.other.imu\",\n\"id\": \"imu\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"noise_accel_stddev_x\": 0.0, \"noise_accel_stddev_y\": 0.0, \"noise_accel_stddev_z\": 0.0,\n\"noise_gyro_stddev_x\": 0.0, \"noise_gyro_stddev_y\": 0.0, \"noise_gyro_stddev_z\": 0.0,\n\"noise_gyro_bias_x\": 0.0, \"noise_gyro_bias_y\": 0.0, \"noise_gyro_bias_z\": 0.0\n},\n{\n\"type\": \"sensor.other.gnss\",\n\"id\": \"gnss\",\n\"spawn_point\": {\"x\": 1.0, \"y\": 0.0, \"z\": 2.0},\n\"noise_alt_stddev\": 0.0, \"noise_lat_stddev\": 0.0, \"noise_lon_stddev\": 0.0,\n\"noise_alt_bias\": 0.0, \"noise_lat_bias\": 0.0, \"noise_lon_bias\": 0.0\n},\n{\n\"type\": \"sensor.other.collision\",\n\"id\": \"collision\",\n\"spawn_point\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0}\n},\n{\n\"type\": \"sensor.other.lane_invasion\",\n\"id\": \"lane_invasion\",\n\"spawn_point\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0}\n},\n{\n\"type\": \"sensor.pseudo.tf\",\n\"id\": \"tf\"\n},\n{\n\"type\": \"sensor.pseudo.objects\",\n\"id\": \"objects\"\n},\n{\n\"type\": \"sensor.pseudo.odom\",\n\"id\": \"odometry\"\n},\n{\n\"type\": \"sensor.pseudo.speedometer\",\n\"id\": \"speedometer\"\n},\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n}\n]\n}I do know that in this link: \nconsists of max acceleration and deceleration adjustment via Carla_Ackermann_Control.EgoVehicleControlInfo.msg . However, I am not using Carla_Ackermann_Control at all so what is the way to know the default value of max acceleration and deceleration of the vehicle that is spawn with config.json file as above?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5560",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "wujingda",
            "datetime": "Jul 4, 2022",
            "body": "Hi everyone,Recently we use the Roadrunner to generate a new map and import it into CARLA. The map works well with cameras but becomes weird when facing the lidar. Specifically, I use the open3d to visualize the lidar data, and the lidar data in this new map does not show the line (ray cast) of the ground. The lidar data in our new map (figure 1) and in an official map (figure 2) are provided for comparison. BTW, the lidar setting is invariant across maps.\nDoes anyone have ideas about the issue? Is it because of the road texture of the map or something... we are stuck by this issue for days. Many thanks.CARLA version: 0.9.13\nPlatform/OS: Win10\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5547",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "omega1497",
            "datetime": "Jun 29, 2022",
            "body": "'''>>> Creating camera {'x': -0.2, 'y': -0.55, 'z': 1.65, 'roll': 0, 'pitch': -10, 'yaw': 270, 'width': 1024, 'height': 768, 'fov': 90, 'sensor_label': 'camera8', 'sensor_type': 'camera'}",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5536",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yanmiao2",
            "datetime": "Jun 23, 2022",
            "body": "Hi Developers,We are on Ubuntu 20.04 and using Carla 0.9.13. We are trying to understand how Depth Camera gives out the depth info in Carla.Here's what we did:However, we observed that the depth calculated is always around 9.3m. Although it's not far from the Ground Truth Distance, it makes us curious about how Depth Camera in Carla is calculating Distance(the only way we can think of is through ground truth, but then why would there be an error?). We check this  but doesn't quite understand.Hope someone can explain this for us, thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yanmiao2",
            "datetime": "Jun 23, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5520",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "xiao6768",
            "datetime": "Jun 16, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xiao6768",
            "datetime": "Jun 20, 2022",
            "body": "Good news! This issue has been resolved . We are planning to merge into main branch, need approve from company.\nAny one please feel free to contact me if you have any questions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shr1997",
            "datetime": "Jul 1, 2022",
            "body": "I've been looking for it for a long time. This is exactly what I need. Is it convenient to share the checkboard map or code? Thank you very much",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AbanobSoliman",
            "datetime": "Jul 1, 2022",
            "body": "Hello everyone,\nAs part of our recent , we needed to have exact (calibrated) values for all CARLA cameras' intrinsic parameters (RGB, Depth, DVS). Accordingly, we added calibration targets (Checkerboard and AprilGrid) to the Town3 map, and calibrated RGB, and DVS cameras using SOTA algorithms. Check out the repo here: .\nThanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shr1997",
            "datetime": "Jul 4, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "xiao6768",
            "datetime": "Jul 4, 2022",
            "body": "Firstly, My compile Env. is ubuntu18.04, carla 0.9.10, and UE4 is 4.24.3.\nYou have to following get UE4 source code compile OK, then compile Carla source code.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5502",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "SoMuchSerenity",
            "datetime": "Jun 10, 2022",
            "body": "CARLA version: 0.9.12, package installation\nPlatform/OS: Windows 11\nProblem you have experienced: No rendering option still outputs camera images\nWhat you expected to happen: No rendering option should return no camera resultsSteps to reproduce:\nI attached a RGB camera to my car and I have triedand also the scripts  to disable the rendering mode, yet still got image output in my designated directory. Just wonder did I do something wrong or is it a possible bug?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "brscholz",
            "datetime": "Jun 28, 2022",
            "body": "The no rendering mode suppresses the generation of the spectator render target. Spawning a camera creates an extra render target for that camera, that's why you still get output images. But why would you add a camera in no rendering mode?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SoMuchSerenity",
            "datetime": "Jul 1, 2022",
            "body": ". Hi Scholz, the reason I added a camera in no rendering mode is that according to this tutorial:  , any camera or GPU sensor should return empty data. It also says UE engine would skip everything regarding graphics, so I was experimenting with it.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5483",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "k-nayak",
            "datetime": "Jun 1, 2022",
            "body": "Hello all,I have implemented a lane detection model to detect lanes in carla and it provides a list of xy pixel coodinates. I want to check if it is possible to get the XY map coordinates (in my case town 07) to generate the trajectory along the way for the car to drive. And if it is possible what kind of things are supposed to be taken care of.\nThe green lane is the lane i am working on and if any one can help me out as to how to get the map coordinates of the points, it would be a great help. Thank you in advance for your time and advice.Carla version: 0.9.11\nOS: Windows\nCode in Python",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "k-nayak",
            "datetime": "Jun 2, 2022",
            "body": "Hello ,Could you please give me some feedback on if Is it possible to use the following functions to achieve this?carla.Location and carla.GeoLocationIf I have the location of the car and the points in meter wrt to the camera?Thanks in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "k-nayak",
            "datetime": "Jul 5, 2022",
            "body": "Hello all, i have figured some aspects of my problem so far. Instead of the above mentioned functions i have used rgb_camera.get_transform.get_matrix to get the pose of the camera wrt to world. The camera position and the lane points close to the vehicle have good coordinate values that match the vehicles trajectory location, although as the lane points go farther the coordinate values move away from the trajectory. I am also not sure if the vehicle location z=0.027961 , camera position at a height of 1.5 from the road surface has z= 1.527961, But the lane points after transformation has z values ranging from  This is somewhat unexpected, could some one explain or give any comment on this? Would mean a lot if someone can clear the doubt.Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "k-nayak",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5455",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "knelk",
            "datetime": "May 19, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhidalgocarrio",
            "datetime": "May 19, 2022",
            "body": "Yes, I also encountered the same effects. Significantly, the aliasing effect at a low image resolution.\nHere is a video in Town10: \nI thought this issue was solved in the newest releases. Any idea ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "knelk",
            "datetime": "May 19, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5417",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "tecena",
            "datetime": "May 6, 2022",
            "body": "Hello,Carla supports a number of sensors Camera,Radar,Lidar etc. Each sensor has a number of attributes as mentioned in the link  but only the default values of attributes are mentioned in the doc, but we want to know the range of the attributes of all sensors, Where are the value ranges of attribute mentioned, From where should we get it.Thank you",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5367",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "zxiaomzxm",
            "datetime": "Apr 15, 2022",
            "body": "CARLA version: 0.19.13\nPlatform/OS: ubuntu 20.04\nProblem you have experienced:\nI use instance segmentation camera in , but capture strange results:\nrgb image\n\ninstance image\n\nIt seems that the instance camera can not see the mound in the right side and ray casting just pass through it.\nOther maps has no such phenomenon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "serwansj",
            "datetime": "May 6, 2022",
            "body": "i am experiencing the same issue right now. did you manage to fix this somehow ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "May 24, 2022",
            "body": "Hi , You wrote \"CARLA version: 0.19.3\", do you mean the \" CARLA version 0.9.13\", right?. I will take a look. Thank you for letting us know",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zxiaomzxm",
            "datetime": "Jun 1, 2022",
            "body": "yep, it's 0.9.13. And I found town04 have similar problem.\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ghh116553",
            "datetime": "Jun 22, 2022",
            "body": "I meet the same issue. InstanceCamera can not show the landscape correctly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "May 24, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "May 24, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5362",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Chup123",
            "datetime": "Apr 13, 2022",
            "body": "CARLA version: 0.9.12\nPlatform/OS: ubuntu 18.04\nProblem you have experienced: When I move the mouse in any direction, the camera will always do the same thing. It will look at the ground and spin to the right. Even when I try to move the mouse somewhere else.\nWhat you expected to happen: When i point the mouse up, I except the camera to move to the position I put my mouse, just like in a video game\nSteps to reproduce: Move the mouse in any map. It will always move in this direction\nOther information (documentation you consulted, workarounds you tried): I tried to lower to mouse sensitivity in the DefaultInput.ini file. This made it much more clear that the mouse always moves in this direction",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5321",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "makaveli10",
            "datetime": "Apr 4, 2022",
            "body": "Hello,\nI am trying to generate 3D bounding box from ,  and . Turns out somethings is missing from what I have put together as the boxes I get are disoriented.\nThis is what I am using right now with the .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "makaveli10",
            "datetime": "Apr 11, 2022",
            "body": " You seem to have solved this on . So, if you could throw some pointers on how to make progress here, that would be very helpful.\nThanks",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5254",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "joako1991",
            "datetime": "Mar 14, 2022",
            "body": "CARLA version: latest\nPlatform/OS: LinuxHi everybody,I would like to know if there is any efforts in course or consideration to include the polarization state of the light as part of the simulation engine. I work with DoFP RGB cameras, and I would like to test the simulator with this modality.If there is not, could you kindly guide me where should I look at in order to implement it myself ?Thank you very much in advance!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5227",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JiulongGao",
            "datetime": "Mar 3, 2022",
            "body": "In the Carla simulator, when the manual control mode is turned on and the camera picture is saved, the screen will be stuck and the frame drop will occur.What should be done?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 2, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 2, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5223",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ll7",
            "datetime": "Mar 1, 2022",
            "body": "Hello,I wondered why the  uses two different ways to destroy an actor:Is this redundant because we appended the camera to the ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 30, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 30, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5193",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "pespen",
            "datetime": "Feb 22, 2022",
            "body": "I am trying to use the Carla lidar sensor to create 2D images from 3D point cloud data(similarly to how Ouster does it), and have so far been successful with the range and intensity images. Ouster also creates what they call Ambient images, where they somehow use the reflectivity that they get from their sensor to create a more life-like image. Is it possible to get this information from Carla? I basically need to know the number of photons over time the sensor registers. See the image for reference",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GruNyv",
            "datetime": "Mar 3, 2022",
            "body": "Hi, sounds like we are working on pretty similar problems. However, note that the ambient images are not the same as the reflectivity image. The ambient image are created based on the photons coming from other sources than the lidar (the sun), while the reflectivity images are created by using the inverse square law for the intensity. Btw, could I ask you if you have got any realistic results for the intensity image in Carla? From my experience the simulated values are pretty unrealistic for the intensity image you get from the Ouster sensor.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pespen",
            "datetime": "Mar 3, 2022",
            "body": "Alright, that makes sense about the reflectivity and ambient images. My images are not as good as the ones from Ouster, I'm not sure if it's because they do more processing or if they just have more data available. Here are some examples of range and intensity.\n\nThis is part of my Graduate degree, and my supervisor said that these images are good enough for our purpose. I might try to adjust the values closest to the camera as it is a bit hard to spot objects there",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GruNyv",
            "datetime": "Mar 3, 2022",
            "body": "Ouster does not project the point cloud from 3D space to 2D as I assume you do here to create the 2D image. Instead they use the values that each pixel in the sensor itself measures, since these are organized into an array of num_channels * width. The range image however contains holes, as they are not able to calculate the range for noisy points, but they still got the intensity and ambient value from the sensor.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GruNyv",
            "datetime": "Mar 3, 2022",
            "body": "Btw, if you want we could talk together about this problem on an other platform, since I do not want to talk about all parts of my thesis in public. Could be interesting to get some other input",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pespen",
            "datetime": "Mar 3, 2022",
            "body": "Sure! Shoot me an email at , and we can take it from there (probably use Discord or something if that is OK)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 2, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 2, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5163",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ValeriaProiettiDante",
            "datetime": "Feb 14, 2022",
            "body": "Hello everyone!I have an Ubuntu system with an NVIDIA 3080ti with CARLA and Unreal Engine 4.26.When CARLA is not in server mode, I have 60fps, but when I run the server, the fps go down to 8.I have already tried to uncheck the \"Use less CPU\" parameter in the Unreal editor.Anyone have any suggestions?Thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MunchZkin",
            "datetime": "Feb 16, 2022",
            "body": "I personally don't use Carla on Linux due to poor GPU support. But when you say \"run the server\" what do you mean?\nGenerally, when you have more actors in Carla you usually tend to loose fps, and these actors can be cameras, other vehicles/ walkers, streetlamps etc. So try restricting these actors needed specifically for you use case.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ValeriaProiettiDante",
            "datetime": "Feb 17, 2022",
            "body": "Thanks  for reply.For \"run the server\", I mean press play on UnrealEngine editor and it starts on server mode for CARLA simulator and, in my case, ROS2.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 18, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 18, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5130",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "meso17",
            "datetime": "Feb 2, 2022",
            "body": "Carla version 0.9.5 on windows 10 python 3.7.9. But when I run the PythonAPI code examples, there is no error. but I write the code below it gives an error.camera_bp = blueprint_library.find('sensor.camera.rgb')\ncamera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\ncamera_bp.set_attribute('image_size_x', '800')\ncamera_bp.set_attribute('image_size_y', '600')\ncamera_bp.set_attribute('fov', '105')",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5102",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "cerebro19288485",
            "datetime": "Jan 22, 2022",
            "body": "Hi,\nI am trying to  to train a reinforcement learning agent. I am able to print depth but not able to store it in a variable or write onto a numpy/text file for further use.Below code lets me print Radar depth DataReports depth data like this:\nBut if i try to append Depth data to a list, its not happening. (Tried writing to numpy and text file also, but couldnt do it)Reports like this:\nI want to know if there is a way to extract radar data for every point in each frame and save it to a numpy/text file.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5100",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "kanji95",
            "datetime": "Jan 21, 2022",
            "body": "I am having trouble getting the 3D world coordinates in CARLA. Basically, I use mouse click to select the destination point (pixel coordinate) in the front-view RGB camera image and get the depth of that point through a depth sensor. Now, I get the 3D coordinate using the following,\n\nHere, K is the intrinsic camera matrix and T_cam is Camera extrinsic transformation matrix. However, the final point I get after the above transformation is not correct, it is basically a point behind the vehicle's starting position. I am unable to figure out what I am doing wrong.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "brscholz",
            "datetime": "Feb 7, 2022",
            "body": "Correct me if I'm wrong, but I think that won't work anyway because you have zero clue as to where along its way the ray that is represented by the pixel met a surface? How do you want to extract that depth information from the image?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5082",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ivannson",
            "datetime": "Jan 18, 2022",
            "body": "I was wondering how the collision sensor can be configured to work in synchronous mode?I added it to the  alongside other sensors and added some exception handling to deal with empty queue when there is no data (since there are no collisions), but adding the collision sensor that way makes the simulation run really really slowly (dropping from 20 fps to 0 fps).This is what I'm currently using (I took bits which are not relevant to the collision sensor out)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5078",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "werewolfdev",
            "datetime": "Jan 17, 2022",
            "body": "CARLA version: 0.9.11\nPlatform/OS: Ubuntu 18.04Problem you have experienced: Hi Team, I am trying to update the location of the ego vehicle in a scenario using location data sent from another software. The data packets are reaching at a frequency of 100 packets per second into the Python API. Hence I would like to know if I could update my location data in CARLA at 100 FPS. I tried putting  \"-benchmark -fps=100\", but the client FPS still stays around 60. I tried adjusting in config.py the delta time seconds and the end result is also not as expected. Could anyone kindly let me know if 100 FPS is possible with CARLA (I am not using any Cameras or any other sensors as of now) and if yes, what is missing from my end. Thanks a lot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 16, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5026",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "SM1991CODES",
            "datetime": "Jan 3, 2022",
            "body": "I have gone through the basic setup and have a working simulation where there are 50 cars and my ego vehicle.\nI have a lidar mounted which I have configured at 128 channels.\nI have also configured the rotation frequency to 100 => 1ms per rotation.\nTherefore, I expect the lidar to generate a frame of point clouds every 1ms. This should in turn trigger my data collection callback, is that correct?So the idea is, if my sensor frame rate is higher than simulation frame rate (which I assume is < 100 FPS in this case), I should always get a full 360 degree frame each time the callback is triggered, is that right?I see this:(The white box is made by me)., then I had an idea and removed the power supply of my laptop, it seems, the simulator now runs at a slower speed leading to actual full frames. I see this:Does this look correct?\nSuprisingly, I still do not see any cars, even though I have 50 cars in the simulation.,I get only about 30k points on average whereas I have configured the lidar for 120k points.\nI have also reduced drop-off (please see code).Any idea what could be going wrong here?Please help!!!\nPlease find the code here:`\nimport carla\nimport numpy as np\nimport time\nimport carla_configs as ccssensor_dict = {\"imu\": \"sensor.other.imu\",\n\"gps\": \"sensor.other.gnss\",\n\"camera_rgb\": \"sensor.camera.rgb\",\n\"lidar\": \"sensor.lidar.ray_cast\"}lidar_data_buffer = []class CarlaSensors:class CarlaManager:if  == '':`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 17, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 17, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5024",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "441599828",
            "datetime": "Jan 3, 2022",
            "body": "CARLA version:0.9.13\nPlatform/OS:Ubuntu20.04\nProblem: the parameters in sensor.camera.rgb, I changed the parameters fstop and focal_distance but nothing changed in the Images,\nfstop=0.8 focal_distance=100\n\nfstop=8 focal_distance=1000\n\nin ue4 doc, it suppose to have the blur effect for the background ( )\n\nExpected to happen: to have a bigger aperture, shallower depth field as mentioned in ue4 docSteps to reproduce:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 17, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 17, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3131",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "NielsRogge",
            "datetime": "Oct 21, 2021",
            "body": "Instructions to add a new dataset can be found .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dnaveenr",
            "datetime": "Mar 22, 2022",
            "body": "I think we can close this issue since PR  solves this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NielsRogge",
            "datetime": "Oct 21, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Dec 8, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/839",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "loganlebanoff",
            "datetime": "Nov 11, 2020",
            "body": "I noticed that the XSum dataset has no space between sentences. This could lead to worse results for anyone training or testing on it. Here's an example (0th entry in the test set):",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/dmlc/gluon-cv/issues/229",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Zikingz",
            "datetime": "Aug 2, 2018",
            "body": "Hi. I am a new one here. It is a awesome lib for CV, .\nNow,  I am encountering a problem that I trained  on my own dataset, the dataset has . After training, I got a  file. When I want to load the parmeters and predict on new images, it return an error, logs as it,\n\nThe command I used as follow,\n\n\n\n\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zikingz",
            "datetime": "Aug 3, 2018",
            "body": "And could you please add a function draw boxes on image then return this image? I found it is difficult for me to use cv2.imshow() show processed images in real time detection.\nThank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhreshold",
            "datetime": "Aug 3, 2018",
            "body": "because you are assigning your customized weights to voc models, the shape of class prediction is different.\nYou can load the same net in your training script.I have added a convenient getter for custom dataset just merged in masterYou can use it to create a custom network quickly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zikingz",
            "datetime": "Aug 8, 2018",
            "body": "Thank you for your useful reply :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zikingz",
            "datetime": "Aug 8, 2018",
            "body": "Thank you again for your reply, that really solved my problem.\nBut there is a new question that I wanna use this trained model make a webcam real-time detection. I use opencv to get the images and call cv2.imshow() to plot images with boxes.\nHow can I get the image matrix with boxes in real time?\nThanks.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ZengyuanYu",
            "datetime": "Dec 4, 2018",
            "body": "@xDooooot Hi LaoGe, I trained SSD and get some params, when I load it and predict my test image, it is pick a issue, please you upload some code about you?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zikingz",
            "datetime": "Dec 12, 2018",
            "body": "Here is what I used.\n``\n希望能帮到你 :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zikingz",
            "datetime": "Dec 12, 2018",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ZengyuanYu",
            "datetime": "Dec 13, 2018",
            "body": "@xDooooot  Thank you! I got it though others ask. But use camera also funny to try it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "FAFACHR",
            "datetime": "Mar 26, 2019",
            "body": "Hello, I'm new in computer vision field. I'm using gluoncv to do instance segmentation with mask-rcnn but i have some problems.\nI did my own dataset similar to coco format and I trained with this model \"mask_rcnn_resnet50_v1b_coco\", but my dataset has just 2 classes. After training i got the parameter file and when i loaded the parameters and predict a new image, the classification was wrong and still loaded all classes of the coco dataset (80 classes). So is there any solution to customize the number of classes?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhreshold",
            "datetime": "Aug 3, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "zhreshold",
            "datetime": "Aug 28, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "narutobns",
            "datetime": "Jun 29, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#844"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-skainet/issues/58",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ValeraDanger",
            "datetime": "Aug 14, 2022",
            "body": "I am using TTGO T-camera  with I2S microphone. It's pinout:This module has ESP32 (not S3) chip, so i am using . I'm trying to change i2s pins. I've founded  file, where I changed  block (not , cause it seems to be second mic). This didn't work, so I disabled all useless blocks (change  parametrs to zero). Here you can see my actual full  file:This didn't help too. I appended  in  file in  function. to see, is there any data from my mic. . There is this func's code:But monitor is empty\n\nSo, there is not any data from my microphone,\nAlso, I've noticed, this warning\n\nI think, the problem is the absence of any codec on my board (before, I took data directly from mic by DMA (this sample worked for me   ))How can I tune this code to force it working on this board?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Aug 15, 2022",
            "body": "I think you should refer to  to modify your board.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ValeraDanger",
            "datetime": "Aug 15, 2022",
            "body": "I have renamed  to  and tryied to flash my board. While flashing, I tooked this error, cause my chip is not S3.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Aug 23, 2022",
            "body": "I mean you can refer to  to modify I2S setting.\nThe microphone of esp32_korvo_v1_1 is analog microphone.\nThe microphone of esp32s3-sys is digital microphone and I guess your mircophone is digital microphone too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Aug 14, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/voxel51/fiftyone/issues/1946",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "brimoor",
            "datetime": "Jul 15, 2022",
            "body": "With point cloud visualization coming soon (cf ), we should probably add  types capturing the default formats that our 3D visualizer will expect.Note that an alternative would be to simply expect users to use the existing  and  types and add the 3D-specified info as dynamic attributes of same/similar names as below.Here's some proposed formats, inspired by :",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "brimoor",
            "datetime": "Jul 15, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#752"
        }
    ]
},
{
    "issue_url": "https://github.com/voxel51/fiftyone/issues/1760",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "brimoor",
            "datetime": "May 20, 2022",
            "body": "In many use cases, it makes sense for a  to have multiple versions of its media. For example:Note that we're not talking about multiple views of the scene (eg left vs right camera) here, we're talking about different versions of the exact same scene. Any  labels, for example, would be renderable on any version of the media.This could be achieved by adding a  subtype of  that allows the user to declare that the field contains a media path that should be included in a dropdown menu in the App that controls which media field is being used to source media files.The required  field would have this type, but so could new fields:Additional elementsShould the dropdown go under settings? Or somewhere else?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "brimoor",
            "datetime": "Jun 2, 2022",
            "body": " Here's some pseudocode on how this feature could be exposed in the Python API:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "brimoor",
            "datetime": "May 20, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ritch",
            "datetime": "Jun 1, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#1811"
        },
        {
            "user_name": "brimoor",
            "datetime": "Jun 1, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#1520"
        },
        {
            "user_name": "ritch",
            "datetime": "Jun 2, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "benjaminpkane",
            "datetime": "Jun 15, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#1841"
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/4801",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "RafaelsNeurons",
            "datetime": "Aug 11, 2022",
            "body": "Hey :),\nI‘m trying to Train a 3D object detection Model with my custom 3D dataset created with cvat.\nBut I‘m confused I exported in Kitti formate but was Not able to do transfer-learning with it.\nCan somebody recomment a model which takes the exported Data as training?Thanks :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "holtvogt",
            "datetime": "Aug 12, 2022",
            "body": "Usually, in order to train a KITTI-based model, the KITTI format requires label files for each point cloud or image frame in the following structure:000042.txt:With that being sad, the information you receive from the CVAT export is in KITTI Raw Format 1.0 which will generate a  file from which you need to convert the corresponding tag values to the KITTI format mentioned above. I recently created an issue to make sure I understood that properly. You can have a look . I hope that helps! :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RafaelsNeurons",
            "datetime": "Aug 14, 2022",
            "body": "Thank you very much! This was exactly what I was looking for. I will also try it with MMDETECTION3D.So the worksteps are:Or did I forgot a step ? :)Again, thank you very much!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "holtvogt",
            "datetime": "Aug 14, 2022",
            "body": "Correct. The CVAT export always provides a  which basically contains a dictionary with frame index in ascending order and its corresponding  file name as value. That comes in handy when trying to automate that conversion. I have already written a tracklet parser based on the CVAT export. You can find the link .That's actually a huge point as you'll have to dig deep into the documentation and write your own dataset configuration. But yes, that's also correct.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RafaelsNeurons",
            "datetime": "Aug 17, 2022",
            "body": "Thank you very much, you are a hero!\nI really appreciate that!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Aug 25, 2022",
            "body": "That is not exactly CVAT issue, but I hope you were helped :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Aug 25, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/4781",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "copaah",
            "datetime": "Jul 27, 2022",
            "body": "Support rosbag format as input data source. Allow the annotator to skip through Image msgs for image/video annotation and write the annotation back into the rosbag using appropriate ROS messages.Currently you will need to extract images and annotate these and then manually write back into rosbags.Utilize the rosbag python API to load images and use appropriate ROS msgs for the supported annotation types.it is error prone and tidious to dump images from rosbags and load to CVAT and then subsequently write those annotations back into the rosbag. Annotating directly from the rosbag is less error prone and also allows to utilize other sensory information in the future such as multiple-view cameras, kinematics, IMUs etc.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kirill-sizov",
            "datetime": "Aug 29, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "kirill-sizov",
            "datetime": "Aug 29, 2022",
            "body": [],
            "type": "added this to",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/4762",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Steven-m2ai",
            "datetime": "Jul 15, 2022",
            "body": "Hello,I have images taken by a top down camera (~45 degrees looking downwards). As such, the objects have a tilt angle, and the front face of the cuboid thus will not be a \"perfect rectangle\" (it is distorted because of the object's tilt).Is there any way to label this using the cuboid?Thanks",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/4587",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "xiongda777",
            "datetime": "Apr 22, 2022",
            "body": "\nunion annotate is needed.when I draw a point/box in bird -eye-view pictrue,I want project the point/box to camera pictrue with camera Intrinsics and Extrinsics(or methods I customize).\nwhat I want is drawing an obstacle in  bird -eye-view pictrue,I know the position of obstacle in camera pictrue and I know whether I annotate correctly.No such functionCan you design an Interface to get union annotation？You may  channel for community support.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/4303",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "kkju",
            "datetime": "Feb 9, 2022",
            "body": "When marking the 3D box with cloud points, the  view  and  the  view , can not see the car's shape clear as follow picture:The better view to see, the cuboid need to move to the car's left:How can i move the camera view to the left side instead of using the right side to view the cloud points?\nThanks for giving some advices~",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Feb 10, 2022",
            "body": "Hi,3D views are constant for now, but your request is relevant. Let me mark it as a feature for the future.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kkju",
            "datetime": "Feb 9, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "kkju",
            "datetime": "Feb 9, 2022",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Feb 10, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/8",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "headdab",
            "datetime": "Jul 13, 2018",
            "body": "Another question and likely feature suggestion.When start a job, if I wait long enough, will all the frames be loaded into the browser?\nOr, are they loaded on demand as I seek through the video?\nAre they cached locally in memory?I'm working with 4k video and the interface isn't that usable, at least for my current use model, until all frames have been loaded.Based on the answer above, it would be great to have feedback as to whether the frames have all been loaded or, better, which frames have been loaded.   What I've seen that works well is using a different color on the seek bar for frames that have been loaded.If they are demand loaded, it would be nice to have a way to force it to load them all (as long as there's enough memory available).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 13, 2018",
            "body": "No, they don't. To reduce server load it will try to preload next 500 frames and it will continue preload other frames as soon as necessary. After jump it will start preload next frames from the new position.Could you please describe your annotation use case? Another way to optimize your case is to resize the video before uploading to CVAT.I like the feature. I will add it into our internal roadmap. It should not be difficult to implement.Let's understand your use case. If you annotate with a reasonable speed (e.g. several seconds per image) CVAT should be fast enough to preload next frames for you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "headdab",
            "datetime": "Jul 13, 2018",
            "body": "We're annotating sports video from stationary cameras.  Sometimes the players and ball are rather small in the resulting video making scaling the video down less desirable.  At the end of the day, we have to balance accuracy of the annotations with the time it takes to download and tag the videos.  I realize I can scale down the video, or compress the frames more, but need to understand the loading and caching strategy to determine the appropriate trade-offs.  How is the frame cache size determined?  Currently, I'm annotating 30s clips.  I'm guessing the segmentation options when creating jobs is so you can break big jobs into smaller ones, to handle related issues.The model I'm currently using, in interpolation mode, is to step through the video using at a coarse level (c/v keys) labeling the boxes for a given player.  I think that's the best way to get good per player tracks.  Then I use the scroll bar and single step to quickly slide back and forth through the video, stopping when necessary to add more keyframes to the track.We're still trying to find the most effective way to annotate these videos, but that's what we're doing so far.  I recall in the research that its easier to annotate one object at a time.  I use the filtering functionality to only show the boxes for the current object.  If you have other suggestions or ideas, please share.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 22, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Sep 20, 2018",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Sep 26, 2018",
            "body": [],
            "type": "issue",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Oct 15, 2018",
            "body": [],
            "type": "issue",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Oct 17, 2018",
            "body": [],
            "type": "issue",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Oct 29, 2018",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 20, 2018",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 22, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 22, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 22, 2019",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Feb 26, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "l3xis",
            "datetime": "Mar 1, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#1219"
        },
        {
            "user_name": "nmanovic",
            "datetime": "Mar 21, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Apr 15, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Jun 22, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 29, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "ActiveChooN",
            "datetime": "Sep 1, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Dec 15, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Mar 10, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "TOsmanov",
            "datetime": "Aug 23, 2021",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "korshunovdv",
            "datetime": "Aug 23, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "removed this from",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "removed this from the",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "removed this from",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 28, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "StellaASchlotter",
            "datetime": "Aug 3, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/5423",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "pooja-botpress",
            "datetime": "Sep 8, 2021",
            "body": "When a new Botpress instance is spun up, the first Super Admin Login should always be local",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "laurentlp",
            "datetime": "Sep 9, 2021",
            "body": "Hey,  Can you please provide a little more details on what you mean by \"the first Super Admin Login should always be \"?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "charlescatta",
            "datetime": "Oct 6, 2021",
            "body": "What I understand: When an SSO strategy is enabled and the default auth strategy is disabled on first boot, the registration screen to create the super admin does not appear.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JustusNBB",
            "datetime": "Oct 8, 2021",
            "body": "I would like the ability to disable the user/pass local strategy from the login page while keeping accessibility through APIs (CI User = Superadmin)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Michael-N-M",
            "datetime": "Sep 9, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "laurentlp",
            "datetime": "Oct 13, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#5564"
        },
        {
            "user_name": "laurentlp",
            "datetime": "Oct 14, 2021",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Oct 22, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#5595"
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/2170",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "moeidsaleem",
            "datetime": "Jul 29, 2019",
            "body": "\nNo, it actually an accessibility feature, a nice to have. This can be for someone who doesn't understand English that well or prefer to listen or only limited to listening - Accessibility feature. A audio button with every message to say that message loud in the user selected language.\nWe can use say.js for mac and festival for Linux, a simplest implementation of this and for converting text, a language server would be good start to provide content.\nWe can use azure cognitive or IBM Watson or any other cloud service available for achieving the following task.\nAs i am already, working on implementing this with say.js / festival and also Microsoft azure cloud support.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Aug 16, 2019",
            "body": "That's indeed pretty interesting. At the moment, we're focusing on improving NLU and flow editor. Indeed HITL modules needs some love and speech would be very nice but would require a decent amount of work to support multiple languages and to keep this available on-prem. Integrating with cloud providers would be a good start though.Please share your work and your progress.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Feb 12, 2020",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "moeidsaleem",
            "datetime": "Jul 29, 2019",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Feb 12, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Mar 13, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/3939",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 23, 2021",
            "body": "Hi,\nWhy does annotation was not there on the downloaded dataset images ..?\nWhy the structure of datasets like market1510 , yolo ,pascal VOC etc.. in CVAT looks different when compare to original..?\nCan we able to make the structure of datasets similar to the original one..?Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 23, 2021",
            "body": " , thanks for your comments. Could you please describe in more details and give us a couple of examples? What is the difference?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 23, 2021",
            "body": "Hi,lets take some of the dataset like Market-1510.\nWhen I downloaded the Market-1510 dataset from the online.\nit has a folders of Train , test  etc..I used CVAT tool for re-identification. First I had done detection and the re-identification using auto annotation.\nWhen I downloaded that dataset from CVAT. The folder has only some images(save images from the Export dataset) and txt file.So, what's the difference here..?\nand How can I make CVAT Market-1510 dataset into original one..?Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 23, 2021",
            "body": " , could you please look at answer?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 24, 2021",
            "body": "Hi guys,\nany update..?Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kirill-sizov",
            "datetime": "Nov 24, 2021",
            "body": "Hi, do you use Tag= during your annotation process in CVAT?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 24, 2021",
            "body": "Thanks for the reply .Am new to this. where can i find that..?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kirill-sizov",
            "datetime": "Nov 24, 2021",
            "body": "If you want to get expected result of exporting into the Market-1501 you should use Label with value  and attributes: , , . Particular use case for Market-1501 dataset it's when you have cropped images with people: one person per image. And after that using CVAT you can fill information about person for each image: , , . We implemented such logic based on .It will be interesting to get more information about your use case: what kind of images do you have? what types of annotation objects did you try to use in CVAT for getting Market-1501 dataset?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 24, 2021",
            "body": "Thanks,I just annotated the data with person detection and next with reidentification.\nThen i exported the dataset from the option that you had provided.\nI didn't attached any tags.I want the dataset like you mentioned in above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 24, 2021",
            "body": "First thing I want to know how to set these tags that you said in the above message.Coming to our use case. We need the dataset from CVAT like the original one from the online.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 24, 2021",
            "body": "HI, Is this is the thing that you are trying to mention.Let me know if it is wrong or right..?\nIf right what needs to be update in query attribute..?\nif right what needs to do after...?Thanks,",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kirill-sizov",
            "datetime": "Nov 24, 2021",
            "body": "It's better to specify type for attributes, try to use this labels definition in Raw (instead of Constructor):Also don't forget to change values for attributes  and  according your dataset (convention for list of values: )Once you create a task with a label definition like this, you can annotate your images with tags, see the documentation to read more about this   and about the attribute annotation mode .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 25, 2021",
            "body": "Thanks .\nWill do this and let you know.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 25, 2021",
            "body": "HI \nIt works.I had a doubt what does the folder \"query\" called..?\nHow it get structured..? Does it selects one random photo from the video or it selects one random photo from one person (like 2 photos for 2 persons).Does CVAT provides any option to crop the images. ? If not how can you crop them ..?Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 25, 2021",
            "body": "HI . I had tried adding my own dl models into nuclio.\nCVAT uses openvino_2020.2 version of models. I modified the code to use other versions of openvino or other type of models in the same version. (like 0031 reidentiication model instead of 0300).\nI tried for detection models too. The function has deployed completely.But it didn't got added to the list of CVAT models.\nAny reasons and how to solve it.?\nThanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhiltsov-max",
            "datetime": "Nov 28, 2021",
            "body": "You can find a simple explanation of query and gallery subsets in ReID context  and . Basically, \"query\" subset means \"identities to be classified\".",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 28, 2021",
            "body": "Thanks .So, for every person we have one query image to match with the gallery images.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Nov 29, 2021",
            "body": "Hi. Any updates..?Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Dec 9, 2021",
            "body": "Hi  we are still working on it. I just need the dataset of market-1501 from CVAT to be the same with the original one which we can get from online.\nwe are annotating on a video. (first detection model and reid model). I updated the labels as same as you said before. And I am not able get the folders like query, gt_query, gt_bbox when we export the dataset into market-1501.\nBut as you mentioned before about cropping images.\n(like taking cropped images and attaching the labels or how can i do that..?)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhiltsov-max",
            "datetime": "Jan 14, 2022",
            "body": "Hi, you could crop images with Datumaro. How would you like to crop them? By fixed coordinates, by a bbox or something else? From the conversation above, I can assume, you would like to auto-annotate images with a model and then crop by a bbox, is it correct?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bhargav-sudo",
            "datetime": "Jan 16, 2022",
            "body": "Hi I want to crop the images using the data of person that i get from the CVAT tool.Yes, you are correct.\nwe need the data of person_reid to train the model.\nAnd we need the that data in the market-1501 format where it has  these folder structure.How can I get them..?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bsekachev",
            "datetime": "Sep 2, 2022",
            "body": "Sorry for lack of response from our side. There are too many issues opened. I am trying to reduce them now and I will close this issue.Please, if the question is still relevant, let us know and do not hesitate to reopen.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 23, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "zhiltsov-max",
            "datetime": "Nov 29, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "openvinotoolkit/datumaro#570"
        },
        {
            "user_name": "bsekachev",
            "datetime": "Sep 2, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/3387",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "hnuzhy",
            "datetime": "Jul 4, 2021",
            "body": "I have read and searched the official docs and past issues for the solution. No one had the same problem with me.I want to annotate the head orientation of people in 2D image with a standard 3D cube. Here, the head is a rigid object. A standard cube is defined as follows: three sides of any vertex are perpendicular to each other, and all twelve sides are equal in length, or in unit length.After labeling, we could get the eight projected vertices of the cube in the two-dimensional coordinate system. If three Euler angles (pitch, yaw, roll) are used to represent the orientation of the head, these precise projection points can be converted into corresponding angles.I have three suggestions or roadmaps for adding unit  label in the new version of CVAT.Here are two examples of 3D model interaction. The first is the rotation interaction of a 3D head model in mayavi. The interactive operation needs to rely on both mouse and keyboard. The second is to use the 3D image editing tool in Windows 10 to place and operate 3D models on 2D images. All you need to do is use the mouse.\nExample 1\nExample 2Looking forward to your reply. I will be willing to do whatever I can to advance this functional part.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiehpower",
            "datetime": "Jul 7, 2021",
            "body": "this is so cool feature ...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 7, 2021",
            "body": " , I agree that we need to improve the functionality. Your explanation is really helpful. Could you please describe your research area and organization? Unfortunately my team has huge amount of requests and we already have an approximate roadmap for Q3'21 and Q4'21. Thus I'm trying to clarify details which will help me to increase the priority of the feature.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hnuzhy",
            "datetime": "Jul 8, 2021",
            "body": "Yes, it is a pretty cool function which is not easy to realize :-(",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hnuzhy",
            "datetime": "Jul 8, 2021",
            "body": " Hi, I'm glad you agree to my proposal. I am a PhD student in computer department from SJTU University. My research field is the intersection of AI and education. The detailed research direction is object detection and pose estimation in computer vision. I would like to talk about the motivation of this question from two aspects.Recently, I've been studying the methods of attention detection for students in the classroom. Among them, head orientation (head pose estimation) is one of the key factors. However, as far as I know, the head pose estimation algorithm of multi-person in 2D image is not well developed. At present, there are some SOTA algorithms for head pose estimation of a single well cropped head, including  and . But their effect is not ideal, and it is not easy to extend to the case of multiple people in a single image. Most importantly, the datasets used by these algorithms are obtained by 3D head projection (), or the 3D Euler collected by depth camera in the experimental scene ().\nPrediction example 1 of FSA-Net (The input can only be a single person's head with visible face.)\nPrediction example 2 of FSA-Net (First, the head bbox of each person is detected by MTCNN, and then the single head is estimated. Therefore, this is not an efficient or essential multi-person head pose estimation algorithm.)\nPrediction example of WHE-Net (The input can only be a single person's head with wide range pose. The predictable yaw angle of the head is omnidirectional.)Dataset has always been the cornerstone of deep learning algorithms, so is head pose estimation. Therefore, I want to try to annotate the 3D head orientation, or three Euler angles of the head directly in the 2D image. As mentioned for the first time in this issue, the most accurate annotation scheme focuses on how to use 3D cube to interact freely on 2D images. In my opinion, once such a dataset is constructed, it will help promote the great progress of the corresponding algorithm research. For example, a bottom-up method could be designed to directly predict the pose of all heads in the image at one time. At the same time, compared with a single captured head image, the complete scene and human body information in the original image can assist more accurate head pose estimation.:After investigation, I didn't find tools with real 3D cube annotation. Fortunately, close functional options were found in CVAT. The first is . However, the new builded cuboid lacks rotation freedom. The second is . By annotating three consecutive non coplanar edges of a 3D cube approaching the head orientation, we can deduce the approximate Euler angle. Unfortunately, there is a great subjectivity error in this annotation process. We can't see the actual pose of the generated cube directly, unless we use a real 3D cube to annotate interactively. If we use this method reluctantly, the credibility of the final annotation will be questioned.Here are three examples of rough annotation results with . Images are all from the public  dataset. The object we annotate is the head with any orientation in the image, including the visible, occluded and invisible face. In many cases, the current method of  annotation is difficult and inaccurate.In a word, it is very useful to add interactive annotation of rigid 3D graphics (which can only be rotated, translated and scaled) to 2D images. In addition to supporting the head orientation marking, the new function can also be extended to the annotation of other rigid objects. After the construction of similar datasets about general objects, we can try to develop a simple and direct 3D object pose estimation algorithm only based on 2D images. We expect that this method can be comparable to estimation algorithms based on RGB-D or 3D point cloud.Finally, I am not good at giving the overall improvement framework of CVAT about this enhancement from UI design or code addition, but I am willing to do what I can. I sincerely thank CVAT's main contributors for their work, and hope to carefully consider adding this task to roadmap.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Kucev",
            "datetime": "Apr 7, 2022",
            "body": "I support the request. We also have a need for such functionality.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "schliffen",
            "datetime": "May 28, 2022",
            "body": "This is a growing request from automotive industry as well, we need cuboid annotations to be done on RGB images not points clouds.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hnuzhy",
            "datetime": "May 28, 2022",
            "body": "Yes, you are right. Actually, I have written a simple 2D head pose annotation tool using  last year. As shown below, the annotator can label one head with adding a bounding box in the 2D image, and adjust the 3D head model through mouse or keyboard in the right area to make it have the similar orientation/pose with boxed head. The co-existed 3D cube will be projected in the 2D image. For every appearing head pose status, we will record the corresponding Euler angles.\n\nHowever, this tool can only run in desktop, and is not fully as what I expected originally (refer above question for details). Then, I was busy on other things until now. I did not update and perfect this tool for a long time. If possible, I still look forward to seeing this annotation function in .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Jul 7, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 20, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Nov 20, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/14644",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "mik-laj",
            "datetime": "Mar 6, 2021",
            "body": "Hello,Currently, documentation for the Docker image is available in the middle of the . This makes it difficult for the user to find this documentation. Also, this section is called \"Production deployment\", but this image is also used in the development environment. I think we should extract the image sections, but I'm not sure where. I am thinking of a few solutions:\na) create a new page/section in  package. Then the documentation will available as a new menu item in the current documentation package. See \"Content\" section on \nb) create a new documentation package. Then the documentation will be available as a new item on I prefer the second solution, but I am open to discussions.It is worth considering that I am working on creating a new package for the Helm Chart. See:Probably related: CC:      ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ashb",
            "datetime": "Mar 6, 2021",
            "body": "Docs for the helm chart feel right as a new \"package\" (I really like that idea!) but docker image feels more like it is about/for apache-airflow so I favour option a.My thinking is that if you want to learn how to run Airflow, your going to be looking at the docs of apache-airflow for config reference etc, and to me docker sort of fits in there. I think the other reason why it feels wrong as separate doc package is because it's not a package we actually release.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Mar 6, 2021",
            "body": "Keycloak publishes documentation for an image separately from the documentation for a project.\nKeycloak documentation index: \nKeycloak server container documentation: Jupyter has very extensive documentation for the image and publishes it separately also.\nDocumentation index: \nDocker image: Have you seen a project with non-trivial docker-images that describes building images in the main project documentation? I'd love to see it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ashb",
            "datetime": "Mar 7, 2021",
            "body": "Good question, I'll have a look around",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ashb",
            "datetime": "Mar 8, 2021",
            "body": "(Screenshots are of the section of the doc Table of Contents of the projects)Elastic(search):Prisma (v1, v2 has changed the architecture to become just a library so doesn't have a server component any longer:Keycloak is kind of a hybrid -- it's in the normal docs menu, it just links out to a README.md.It's a fairly uncommon case that projects document much about their docker file anyway from what I can see, jupyter seems to be the exception in having it's docker docks on a totally different url to the project docs from my search just now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Mar 9, 2021",
            "body": "I think many people are asking for docker image and have hard time finding it.  Separate section (option b) is fine). However  think there is one caveat with the current airflow/providers/split which it will only reinforce.Once you are in airflow documentation, it is not easily to realize that there are more components (providers and image when it is separated out). Most people will not go to the docs of airflow via url - they will reach it via Google search or history in the browser. Then - when you are already in airflow docs, there is no way of knowing that you should click on the airflow logo to get to the airflow + providers + image directory. And it is not very accessibility friendly. So while separating it out is fine, we should - i think also add some way for people. To know that provider/image documentation is there - other than clicking at the Airflow logo. Maybe a separate section in the table of content should be added ? Something that will clearly say hat You can reach out to full set of airflow- related docs this way?Not sure about name though? Additional documentation? Beyond Airflow ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Mar 9, 2021",
            "body": " Terraform has a drop down menu in the section under the link with the documentation.\nThey also have links to other documentation packages in the side menu.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Mar 10, 2021",
            "body": "I would like to add a few comments. I am working on more docker-composer example files so that users can test more easily.\nSee: \nWhen we create a new package of documentation for all things related to Docker, it will be easier to put it together.Another thing that I am still missing is the description of how to use  together with Docker-compose. Many users try to use this together but have problems with it because  cannot be read by the  user. The simplest solution is: to use. See: So this documentation will increasingly describe how to deploy Airflow in a Docker/docker-compose environment much like the documentation for Helm Chart describes how to deploy Airflow in a Kubernetes environment.Also, note that the documentation that describes how to build a Docker image describes building an image for multiple versions of Airflow. You can use the same instruction to build the image for Airflow 1.10 and for Airflow 2.0. The only difference is the additional flag.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Mar 6, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ashb",
            "datetime": "Mar 6, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#14764"
        },
        {
            "user_name": "mik-laj",
            "datetime": "Mar 17, 2021",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/13114",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "atolopko-czi",
            "datetime": "Dec 16, 2020",
            "body": "The coloration of DAG operator statuses in the DAG \"Tree\" and \"Graph\" views are not easy to discern by color blind individuals. There is no textual representation of the status in the hover popup window.YesNo",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "boring-cyborg",
            "datetime": "Dec 16, 2020",
            "body": "Thanks for opening your first issue here! Be sure to follow the issue template!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eladkal",
            "datetime": "Dec 16, 2020",
            "body": "What Airflow version are you running? There has been improve in that area in 1.10.11\nsee issue , PR , Also in Graph View when you over one of the statuses in the legend it makes all the tasks in that status highlighted:\nCan you give more information? (maybe a mock up of what you expect?)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "atolopko-czi",
            "datetime": "Dec 16, 2020",
            "body": "Version : 1.10.10The Tree view status-hover highlighting is indeed helpful, and I was unaware! The Graph view doesn't provide the same feature, which would be helpful.  So it would still be helpful if these popups contained the operator status:\nUse of custom colors may also be able to address this for particular degrees of color blindness, which I will try.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Dec 17, 2020",
            "body": "You can customize those colors: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ashb",
            "datetime": "Dec 17, 2020",
            "body": "And if you can suggest new default colours we can look at changing them too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eladkal",
            "datetime": "Jan 29, 2021",
            "body": " Do you think there are still issues needs to be addressed?  If you can please describe your suggestions",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "atolopko-czi",
            "datetime": "Feb 1, 2021",
            "body": "I still feel it is helpful if the Tree view status hover popups simply contained the operator status as text. Colors can be useful conveyors of information, but text is more accessible for the color-blind. I have not tried setting different default colors, since that would require consensus across our organization.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Feb 1, 2021",
            "body": "The tree view in 2.0.0 should already have that",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "atolopko-czi",
            "datetime": "Feb 1, 2021",
            "body": "Will upgrade! Thanks !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "atolopko-czi",
            "datetime": "Dec 16, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "eladkal",
            "datetime": "Dec 16, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "atolopko-czi",
            "datetime": "Feb 1, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/2980",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "cengizmurat",
            "datetime": "Feb 20, 2020",
            "body": "Hello,\nI have a  in our team and he has some difficulties to use Botpress chatbot.\nHe works with two screen readers  and , and there are different issues for interacting with Botpress chatbot :Thank you very much,\nBest regards",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Feb 20, 2020",
            "body": " Thank you for sharing your feedback. While we were aware that Botpress' interface is not accessible, we had no idea where to start as you might imagine there's quite a lot to tackle. Thanks for sharing your thoughts, we'll take this into account and do our best to make the interface more accessible for blind people (starting with the webchat).However, if you are willing to contribute this would help a lot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patlachance",
            "datetime": "Feb 21, 2020",
            "body": " /  :  is in my team, you can count on us to help you improving botpress accessibility by submitting PR. We may need your help to pinpoint appropriate sections to improve.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SEPOL14",
            "datetime": "Feb 21, 2020",
            "body": "Hello,I am the blind developer who use your chatbot.How would you like that we work together ?Do you wish that we send any accessible code examples to you, or do you first prefer to search the informations by yourself ?Best regards.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "asashour",
            "datetime": "Feb 21, 2020",
            "body": ", , please don't hesitate to ask, also in  if you need further assistance I would suggest that we try to resolve the most important things first",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SEPOL14",
            "datetime": "Apr 29, 2020",
            "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SEPOL14",
            "datetime": "Apr 29, 2020",
            "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2021",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Aug 17, 2021",
            "body": "We will close this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cengizmurat",
            "datetime": "Feb 20, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Feb 20, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "slvnperron",
            "datetime": "Oct 22, 2020",
            "body": [],
            "type": "unassigned",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 28, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Aug 17, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/opencv/cvat/issues/219",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "bluecamel",
            "datetime": "Dec 5, 2018",
            "body": "On a laptop touchpad, zooming is too fast to control well.  I've been poking through svg.js, but it's not clear how to change the zoom speed.  Thanks so much for any info!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Dec 5, 2018",
            "body": "Hi  ,I believe it depends on OS. For example, . In our case we use mostly mouse to annotate data. Tried a trackball but it worked bad for our team.Does it answer your question?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Dec 8, 2018",
            "body": "Hi  ,I will close the issue. Feel free to reopen it if your question isn't answered.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JeremyKeustersML6",
            "datetime": "Apr 7, 2021",
            "body": "Hi all,I'm opening this issue again as I'm experiencing this issue on my MacBook Pro. I already tried to change the scrolling speed in the Accessibility options, but the zoom in CVAT remains extremely sensitive when using the trackpad. Any workarounds for this? Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Dec 5, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "nmanovic",
            "datetime": "Dec 8, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/582",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "awallin",
            "datetime": "Nov 15, 2019",
            "body": "Based on user testing feedback an audio response for requests is desired by some users. In addition, this would be a powerful accessibility feature.For utterances that return card result we can provide a brief verbal response using available TTS provided by the OS.Responses for different card types are available in this doc as well as the attached mockup.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Dec 3, 2019",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jun 8, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jun 8, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "jcambre",
            "datetime": "Jun 29, 2020",
            "body": [],
            "type": "pull",
            "related_issue": "#1786"
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 1, 2020",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 1, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/EconML/issues/614",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "juandavidgutier",
            "datetime": "May 9, 2022",
            "body": "Hello ,I have a model with a X variable=Hesitant  and I can get the shap_values for my model too. The model has PolynomialFeatures(degree=3). Simillarly, I estimated the CATE for the variable Hesitant. These are the figures I obtain for shap values and CATE:My question is how to interpret the CATE results? It means that if the X0 row in the figure of shape values shows that high values of Hesitant are more likely to give high values of output variable. But in the CATE figure, the high values of Hesitant trend to reduce the effect of treatment on the output variable.I assume that the difference can be explained by the PolynomialFeatures argument, but if you can give me details, I'll appreciate it.By the way, is possible that my shap values figure shows the name of the X variable?I'll appreciate a lot your cooperation.Here is my dataset\nand here is my code:\n`\nimport os, warnings, random\nimport dowhy\nimport econml\nfrom dowhy import CausalModel\nimport pandas as pd\nimport numpy as np\nimport econml\nfrom econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML, CausalForestDML\nfrom econml.dr import DRLearner, ForestDRLearner, SparseLinearDRLearner\nfrom econml.orf import DROrthoForest, DMLOrthoForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV\nfrom econml.inference import BootstrapInference\nimport numpy as np, scipy.stats as st\nimport arviz as az\nimport scipy.stats as stats\nfrom econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nfrom zepid.graphics import EffectMeasurePlot\nfrom zepid.causal.causalgraph import DirectedAcyclicGraph\nfrom joblib import Parallel, delayed\nfrom econml.score import RScorer\nfrom plotnine import ggplot, aes, geom_line, geom_ribbon, ggtitle, labs\nimport shap\nfrom sklearn.model_selection import train_test_split\nimport warningsdef seed_everything(seed=123):\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'seed = 123\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '%.2f' % x)#import data\ndata = pd.read_csv(\"D:/dataset_covid.csv\", encoding='latin-1')\ndata = data.dropna()Constrained = data[['output50', 'Constrained', 'SVI', 'Access', 'HC_Accessibility_Barriers', 'Hesitant', 'Sociodemographic_Barriers', 'PopDensity', 'broadband']]\nprint(Constrained.std())\nConstrained.PopDensity = stats.zscore(Constrained.PopDensity)\nConstrained.Hesitant = stats.zscore(Constrained.Hesitant)Y = Constrained.output50.to_numpy() #Y = data_card['incidencia100k_cardiovasculares'].values\nT = Constrained.Constrained.to_numpy()\nW = Constrained[['SVI', 'HC_Accessibility_Barriers', 'Access']].to_numpy().reshape(-1, 3)\nX = Constrained[['Hesitant']].to_numpy().reshape(-1, 1)X_train, X_val, T_train, T_val, Y_train, Y_val, W_train, W_val = train_test_split(X, T, Y, W, test_size=.4)warnings.filterwarnings('ignore')reg1 = lambda: GradientBoostingClassifier()\nreg2 = lambda: GradientBoostingRegressor()models = [\n('ldml', LinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nlinear_first_stages=False, cv=3, random_state=123)),\n('sldml', SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)),\n('dml', DML(model_y=reg1(), model_t=reg2(), model_final=LassoCV(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3),\nlinear_first_stages=False, cv=3, random_state=123)),\n#('ortho', DMLOrthoForest(model_Y=reg1(), model_T=reg2(), model_T_final=LassoCV(), model_Y_final=LassoCV(),\n#                     discrete_treatment=False, global_res_cv=3, random_state=123)),\n('forest', CausalForestDML(model_y=reg1(), model_t=reg2(),\nfeaturizer=PolynomialFeatures(degree=3),\ndiscrete_treatment=False, cv=3, random_state=123)),\n]def fit_model(name, model):\nreturn name, model.fit(Y_train, T_train, X=X_train, W=W_train)models = Parallel(n_jobs=-1, verbose=1, backend=\"threading\")(delayed(fit_model)(name, mdl) for name, mdl in models)#Choose model with highest RScore\nscorer = RScorer(model_y=reg1(), model_t=reg2(),\ndiscrete_treatment=False, cv=3,\nmc_iters=3, mc_agg='median')scorer.fit(Y_val, T_val, X=X_val, W=W_val)rscore = [scorer.score(mdl) for _, mdl in models]\nprint(rscore)#best model SparseLinearDML#Step 1: Modeforestg the causal mechanism\nmodel_Constrained=CausalModel(\ndata = Constrained,\ntreatment=['Constrained'],\noutcome=['output50'],\ngraph= \"\"\"graph[directed 1 node[id \"Constrained\" label \"Constrained\"]\nnode[id \"output50\" label \"output50\"]\nnode[id \"SVI\" label \"SVI\"]\nnode[id \"HC_Accessibility_Barriers\" label \"HC_Accessibility_Barriers\"]\nnode[id \"Access\" label \"Access\"]\nnode[id \"Hesitant\" label \"Hesitant\"]\nedge[source \"Access\" target \"SVI\"]\nedge[source \"Access\" target \"HC_Accessibility_Barriers\"]\nedge[source \"Access\" target \"Hesitant\"]\nedge[source \"Access\" target \"output50\"]\nedge[source \"Access\" target \"Constrained\"]\nedge[source \"SVI\" target \"Constrained\"]\nedge[source \"SVI\" target \"output50\"]\nedge[source \"HC_Accessibility_Barriers\" target \"Constrained\"]\nedge[source \"HC_Accessibility_Barriers\" target \"output50\"]\nedge[source \"SVI\" target \"HC_Accessibility_Barriers\"]\nedge[source \"SVI\" target \"Hesitant\"]\nedge[source \"HC_Accessibility_Barriers\" target \"Hesitant\"]\nedge[source \"Constrained\" target \"Hesitant\"]\nedge[source \"Constrained\" target \"output50\"]\nedge[source \"Hesitant\" target \"output50\"]\n]\"\"\"\n)#view model\n#model_Constrained.view_model()#Step 2: Identifying effects\nidentified_estimand_Constrained = model_Constrained.identify_effect(proceed_when_unidentifiable=False)\nprint(identified_estimand_Constrained)#Step 3: Estimating effects\nestimate_Constrained = SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)estimate_Constrained = estimate_Constrained.dowhyestimate_Constrained.fit(Y=Y, T=T, X=X, W=W, inference='bootstrap')estimate_Constrained.effect(X)ate_Constrained = estimate_Constrained.ate(X)\nprint(ate_Constrained)ci_Constrained = estimate_Constrained.ate_interval(X)\nprint(ci_Constrained)#shap\nshap_values = estimate_Constrained.shap_values(X)\n#x0='Hesitancy'\nshap.plots.beeswarm(shap_values['Y0']['T0'])\nshap.summary_plot(shap_values['Y0']['T0'], plot_type=\"violin\")#CATE\n#range of hesitancy\nmin_Hesitant = -2.45\nmax_Hesitant = 2.35\ndelta = (max_Hesitant - min_Hesitant) / 100\nX_test = np.arange(min_Hesitant, max_Hesitant + delta - 0.001, delta).reshape(-1, 1)treatment_effects = estimate_Constrained.const_marginal_effect(X_test)te_upper, te_lower = estimate_Constrained.const_marginal_effect_interval(X_test)est2_Constrained = SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=2, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)est2_Constrained.fit(Y=Y, T=T, X=X, inference=\"bootstrap\")treatment_effects2 = est2_Constrained.effect(X_test)\nte_lower2_cons, te_upper2_cons = est2_Constrained.effect_interval(X_test)#plot elasticity(\nggplot(aes(x=X_test.flatten(), y=treatment_effects2))`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kbattocchi",
            "datetime": "May 9, 2022",
            "body": "From your code, it looks like you're plotting the results of est2_Constrained, which has a degree 2 polynomial rather than the degree 3 polynomial that corresponds to the shap results.The plot basically says that if 'Hesitant' is 0, then a one-unit increase in T should cause a roughly 0.48 unit increase in Y, if 'Hesitant' is 2, then a one-unit increase in T should cause a roughly 0.1 unit , etc. (I'm just reading off the approximate values from the chart).As a side note, you can probably get more accurate confidence intervals from the default \"debiasedLasso\" inference rather than explicitly using \"bootstrap\".I believe you should be able to pass the feature names through like this: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "juandavidgutier",
            "datetime": "May 10, 2022",
            "body": "OK , thanks for your useful cooperation.Regards",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "juandavidgutier",
            "datetime": "May 10, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/560",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Nov 11, 2019",
            "body": "There are conflicts sometimes (with Multi-Account containers per , and with 1Password per ). We can't choose a keyboard shortcut that works for everyone, so we should just make it configurable.This would be a new field in the options page. I think we can just force people to type in the keyboard shortcut syntax per the We might leave  and make this an additional keyboard shortcut...?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bnakamoto",
            "datetime": "Jan 15, 2020",
            "body": "I had mostly composed (last night) a reply to  about my feedback about Command-Period being a long-time expected keyboard shortcut on macOS, but it appears that .  had referenced an issue that referenced this issue in his reply so this seems like the best place to share my follow-up; apologies in advance if not.I surprisingly haven't been able to find any Apple developer docs about Command-Period. They have , but there's only reference to Control-Option-Command-Period (to increase screen contrast, part of ). There is evidence that Apple has standardized on Command-Period as a cancel/escape/break operation; for example, in their keyboard shortcut docs for  and . There's also third party docs such as  about Command-Period being a broad \"'cancel' feature\":Apple has associated Command-Period with \"cancel\" on Macs since before Mac OS X/macOS, the System  days. Thus, I hope that the  keyboard shortcut for Firefox Voice is changed to something other than Command-Period.Thank you!\n-Brian",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jan 17, 2020",
            "body": "Per the original thread disappearing: apparently our comment submission process was flagged as suspicious by GitHub, and all comments made that way were hidden. We're working on getting them back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Nov 11, 2019",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#550"
        },
        {
            "user_name": "ianb",
            "datetime": "Nov 21, 2019",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Dec 3, 2019",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Dec 5, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Dec 5, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/57",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Aug 8, 2019",
            "body": "Right now the text input seems to be a . This is extra work and has accessibility problems. We should just use an input. All the styles can still be overridden so it can look like whatever (though it takes somewhat more work).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Aug 12, 2019",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Aug 30, 2019",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Sep 16, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/EconML/issues/612",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "juandavidgutier",
            "datetime": "May 7, 2022",
            "body": "Hello ,First, I am new in shap. I have two variables: 'Access' and 'Hesitant' in X and I can get the shap_values for my model. However, when I want to plot the shap_values with beeswarm, I see other different variables e.g. X0^3, X1^3 and others, but not my two variables of X. Why it happens?. I have shap version 0.39.0I get this plot:I'll appreciate a lot your cooperation.Here is my dataset\nand here is my code:`# importing required libraries\nimport os, warnings, random\nimport dowhy\nimport econml\nfrom dowhy import CausalModel\nimport pandas as pd\nimport numpy as np\nimport econml\nfrom econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML, CausalForestDML\nfrom econml.dr import DRLearner, ForestDRLearner, SparseLinearDRLearner\nfrom econml.orf import DROrthoForest, DMLOrthoForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV\nfrom econml.inference import BootstrapInference\nimport numpy as np, scipy.stats as st\nimport arviz as az\nimport scipy.stats as stats\nfrom econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nfrom zepid.graphics import EffectMeasurePlot\nfrom zepid.causal.causalgraph import DirectedAcyclicGraph\nfrom joblib import Parallel, delayed\nfrom econml.score import RScorer\nfrom plotnine import ggplot, aes, geom_line, geom_ribbon, ggtitle, labs\nimport shap\nfrom sklearn.model_selection import train_test_split\nimport warningsdef seed_everything(seed=123):\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'seed = 123\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '%.2f' % x)#import data\ndata = pd.read_csv(\"D:/dataset_covid.csv\", encoding='latin-1')\ndata = data.dropna()#Constrained\nConstrained = data[['output50', 'Constrained', 'SVI', 'Access', 'HC_Accessibility_Barriers', 'Hesitant', 'Sociodemographic_Barriers', 'PopDensity', 'broadband']]\nprint(Constrained.std())Yc = Constrained.output50.to_numpy()\nTc = Constrained.Constrained.to_numpy()\nWc = Constrained[['SVI', 'HC_Accessibility_Barriers']].to_numpy().reshape(-1, 2)\nXc = Constrained[['Access', 'Hesitant']].to_numpy().reshape(-1, 2)reg1 = lambda: GradientBoostingClassifier()\nreg2 = lambda: GradientBoostingRegressor()#Step 1: Modeling the causal mechanism\nmodel_Constrained=CausalModel(\ndata = Constrained,\ntreatment=['Constrained'],\noutcome=['output50'],\ngraph= \"\"\"graph[directed 1 node[id \"Constrained\" label \"Constrained\"]\nnode[id \"output50\" label \"output50\"]\nnode[id \"SVI\" label \"SVI\"]\nnode[id \"HC_Accessibility_Barriers\" label \"HC_Accessibility_Barriers\"]\nnode[id \"Access\" label \"Access\"]\nnode[id \"Hesitant\" label \"Hesitant\"]identified_estimand_Constrained = model_Constrained.identify_effect(proceed_when_unidentifiable=False)\nprint(identified_estimand_Constrained)estimate_Constrained = DML(model_y=reg1(), model_t=reg2(), model_final=LassoCV(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3),\nlinear_first_stages=False, cv=3, random_state=123)estimate_Constrained.fit(Y=Yc, T=Tc, X=Xc, W=Wc, inference='bootstrap')#shap\nshap_values = estimate_Constrained.shap_values(Xc[:200])\nshap.plots.beeswarm(shap_values['Y0']['T0'])\n`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kbattocchi",
            "datetime": "May 9, 2022",
            "body": "This is because you're using  when you instantiate the estimator - this is saying that you want to estimate the effect Θ(X) as a sparse linear function of terms of up to degree 3 in the columns of X.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "juandavidgutier",
            "datetime": "May 9, 2022",
            "body": "Hello ,Thanks a lot for the explanation.Best regards",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "juandavidgutier",
            "datetime": "May 9, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/google/sentencepiece/issues/418",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "blaizeberry4",
            "datetime": "Nov 3, 2019",
            "body": "Wondering when version 0.1.84 will be released to PyPI as from the , it appears the most recent version is still 0.1.83 for both  and  on PyPI. From comparing the Github release history and PyPI release history, it seems that the release date has typically been the same, so curious if the lack of accessibility via PyPI is intended. Thanks in advance for the assistance!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "taku910",
            "datetime": "Dec 16, 2019",
            "body": "Released 0.1.85",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "taku910",
            "datetime": "Dec 16, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/facebookresearch/ParlAI/issues/4642",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "daje0601",
            "datetime": "Jul 1, 2022",
            "body": "Hello, An error occurs when installing the bart model while inference to Blenderbot 2.0, which can be searched on the Internet.\nI proceeded as follows. The Internet server downloaded the server from and used it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "daje0601",
            "datetime": "Jul 4, 2022",
            "body": "I solve this problem and close the issu page~",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "daje0601",
            "datetime": "Jul 4, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/Lightning-AI/lightning/issues/4445",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "edenlightning",
            "datetime": "Oct 30, 2020",
            "body": "to promote NeMo/lightning further in a medium article showing accessibility and ease to make audio research headway",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "edenlightning",
            "datetime": "Oct 30, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/RasaHQ/rasa/issues/6765",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "erohmensing",
            "datetime": "Sep 23, 2020",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "erohmensing",
            "datetime": "Sep 23, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "m-vdb",
            "datetime": "Sep 23, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "erohmensing",
            "datetime": "Sep 28, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "erohmensing",
            "datetime": "Sep 28, 2020",
            "body": [],
            "type": "pull",
            "related_issue": "#6822"
        },
        {
            "user_name": "rasabot",
            "datetime": "Oct 3, 2020",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/iNavFlight/inav/issues/2070",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Dronek",
            "datetime": "Sep 3, 2017",
            "body": "I would like to address the fact / the \"issue\" that the frequency of iNav (pre-)releases has been obviously decreasing since the beginning of 2017 (at least) while popularity and support have grown at the same time. To be clear up front, this is  meant to criticize or discuss the development pace of iNav whatsoever.My request is rather to increase the accessibility of experimental features to \"regular\" pilots / users. Many of us were \"brave enough\" to fly alpha releases of iNav on our aircraft in order to test new features. As far as I recall, this had a positive effect of improved contribution from non-developers in terms of functionality-oriented feedback on RCG. Over the last several months discussions on RCG have seemingly reduced towards getting supported hardware to work and configuration.Would it be feasible to make the future releases more granular / frequent, including \"alpha-releases\"?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 4, 2017",
            "body": " good point. It's all comes to balance between having enough time, testing everything and desire to have as many new bells and whistles as possible in new releases.How about dropping the release-candidate cycle and changing the versioning:\nThree-digit versions will be \"unstable releases\"\nTwo digit versions will be \"stable releases\".This way we'll be able to do more frequent releases for those who are brave while still keeping somewhat adequate stable release cycle.Thoughts?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stronnag",
            "datetime": "Sep 4, 2017",
            "body": "I agree witj . We should perhaps try to have no more than a month between minor releases. Now we seem to have settled into a culture of 'just one more big feature' and the cycle stretches. We seem to get a lot of '' requests on RGC. So how about a feature freeze and 1.7.3 in the new 'semi-stable' scheme by the end of the week? Do we have any show-stoppers in the works?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 4, 2017",
            "body": "We don't have any show-stoppers for 1.7.2 apart from few minor features. We can have a semi-stable 1.7.3 by end of the week for sure.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "oleost",
            "datetime": "Sep 5, 2017",
            "body": "How about release every 4 week.So it would go like this:A stable release is done.\nAfter 3 weeks release an release candidate. Freeze new feature pullrequest, only bugfix.\nAfter 4 weeks if no outstanding critical bug, release a new stable version. If still outstanding bugs release an release candiate. It will be stuck in this stage until bug are fixed and a new stable version is out.( So this would mean it can be extended more than 4 weeks, and its noth calender based )This would leave 3 week where one can implement new features, and 1 week to fix any issues. ( Or more if necessary. )Versions numbers doesnt matter, as they will probably live they own life depending on what have been done the last 4 weeks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 5, 2017",
            "body": "At the time of building it's hard to state that this will be the stable version. For me, major/minor number increase could indicate new features, while the micro number could be added for bugfix releases.\nI also vote for short release cycle, 1 month at most. In case problems are detected bugfix release(s) should follow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 5, 2017",
            "body": " what about release-candidates? Releasing RC's makes release-cycle considerably longer.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 5, 2017",
            "body": "For me, additional RC phase could be skipped.\nEach \"new feature\" release (major/minor bump) could be the \"RC\" (regardless if we name them so or not), followed by bugfix release (could be considered \"stable\", but if more problems will occur, we may build another \"stable\" with micro number bumped).\nI'm not a fan of releasing a version named x.x-RC, and then not releasing the \"stable\" shortly (e.g. because there are no problems, or because there are some severe ones that should go to the release). It makes the impression that something is not done to the end (at least for me).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "martinbudden",
            "datetime": "Sep 5, 2017",
            "body": "Personally I think 1 month is too short. A big new feature will take more than one month to develop and so will have to be done in a way that spans more than one release. This will discourage development of \"big\" features.What really is required is to make the latest builds available to \"test pilots\", so they can fly them and test them. I think the way to do this is to set up a \"nightly\" build server that builds development every night and makes it available. Betaflight does this with the Jenkins build server, iNav should do something similar.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 5, 2017",
            "body": "Nightly builds are welcome and this will offload developers from the \"please build this for me to test\" requests.\nHowever I don't get why developing a feature that takes longer than release cycle could discourage one from work...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "martinbudden",
            "datetime": "Sep 5, 2017",
            "body": "Because then you have to spend more time doing code merges/rebases and that is always a pain...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 5, 2017",
            "body": "Nightly builds are good - maybe it's possible to use same server as Betaflight to build INAV's development branch?Regarding big features - in most cases they could be done incrementally, without affecting end users much. But I agree with  - there are features that are big in terms of required coding or testing and also atomic (couldn't be split) - we had a few of those and they were a pain.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 5, 2017",
            "body": "Merges/rebases could be pain, I agree. But... should this block other features from being delivered?\nI see this opposite - having developed a feature, publishing PR and waiting months for this thing to be released into public could be discouraging... Just my humble opinion.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "giacomo892",
            "datetime": "Sep 5, 2017",
            "body": "It could be nice releasing a build when something really needs to be tested on many machines, without a fixed schedule, specifying what changed and what to test.Also inav-configurator needs to be released to match the current build. (no-go with nightly)That can really help the development with logs and bug reports.Letting users to run straight nightly builds without letting them know what's really changed can led to some dangerous situations or a led to a general lack of development awareness.When I build a firmware from development I usually take a look at every commit to understand where the project went and then I'm ready to flight.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 5, 2017",
            "body": " good point. Configurator has to match the build number.The way I see it: For minor releases stick to 1mo release cycle, postpone features that didn't make it to the code yet. Declare feature freeze 1 week before release and only accept bugfixes. These releases could be done straight from development branch. We'll mark these releases as \"pre-release\" on github.Minor releases would essentially be a release-candidates for a bigger release.What do you think?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Dronek",
            "datetime": "Sep 5, 2017",
            "body": "I were about to suggest a similar thing to Jenkins. However, it could actually be more harmful to provide nightlies for systems with navigation / autonomous capabilities compared to BF. If so, nightlies should be rather considered for bench testing only.\nAs for release numbering, e.g. 1.7.x releases could be considered as \"testing release\" and 1.x \"stable\". Testing releases could include experimental features which might not necessarily make it into stable releases, therefore being different to a release candidate. I support the idea of getting rid of release candidates. Instead, x.x.9 could indicate that no new features will be added, being the last testing release in order to find bugs before next stable release.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 5, 2017",
            "body": "I'd point out that simplicity and avoiding doubts should be the principle.\nE.g. bigger number should always indicate newer version.\nIf we can provide stable releases - great. They should be clearly marked then as such. As well as bugfix releases to the stable releases in case the latter require any.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DzikuVx",
            "datetime": "Sep 6, 2017",
            "body": "OK, here are my thoughts on the topic:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Dronek",
            "datetime": "Sep 6, 2017",
            "body": "I agree that defining and sticking to a fixed release cycle might be rather unrealistic for projects like this. I experience roadmap changes and postponed deadlines on a regular basis in a professional context. Not to mention how much overhead project management creates.\nHowever, please keep \"test pilots\" in mind who are also regular dudes and who like to contribute experience from the field in order to help shaping iNav's functionality... in their spare time as well.\nRegarding safety concerns: every user has the choice to enable the \"show unstable versions\" switch in the firmware flasher or to keep this switch off to stay more or less safe. As of now, this switch is quite useless.\nPrividing testing releases in a more frequent (not necessarily fixed) manner could provide a broader feedback on a comparable basis.\nThere is quite a gap between flying a stable release and bench testing development builds.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 7, 2017",
            "body": "My thougs on this:I agree with  that 1mo releases are unrealistic. However, I think we should make an \"unstable\" release approx. each month regardless of new features added/removed.Stable release should be made every 2-3 months. RC phase (assuming feature-freeze) for 4 weeks is probably an overkill - 2 weeks should be enough. Our release schedule will look like this:1.7.3 (unstable) --- (1mo) ---> 1.7.4 (unstable, RC) ---> (1w, some bugs fixed) --->1.7.5 --- (2 days, critical bug found and fixed) ---> 1.7.6 --- (1w, no new bugs) ---> 1.8 (stable) ---> (1mo) ---> 1.8.1 ...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "martinbudden",
            "datetime": "Sep 7, 2017",
            "body": "If we make an \"unstable\" release then it should be clearly named as such. Ie we should not use the number to denote stability. Ie it should be 1.7.4-RC1, or 1.7.4-alpha1 or some such name.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 7, 2017",
            "body": "On GitHub it's possible to mark release as \"Pre release\" and in Configurator Flasher it won't be shown unless you tick the \"Show unstable releases\" switch and unstable releases will be shown as \"(release candidate)\" in the list:I think it would be enough - no need to explicitly add anything to version number.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 7, 2017",
            "body": "The first impression to users could be that 1.8.1 is a bugfix release to 1.8 and not something completely new... However I don't have any constructive proposal how to make it more clear...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "martinbudden",
            "datetime": "Sep 7, 2017",
            "body": "That assumes people are not using local files to flash with.What is the reason not to add something to the version number?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "krzysztofmatula",
            "datetime": "Sep 7, 2017",
            "body": "Maybe: 1.8.dev1 -> 1.8.dev2 -> 1.8 (stable) -> 1.8.1 (potential bugfix) -> 1.9.dev1I'm not sure about this \"dev\"... But anything like \"alpha\", \"beta\" or \"rc\", all have the meaning that the feature set is fully implemented, but not completely stable yet. This is not the case of development unstable snapshots.Maybe \"1.8.pre1\" like \"preview\" or \"pre-release\"...?\nI vote for including the information that build is not stable in the build name.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "martinbudden",
            "datetime": "Sep 7, 2017",
            "body": "That is not correct. Alpha specifically means that the feature set may not be fully implemented. Once feature set is complete the software becomes beta.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 7, 2017",
            "body": "Local file only has 3-digit version number anyhow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 9, 2017",
            "body": "Let's keep the 3-digit versions and use the following conventions:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 9, 2017",
            "body": "Using this convention our next release will be unstable  and after that - stable ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 4, 2017",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 4, 2017",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "digitalentity",
            "datetime": "Sep 9, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/Lightning-AI/lightning/issues/10760",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 25, 2021",
            "body": "See title:\nSimilar issues: , .. I'll keep linking more.This is a highly requested feature from the lightning community. Total training steps is being used by some of the lr schedulers, especially when using transformer models, but since there are a lot of arguments/flags involved for computing it, it's not easy for a user to create one that can work on all the possible edge cases with no-code change. Also we make updates to some of these flags and accessibility to some of its components (for eg. train_datalaoders in v1.5), so there is a possibility that a custom one created by a user might get outdated soon, and one has to write a new one which is only possible if they are well aware with the codebase internals. But we as core-contributors and maintain it with some tests of course.would like to credit  for helping out :)cc \ncc @PyTorchLightning/core-contributors",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tchaton",
            "datetime": "Nov 25, 2021",
            "body": "I have definitely seen this question being asked over and over.So I believe we should do it !    ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Nov 25, 2021",
            "body": "The last time this was discussed, I remember somebody mentioning that the problem with this is that it will not be correct in all circumstances and there's no way for us to know it. A problem of silent correctness.Also, this property cannot be called from anywhere, the attributes used inside need to be computed first.Just pointing out possible problems, not saying we shouldn't add it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Nov 25, 2021",
            "body": "I raised the concern for correctness earlier. However, I do believe it becomes more important to provide such utility because Lightning is getting more complex and harder to understand what's happening under the hood for the regular user. And if users ask for it, we should provide we should provide the most accurate estimation that is possible, then at the same time document what the unknowns are (e.g., accumulation scheduling via callback).Also, which implementation are the users asking for? The number of training steps, i.e., the number of times the training_step will be called / the size of the \"dataloader\", OR the number of optimization steps?\nBoth may be used for learning rate scheduling (?), and if you are converting from PyTorch to Lightning you may have to choose one or the other (?).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SeanNaren",
            "datetime": "Nov 25, 2021",
            "body": "Thanks for picking this up  again, I'm very for this :) I've added both functionality manually into a class for Flash and Transformers and it would be nice to have a single function available through the trainer.Regarding correctness, I think the cases this function breaks down I detailed above. However in my opinion I think the gains outweigh the issue of correctness. It may be worth putting a disclaimer in the docstring that in certain cases we cannot estimate correctly!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 26, 2021",
            "body": "I think except for the dataloader, every other argument will already be there when we initialize the trainer. but yeah dataloader might need a few more things that might not be there for eg. dataset, batch_size... so won't be able to access it inside  (which I guess is okay because we don't expect users to do that inside ).accumulation via callback will still be available right? since we will have it during Trainer init?its number of optimization steps. We can rename the method to  if required.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justusschock",
            "datetime": "Nov 26, 2021",
            "body": "AFAIK for lr scheduling you only use the number of optimization steps, since this is the only thing that may influence the model and the optimizer :) So I think it's fine to go with that and not provide the size of the \"dataloader\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Nov 26, 2021",
            "body": "In that case, after one epoch, ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 26, 2021",
            "body": "I think yes.. it should be. any edge case we missed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariomeissner",
            "datetime": "Nov 28, 2021",
            "body": "I got  when using the function provided here. Running lightning 1.5.3.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 28, 2021",
            "body": " in which hook/method did you call this function?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariomeissner",
            "datetime": "Nov 29, 2021",
            "body": " I call it inside  to set up the scheduler!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Nov 29, 2021",
            "body": "You probably need to change this line\n\nto(the error  pretty much gives it away)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lukasschmit",
            "datetime": "Jan 19, 2022",
            "body": "One thing I noticed with the provided solution, if passing  to the trainer the total number of training steps will end up being negative as  will be -1 as well",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Jan 19, 2022",
            "body": "good catch! I think we should use  instead. Need to check whether it will hold true for TPU and DDP2 or not.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SeanNaren",
            "datetime": "Jan 20, 2022",
            "body": " can we add this as a property now? Definitely would be a great feature I'm interested in :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Jan 20, 2022",
            "body": "yes! just waiting for final approvals.\ncc   ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Jan 20, 2022",
            "body": "Sounds good to me.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ananthsub",
            "datetime": "Jan 20, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Jan 20, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Jan 20, 2022",
            "body": "For the name, I would suggest something like  to differentiate it from count variables that get updated as training progresses",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tchaton",
            "datetime": "Jan 21, 2022",
            "body": "I believe  and  might be less confusing for the users.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 25, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Nov 25, 2021",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "tchaton",
            "datetime": "Nov 25, 2021",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "tchaton",
            "datetime": "Nov 25, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Dec 1, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#10275"
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Jan 24, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#11599"
        },
        {
            "user_name": "carmocca",
            "datetime": "Feb 16, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "rohitgr7",
            "datetime": "Feb 28, 2022",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/Lightning-AI/lightning/issues/9220",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "awaelchli",
            "datetime": "Aug 31, 2021",
            "body": "Provide a means to share data in a stack of loops, for example in the training loop which is made of\nFitLoop > TrainingEpochLoop > TrainingBatchLoop > OptimizerLoop, where \"X > Y\"  denotes Y is a child loop of X.We currently have several attributes like running_loss, skip_backward, etc. which need to be accessed externally or from within subloops.\nThe current way is to reach into these loops like so:If a subloop needs to access a parent loop's attribute like this, then it has several drawbacks.Introduce a dataclass or dictionary:(the attributes here are for illustrative purpose, whether they get included or not is for discussion)This approach may still be too strict and it could be extended to make the data holder dynamic and let each loop register attributes at the time of instantiation.We are currently extracting an optimizer loop in  which is the origin of this RFC. There we are splitting the code path of manual and automatic optimization. In these cases, the updating logic for the attributes \nshould also be different but currently these attributes are owned by the . A data holder would greatly improve the situation there.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Aug 31, 2021",
            "body": "   ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tchaton",
            "datetime": "Aug 31, 2021",
            "body": "Hey ,Quick questions to better understand the background:Best,\nT.C",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Aug 31, 2021",
            "body": "As it stands, all the instances where we need to access shared data are points where the code could be improved, perhaps with the exception of  which is really a global value to track.This is why I don't think this refactor is important right now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ananthsub",
            "datetime": "Sep 1, 2021",
            "body": " i'm curious to hear your ideas on this! needing access to the trainer to walk down the loop hierarchy to access state somewhere is very roundabout right now.i also wonder: how easy is it to read this code now vs having dedicated implementations of training loops that can be subbed into the fit loop to run. it seems like this problem arises only in the training loops compared to the evaluation and prediction loops, given the differences in training modes lightning supports.if we have pre-packaged loop implementations for training, the code/state is all in one place. the somewhat hacky part is needing a loop selector to pick the training loop implementation based on the lightning module properties.vs.with fit_loop.epoch_loop.batch_loop.optimizer_loop this is hopping across 4 different classes to trace through the execution.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Oct 1, 2021",
            "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "carmocca",
            "datetime": "Oct 6, 2021",
            "body": "It's a case-by-case thing where we need to check each occurrence and thing about why it was done that way and how we can group things in self-contained classes.This is a matter of perspective, if you ask me, its as easy as it has ever been, perhaps with the only disadvantage of having to jump across files. The non-fit loops don't suffer from this as much because they don't have nearly as many features and customizations.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Nov 6, 2021",
            "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Aug 31, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Aug 31, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#9191"
        },
        {
            "user_name": "awaelchli",
            "datetime": "Sep 5, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#9317"
        },
        {
            "user_name": "stale",
            "datetime": "Oct 1, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Oct 6, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Nov 6, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "awaelchli",
            "datetime": "Nov 7, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/malmo/issues/431",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "nottug",
            "datetime": "Dec 20, 2016",
            "body": "I'm trying to have this running of my Jetson TX1 (Ubuntu 16.04), but I keep getting a build error when running . This is just the part where it fails, I can post the full output if necessary. I ran it with --debug and --stacktrace for a more verbose output.`14:39:33.080 [INFO] [org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter] Skipping task ':extractNatives' as task onlyIf is false.\n14:39:33.080 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':extractNatives'\n14:39:33.081 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractNatives SKIPPED\n14:39:33.081 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractNatives (Thread[main,5,main]) completed. Took 0.107 secs.\n14:39:33.082 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractUserdev (Thread[main,5,main]) started.\n14:39:33.082 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractUserdev\n14:39:33.082 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Starting to execute task ':extractUserdev'\n14:39:33.083 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: {CACHE_DIR}/{API_GROUP_DIR}/{API_NAME}/{API_VERSION}/userdev\n14:39:33.083 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: /home/ubuntu/.gradle/caches/minecraft/net/minecraftforge/forge/1.8-11.14.3.1543/userdev\n14:39:33.084 [DEBUG] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Determining if task ':extractUserdev' is up-to-date\n14:39:33.089 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.DefaultArtifactDependencyResolver] Resolving configuration ':forgeGradleUserDevPackage'\n14:39:33.091 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'forge' [58d026f73bd51b2d7316bd092d31aca0].\n14:39:33.093 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'MavenRepo' [4471c3b2f5ea2d40ffaa8b3948cdbacd].\n14:39:33.096 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'minecraft' [1a583fa30149d818dec07b24fb2d3986].\n14:39:33.098 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'deobfDeps' [78379ea6164128758068fc0273acf515].\n14:39:33.099 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'TweakerMcRepo' [9bade229c9af7de517a76b0c8c6204b2].\n14:39:33.103 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting configuration com.microsoft.MalmoMod:Minecraft:unspecified(forgeGradleUserDevPackage).\n14:39:33.105 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting dependency com.microsoft.MalmoMod:Minecraft:unspecified(forgeGradleUserDevPackage) -> net.minecraftforge:forge:1.8-11.14.3.1543(forgeGradleUserDevPackage)\n14:39:33.105 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Selecting new module version net.minecraftforge:forge:1.8-11.14.3.1543\n14:39:33.106 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.RepositoryChainComponentMetaDataResolver] Attempting to resolve component for net.minecraftforge:forge:1.8-11.14.3.1543 using repositories [forge, MavenRepo, minecraft, deobfDeps, TweakerMcRepo]\n14:39:33.109 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.parser.IvyXmlModuleDescriptorParser] post 1.3 ivy file: using exact as default matcher\n14:39:33.113 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.CachingModuleComponentRepository] Using cached module metadata for module 'net.minecraftforge:forge:1.8-11.14.3.1543' in 'forge'\n14:39:33.113 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.RepositoryChainComponentMetaDataResolver] Using net.minecraftforge:forge:1.8-11.14.3.1543 from Maven repository 'forge'\n14:39:33.114 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting configuration net.minecraftforge:forge:1.8-11.14.3.1543(default).\n14:39:33.114 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] net.minecraftforge:forge:1.8-11.14.3.1543(default) has no transitive incoming edges. ignoring outgoing edges.\n14:39:33.115 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.oldresult.TransientConfigurationResultsBuilder] Flushing resolved configuration data in Binary store in /tmp/gradle2012945065804096886.bin. Wrote root com.microsoft.MalmoMod:Minecraft:unspecified:forgeGradleUserDevPackage.\n14:39:33.116 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.CachingModuleComponentRepository] Found artifact 'forge-userdev.jar (net.minecraftforge:forge:1.8-11.14.3.1543)' in resolver cache: /home/ubuntu/.gradle/caches/modules-2/files-2.1/net.minecraftforge/forge/1.8-11.14.3.1543/88bee89f68318739e0c0a1936f0f0a563c0cad8d/forge-1.8-11.14.3.1543-userdev.jar\n14:39:33.117 [INFO] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Skipping task ':extractUserdev' as it is up-to-date (took 0.033 secs).\n14:39:33.118 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':extractUserdev'\n14:39:33.118 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractUserdev UP-TO-DATE\n14:39:33.118 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractUserdev (Thread[main,5,main]) completed. Took 0.036 secs.\n14:39:33.120 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :getAssetIndex (Thread[main,5,main]) started.\n14:39:33.120 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :getAssetIndex\n14:39:33.120 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Starting to execute task ':getAssetIndex'\n14:39:33.121 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: \n14:39:33.121 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: \n14:39:33.122 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: {CACHE_DIR}/assets/indexes/{ASSET_INDEX}.json\n14:39:33.122 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: /home/ubuntu/.gradle/caches/minecraft/assets/indexes/1.8.json\n14:39:33.123 [DEBUG] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Determining if task ':getAssetIndex' is up-to-date\n14:39:33.131 [INFO] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Executing task ':getAssetIndex' (up-to-date check took 0.008 secs) due to:\nTask.upToDateWhen is false.\n14:39:33.131 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter] Executing actions for task ':getAssetIndex'.\n14:39:33.541 [DEBUG] [sun.net.www.protocol.http.HttpURLConnection] sun.net.www.MessageHeader@5df920896 pairs: {GET /Minecraft.Download/indexes/1.8.json HTTP/1.1: null}{User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.95 Safari/537.11}{If-None-Match: \"ec766c1642f6db8ab4469933d6aeae31\"}{Host: s3.amazonaws.com}{Accept: text/html, image/gif, image/jpeg, .tar=01;31:.arc=01;31:.taz=01;31:.lz4=01;31:.lzma=01;31:.txz=01;31:.t7z=01;31:.z=01;31:.dz=01;31:.lrz=01;31:.lzo=01;31:.bz2=01;31:.tbz=01;31:.tz=01;31:.rpm=01;31:.war=01;31:.sar=01;31:.alz=01;31:.zoo=01;31:.7z=01;31:.cab=01;31:.jpeg=01;35:.bmp=01;35:.pgm=01;35:.tga=01;35:.xpm=01;35:.tiff=01;35:.svg=01;35:.mng=01;35:.mov=01;35:.mpeg=01;35:.mkv=01;35:.ogm=01;35:.m4v=01;35:.vob=01;35:.nuv=01;35:.asf=01;35:.rmvb=01;35:.avi=01;35:.flv=01;35:.dl=01;35:.xwd=01;35:.cgm=01;35:.ogv=01;35:.aac=00;36:.flac=00;36:.mid=00;36:.mka=00;36:.mpc=00;36:.ra=00;36:.oga=00;36:.spx=00;36:*.xspf=00;36:, SHLVL=2, LESSCLOSE=/usr/bin/lesspipe %s %s, COMPIZ_CONFIG_PROFILE=ubuntu, UPSTART_JOB=unity-settings-daemon, QT_IM_MODULE=ibus, TERM=xterm-256color, XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/usr/share/upstart/xdg:/etc/xdg, LANG=en_US.UTF-8, XDG_SEAT_PATH=/org/freedesktop/DisplayManager/Seat0, GNOME_KEYRING_CONTROL=, XDG_SESSION_TYPE=x11, XDG_SESSION_ID=c1, DISPLAY=:0, GDM_LANG=en_US, XDG_GREETER_DATA_DIR=/var/lib/lightdm-data/ubuntu, UPSTART_EVENTS=started starting, DESKTOP_SESSION=ubuntu, GPG_AGENT_INFO=/home/ubuntu/.gnupg/S.gpg-agent:0:1, USER=ubuntu, QT_ACCESSIBILITY=1, VTE_VERSION=4205, XDG_SEAT=seat0, SSH_AUTH_SOCK=/run/user/1000/keyring/ssh, QT_QPA_PLATFORMTHEME=appmenu-qt5, XDG_RUNTIME_DIR=/run/user/1000, XDG_VTNR=7, HOME=/home/ubuntu, GNOME_KEYRING_PID=}\n14:39:35.892 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING\n14:39:35.896 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'.\n14:39:35.909 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED\n14:39:35.916 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java''\n14:39:35.916 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...\n14:39:37.057 [QUIET] [system.out] [14:39:37] [main/INFO] [GradleStart]: Extra: []\n14:39:37.058 [QUIET] [system.out] [14:39:37] [main/INFO] [GradleStart]: Running with arguments: [--userProperties, {}, --assetsDir, /home/ubuntu/.gradle/caches/minecraft/assets, --assetIndex, 1.8, --accessToken{REDACTED}, --version, 1.8, --tweakClass, net.minecraftforge.fml.common.launcher.FMLTweaker]\n14:39:37.114 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.151 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Using primary tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.152 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.202 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Forge Mod Loader version 11.14.3.1543 for Minecraft 1.8 loading\n14:39:37.204 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Java is OpenJDK 64-Bit Server VM, version 1.8.0_111, running on Linux:aarch64:3.10.96-tegra, installed at /usr/lib/jvm/java-8-openjdk-arm64/jre\n14:39:37.233 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Managed to load a deobfuscated Minecraft name- we are in a deobfuscated environment. Skipping runtime deobfuscation\n14:39:37.248 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Found a command line coremod : com.microsoft.Malmo.OverclockingPlugin\n14:39:37.280 [QUIET] [system.out] [14:39:37] [main/WARN] [FML]: The coremod com.microsoft.Malmo.OverclockingPlugin does not have a MCVersion annotation, it may cause issues with this version of Minecraft\n14:39:37.291 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.293 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n14:39:37.296 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.298 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.299 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:37.497 [QUIET] [system.out] [14:39:37] [main/ERROR] [FML]: The binary patch set is missing. Either you are in a development environment, or things are not going to work!\n14:39:38.785 [QUIET] [system.out] [14:39:38] [main/ERROR] [FML]: FML appears to be missing any signature data. This is not a good thing\n14:39:38.787 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:38.799 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:38.838 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n14:39:39.594 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.TerminalTweaker\n14:39:39.596 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.TerminalTweaker\n14:39:39.622 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Launching wrapped minecraft {net.minecraft.client.main.Main}\n14:39:39.768 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:52]: MALMO: Attempting to transform MinecraftServer\n14:39:39.779 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:147]: MALMO: Found Minecraft, attempting to transform it\n14:39:39.781 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:153]: MALMO: Found Minecraft.runGameLoop() method, attempting to transform it\n14:39:39.783 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:168]: MALMO: Hooked into call to Minecraft.updateDisplay()\n14:39:40.596 [QUIET] [system.out] [14:39:40] [main/ERROR] [LaunchWrapper]: Unable to launch\n14:39:40.596 [QUIET] [system.out] java.lang.reflect.InvocationTargetException\n14:39:40.596 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat net.minecraft.launchwrapper.Launch.launch(Launch.java:135) [launchwrapper-1.12.jar:?]\n14:39:40.597 [QUIET] [system.out] \tat net.minecraft.launchwrapper.Launch.main(Launch.java:28) [launchwrapper-1.12.jar:?]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat net.minecraftforge.gradle.GradleStartCommon.launch(Unknown Source) [start/:?]\n14:39:40.598 [QUIET] [system.out] \tat GradleStart.main(Unknown Source) [start/:?]\n14:39:40.598 [QUIET] [system.out] Caused by: java.lang.UnsatisfiedLinkError: /home/ubuntu/.gradle/caches/minecraft/net/minecraft/natives/1.8/liblwjgl.so: /home/ubuntu/.gradle/caches/minecraft/net/minecraft/natives/1.8/liblwjgl.so: wrong ELF class: ELFCLASS32 (Possible cause: architecture word width mismatch)\n14:39:40.598 [QUIET] [system.out] \tat java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.Runtime.loadLibrary0(Runtime.java:870) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.System.loadLibrary(System.java:1122) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys$1.run(Sys.java:73) ~[lwjgl-2.9.1.jar:?]\n14:39:40.599 [QUIET] [system.out] \tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys.doLoadLibrary(Sys.java:66) ~[lwjgl-2.9.1.jar:?]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys.loadLibrary(Sys.java:95) ~[lwjgl-2.9.1.jar:?]\n14:39:40.600 [QUIET] [system.out] \tat org.lwjgl.Sys.(Sys.java:112) ~[lwjgl-2.9.1.jar:?]\n14:39:40.600 [QUIET] [system.out] \tat net.minecraft.client.Minecraft.getSystemTime(Unknown Source) ~[Minecraft.class:?]\n14:39:40.600 [QUIET] [system.out] \tat net.minecraft.client.main.Main.main(Unknown Source) ~[Main.class:?]\n14:39:40.600 [QUIET] [system.out] \t... 12 more\n14:39:40.602 [ERROR] [system.err] Exception in thread \"main\" 14:39:40.604 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: java.lang.reflect.InvocationTargetException\n14:39:40.606 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n14:39:40.608 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n14:39:40.610 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n14:39:40.623 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.reflect.Method.invoke(Method.java:498)\n14:39:40.625 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraftforge.gradle.GradleStartCommon.launch(Unknown Source)\n14:39:40.626 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat GradleStart.main(Unknown Source)\n14:39:40.628 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: Caused by: net.minecraftforge.fml.relauncher.FMLSecurityManager$ExitTrappedException\n14:39:40.629 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraftforge.fml.relauncher.FMLSecurityManager.checkPermission(Unknown Source)\n14:39:40.635 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.SecurityManager.checkExit(SecurityManager.java:761)\n14:39:40.636 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.Runtime.exit(Runtime.java:107)\n14:39:40.637 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.System.exit(System.java:971)\n14:39:40.642 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraft.launchwrapper.Launch.launch(Launch.java:138)\n14:39:40.643 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraft.launchwrapper.Launch.main(Launch.java:28)\n14:39:40.649 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \t... 6 more\n14:39:41.001 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: FAILED\n14:39:41.003 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with exit value 1 (state: FAILED)\n14:39:41.005 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':runClient'\n14:39:41.005 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :runClient FAILED\n14:39:41.006 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :runClient (Thread[main,5,main]) completed. Took 5.193 secs.\n14:39:41.006 [DEBUG] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] Task worker [Thread[main,5,main]] finished, busy: 12.272 secs, idle: 0.024 secs14:39:41.023 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.030 [ERROR] [org.gradle.BuildExceptionReporter] FAILURE: Build failed with an exception.\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter] * What went wrong:\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter] Execution failed for task ':runClient'.\n14:39:41.032 [ERROR] [org.gradle.BuildExceptionReporter] > Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with non-zero exit value 1\n14:39:41.033 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.033 [ERROR] [org.gradle.BuildExceptionReporter] * Exception is:\n14:39:41.035 [ERROR] [org.gradle.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':runClient'.\n14:39:41.036 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69)\n14:39:41.037 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46)\n14:39:41.038 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35)\n14:39:41.038 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)\n14:39:41.045 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)\n14:39:41.046 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52)\n14:39:41.047 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)\n14:39:41.048 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53)\n14:39:41.049 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n14:39:41.049 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203)\n14:39:41.050 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185)\n14:39:41.051 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:62)\n14:39:41.052 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50)\n14:39:41.053 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25)\n14:39:41.054 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110)\n14:39:41.055 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)\n14:39:41.056 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)\n14:39:41.056 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)\n14:39:41.057 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)\n14:39:41.058 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)\n14:39:41.059 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)\n14:39:41.059 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)\n14:39:41.060 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:155)\n14:39:41.061 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.Factories$1.create(Factories.java:22)\n14:39:41.062 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n14:39:41.062 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)\n14:39:41.063 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:152)\n14:39:41.064 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33)\n14:39:41.064 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:100)\n14:39:41.065 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:94)\n14:39:41.066 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n14:39:41.067 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)\n14:39:41.068 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:94)\n14:39:41.069 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83)\n14:39:41.070 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47)\n14:39:41.076 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51)\n14:39:41.077 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28)\n14:39:41.077 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43)\n14:39:41.078 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170)\n14:39:41.079 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237)\n14:39:41.079 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210)\n14:39:41.080 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35)\n14:39:41.080 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24)\n14:39:41.082 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206)\n14:39:41.085 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.Main.doAction(Main.java:33)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.GradleMain.main(GradleMain.java:23)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:127)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61)\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with non-zero exit value 1\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367)\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.process.internal.DefaultJavaExecAction.execute(DefaultJavaExecAction.java:31)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.tasks.JavaExec.exec(JavaExec.java:60)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:226)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:219)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:208)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter] \t... 60 more\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.092 [LIFECYCLE] [org.gradle.BuildResultLogger]\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger] BUILD FAILED\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger]\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger] Total time: 22.741 secs\n14:39:41.095 [DEBUG] [org.gradle.api.internal.tasks.compile.daemon.CompilerDaemonManager] Stopping 0 compiler daemon(s).\n14:39:41.096 [INFO] [org.gradle.api.internal.tasks.compile.daemon.CompilerDaemonManager] Stopped 0 compiler daemon(s).\n14:39:41.099 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on cp_proj class cache for build file '/home/ubuntu/Malmo/Minecraft/build.gradle' (/home/ubuntu/.gradle/caches/2.7/scripts/build_28dl6d7gt773pezkarluvxmug/cp_proj).\n14:39:41.099 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on proj class cache for build file '/home/ubuntu/Malmo/Minecraft/build.gradle' (/home/ubuntu/.gradle/caches/2.7/scripts/build_28dl6d7gt773pezkarluvxmug/proj).\n14:39:41.101 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache plugin-use-metadata.bin (/home/ubuntu/.gradle/caches/2.7/plugin-resolution/plugin-use-metadata.bin)\n14:39:41.101 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on Plugin Resolution Cache (/home/ubuntu/.gradle/caches/2.7/plugin-resolution).\n14:39:41.104 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache module-metadata.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/module-metadata.bin)\n14:39:41.105 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache artifact-at-repository.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/artifact-at-repository.bin)\n14:39:41.105 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache artifact-at-url.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/artifact-at-url.bin)\n14:39:41.106 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on artifact cache (/home/ubuntu/.gradle/caches/modules-2).\n14:39:41.120 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache outputFileStates.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/outputFileStates.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache fileSnapshots.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/fileSnapshots.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache fileHashes.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/fileHashes.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache taskArtifacts.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/taskArtifacts.bin)\n14:39:41.122 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on task history cache (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts).\n14:39:41.123 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.CachedStoreFactory] Resolved configuration cache closed. Cache reads: 0, disk reads: 0 (avg: 0.0 secs, total: 0.0 secs)\n14:39:41.123 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.CachedStoreFactory] Resolution result cache closed. Cache reads: 0, disk reads: 0 (avg: 0.0 secs, total: 0.0 secs)\n14:39:41.124 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.ResolutionResultsStoreFactory] Deleted 2 resolution results binary files in 0.002 secs\n14:39:41.124 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] In-memory dependency metadata cache closed. Repos cached: 44, cache instances: 6, modules served from cache: 4, artifacts: 2`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "katja-hofmann",
            "datetime": "Dec 20, 2016",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "nottug",
            "datetime": "Dec 20, 2016",
            "body": "Yeah I have 3 different Java 8s installed but I've tried all of them. If I attempt to run the others, I get an immediate build failure. This is the",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nottug",
            "datetime": "Dec 20, 2016",
            "body": "default 64 bit sdk. The issue might be the Tegra architecture not being recognized, but I'm not sure.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mfuntowicz",
            "datetime": "Jan 9, 2017",
            "body": "Hi @traw1234, are you trying to run Malmo experiment through SSH connection on the Jetson ?\nI suspect that there is no desktop environment initialized on the device, so Minecraft cannot start.Can you try starting Minecraft on a Desktop environment and launching only the agent on embedded device ?Morgan",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DaveyBiggers",
            "datetime": "Jan 25, 2017",
            "body": "Am closing this due to inactivity; @traw1234 please re-open and provide more details if this is still a problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nottug",
            "datetime": "Dec 20, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "tambetm/minecraft-py#1"
        },
        {
            "user_name": "DaveyBiggers",
            "datetime": "Jan 25, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/5405",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "fresejoerg",
            "datetime": "May 5, 2020",
            "body": "I am using the CLI training interface to train a custom tagger and parser. The dependency labels are a custom set of semantic labels. The training data is converted from conll format.\nI am not using the --base-model argument, so I believe I'm starting from a blank model. Also, the output directory does not exist prior to training.\nAfter training, the model sometimes outputs an unexpected dependency tag ('dep') which is not part of my training data.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 6, 2020",
            "body": "I'm not entirely sure, but I think this may be a bug related to using  as a label because the parser (which is used underneath for both  and ) has some special cases related to  to process labels like  for NER.Do you have the same results if you replace  with a different dummy label like ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "May 6, 2020",
            "body": "Thanks! I replaced  with  and haven't observed the issue again, so I think your intuition was correct.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 6, 2020",
            "body": "Glad to hear it! I have to look into the details about how complicated it might be to fix the parser directly (since you really should be able to use any string label), but at least for now we should show an error when training data is loaded with  in a dependency label to avoid this problem.A side note: unless you are only processing single sentences with your model when it's in use, I would recommend against using . If you use this option, the parser won't learn to split sentences because it also splits the training data up into individual sentences while training and the parser never sees any sentence boundaries.If you want to train with gold tokenization, then just remove the  texts from your training data (if you have them) and it will learn from the gold tokens without splitting up documents. (Gold tokenization and single-sentence training ended up grouped together in this option for specific kinds of parser evaluations, when separate options would been better.)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "May 6, 2020",
            "body": "Thanks for the pointer regarding . I don't have the  string in my data and the inputs are relatively short, non-grammatical fragments (search queries, in particular), so I'm not concerned about sentence splitting at the moment. But I'll do some experiments to see how omitting  affects LAS.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "May 7, 2020",
            "body": "The original issue just re-emerged for me. A freshly trained version of my model is predicting  as a label. I verified that no  labels are in my training or dev dataset. So it appears that the root cause is unrelated to special handling for this character.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 14, 2020",
            "body": "Hmm, what is  for your model? Are there any warnings/errors for your data with ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "May 14, 2020",
            "body": "this is what I get from :which is a subset of all training labels (with the exception of , which is not in my training data.Here's the output from debugging the training data:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 29, 2020",
            "body": "I noticed that one of our example scripts uses  as a label in for a similar case without issues, so it must be something else.I don't understand why  would show 21 labels but you don't end up with all of them in the model labels. How many training docs do you have? There was a minor issue where the parser peeked at the first 1000 examples instead of examining all of them when adding labels. This peeking is still in v2.2.4, but will be removed in v2.3.0 (to be released soon, change in ).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "May 29, 2020",
            "body": "My sense is that the number of training examples is related to this issue, but probably not to the one you're referencing. The  output above is for a training data set with 260 examples. I've since added some examples and have re-trained the model with 344 training examples, but still well below 1,000.I noticed that there were previously two labels ( and ) which didn't make it into the model but also did not receive a  warning. These two labels are now in the model.Unfortunately, adding more training data didn't prevent  from showing up in the model.\nLooking at specific examples where the model actually predicts , I noticed that that occurs in cases where either the correct label would have been one of those  with a  warning or in edge cases where even a human expert can't confidently assign the correct label.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "Jun 2, 2020",
            "body": "I think I figured out what's going on. There's a minimum label frequency parameter with a default value of 30, which explains why some labels are missing in the model. You can lower this by passing the parameter  to .  uses a cutoff of 20 instead of 30, which is confusing here.The  label is coming from here as a backoff if there's no other good move:spacy v2 has a number of parameters and defaults that are spread across the code and hard to track down. The rewrite of thinc for spacy v3 uses a much better config system where models can be saved with a complete config file and there shouldn't be as many frustrating issues with hidden defaults.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "Jun 2, 2020",
            "body": "Thanks for sticking with this. I'm closing this issue as resolved.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Nov 5, 2021",
            "body": "This thread has been automatically locked since there  has not been any recent activity after it was closed.  Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "svlandeg",
            "datetime": "May 5, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 6, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "no-response",
            "datetime": "May 6, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 6, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "adrianeboyd",
            "datetime": "May 6, 2020",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "fresejoerg",
            "datetime": "Jun 2, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Nov 5, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/Project-MONAI/MONAI/issues/3482",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 13, 2021",
            "body": "\nThanks for the interesting technical discussion with     , as we still have many unclear requirements and unknown use cases, we plan to develop the model package feature step by step, May adjust the design based on feedback during the development.For the initial step, the core team aligned to develop a small but typical example for  first, it will use JSON config files to define environments, components and workflow, save the config and model into TorchScript model. then other projects can easily reconstruct the exact same python program and parameters to reproduce the inference. When the small MVP is ready, will share and discuss within the team for the next steps.I will try to implement the MVP referring to some existing solutions, like NVIDIA Clara MMAR, ignite online package, etc. Basic task steps:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dbericat",
            "datetime": "Dec 13, 2021",
            "body": " this is the current PR to expand model metadata.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 14, 2021",
            "body": "Hi     ,I updated the ticket description to make it more clear.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wyli",
            "datetime": "Dec 14, 2021",
            "body": "We've discussed that the MVP should focus on the minimal structure, including model weights, basic environment/system info, maintaining changelog/versioning, how to ensure accessibility for both human and machine.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 14, 2021",
            "body": "Hi  ,Sure, I added these items to the MVP task list.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dbericat",
            "datetime": "Dec 14, 2021",
            "body": "       thoughts on proposed MVP?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Dec 14, 2021",
            "body": "We should discuss a clear definition of requirements and objectives. We want to define a format of a single file or multiples files which contains the model weights at least with secondary information describing how to use it for various use cases. This will allow a human or a program to determine what sort of model it is, how to use the model, and what tasks to use it for. For our MVP we want to consider the starting position of what the model weight storage and metadata storage would look like and if it would achieve that objective to some degree.The base level set of requirements I would suggest are:One use case for this information is a human user looking into how the model is used in a particular task. They would want a clear idea of what inputs are expected and what the outputs mean. Whatever format this information is can be either easily read by a human or easily converted into a convenient format using included tools.A second use case is a deployment environment which automatically constructs whatever infrastructure is needed around a model to present it through a standard interface. This would require generating transform sequences automatically to pre- and post-process data, and load the model through some script defining the workflow. This would used by MONAI Deploy to automatically generate a MAP from the package, or another script automatically create a Docker image to serve the model as a command line tool or through Flask, or another script to interface with existing hosting services to upload the model and whatever other information is needed.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 15, 2021",
            "body": "Hi  ,Thanks very much for your detailed description.\nI will try to prepare a draft model package for discussion according to your summary and our basic ideas, and then develop the necessary features to support it.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wyli",
            "datetime": "Dec 16, 2021",
            "body": "I'd strongly recommend that the MVP should support the essential features for fast prototyping of model packages. The following uses the popular  package as an example:These are mainly to ensure a good flexibility and extensibility of the packaging.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 16, 2021",
            "body": "Hi  ,Thanks for your great suggestions.\nThese are very good references for config parsing logic in steps 3 and 4 of this ticket, we definitely need similar features for flexibility and extensibility.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 16, 2021",
            "body": "Hi     ,I tried to prepare a draft model package structure for the  of this ticket in the tutorial PR.\n\nCould you please help take a look and share some feedback for the model package structure?\n(1) I didn't implement any config-parsing related logic so far, I think that's later steps.\n(2) I am not very sure to put some description strings in  or , so I put in both sides, would like to see your ideas.Thanks in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Dec 23, 2021",
            "body": "To recap where we are with existing issues/PRs:Related issues:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 23, 2021",
            "body": "Hi  ,Thanks for your great summary!\nThere is another related PR: .Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wyli",
            "datetime": "Jan 10, 2022",
            "body": "for the record, we could consider this cache dataset config vs content Integrity issue .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Jan 17, 2022",
            "body": "Hi    ,Another thing I want to mention is that: we can also consider to add the package format verification with predefined , for example: check the JSON syntax, check necessary fields in the meta data, check essential folders, etc.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Jan 17, 2022",
            "body": "We'll certainly want to check the format of specific fields in the json data so that they correctly adhere to a standard way of describing transforms and inference so that code can be generated from them. With the other data that can be added it would be preferrable that this can be as free-form as people like, so I would expect standardized elements in the metadat adhering to our format mixed in with ones that are application-specific in any format.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Jan 17, 2022",
            "body": " sounds good to me! We can do that when we finalized the first MVP MMAR example.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MMelQin",
            "datetime": "Jan 21, 2022",
            "body": "As for the machine consumer of the model package, specifically TrochScript, we do have a specific one, . Triton can automatically generate runtime config for all the backends it supports, except PyTorch. Triton team had logged an issue for PyTorch, though based on the discussions we've already had here, the requested metadata are covered in the MONAI model package. Here is the ,",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MMelQin",
            "datetime": "Jan 21, 2022",
            "body": "For deployment of AI apps/models in clinical environment, there are a number of persona and user stories.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Jan 21, 2022",
            "body": "Hi  ,Thanks very much for your detailed feedback!\nThese are important information for metadata. It's a long way to finalize the metadata content and format.\nLet's try to provide a minimum metadata for the first version, then add or adjust according to users' feedback.\nWe also definitely should define the metadata , , , etc.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 19, 2022",
            "body": "Another related PR relating to the configuration parsing: .Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 21, 2022",
            "body": "Related PR for common training, evaluation and inference:  .Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 21, 2022",
            "body": "As we are still developing the  in PR , I compared it with the Hydra : \nThere are several interesting features of it we still not support:I think we can try to support 3 with enhanced idea: use the \"@\" mark to define a placeholder, then parser gives the value.\nAnd we can try to support 4 to make the base config similar to \"base class with abstractmethod\" that must be overridden.For other features we may consider later.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Feb 21, 2022",
            "body": "I have a PR now on the specification document  which should have been a draft PR but oh well. It's very minimal compared to what  prototyped  and lacks details about intent, use cases, design expectations, etc.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wyli",
            "datetime": "Feb 25, 2022",
            "body": "I think  is now intuitive and flexible after a few rounds of refactoring. We may want to provide a system-wide flag to optionally disable  because it is too powerful and unsafe..",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 28, 2022",
            "body": "I put all the links of related PRs to the task steps in the above ticket description.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Mar 7, 2022",
            "body": "Related PR for inference example bundle: .Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wyli",
            "datetime": "Mar 20, 2022",
            "body": "follow-up from the dev meeting:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Mar 21, 2022",
            "body": "Revising this tutorial to use our bundle would be a good exemplar: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Mar 22, 2022",
            "body": "After sharing with the internal NVIDIA Clara team, I put 2 missing features in the task feature.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Mar 30, 2022",
            "body": "Feedback from MONAI deploy team:Welcome more testing and feedback later.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Apr 4, 2022",
            "body": "Most of the tasks are completed, I created several new tickets to track the left items, and let's close this big first ticket now.Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 13, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 13, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "Project-MONAI/monai-deploy-app-sdk#212"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 15, 2021",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "ericspod",
            "datetime": "Dec 15, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 16, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "Project-MONAI/tutorials#486"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Dec 20, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#3518"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Jan 6, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3593"
        },
        {
            "user_name": "gigony",
            "datetime": "Jan 13, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "Project-MONAI/monai-deploy-app-sdk#213"
        },
        {
            "user_name": "dbericat",
            "datetime": "Jan 13, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 19, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3822"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 21, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3832"
        },
        {
            "user_name": "ericspod",
            "datetime": "Feb 21, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3834"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Feb 28, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3865"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Mar 2, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "google/python-fire#356"
        },
        {
            "user_name": "wyli",
            "datetime": "Mar 11, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#3927"
        },
        {
            "user_name": "wyli",
            "datetime": "Mar 17, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#3966"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#3974"
        },
        {
            "user_name": "wyli",
            "datetime": "Mar 22, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3982"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Mar 24, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3994"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#4029"
        },
        {
            "user_name": "Nic-Ma",
            "datetime": "Apr 4, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/4046",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "tomtel",
            "datetime": "Jul 30, 2019",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "BreakBB",
            "datetime": "Jul 30, 2019",
            "body": "It would be great if you could add some information to your post. Moreover you should put your environment information into the place of the template instead of the title.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tomtel",
            "datetime": "Jul 30, 2019",
            "body": "How to reproduce the problem\nMy error message:(base) user0001@LINUX0001:~$ pip install -U spacy[cuda]\nCollecting spacy[cuda]\nUsing cached \nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy[cuda])\nUsing cached \nCollecting cymem<2.1.0,>=2.0.2 (from spacy[cuda])\nUsing cached \nCollecting thinc<7.1.0,>=7.0.8 (from spacy[cuda])\nUsing cached \nRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in ./anaconda3/lib/python3.7/site-packages (from spacy[cuda]) (1.16.4)\nCollecting plac<1.0.0,>=0.9.6 (from spacy[cuda])\nUsing cached \nCollecting preshed<2.1.0,>=2.0.1 (from spacy[cuda])\nUsing cached \nRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.7/site-packages (from spacy[cuda]) (2.22.0)\nCollecting wasabi<1.1.0,>=0.2.0 (from spacy[cuda])\nUsing cached \nCollecting blis<0.3.0,>=0.2.2 (from spacy[cuda])\nUsing cached \nCollecting srsly<1.1.0,>=0.0.6 (from spacy[cuda])\nUsing cached \nCollecting thinc-gpu-ops<0.1.0,>=0.0.1; extra == \"cuda\" (from spacy[cuda])\nUsing cached \nCollecting cupy>=5.0.0b4; extra == \"cuda\" (from spacy[cuda])\nUsing cached \nERROR: Command errored out with exit status 1:\ncommand: /home/user0001/anaconda3/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6ku2puxn/cupy/setup.py'\"'\"'; ='\"'\"'/tmp/pip-install-6ku2puxn/cupy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)();code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, , '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\ncwd: /tmp/pip-install-6ku2puxn/cupy/\nComplete output (43 lines):\nOptions: {'package_name': 'cupy', 'long_description': None, 'wheel_libs': [], 'wheel_includes': [], 'no_rpath': False, 'profile': False, 'linetrace': False, 'annotate': False, 'no_cuda': False}ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n(base) user0001@LINUX0001:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tomtel",
            "datetime": "Jul 30, 2019",
            "body": "My Environment\nOperating System:\nx86_64\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=18.04\nDISTRIB_CODENAME=bionic\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.2 LTS\"\nNAME=\"Ubuntu\"\nVERSION=\"18.04.2 LTS (Bionic Beaver)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 18.04.2 LTS\"\nVERSION_ID=\"18.04\"\nHOME_URL=\"\"\nSUPPORT_URL=\"\"\nBUG_REPORT_URL=\"\"\nPRIVACY_POLICY_URL=\"\"\nVERSION_CODENAME=bionic\nUBUNTU_CODENAME=bionicPython Version Used:\nPython 3.7.3spaCy Version Used:\nspaCy is not installedEnvironment Information:\n(base) user0001@LINUX0001:~$ printenv\nCLUTTER_IM_MODULE=xim\nCONDA_SHLVL=1\nLD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tgz=01;31:.arj=01;31:.lha=01;31:.lzh=01;31:.tlz=01;31:.tzo=01;31:.zip=01;31:.Z=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.tzst=01;31:.bz=01;31:.tbz2=01;31:.deb=01;31:.jar=01;31:.ear=01;31:.rar=01;31:.ace=01;31:.cpio=01;31:.rz=01;31:.wim=01;31:.dwm=01;31:.jpg=01;35:.mjpg=01;35:.gif=01;35:.pbm=01;35:.ppm=01;35:.xbm=01;35:.tif=01;35:.png=01;35:.svgz=01;35:.pcx=01;35:.mpg=01;35:.m2v=01;35:.webm=01;35:.mp4=01;35:.mp4v=01;35:.qt=01;35:.wmv=01;35:.rm=01;35:.flc=01;35:.fli=01;35:.gl=01;35:.xcf=01;35:.yuv=01;35:.emf=01;35:.ogx=01;35:.au=00;36:.m4a=00;36:.midi=00;36:.mp3=00;36:.ogg=00;36:.wav=00;36:.opus=00;36:.xspf=00;36:\nCONDA_EXE=/home/user0001/anaconda3/bin/conda\nLESSCLOSE=/usr/bin/lesspipe %s %s\nXDG_MENU_PREFIX=gnome-\nLANG=de_DE.UTF-8\nDISPLAY=:1\nGNOME_SHELL_SESSION_MODE=ubuntu\nCOLORTERM=truecolor\nCUDA_PATH=/usr/local/cuda-10.1\nUSERNAME=user0001\nCONDA_PREFIX=/home/user0001/anaconda3\nXDG_VTNR=2\nSSH_AUTH_SOCK=/run/user/1000/keyring/ssh\n_CE_M=\nXDG_SESSION_ID=3\nUSER=user0001\nDESKTOP_SESSION=ubuntu\nQT4_IM_MODULE=xim\nTEXTDOMAINDIR=/usr/share/locale/\nGNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/d1324337_0a68_424d_b658_e7e868284cbf\nPWD=/home/user0001\nHOME=/home/user0001\nCONDA_PYTHON_EXE=/home/user0001/anaconda3/bin/python\nTEXTDOMAIN=im-config\nSSH_AGENT_PID=1885\nQT_ACCESSIBILITY=1\nXDG_SESSION_TYPE=x11\nXDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share:/usr/share:/var/lib/snapd/desktop\n_CE_CONDA=\nXDG_SESSION_DESKTOP=ubuntu\nGJS_DEBUG_OUTPUT=stderr\nCONDA_PROMPT_MODIFIER=(base)\nGTK_MODULES=gail:atk-bridge\nWINDOWPATH=2\nTERM=xterm-256color\nSHELL=/bin/bash\nVTE_VERSION=5202\nQT_IM_MODULE=xim\nXMODIFIERS==ibus\nIM_CONFIG_PHASE=2\nXDG_CURRENT_DESKTOP=ubuntu:GNOME\nGPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1\nGNOME_TERMINAL_SERVICE=:1.114\nXDG_SEAT=seat0\nSHLVL=1\nGDMSESSION=ubuntu\nGNOME_DESKTOP_SESSION_ID=this-is-deprecated\nLOGNAME=user0001\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\nXDG_RUNTIME_DIR=/run/user/1000\nXAUTHORITY=/run/user/1000/gdm/Xauthority\nXDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg\nPATH=/usr/local/cuda-10.1/bin:/home/user0001/anaconda3/bin:/home/user0001/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nCONDA_DEFAULT_ENV=base\nGJS_DEBUG_TOPICS=JS ERROR;JS LOG\nSESSION_MANAGER=local/LINUX0001:@/tmp/.ICE-unix/1790,unix/LINUX0001:/tmp/.ICE-unix/1790\nLESSOPEN=| /usr/bin/lesspipe %s\nGTK_IM_MODULE=ibus\n_=/usr/bin/printenv",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jul 30, 2019",
            "body": "This is likely not a problem with spaCy. Check out your error log:Looks like cupy (which spaCy uses) can't find CUDA. Once you've fixed that, spaCy should also work as expected.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Aug 29, 2019",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tomtel",
            "datetime": "Jul 30, 2019",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jul 30, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "no-response",
            "datetime": "Jul 30, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jul 30, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Jul 30, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Aug 29, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/1471",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "k-nut",
            "datetime": "Oct 28, 2017",
            "body": "I was reading the  and could not understand what the last column means. The header suggests that this is a boolean value but there are three options (red/green/gray). Does green mean that it does need a model or not? It would also be cool to avoid using only color as a means of encoding this information because it makes it difficult for colourblind red/green-blind people to judge the information.I would propose to use symbols here or at least add a legend or tooltips on hover.If you can explain the three colors to me I am more than happy to send a pull request or some mockups for how this could be displayed differently. ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 28, 2017",
            "body": "Thanks for the feedback – this is super valuable! The original idea here was green = yes (based on model predictions, so needs model data), red = no, grey = neutral / \"it depends\" (which, admittedly, is a bit vague – but both serialization and training don't actually require a model to be loaded in order to work - like dependency parsing for example – but they can be used in combination with one). Either way, I totally see how this is confusing.Honestly, the more I look at this table, the more I'm like \"wtf was I thinking when I built this\", haha. It's also an accessibility nightmare btw – important information expressed with only icons and no other distinction except for colour (whyyyy?), and red/green/grey out of all colours. And  okay, I did add ARIA labels, but they were bad, so screen readers can't parse this mess either. So yeah, sorry about that. No idea why I did this.As a solution, how about replacing the icons with \"yes\" and \"no\", and maybe adding an additional note to the \"depends\" features? The same also applies to the feature comparison . And while I'm at it, I'll also add some proper ARIA attributes to the before/after code examples (e.g.  – it's more obvious visually, but still completely inaccessible).I'm happy to take care of this, since it'll require some more fixes and decisions on the front-end – but let me know if this makes sense, and if you have any other suggestions! I've been playing around with this, and the \"needs model\" thing is slightly trickier than I thought. I do think it's important to get this information across as early as possible, since it's one of the main sources of confusion for beginners. It's not immediately intuitive which functionality is built into spaCy, and what needs a statistical model. On the other hand, it's not always so black and white. The  in English uses the part-of-speech tags to predict a token's lemma – however, many languages also come with a lookup table that doesn't require a model at all. But this distinction is a little complex for one of the very first sections of the beginners guide... ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 28, 2017",
            "body": "Current work in progress: now with distinctive icons and label. I also think that a better policy for categorising the features that \"need a model\" is to interpret \"need\" as \"absolutely need\", i.e. \"can't do without\" (at least, not by default in spaCy).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "emste",
            "datetime": "Oct 30, 2017",
            "body": "Thanks for improving the website / documentation!\nMaybe it's worth adding a sentence above the table to describe the \"needs model\" column? If I didn't know SpaCy or NLP tools in general, I might wonder what it exactly means. Does it mean SpaCy is not feature complete? The red mark looks like something is \"missing\", \"not yet supported\" or \"not available\". In conjunction with \"needs model\" it sounds like we still wait for somebody to train a model for tokenization.Maybe something like this:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 31, 2017",
            "body": " Good point, thanks! I hadn't really thought about the problems with the positive/negative connotation of the icons in this context. We definitely don't want new users to get the impression that tokenization not requiring a statistical model is somehow \"bad\" or \"worse\". (In fact, the main point of the table should be to demystify the processing pipeline and move away from just reducing it to  – which I feel like hasn't particularly come across well in the past.)Maybe we should just remove the \"needs model\" column and replace it with a column that describes how spaCy \"does it\" and where the information comes from? For example, something like:I really like your text suggestion btw. We could use that for the intro, plus maybe the text that's currently in the aside on the right (where it kinda gets lost at the moment – asides should be for additional notes, not for critical information).We could still have an aside there that goes into more details about what exactly a \"statistical model\" is, what it does, where to find the models and install them etc.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "k-nut",
            "datetime": "Oct 31, 2017",
            "body": "I agree with the problem of the positive/negative connotations of the icons and think replacing them would be a good idea.  I like your idea of removing the column entirely. Thinking about it now: The main goal of this table is to show the reader what they can do with spacy, right? So it might not even be necessary to explain the way in which spacy does this because all the readers want to know is: What can I do with this tool? The actual implementation details could be deferred further down to the individual sections.\nOn the other hand it might be interesting to learn something about the internals in the summary table already for people that have some more knowledge of NLP and would like to understand, how spacy implements certain features...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 31, 2017",
            "body": " Yes, I think you're right! The main reason I wanted to cover the models in this section was to show what's statistical and what isn't – and to get this across as early as possible.This may seem obvious to users who already have some knowledge of NLP, but to a beginner, this isn't necessarily intuitive. (And that whole \"magical AI\" narrative that's often used when talking about the technology doesn't help, either.)For example: How does spaCy assign part-of-speech tags or named entities? Are there rules? Does the code in the library do all the magic? And what does it mean if spaCy \"gets it wrong\"? I also think that the concept of loading and installing models – and choosing between the different options – becomes much more obvious once you view the models as \"binary data packs that enable spaCy to make predictions\".In terms of the 101 guide, another option could be to add this as a separate section underneath the features table and cover the components of the statistical models. Or just give a quick overview of what can be included – like, binary weights for the tagger, parser and entity recognizer, lexical entries in the vocabulary, word vectors etc.If we manage to get this across right, I hope that what a new users takes away from this will be: \"Ah, cool, so I can use spaCy to find entities in my text. All I need to do is plug in a binary data pack spaCy can use to make the predictions. And if I want different results for a different language or domain, I need to plug in a different data pack – or create my own!\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "k-nut",
            "datetime": "Nov 1, 2017",
            "body": "I really like this new approach os separating the two issues! ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Nov 1, 2017",
            "body": "Yay – it's all on  now and will be uploaded as soon as  is out. So I'm closing this issue for now. Thanks again for your feedback!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "May 8, 2018",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 28, 2017",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 28, 2017",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 28, 2017",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Oct 31, 2017",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Nov 1, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "May 8, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/explosion/spaCy/issues/3356",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 5, 2019",
            "body": "I found a bug where tokenization is completely not working with version 2.1.0a10 on python 2.7. I have reproduced this on three of my machines.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 6, 2019",
            "body": "Hmm that's very confusing. I don't see how our tests could be passing if this is the case. I don't suppose you're somehow getting a narrow unicode build of Python?Another thing to check is that your  line is actually running the version of pip you expect. But then if it werent't I don't see how spaCy would be in your environment. Hmm.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 6, 2019",
            "body": "It's quite easy to reproduce for me. The following is another run. I also checked my python and pip and they are correct.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 6, 2019",
            "body": "Okay, thanks! Definitely seems bad --- will look into it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "svlandeg",
            "datetime": "Mar 7, 2019",
            "body": "I can't reproduce either :(\n : what happens if you use the default English model instead?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "handsomezebra",
            "datetime": "Mar 7, 2019",
            "body": "Using the default English model the result is correct. But using spacy.load('en_core_web_sm') the result is the same. I am guessing maybe there are conflicts with previous installed spacy models?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "svlandeg",
            "datetime": "Mar 7, 2019",
            "body": "  : Any chance you could run this entire sentence through the  pipeline?\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 7, 2019",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 7, 2019",
            "body": "Thanks for your patience on this, this is definitely mysterious!And yeah, maybe what you're loading as  here isn't actually the correct model package? Could you print  and see what that returns?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 7, 2019",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 19, 2019",
            "body": "I still got the same issue even using the stable release of 2.1.0",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 20, 2019",
            "body": "I suspect there might actually be a connection to , especially since you and  both had the same problems  we weren't able to reproduce either of them yet on any of our test machines and CI... So there must be  configuration here that we might be missing? I'm not sure, this is so confusing ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "The tests are green in the exact same environment as produces the tokenization issues:(.env) [  6:30PM ]  [ jfelectron@madmax:~/Documents/Repos/spaCy/spacy/tests(master) ]\n$ pytest -svv\n==================================================================================================================================================================================== test session starts =====================================================================================================================================================================================\nplatform linux2 -- Python 2.7.15+, pytest-4.0.2, py-1.7.0, pluggy-0.8.0 -- /usr/bin/python\ncachedir: .pytest_cache\nmetadata: {'Python': '2.7.15+', 'Platform': 'Linux-4.18.0-10-generic-x86_64-with-Ubuntu-18.10-cosmic', 'Packages': {'py': '1.7.0', 'pytest': '4.0.2', 'pluggy': '0.8.0'}, 'Plugins': {'html': '1.19.0', 'xdist': '1.15.0', 'timeout': '1.3.3', 'metadata': '1.7.0'}}\nrootdir: /home/jfelectron/Documents/Repos/spaCy, inifile:\nplugins: xdist-1.15.0, timeout-1.3.3, metadata-1.7.0, html-1.19.0\ncollected 2123 items\n...\n1482 passed, 585 skipped, 56 xfailed in 62.04 seconds",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "In [2]: import spacyIn [3]: spacy.\nOut[3]: '2.1.1'In [4]: spacy_nlp = spacy.load('en_core_web_sm')In [5]: [d.text for d in spacy_nlp(u'I have a dog named spot')]\nOut[5]:\n[u'I',\nu'h',\nu'a',\nu'v',\nu'e',\nu'a',\nu'd',\nu'o',\nu'g',\nu'n',\nu'a',\nu'm',\nu'e',\nu'd',\nu's',\nu'p',\nu'o',\nu't']",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 20, 2019",
            "body": "Thanks for running more tests! This is consistent with what's been reported before. The problem seems to occur when deserializing the tokenization rules from the model, not when using a blank language class like . The tests shipped with the library do not test the models, so it makes sense they succeed.If you could run the  command in your environment and post the environment variables that you have set here, that might be helpful (just double-check and make sure to remove stuff like secrets etc.). We're trying to pin down the exact configuration that causes the problems – we tried lots of different combinations but we haven't been able to reproduce it yet.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "How about compiler tool chain or Cython? It looks like spacy leaves many of its deps unpinned to a specific version.Or how about compression/serialization tool chain?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 20, 2019",
            "body": "Now I can reproduce it using miniconda docker image.First create a Dockerfile as below:Then run , you will be able to see:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rulai-huajunzeng",
            "datetime": "Mar 20, 2019",
            "body": "Also tried the following base images, both can reproduce easily.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": " what environments are you using? Linux or OS X?LC_ADDRESS=en_US.UTF-8\nXDG_CONFIG_DIRS=/etc/xdg/xdg-pop:/etc/xdg\nLC_TELEPHONE=en_US.UTF-8\nLANG=en_US.UTF-8\nDISPLAY=:1\nSHLVL=1\nLOGNAME=jfelectron\nLANGUAGE=en_US:en\nMANDATORY_PATH=/usr/share/gconf/pop.mandatory.path\nLC_NAME=en_US.UTF-8\nXDG_VTNR=2\nUSER=jfelectron\nXAUTHORITY=/run/user/1000/gdm/Xauthority\nPWD=/home/jfelectron/Documents/Repos/spaCy\nGTK_IM_MODULE=ibus\nGJS_DEBUG_TOPICS=JS ERROR;JS LOG\nXDG_SESSION_ID=3\nCOLORTERM=truecolor\nGNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/aa51ec0d_5220_4740_957e_4aa554ffa884\nDESKTOP_SESSION=pop\nXDG_SESSION_DESKTOP=pop\nGDMSESSION=pop\nGNOME_DESKTOP_SESSION_ID=this-is-deprecated\nDEFAULTS_PATH=/usr/share/gconf/pop.default.path\nWINDOWPATH=2\nPAPERSIZE=letter\nLC_MEASUREMENT=en_US.UTF-8\nLC_NUMERIC=en_US.UTF-8\nLC_MONETARY=en_US.UTF-8\nLC_PAPER=en_US.UTF-8\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\nVTE_VERSION=5401\nUSERNAME=jfelectron\nCLUTTER_IM_MODULE=xim\nGNOME_TERMINAL_SERVICE=:1.114\nQT4_IM_MODULE=xim\nXDG_DATA_DIRS=/usr/share/pop:/usr/local/share/:/usr/share/\nXDG_MENU_PREFIX=gnome-\nLC_IDENTIFICATION=en_US.UTF-8\nIM_CONFIG_PHASE=2\nSHELL=/usr/bin/zsh\nGNOME_SHELL_SESSION_MODE=pop\nQT_IM_MODULE=xim\nLC_TIME=en_US.UTF-8\nTERM=xterm-256color\nSESSION_MANAGER=local/madmax:@/tmp/.ICE-unix/2674,unix/madmax:/tmp/.ICE-unix/2674\nGJS_DEBUG_OUTPUT=stderr\nXDG_SESSION_TYPE=x11\nGTK_MODULES=gail:atk-bridge\nXDG_CURRENT_DESKTOP=pop:GNOME\nSSH_AGENT_PID=2818\nPATH=/home/jfelectron/Documents/Repos/spaCy/.env/bin:/home/jfelectron/.local/bin:/home/jfelectron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/cuda/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin\nSSH_AUTH_SOCK=/run/user/1000/keyring/ssh\nXMODIFIERS==ibus\nHOME=/home/jfelectron\nXDG_RUNTIME_DIR=/run/user/1000\nGPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1\nXDG_SEAT=seat0\nQT_ACCESSIBILITY=1\nOLDPWD=/home/jfelectron\nPYTHONPATH=:/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext\nZSH=/home/jfelectron/.oh-my-zsh\nPAGER=less\nLESS=-R\nLC_CTYPE=en_US.UTF-8\nLSCOLORS=Gxfxcxdxbxegedabagacad\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tgz=01;31:.arj=01;31:.lha=01;31:.lzh=01;31:.tlz=01;31:.tzo=01;31:.zip=01;31:.Z=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.tzst=01;31:.bz=01;31:.tbz2=01;31:.deb=01;31:.jar=01;31:.ear=01;31:.rar=01;31:.ace=01;31:.cpio=01;31:.rz=01;31:.wim=01;31:.dwm=01;31:.jpg=01;35:.mjpg=01;35:.gif=01;35:.pbm=01;35:.ppm=01;35:.xbm=01;35:.tif=01;35:.png=01;35:.svgz=01;35:.pcx=01;35:.mpg=01;35:.m2v=01;35:.webm=01;35:.mp4=01;35:.mp4v=01;35:.qt=01;35:.wmv=01;35:.rm=01;35:.flc=01;35:.fli=01;35:.gl=01;35:.xcf=01;35:.yuv=01;35:.emf=01;35:.ogx=01;35:.au=00;36:.m4a=00;36:.midi=00;36:.mp3=00;36:.ogg=00;36:.wav=00;36:.opus=00;36:.xspf=00;36:\nVIRTUAL_ENV=/home/jfelectron/Documents/Repos/spaCy/.env\nPS1=(.env) (git_prompt_info)$fg[yellow]$(rvm_prompt_info)$fg_bold[blue] ]$reset_color\n$\n_=/usr/bin/env",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "$ pip freeze\natomicwrites==1.3.0\nattrs==19.1.0\nblis==0.2.4\ncertifi==2019.3.9\nchardet==3.0.4\nconfigparser==3.7.3\ncymem==2.0.2\nCython==0.29.6\nen-core-web-sm==2.1.0\nenum34==1.1.6\nflake8==3.5.0\nfuncsigs==1.0.2\nfunctools32==3.2.3.post2\nidna==2.8\njsonschema==2.6.0\nmccabe==0.6.1\nmock==2.0.0\nmore-itertools==5.0.0\nmurmurhash==1.0.2\nnumpy==1.16.2\npathlib==1.0.1\npathlib2==2.3.3\npbr==5.1.3\nplac==0.9.6\npluggy==0.9.0\npreshed==2.0.1\npy==1.8.0\npycodestyle==2.3.1\npyflakes==1.6.0\npytest==4.0.2\npytest-timeout==1.3.3\nrequests==2.21.0\nscandir==1.10.0\nsix==1.12.0\nspacy==2.1.1\nsrsly==0.0.5\nthinc==7.0.4\ntqdm==4.31.1\nuncommon==1.0.2.14072\nuncommon-testing==1.0.2.11140\nurllib3==1.24.1\nvulcan==1.0.2.14072\nwasabi==0.1.4",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": " we are both using py27, my sense is you and  are using Py3, have you tried a py2.7 venv?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "Otherwise, what the the lowest level thing we can check to identify the root cause here?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 20, 2019",
            "body": " Yes of course we've tried Python 2...We've tried configuring the locale to ASCII, tried setting the , and different combinations of the two. We have Python2.7 in our test suite, so the CI tests Python2.7 for every pull request and every release. Thanks, I'll try that container.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "Your test suite doesn't cover this behavior, they are green for me too, it's irrelevant unless in your CI env you are running integration tests that do load models?? If so how do we invoke them?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 20, 2019",
            "body": "Haven't run the container yet, but I had a look at the definition. I think it's the use of the  locale : I'm not 100% sure but I think your locale looks broken too. Your  environment variable isn't set, which looks suspicious: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "LC_ALL isn't set on ANY Ubuntu based system I have access to.  So either it's a Debian/Ubuntu problem or Spacy is expecting something that most environment simply can't provide.  Why are we only seeing this on 2.1?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 20, 2019",
            "body": "I  my locale, seems to have no impact.In [1]: from vulcan.text_spacy import init_spacyIn [2]: spacy_nlp = init_spacy()In [3]: [d.text for d in spacy_nlp(u'I have a dog named spot')]\nOut[3]:\n[u'I',\nu'h',\nu'a',\nu'v',\nu'e',\nu'a',\nu'd',\nu'o',\nu'g',\nu'n',\nu'a',\nu'm',\nu'e',\nu'd',\nu's',\nu'p',\nu'o',\nu't']In [4]: import osIn [5]: os.environ\nOut[5]: {'LC_NUMERIC': 'en_US.UTF-8', 'LESS': '-R', 'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated', 'GJS_DEBUG_OUTPUT': 'stderr', 'LC_CTYPE': 'en_US.UTF-8', 'XDG_CURRENT_DESKTOP': 'pop:GNOME', 'LC_PAPER': 'en_US.UTF-8', 'QT_IM_MODULE': 'xim', 'LOGNAME': 'jfelectron', 'USER': 'jfelectron', 'HOME': '/home/jfelectron', 'XDG_VTNR': '2', 'PATH': '/home/jfelectron/.local/bin:/home/jfelectron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/cuda/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin', 'ZSH': '/home/jfelectron/.oh-my-zsh', 'DISPLAY': ':1', 'SSH_AGENT_PID': '2727', 'LANG': 'en_US.UTF-8', 'TERM': 'xterm-256color', 'SHELL': '/usr/bin/zsh', 'XAUTHORITY': '/run/user/1000/gdm/Xauthority', 'LANGUAGE': 'en_US.UTF-8', 'SESSION_MANAGER': 'local/madmax:@/tmp/.ICE-unix/2559,unix/madmax:/tmp/.ICE-unix/2559', 'SHLVL': '1', 'MANDATORY_PATH': '/usr/share/gconf/pop.mandatory.path', 'QT_ACCESSIBILITY': '1', 'QT4_IM_MODULE': 'xim', 'CLUTTER_IM_MODULE': 'xim', 'WINDOWPATH': '2', 'IM_CONFIG_PHASE': '2', 'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1', 'LC_MONETARY': 'en_US.UTF-8', 'USERNAME': 'jfelectron', 'XDG_SESSION_DESKTOP': 'pop', 'XDG_RUNTIME_DIR': '/run/user/1000', 'LC_IDENTIFICATION': 'en_US.UTF-8', 'LC_ADDRESS': 'en_US.UTF-8', 'PYTHONPATH': ':/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext:/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext', 'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh', 'VTE_VERSION': '5401', 'GDMSESSION': 'pop', 'XMODIFIERS': '=ibus', 'GNOME_SHELL_SESSION_MODE': 'pop', 'XDG_DATA_DIRS': '/usr/share/pop:/usr/local/share/:/usr/share/', 'LC_ALL': 'en_US.UTF-8',",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 20, 2019",
            "body": "Dude, I said I wasn't sure. I'm working on it. Back off okay?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jfelectron",
            "datetime": "Mar 21, 2019",
            "body": "Don't get me wrong, I think we want to help. The tests are uninformative, so I'm looking for the lowest entry point to break in code (Python or not) where we can figure out what's going on here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Apr 21, 2019",
            "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 6, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 19, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#3438"
        },
        {
            "user_name": "ines",
            "datetime": "Mar 19, 2019",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 19, 2019",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 19, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 20, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 22, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 22, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "honnibal",
            "datetime": "Mar 22, 2019",
            "body": [],
            "type": "pull",
            "related_issue": "#3460"
        },
        {
            "user_name": "ines",
            "datetime": "Mar 22, 2019",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "ines",
            "datetime": "Mar 22, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lock",
            "datetime": "Apr 21, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "Kiku-Reise",
            "datetime": "Jun 18, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
}
]