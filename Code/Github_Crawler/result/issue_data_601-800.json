[
{
    "issue_url": "https://github.com/fossasia/susi.ai/issues/1632",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "fragm3",
            "datetime": "Oct 21, 2018",
            "body": "Currently, the lighthouse score is low for Performance, AccessibilityImprove itGo to Audit in inspect element, then run test for Performance, Accessibility.Current score:\nYes",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fragm3",
            "datetime": "Oct 21, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "fragm3",
            "datetime": "Oct 21, 2018",
            "body": [],
            "type": "pull",
            "related_issue": "#1634"
        },
        {
            "user_name": "fragm3",
            "datetime": "Oct 21, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "anshumanv",
            "datetime": "Oct 26, 2018",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "anshumanv",
            "datetime": "Oct 26, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "pulkit1joshi",
            "datetime": "Jan 20, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/950",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Feb 4, 2020",
            "body": "Since now the wakeword works when Firefox is in the background, it can often be strange to be working with the background browser. Ideally we'd raise the window.Maybe sometimes this isn't desirable, I'm not sure.One of  might do the trick, though I suspect not. If not, then it might require a webexperiment API implementation.Some accessibility advise is specifically  to do this, as it can interfere with accessibility helper tools, but I don't understand the specifics.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 6, 2020",
            "body": "Using  unfortunately does not work – it focuses one window among the other Firefox windows, but does not itself bring Firefox to the front.Implementing this will be more complicated than I hoped as a result. Pushing this to the backlog as a result.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 5, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 5, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 6, 2020",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 6, 2020",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Feb 20, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#1037"
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/77",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "dharani211",
            "datetime": "Aug 28, 2021",
            "body": "I want to add these features to the voice assistantAttaching my sample voice assistant here\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/433",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ArneBinder",
            "datetime": "Jul 24, 2020",
            "body": "I have written a generic dataset for corpora created with the Brat annotation tool (, ). Now I wonder how to use that to create specific dataset instances. What's the recommended way to reuse formats and loading functionality for datasets with a common format?In my case, it took a bit of time to create the Brat dataset and I think others would appreciate to not have to think about that again. Also, I assume there are other formats (e.g. conll) that are widely used, so having this would really ease dataset onboarding and adoption of the library.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "thomwolf",
            "datetime": "Jul 29, 2020",
            "body": "Hi , we have a few \"generic\" datasets which are intended to load data files with a predefined format:You can find more details about this way to load datasets here in the documentation: Maybe your brat loading script could be shared in a similar fashion?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ArneBinder",
            "datetime": "Jul 29, 2020",
            "body": " that was also my first idea and I think I will tackle that in the next days. I separated the code and created a real abstract class  to allow to inherit from that (I've just seen that the dataset_loader loads the first non abstract class), now  is very similar in its functionality to  but inherits from .However, it is still not clear to me how to add a specific dataset (as explained in ) to your repo that uses this format/abstract class, i.e. re-using the  entry of the   object and . Again, by doing so, the only remaining entries/functions to define would be , ,  and  (which is all copy-paste stuff) and .In a lack of better ideas, I tried sth like below, but of course it does not work outside  ( is currently defined in ):Nevertheless, many thanks for tackling the dataset accessibility problem with this great library!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ArneBinder",
            "datetime": "Jul 31, 2020",
            "body": "As temporary fix I've created  (contributions welcome).",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/fossasia/susi_server/issues/632",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "abishekvashok",
            "datetime": "Dec 20, 2017",
            "body": "Travis CI build is not parallelizedTravis CI bulild should be parallelized\nWe should do this to reduce build time and make user interaction with the logs easy and to improve accessibility of the logs. It also helps us the pin point which component was broken.\nJob 1 - build, test, deploy.\n(we can't change this as we should deploy only when we pass all tests and compilation is successful)\nJob 2 - CodecovSee a Travis Ci build log or .travis.yml fileSure, taking it up as part of GCI.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RahulMetre03",
            "datetime": "Dec 21, 2017",
            "body": "I would like to work on that issue.Plz guide about I should do",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "abishekvashok",
            "datetime": "Dec 21, 2017",
            "body": " sorry for that, I have worked on it and opened a PR.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "abishekvashok",
            "datetime": "Dec 20, 2017",
            "body": [],
            "type": "pull",
            "related_issue": "#635"
        },
        {
            "user_name": "the-dagger",
            "datetime": "Dec 30, 2017",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/102",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Wolfman1219",
            "datetime": "Aug 17, 2022",
            "body": "C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\nwarnings.warn(\"No audio backend is available.\")\nTraceback (most recent call last):  File \"c:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\demo\\demo.py\", line 7, in \nfrom engine import SpeechRecognitionEngine  File \"C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\engine.py\", line 10, in \nfrom neuralnet.dataset import get_featurizer\nFile \"C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\neuralnet\\dataset.py\", line 6, in \nfrom utils import TextProcess\nImportError: cannot import name 'TextProcess' from 'utils' (C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\venv\\lib\\site-packages\\utils__.py)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AbdouBou",
            "datetime": "Aug 26, 2022",
            "body": "add this to dataset.py before the from utils import TextProcess Lineimport sys\nsys.path.insert(0, 'PATH_TO_NEURALNET_FOLDER') #C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\neuralnetif it doesn'T resolve the error try to delete the utils (most probably you installed via pip/pip3) using pip uninstall utils.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/359",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "awallin",
            "datetime": "Oct 2, 2019",
            "body": "Enable \"Open developer tools\" and related synonyms:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lelouchB",
            "datetime": "Mar 19, 2020",
            "body": "  Is this still open??",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Mar 19, 2020",
            "body": "Yes, . Are you interested in working on it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lelouchB",
            "datetime": "Mar 19, 2020",
            "body": "Yes, I am",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lelouchB",
            "datetime": "Mar 19, 2020",
            "body": " please assign me",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lelouchB",
            "datetime": "Mar 19, 2020",
            "body": " Thanks, Do you have any idea on how to proceed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Mar 19, 2020",
            "body": " might be able to offer suggestions?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lelouchB",
            "datetime": "Mar 19, 2020",
            "body": "Cool, I'll wait then\nthanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Oct 3, 2019",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Mar 19, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/30",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "pietmlr",
            "datetime": "Nov 27, 2020",
            "body": "I constantly getting this error:\n`During handling of the above exception, another exception occurred:Traceback (most recent call last):\nFile \"/Users/pietmuller/Dokumente/code/sr.venv/speechrecognition/neuralnet/dataset.py\", line 110, in \nlabel = self.text_process.text_to_int_sequence(self.data['text'].iloc[idx])\nFile \"/Users/pietmuller/Dokumente/code/sr.venv/speechrecognition/neuralnet/utils.py\", line 51, in text_to_int_sequence\nch = self.char_map[c]\nKeyError: 'D'`\nI didn't change anything in the utils.py file.\nI'm also wondering why it says that there is a KeyError with the character: \"D\". In the variable char_map_str (in utils.py) is no capital D mentioned. Also I want to train it on a common voice dataset (German to be exact)It' not only 'D', it's 'D' and 'E' alternately but no other characters...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pietmlr",
            "datetime": "Nov 27, 2020",
            "body": "Okay I could fix this by adding some characters to the map and adding .lower() at the and of the label variable in dataset.py\nBut now it says that my spectrograms are to big..",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NoCodeAvaible",
            "datetime": "Mar 5, 2021",
            "body": "Hey mate did you fix it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pietmlr",
            "datetime": "Mar 5, 2021",
            "body": " No, I couldn't, but now I am building my own automatic speech recognition system in Keras/Tensorflow.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NoCodeAvaible",
            "datetime": "Mar 16, 2021",
            "body": "Fixed it! Add me on Discord I'am german too so I can tell u everythink about it:) Name: SheeeshForce1#8083",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NoCodeAvaible",
            "datetime": "Mar 16, 2021",
            "body": "If you not german I could tell you more about this because the KeyError will occur even if you lowercase.\nBut in a different way",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 12, 2021",
            "body": " How did you fix the error? I fixed the KeyError by changing the sentence to only lowercase letters, but I didn't fix the spectrogram which is too big. So I would be very interested how you did that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 12, 2021",
            "body": " it's pretty late for me I will tell you tommorow:)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 11, 2021",
            "body": " Can you tell me now?\nI originally thought I would have fixed it, but it doesn't allow the big letters. (I don't get any error, but it's not liking the big letters)",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/53",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 9, 2021",
            "body": "Hello first of all nice work!\nI wanted to ask if I could use mp3 files instead of wav files and which lines I have to change for that, if this is working?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 9, 2021",
            "body": "Hello  I think that's not possible.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 9, 2021",
            "body": " because the spectograms are made out of the wav files",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 9, 2021",
            "body": "  Thanks for the quick answer.\nBut I also can create  a spectogram with a mp3 file. Admitting I only tested it with tensorflow and I know it works there.\nIf it didn't work with pytorch, would it be possible to do the one task with tensorflow and the rest with pytorch?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 9, 2021",
            "body": " hmmm good question I actually don't know if this is possible but I think yes you can do one part with tensorflow and the other with pytorch you just need to fed the spectograms some how into pytorch.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 9, 2021",
            "body": " but why would you want to use mp3 files instead of wav files it's much easier to handel and format them:)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 10, 2021",
            "body": " First of all I don't have so much storage on my computer.\nBut the bigger problem is my GPU, so I have to train with google colab. Wav Files are too big, mp3 files are not so big. Another idea was only to upload the spectogram to google colab, so the files wouldn't be too big.\nIs there a quality different between wav and mp3? I know that wav files have a better quality, but I don't really think this is so important in training. Or it's better, because the model has to transcript worse quality.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 10, 2021",
            "body": " mp3 files are like any normal files, wav files(wave files ) are constructed different they look quite different too because every sound is displayed as a wave, mp3's on the other hand not. I don't know how it would change the performance of the model or the training. U can add me on discord: SheeeshForce#8083",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 10, 2021",
            "body": " Thanks for explaning. I wanted to test the engine.py, but I got an error \"ImportError: cannot import name 'imsave'\". imsave is from scipy.misc and I found out that stackoverflow means it should be imageio. Now I'm confused, because I think it should work with imsave?! Could you help me out there please?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 10, 2021",
            "body": " I am actually not familiar with imsave and imageio",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 10, 2021",
            "body": " Ok but can you run engine.py without problems?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 10, 2021",
            "body": " at the moment not because for installing the ctcdecoder I have to switch to Linux.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 10, 2021",
            "body": " Ok I'm sorry I'm an idiot, I fixed it. My problem was that I thought neuralnet would be a regular pypi package and not a special self programmed one. Why did you name scripts or folders like other existing packages on pypi :-) (there is sadly no smiley which is laughing)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 11, 2021",
            "body": " Where exactly will the spectrograms be produced? There is so much code with spectrograms, I don't find the exact one.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 15, 2021",
            "body": " Which version of ctcdecode do you use? (Mine worked a few days ago, but than it failed)\nWhat does the ken_lm file mean? Is this the file which did such a good transcription in the video?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 15, 2021",
            "body": " I don't was able to test ctcdecode yet",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 17, 2021",
            "body": " Ok got it.\nDo I have to use the ckpt file for training from a checkpoint (argument for --load_model_from)? And how can I get zip file in the end of training or a ckpt file? I think I need a zip file for transcription with the microphone, but I also would like to  get a ckpt file for further training in the future.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NoCodeAvaible",
            "datetime": "Apr 18, 2021",
            "body": "Hey  yes you have:) The model will be saved automatically as a ckpt file:) Yes I think that you need one too(btw I need also one ) because I think without the zip we get no outputs. Could you please add me on discord please so we could talk there and speed up communication? Name:SheeeshForce1#8083",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 20, 2021",
            "body": "Sorry I don't have discord.\nOk thanks. I'm getting the folowing error when I use the argument --load_model_from speechrecognition.ckpt:\nRuntimeError: Error(s) in loading state_dict for SpeechModule:\nUnexpected key(s) in state_dict: \"model.cnn.0.weight\", \"model.cnn.0.bias\", \"model.cnn.1.norm.weight\", \"model.cnn.1.norm.bias\", \"model.dense.0.weight\", \"model.dense.0.bias\", \"model.dense.1.weight\", \"model.dense.1.bias\", \"model.dense.4.weight\", \"model.dense.4.bias\", \"model.dense.5.weight\", \"model.dense.5.bias\", \"model.lstm.weight_ih_l0\", \"model.lstm.weight_hh_l0\", \"model.lstm.bias_ih_l0\", \"model.lstm.bias_hh_l0\", \"model.layer_norm2.weight\", \"model.layer_norm2.bias\", \"model.final_fc.weight\", \"model.final_fc.bias\".Does anyone now what this means?Then I tried to use the argument --resume_from_checkpoint (I don't know what this argument is doing, sorry) instead of --load_model_from. But this doesn't work, too. Following error:\ncheckpoint_callbacks[-1].best_model_path = checkpoint['checkpoint_callback_best_model_path']\nKeyError: 'checkpoint_callback_best_model_path'",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 21, 2021",
            "body": "Ok I fixed the first error. My version of pytorch_lightning was to old.\nBut what does the --resume_from_checkpoint argument mean?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 21, 2021",
            "body": " It means that you insert an checkpoint file as default or truh the terminal(set required false if you set it as default) and the training is being resumed from this checkpoint. U basically use it to  resume training if you stopped the training, if you want to test the checkpoint(model that you create in optimize_graph.py) or if your pc shuts down for an unknown reason while training.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 24, 2021",
            "body": " Do you know why loss could be \"nan\"? At the beginning it worked with a real float, but now I only see this string there. I researched this, but didn't find a good cause.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 24, 2021",
            "body": " yes Cuda and cudnn are not installed the right way. U can search on YouTube for videos for a correct cuda and cudnn installation:)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 24, 2021",
            "body": " Ouh ok that's interesting thanks. I'm using my CPU.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "Apr 24, 2021",
            "body": " are you working with mp3 files now?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "Apr 25, 2021",
            "body": "With Windows, no, because it doesn't work there with mp3 files, but it is working with Linux. I'm training with my Windows system, I only have Linux as vm.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 1, 2021",
            "body": " Do you know what this ken_lm is for and where I could get it? Is this the file which improved the transcription in the video so much? When not, what was the file which improved the transcription so much?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "May 1, 2021",
            "body": " did you already tested the speechrecognition?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 1, 2021",
            "body": " Yes with the zip model. But it's not so good. But I remember that in the video he used something else, too, to get good results.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CracKCatZ",
            "datetime": "May 1, 2021",
            "body": " hold up did u used the portaudio library because I think that this library is required and can give better results. Could you please tell me if you have portaudio already installed at the beginning of working with this project or if you have to install it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 8, 2021",
            "body": " Sorry for late response. Yes, I think so.\nTo come back to the loss=nan problem: Why isn't loss=nan when I train the same wav file for a few epochs? Could I try to train only one wav file per training or would the result be worse?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 12, 2021",
            "body": "Ok I tried some things and it  seems that there is a problem with the big letters but only in the first line. The second line doesn't care about big letters.\nHave you teained a kenlm model? When yes, how? I don't understand what I have to do, I'm sorry!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Botirjon2009",
            "datetime": "Feb 16, 2022",
            "body": " Hi.. I am going to set my mp3 files. .. Actually I going to know where my mp3 files should be set?  I mean which section should be linked to mp3 files.. scripts\\common_voice_json.. ? right?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/25",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "samano99",
            "datetime": "Oct 27, 2020",
            "body": "File \"/home/User/anaconda3/envs/User/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/optimizer_connector.py\", line 47, in update_learning_rates\nmonitor_key = lr_scheduler['monitor']\nKeyError: 'monitor'pytorch_lightning.utilities.exceptions.MisconfigurationException: ReduceLROnPlateau requires returning a dict from configure_optimizers with the keyword monitor=. For example:return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'your_loss'}Any suggestion on solving this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jochen-sys",
            "datetime": "May 11, 2021",
            "body": " I don't know whether you still have this issue, but a possible solution would be here:\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/21",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "samano99",
            "datetime": "Oct 14, 2020",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LearnedVector",
            "datetime": "Oct 15, 2020",
            "body": "looks like you're trying to train a speech recognition model. I think the size of your spectrogram is too big and maybe causing python to get stuck in a loop. you can change this line of code to accept large spectrogram size to like 2000 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "samano99",
            "datetime": "Oct 17, 2020",
            "body": "in the optimze graph.py, i got \"python stopped working\". it stuck while saving the file\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AI/issues/19",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Bhoot127",
            "datetime": "Nov 1, 2021",
            "body": "import pyttsx3\nimport datetime\nimport speech_recognition as sr\nimport wikipedia\nimport smtplib\nimport webbrowser as wb\nimport os\nimport requests\nfrom pprint import pprint\nimport pyautogui\nimport pyjokesengine = pyttsx3.init()\nvoices = engine.getProperty('voices')       # getting details of current voice\nengine.setProperty('voice', voices[1].id)   # For Female Voicedef speak(audio):\nengine.say(audio)\nengine.runAndWait()def time():\nTime = datetime.datetime.now().strftime(\"%I:%M:%S\")\nprint(Time)\nspeak(\"The current Time is\")\nspeak(Time)def date():\nyear  = int(datetime.datetime.now().year)\nmonth = int(datetime.datetime.now().month)\ndates = int(datetime.datetime.now().day)\nspeak(\"The current date is\")\nprint(dates)\nprint(month)\nprint(year)\nspeak(dates)\nspeak(month)\nspeak(year)def wishme():\nspeak(\"Welcome!\")\nhour = datetime.datetime.now().hour\nif hour >= 6 and hour < 12:\nspeak(\"Good Morning Sir\")\nelif hour >=12 and hour < 18:\nspeak(\"Good Afternoon Sir\")\nelif hour >=18 and hour < 24:\nspeak(\"Good Evening Sir\")\nelse:\nspeak(\"I hope you are enjoying your Night Sir\")\nspeak(\"Friday at your service. Please tell me how can i help you \")def takeCommand():\nr = sr.Recognizer()\nwith sr.Microphone() as source:\nprint(\"Listening...\")\nr.pause_threshold = 1\naudio = r.listen(source)\ntry:\nprint(\"Recognizing...\")\nquery = r.recognize_google(audio, language='en-in')\nprint(f\"You Said: {query}\\n\")def sendEmail(to, content):\nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.ehlo()\nserver.starttls()\nserver.login('', 'Password')\nserver.sendmail('', to, content)\nserver.close()def screenshot():\nimg = pyautogui.screenshot()\nimg.save('C:/Users/Amandeep/Desktop/Friday/screenshot.png')def jokes():\nhaha = pyjokes.get_joke()\nprint(haha)\nspeak(haha)if  == \"\":\nwishme()\nwhile True:\nquery = takeCommand().lower()",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/63",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "wahyouwebid",
            "datetime": "Apr 16, 2021",
            "body": "I have used Kaldi to trigger the speech but it doesn't work, I still have to click the button first\n\nplease for the solution, thank you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rubycho",
            "datetime": "Apr 16, 2021",
            "body": "Though this comment is not the answer for your question, I suggest you to hide your API KEY on your screenshot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wahyouwebid",
            "datetime": "Apr 16, 2021",
            "body": "on the github there is also API KEY, this is proof\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rubycho",
            "datetime": "Apr 16, 2021",
            "body": "Oh it was the key from the repo. I thought it was your own key. Sorry.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cdhiraj40",
            "datetime": "Mar 18, 2022",
            "body": "hey,  did you find a solution for this?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/61",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Tedcas",
            "datetime": "Feb 11, 2021",
            "body": "Hi,I'm working in a new project where I need a custom voice assistant in my android app. I tried this example  and everything works perfectly, however, if I go from MainActivity to another activity and I try to load the assistant on this new activity, tts, stt and wake word doesn't work, I press the floating button and the assitant does its animation, but thats all, my question is, How can I implement something like the example project but extending the assistant in all the activities?Thank you very much in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bgubanov",
            "datetime": "Feb 12, 2021",
            "body": "Hello!\nAre you initializing the Aimybox object from the Application instance, like in the example?\nCan you send code snippet with the second activity?\nI added second activity to example project, copied code of first activity and everything started correctly on each of activities.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Billthebest1",
            "datetime": "Feb 12, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "Tedcas",
            "datetime": "Feb 14, 2021",
            "body": "Hi, first of all many thanks for your reply ¡, I'm sorry I couldn't reply you earlier, answering your question, yes I'm initializing the Aimybox object from Application instance like the example, here is my code for that I've modified it a little bit to adjust to my project:class AimyboxApplication : Application(), AimyboxProvider {And this is the class I use to create the Assistant in every class of my project:class VoiceAssistant constructor(context: Context, container: Int) : Serializable, ActivitiesFather() {So in Activity 1 I call the following methods on onCreate():And then in the next activity I do the same (I simplified this step, calling those two statements in the father of all the Activities of my project)I'm not sure what I'm doing wrong, I must admit I'm not used to Kotlin (nowadays I'm learning the language) and I could make a misstake there.Again many thanks for your help in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Tedcas",
            "datetime": "Apr 23, 2021",
            "body": "Hi, it's been a while since my last posr, after doing some tests I've realized that the problem only happens when I use KaldiVoiceTrigger with the TTS, STT and DialogApi together, if I disable the KaldiVoiceTrigger system, all works perfectly. I've try updating all the dependencies to the last version, but nothing change.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cdhiraj40",
            "datetime": "Jul 5, 2022",
            "body": "does KaldiVoiceTrigger work perfectly for you folks?  \nFor me, it just triggers every time I say anything! Let me know thanks.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/alan-ai/alan-sdk-flutter/issues/12",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "MarsadMaqsood",
            "datetime": "Jul 20, 2022",
            "body": "I'm facing an issue with Flutter - Android. whenever I run this code, the app crashes.and in the studio.alan.app, I added",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Jul 21, 2022",
            "body": "Hey \nWhat do you try to achieve?\nIntent  will be activated on voice phrase.  executes command locally . it should contains an object:If you want to execute command with voice you can do following. In you voice script add  with object:And add command handler to you flutter app:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Jul 20, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4197",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": "I start it: ./NAME.sh\nThe screen gets black (two windows opened, one 'select vehicle' and the other has a longer name.\nOnce it's killed by ctrl+windows+alt F4 ->  eg. Africa_001 all is okay and I see following error messages:And I seeDEFAULTN/ADUDE Github is crashing I can't see what I'm writing",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": " (Issue where I've put short information but I thought it's not related and is closed)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": "Note that if I start Abandoned Park only following error message appears: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": "I discovered this newly: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": "I can steer Africa now with the arrow keys (car) but why does it not drive/fly byself?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 13, 2021",
            "body": "Hi and welcome @debian-user-france1! AirSim vehicles don't drive by themselves. However, AirSim provides an API that you can use to accomplish that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arch-user-france1",
            "datetime": "Dec 6, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 13, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/alan-ai/alan-sdk-ionic/issues/25",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ljudbane",
            "datetime": "Dec 13, 2021",
            "body": "We have encountered an issue where on specific Android devices the voice input is not recognized. When Alan is listening, nothing happens, as if the voice is not recorded. No amount of app re-installs or different builds have helped, the issue is always present.Affected devices are:On Samsung S20 FE we tested your reference app \"Alan Playground\" (installed from Google Play) and voice input also doesn't work in your app. We will also test your app on Note 10 device at a later time and we will update the issue once we get results.I am attaching logcat from running our Ionic app on Samsung S20 FE. In the log i've marked the approximate location when user started speaking (line 416). A bit above that, on line 398 is app's log entry when button state callback fired and state was LISTEN. So somewhere around here Alan started listening.Ionic info:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Dec 14, 2021",
            "body": "Hey \nThank you for detailed issue. We will try to reproduce it.\nDid you get microphone permission popup on this device?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ljudbane",
            "datetime": "Dec 14, 2021",
            "body": " Yes, the permission popup was shown and approved. We also double checked in Android settings and permission for recording audio was enabled in App settings. The microphone does work with phone calls and other apps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Dec 13, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/alan-ai/alan-sdk-ionic/issues/23",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ljudbane",
            "datetime": "Dec 2, 2021",
            "body": "When we call callProjectApi() from ionic application to execute a command from Alan scripts, the callback doesn't get executed when running on Android or iOS device (or simulator). It does work when running in browser (ionic serve).For example we call Alan's function  from ionic:The callback function doesn't get called and those console logs don't ever get printed. Neither is there any caught exception. When testing on Android device we didn't see anything in the logs that would indicate what's the problem. But when running on iOS there is a log entry that could point to the problem:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Dec 2, 2021",
            "body": "Hey \nWe will try to reproduce your issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Dec 10, 2021",
            "body": "Hey \nWe found how to reproduce the issue which is related to your scenario. I will let you know when the fix for your case is ready.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Jan 13, 2022",
            "body": " We have a new release \nPlease take a look - that should resolve this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ljudbane",
            "datetime": "Jan 17, 2022",
            "body": " Thank you, i will check it out and report back if the issue is fixed for me.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ljudbane",
            "datetime": "Jan 19, 2022",
            "body": "I did some testing on iOS simulator.First i updated libs to most recent versions:First call in our app to project API is:And in the log of simulator there are not any console.log outputs from callback function, nor from the catch block:Then i click on a button in our app that launches another callProjectAPI:And in the log i see the same error that was present before at the time of original issue:Every subsequent callProjectApi will trigger the error. Only the first one won't. We plan to test on Android next, so i will report those findings later.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ljudbane",
            "datetime": "Jan 20, 2022",
            "body": "I've also tested on Android and the code in callback function doesn't get executed. I don't see any error in logcat.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Jan 20, 2022",
            "body": " Did you run  or  after plugin update and before run the app?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ljudbane",
            "datetime": "Feb 1, 2022",
            "body": " Yes i did, multiple times. Sorry for the late reply.I will try to replicate the issue in one of your example apps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snyuryev",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/50",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "BennieMatthee",
            "datetime": "Dec 22, 2021",
            "body": "Did you use it with some framework (Angular, React, etc)?\nalan-sdk-webI cant seem to find an example on how to send a command to Alan from a text box.\nI would like to give the user the option to either use the voice command or type in their questions, but I cant find an example on how to do this.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/34",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "a4awesome",
            "datetime": "Aug 14, 2020",
            "body": "Do you have any plans to support Python for voice scripts?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "okolyachko",
            "datetime": "Aug 18, 2020",
            "body": "Hi a4awesome,Thank you for your request, we will pass it to our R&D team and reach out to you as soon as there is any news.\nAt present, voice scripts in the Alan Studio are created in JavaScript that has a pretty easy syntax.Stay tuned!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "andreyryabov",
            "datetime": "Aug 14, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "andreyryabov",
            "datetime": "Aug 17, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/5718",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "bassamtantawi-botpress",
            "datetime": "Nov 26, 2021",
            "body": "\nIF you are using a carousel, and on the carousel you are adding a button, the only way to choose a Value is by clicking it.\nSteps to reproduce the behavior:\nTo be able to deal with the carousel like how you deal with choice field",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Dec 14, 2021",
            "body": "Could you please give a detailed example of what the problem is.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bassamtantawi-botpress",
            "datetime": "Dec 14, 2021",
            "body": "I want to type the option I want to select instead of clicking it. We had a prospect who wanted to deal with the same bot but from web channel and voice channel.From web channel, the carousel is fine since I can click the button associated to it. But when it comes to voice it will be impossible to choose the value.So the bug is: if I have a choice field with values (yes/no) I can write (yes) and then the flow continues. But in carousel we can't do this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Dec 14, 2021",
            "body": "Thanks - this is not a bug, but a feature request. Will file accordingly",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bassamtantawi-botpress",
            "datetime": "Nov 26, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/22532",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "KulykDmytro",
            "datetime": "Mar 25, 2022",
            "body": "2.2.4 (latest released)When inspecting metadata in airflow tables about sla stuff found thatthis prevents to create any custom SLA analisys dashboards/metrics because unable to join  and / in direct and clear wayclicking on daily task's link on SLA Miss dashboard forwarding to task instance which was never executed (because it is in \"future\") and have no runtime metrics in All screenshots are taken @ 2022-03-25 17:xx:xx Z\n should be same as  in all casessetup a dag with any  daily schedule\nsetup an SLA on task\nwait  to trigger\nobserve in  dashboard - incorrect incorrect execution dateamazon linuxOfficial Apache Airflow Helm Chart",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Mar 31, 2022",
            "body": "SLA feature is genreally broken in many ways and need to be completely rewritten. I think there is no point to fix it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "uranusjr",
            "datetime": "Apr 1, 2022",
            "body": "We should advertise 2.4 as “SLA now actually works!”",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Apr 1, 2022",
            "body": "Providing that we will fix it :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sarinarw",
            "datetime": "Jun 15, 2022",
            "body": "Bumping this as we're also experiencing this and it seems to align with data interval end here as well. (v2.2.3)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "argibbs",
            "datetime": "Jul 18, 2022",
            "body": "  Can I check if the above is tongue in cheek, or if there is a rewrite of SLAs planned for 2.4(.x)?I'm hitting an issue in 2.3.3 with SLAs preventing processing of the dags (similar to  which I raised a while back - the way in which it fails has changed, but the behaviour is the same; dags don't get recreated, etc. etc.)I ask because I'm trying to judge whether I should write SLAs off in 2.3.3 and try again in 2.4, or if it's worth putting effort in to work out what's going wrong...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "argibbs",
            "datetime": "Jul 18, 2022",
            "body": "Ok, I got nerd sniped, and dug into it anyway, and now have a reasonable idea of how I might fix it.So, the question still stands: is it worth me attempting to patch this locally for myself (and then raising the change as an MR if successful) or is this pointless since it's already being worked on?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Jul 18, 2022",
            "body": "If you have smple fix that is easy to review you can make a PR fixing it. Happy to review. For now no-one works on rewritinug SLA as far as I know and it is unlikely to happen in 2.4. You can also see discussion I started on it -  - seems like everyone agrees SLA is broken, but there is not enough will (or courage maybe ?) to deprecate it nor incentive to rewrite it, So we are a bit in a limbo - and anything you might want to improve there with very little cost is welcome.Feel free also to chime in the discussion if you have strong feelings about it  - I think more stronger voices are needed if we want to improve the situation a bit more than just patch here or there (or maybe by patching it here and there we might get it to a usable state - who knows :) ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Jul 18, 2022",
            "body": "Actully - If you are much into it and REALLY want to dig into it and see if you can incrementally improve it - I think that might be a super-valuable contribution to the community :). So if you have the will, I am happy to help with reviews and comments :). Just ping me in the PRs.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KulykDmytro",
            "datetime": "Mar 25, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "argibbs",
            "datetime": "Aug 2, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#25489"
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/5714",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "bassamtantawi-botpress",
            "datetime": "Nov 26, 2021",
            "body": "\nWhen there is a markdown in the text, the TTS reads it, in addition to the emojis.\nSteps to reproduce the behavior:\nSpecial Characters are skipped. But things like \"!\" and \"?\" can affect the tone of the voice.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bassamtantawi-botpress",
            "datetime": "Nov 26, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/446",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ubilrodriguez",
            "datetime": "Mar 2, 2022",
            "body": "excuse a query. in the annyang.min.js library you can also manipulate the coordinates in 3D? For example, I have an avartar project that captures those coordinates and with the annyang.min.js library I want to move with the voice.\n\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/445",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "vesper8",
            "datetime": "Jan 30, 2022",
            "body": "I'm using annyang to input chess moves and I'm finding it very challenging to have it correctly recognize commands such as \"D 2\" and \"B 2\", \"E 2\" and so on.Is there a browser in particular that might do a better job than others? Clearly my own voice and pronunciation is a factor in this, I'm just finding it borderline unfeasible at all with how often it incorrectly understands my input, as simple as those inputs are meant to be (64 sounds ranging from \"A 1\" to \"H 8\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limury",
            "datetime": "Aug 10, 2022",
            "body": "Hi! So different browsers will definitely perform differently since Annyang uses the Web Speech API for processing, and the algorithm used by the Web Speech API is different across browsers. However, I unfortunately do not think that there are any reports on what browser is the best and the worst. Also since the functionality is very niche, your best bet is probably just to test them out yourself.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/441",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "inglesuniversal",
            "datetime": "Aug 22, 2021",
            "body": "With the option to extend and use external cloud voices from AWS, GCP, AZURE, IBMBest regards",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/2955",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "AOS55",
            "datetime": "Aug 20, 2020",
            "body": "I am working on adding a FixedWing aircraft class into Airsim, so far I have been documenting my work under issue  but as suggested by  it would be usefult to move this work to a new issue so as to improve visibility for others to work on.Please see this  for a detailed breakdown of what has been added in my . I will aim to keep updates on this issue of new files, classes, methods & functions with a view to write comprehensive documentation as the code begins to get to a state where it can be merged back into the master branch on the main repo.Since the  on  I have been updating the FixedWingLibClient files to enable communication with Ardupilot. A linked issue has been opened within the .Initially I am aiming to work to resolve all the known issues and turn all the  in the state to . Following from this I will aim to add the following functionality:\n[ ] Construct a similar Python API as in multirotor <-- hopefully a short term objective\n[ ] Implement Undercarriage for fixedWingLandings\n[ ] Deep Reinforcement Learning (DRL) based FW landings <-- main PhD objective, expect to write & publish a paper based on this\n[ ] Validate model and control on real UAV and FW aircraft, ties in with above.Principally this is for my PhD at the  where I am aiming to use a combination of computer vision and Deep Reinforcement Learning to train an autonomous fixedwing agent/aircraft to navigate to and safely land in an unknown location. This is both with and without power and on a range of fixedwing aircraft including fixedWing UAVs, gliders & multiengine light piston aircraft (<5700Kg). Not only this but when discussing AirSim with my colleagues we believe that having a fixedWing simulator capable of highfidelity graphics and SITL that is efficient will be of great benefit.Several users have already been realy helpful ( ), if anyone has any ideas about how to solve some of the issues I have described above all help is greatly appreciated. In particular if anyone is able to have any success compiling the  please let me know as I'm a little stumped  as to why it won't compile.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 21, 2020",
            "body": "I had a try at compiling your branch, the error message was really quite useless.\nOpened  which fixes that and brings us back to normal compilation errors with understandable messages (for now)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 24, 2020",
            "body": "I have been continuing to work on the FixedWingPhysics simulation aiming to resolve compilation errors. I have vectorised the control surfaces so ideally they behave as a  tyoe vector. With this I introduced a new file  that instants each control from the  file as a PhysicsBodyVertex class. I have not introduced the thrust vector yet but this shouldn't be too much of a challenge.Aircraft links to the structs in  and applies the aero-forces via . Most fixedwing dynamic models/databases, define the forces and moments around the CG, as . setWrench applies these and then is updated in the loop by , I'm a little lost as to how the PhysicsBody class updates the aircrafts pose via . The way I've set up the classes may also be an antipattern and I might need to refactor some of the structs, please let me know your thoughts on how you might setup the structures.When I run the repo from visual studio I get the following error :Im not sure what this error is being caused by, I'm guessing there is no termination condition in the recursive call. If anyone has experienced this before I'd love to get your thoughts.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 24, 2020",
            "body": "That's happening cause the function is calling itself here - \nIt should instead be calling , from it's parent classNote that I haven't tested this myself yet",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 24, 2020",
            "body": "Thanks yep that was it was quite obvious now I think about it in more detail. It several unresolved external symbol errors now but I think there a case of sorting out further problems with the API.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 24, 2020",
            "body": "Does anyone have any thoughts on this error coming from , I thought it was to do with the link between the base class and Mavlink/Px4 implementation override. I can't see anything untawards though:\nLNK2001\tunresolved external symbol \"protected: virtual float __cdecl msr::airlib::FixedWingApiBase::getAutoLookahead(float,float,float,float)const \" (?getAutoLookahead@FixedWingApiBase@airlib@msr@@MEBAMMMMM@Z) Blocks C:\\Users\\quessy\\Documents\\Unreal Projects\\AirSim\\Unreal\\Environments\\Blocks\\Intermediate\\ProjectFiles\\Module.AirSim.cpp.obj\t1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 24, 2020",
            "body": "Functions seem to be missing, are all the files uncommented in the ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 24, 2020",
            "body": "Thats a good point I've uncommented them now, from , but Im still getting the same link errors from the compiler. When running build.cmd I also get some errors in the command prompt that are slightly different, . These are quite clearly what I should have been looking at, I didn't appreciate the importance of the API src client, server and base files.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 24, 2020",
            "body": "Resolved all build errors now except for one rather unusual one:\nNot sure where this one links to as when I look at the location of the error in object.hpp which I'm not sure where it lives in the repo as dependencies folder not included in push/pull requests.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "The RPC error was due to the fact that it doesn't know how to pack the Vector3r object, need to convert it into the struct defined in RpcLibAdaptorsBase and then pass it. Opened  which fixes that, and adds the files to CMake.\nAfter this AirLib compiles, but there are still lots of errors when building the entire Blocks env, undefined references in FixedWingApiBase.cpp, due to function declarations in .hpp, and usage of commented out methods",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Great thanks very much Ill take a look at those errors today, I merged the issue back into fixedwing and have no build.cmd errors on my end too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Any thoughts on why I'm getting the following include/visibility related build error from in visual studio? I can see the source file  is still on the repo, could it be related to cmake changes?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "That's very strange, Windows shouldn't be affected AFAIK since it uses the .sln files. And if this were a problem, then it would have appeared way earlier since support for Rover has been present for many months\nDoes this appear when running ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "No thats the unusual thing build.cmd doesnt present any errors when run",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "Hmm, maybe try running  from the root directory, that should rebuild everything.\nDo commit any changes made in the files present in the Blocks directory like in the AirLib folder before running though",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Great thanks that seems to have resolved it back to linking problems probably due to decleration again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "Nice! Still have no idea why VS decided to remove the header file\nSide note, this does somewhat bring into mind that GitHub issues might not be the best way to communicate, especially on smaller problems. Maybe something for future",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Yeah not sure its unusual.\nYes certainly, I'm surprised there isn't a microsoft/AirSim slack setup or equivalent for more direct communication. As you said something for the future.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "There is a Facebook group, but I myself haven't used it since a long time.\nAP uses Discord (earlier Gitter) and has a forum as well, similar with PX4. Maybe after there is more activity and members. Having something like this would definitely reduce the number of issues also, since more confirmed issues will be created rather than duplicate and support ones",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Yes I agree thats a good idea, the documentation for AirSim would need to reflect this too as there is not an obvious point of contact when onboarding new users/developers with the application.Also after commenting out the linking errors in the application I have managed to compile through to the blocks environment .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "Awesome, now comes the probably more difficult part of modelling, physics and making sure everything works!\nHaving a more direct method of communication will also increase community interaction a lot. Which platform to select, etc will have to be decided by the maintainers though, might require collecting some opinions from the users",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Yes I agree, would you like to open up a communication issue, I'm not sure if you can do a poll on github?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "Don't think I'm the best person to open such a issue, a maintainer should do this probably. They'll also have to setup the platform and maybe maintain it (no other word is coming to my mind right now)\nGithub doesn't have polls AFAIK",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 25, 2020",
            "body": "Yes that makes sense, a moderator or project manager sort of thing. Perhaps if you know anyone you could  them into this issue so they can set it up. Ok sure you could always make the proposed solution as a comment and get people to vote   or  on the comment.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 25, 2020",
            "body": "    Do you think having another (more chat & voice type) communication platform will be worthwhile? This will definitely increase community interaction, and will probably reduce support issues.\nJust wanted to know your opinions, and definitely don't want to increase your (already heavy) workload too much.\nLooking forward to hearing from you!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 26, 2020",
            "body": " AirLib is actually not compiling for me, getting some undeclared identifier errors for the function declarations which have been commented out. Here's a Travis build - , even the Windows build fails.\nDid some files not get committed when commenting out the API methods?Would also recommend activating Travis on your Github repositories (atleast Airsim), that has saved me a few times when making some changes which don't compile on Windows, or breaking something else. (Side note, I did break the Windows build in my first contribution to Airsim (and others later on also), that was a big motivation to get Travis setup properly on Airsim)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 26, 2020",
            "body": " hmm ok, this is interesting could you please send me the identifier errors you are getting I will cross-check them on my local machine, I just ran update_to_git.bat and had no further changes and I also ran update_from_git.bat in Blocks and it still compiles ok on my end. A couple of thoughts:Nice Idea, I have setup , and it also appears to fail. Looking at the job log it has the following on line 1457. I wonder if this is unity related or just what the  script should print to the command line.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rajat2004",
            "datetime": "Aug 26, 2020",
            "body": "Very strange that it works on your machine, these are the errors I'm getting -AirLib compilation doesn't depend on UE, Travis also doesn't have UE, it just tests whether AirLib, ROS wrapper compiles, and there's the recently added Azure Pipelines CI which compiles the entire Blocks project on Ubuntu and Windows and packages it.Yup, I'm rebuilding from the root directory, .That's just Travis printing the entire command which failed to run, the errors can be seen above it, starting from ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seanmcleod",
            "datetime": "Oct 19, 2020",
            "body": "I mentioned that option, although I suggested using JSBSim for running the physics/aerodynamics, see my comment a couple of comments back - ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xxEoD2242",
            "datetime": "Nov 18, 2020",
            "body": "Have you thought about changing this value in the Fast Physics Engine? I linked to the place where  is instantiated. I'd be really careful with this, as the time step calculations for this process may be heavily dependent on a sufficient timestep. Have you thought about taking the underpinnings of JBSim and modifying the FastPhysicsEngine to be something like kindFastPhysicsEngine with more realism?Myself and a team are looking to implement a fixed wing drone for a drone competition but we also want to update the physics models to be a bit more realistic for dynamics information so that we can implement our state-space models more efficiently then writing them externally in Python.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Nov 18, 2020",
            "body": "Hi  thanks for this. I think a JSBSim physics engine is what we should be heading towards it makes a lot of sense and prevents us reinventing the wheel. I have been working on a python implementation over the last month. But having it native to airsim in C++ would be ideal. The problem I've begun to realise over the past couple of days is that by pose forwarding JSBSim to airsim in CV-mode the collisions cannot be easily calculated from unreal as there is no physical mesh geometry like you get in the multirotor or car pawn.I'd certainly be keen to work together on this though. A native C++ framework is certainly the way to go long term, with a python API similar to multirotor. I think it might be best to let JSBSim deal with all of the fixedwing physics. JSBSim also has a ground interaction component, but I'm not entirely sure how it works yet, the aircraft I have defined so far does not have any ground-reactions in the aircraft XML file.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xxEoD2242",
            "datetime": "Nov 18, 2020",
            "body": " That makes sense. I think the appropriate solution would be to implement JBSim natively in the build process with AirSim and have the selection of JBSim for dynamics information. I work with Purdue University researchers a lot so I'll see if there would be interest from the faculty there in exploring how we would achieve this. Signal routing through the same FastPhysicsEngine routes would be my biggest concern, especially from a memory standpoint to ensure that AirSim stays as optimized as possible. If we decide to go down this route, I'll let you know and possibly fork your branch since you have so much done.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Nov 18, 2020",
            "body": "The documentation and tests leave a little to be desired at the moment but you might find my  repo useful, it just implements the pose forwarding with CV mode described above and then runs JSBSim for the flight physics.The key classes are in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xxEoD2242",
            "datetime": "Nov 18, 2020",
            "body": "Awesome. Thanks!I think there is a desire to implement the physical drone as well and I have a teammate who is working on creating custom environments and scenarios. If we can figure out the blueprints and meshes correctly, we will try to implement that as well, which should help with the issue you are having in CV mode.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Nov 18, 2020",
            "body": "Great that would be really useful and its always useful to get others opinion on how to best implement this. Yes I'm not sure on the difference in efficiency JSBSim presents  may be able to help there.Great also would be very helpful",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seanmcleod",
            "datetime": "Nov 18, 2020",
            "body": "Yep, I see a feature request was raised for collision detection in CV-mode in Aug 2020 - So for now you would need to make use of JSBSim's collision detection via it's ground callback mechanism etc.I had originally suggested the JSBSim pose forwarding idea to Airsim in CV mode as a quick and fairly easy mechanism to implement. Linking JSBSim into AirSim and having some integration and switching mechanism with Airsim's physics engine is definitely doable but a lot more work.Would the Airsim repo maintainers be interested in incorporating JSBSim in this way? Or would you have to maintain your own fork?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Nov 30, 2020",
            "body": "Hey a couple of questions for the airsim maintainers and perhaps JSBSim too. I have been looking at integrating JSBSim into AirSim  over the past week, using the car and multirotor pawns as a base. Im not sure how best to structure the code to get it to work with airsim.As  stated above jsbsim has a collision model built in for a STRUCTURE and BOGEY (undercarriage) element. There is a wrapper class in JSBSim  that is used to instantiate a jsbsim object and then the simulation can be advanced with run and properties accessed and set.Im not sure how to connect the class of JSBSim to an Airsim UE4 pawn object necessary for physics and then send back the terrain information (this can probably come later though). I believe I need to call the wrapper class on a class derived on AirSim's\n. I have created a simple class  that should serve this purpose. I'm not sure how to call this method with a pawn, I initially followed a similar structure to the car api but did not realize it used Unreals inbuilt physics engine. I'm not sure where multirotor builds its FastPhysicsEngine model either.I believe the model needs to do the following and I have started making classes for each of these but am unsure of the interaction:Does anyone have any ideas about the best way to link the JSBSim physics library with the airsim UE4 pawn library. The collision information from UE4 would ideally be sent to JSBSim's ground callback mechanics or we could do it in UE4 and set the acceleration/state. Also not sure if the JSBSim python instance can be used this way I think the best way to interface with the running program will be to setup a pythonclient the same as airsim's multirotor and fixedwing not sure what the way to do that will be.Are there any UML type diagrams for AirSim the maintainers or microsoft have they might be able to publish to show the derived class interaction for the Car and the Multirotor this might make the task of adding a new vehicle class easier in the future too. Thanks for everyone's help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Dec 1, 2020",
            "body": "Following up from my previous post, I have been working on the JSBSim addition today. Im still not 100% how information is supposed to be sent from the Pawn to the API. In particular I have 2 structs  and .I believe the interaction between JSBSim's wrapper FGFDMExec should be something similar to the block diagram shown below:\n\nWith  responsible for getting state information from JSBSim along with calling JSBSim update (via run) and instantiating JSBSim. The class should also pass information to JSBSimPawn to set the aircraft's pose in UE4. Information from JSBSimPawnSim should then be passed to JSBSimApiBase used as the base for a plane specific api (JSBSimPlaneApi) in here all the autopilot functions can be created to control the aircraft and update the JSBSim aircraft. I have the following outstanding questions and it would great to get anyones thoughts:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Dec 9, 2020",
            "body": "If anyone has any ideas on what Im missing in  that would be really useful I am trying to sort out linking the static JSBSim library to AirSim but I am having problems with visual studio.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Dec 30, 2020",
            "body": "Hi ! Thanks for the amazing work you are driving here! I think we can simplify something in your design. I don't know if there is a simple controller for JSBSim (like SimpleFlight), but it is integrated with the autopilots. So it would seem like a good idea to leave the control on the JSBSim side and leave AirSim alone with perception and reporting collisions. What do you all think?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Jan 5, 2021",
            "body": "Hi  thanks a lot. Yes there is xml files for a lot of the aircraft that sets the gains for controllers defined inside JSBSim, . I have also found it useful to directly control the aircraft using a python script which I did for the . I expect to use the same script to control the aircraft but with collisions also being calculated by airsim, i.e.  should also return a collision value and the connection between airsim and jsbsim will happen in the airsim C++ side rather than in the python script, this should be what .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 6, 2021",
            "body": "Great! This development would have an incredible impact if it could be used for multirotors as well. Why is it that I don't see much work done in JSBSim for that? It is not the best option?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Jan 6, 2021",
            "body": "Thanks, yes certainly there are quite a few useful physics simulations included with JSBSim too such as heavier than air powered rotorcraft (helicopters), airships etc that I haven't looked at but I'm sure others would find useful.  My best guess as to why multi rotors haven't been extensively developed is probably age? I think JSBSim started in the late 90s or early 2000s and they just weren't considered significantly at this time except for rc aircraft  may know more?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seanmcleod",
            "datetime": "Jan 7, 2021",
            "body": "In terms of JSBSim and multirotors take a look at the following - ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 11, 2021",
            "body": "Thanks, ! I opened a discussion about integrating with other simulators here: . It would be great if anyone of you can join.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tpool2",
            "datetime": "Feb 16, 2021",
            "body": "Hi all. I'm very interested in the integration of AirSim with JSBSim physics in order to model transitioning vehicles. Just wondering what the current status of this project is and how I can help move it forward.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Feb 19, 2021",
            "body": "Hi  thanks for your interest I'll put a general summary out of where I have gotten to with JSBSim integration with AirSim:I spent a while getting JSBSim to compile and link in Visual Studio and with mac/linux by adding the relevant commands to the  and  or .When first building in Unreal I've had to add  to a couple of the jsbsim header files, I thought I added the relevant code to the airlib project file [https://github.com/AOS55/AirSim-1/blob/FixedWing-JSBSim/AirLib/AirLib.vcxproj] but I've tried playing around with it and can't seem to get it to work.When run with settings.json as:The programme runs until it gets the following error:Looking at the call stack this appears to come from here the  function. I haven't made much more progress past this unfortunately and its proven to be a real challenge.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nikitabeebe",
            "datetime": "May 28, 2021",
            "body": "Has there been any update on the status of this? I'm looking to implement a fixed wing UAV for a camera GPS project I'm working on, and need to do it via airsim.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jun 1, 2021",
            "body": "Hi ! you can use  fork: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Jun 1, 2021",
            "body": "Hi  sorry for the delay in getting back to you the branch jony suggested is still wip I haven’t looked at it in a while hope to get back soon you might find the fixedwing-airsim repo of more use which uses a python backend with the client pose forwarding. I’m going to put some more documentation in this week happy to setup a meeting to talk through it too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Jaeyoung-Lim",
            "datetime": "Jul 27, 2021",
            "body": " Thanks for the great work on getting fixedwing working with Airsim!I am interested in supporting fixedwing vehicles in Airsim with a autopilot (PX4) as a SITL simulation.I have looked into , but it doesn't seem very obvious how this would work with the implementation using python bindings of jsbsim. Would you have any insights how you envision this would work and how this would maybe fit the jsbsim integration on PX4 side as in  ?Thank you in advance",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "redhat2299",
            "datetime": "Dec 4, 2021",
            "body": "  really great work ,I like to know what is the progress ,when i tried to open fixed fixed airsim's block environment in unreal I'm getting errors ,Sir please help me out",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "airsimdevdd",
            "datetime": "Dec 8, 2021",
            "body": "Fixed wing in Airsim , Any Modified version of Airsim  to work with fixed wing please help   \n       ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AOS55",
            "datetime": "Aug 20, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#2508"
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 20, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Nov 16, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#3141"
        },
        {
            "user_name": "AOS55",
            "datetime": "Dec 3, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#3181"
        },
        {
            "user_name": "ahmed-elsaharti",
            "datetime": "Dec 16, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#3203"
        },
        {
            "user_name": "Jaeyoung-Lim",
            "datetime": "Aug 23, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#3978"
        },
        {
            "user_name": "firemount",
            "datetime": "Jun 22, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#4588"
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/439",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Baudin999",
            "datetime": "Jun 29, 2021",
            "body": "Annyang does not seem to work on in my browser (see details below).  The Annyang main page (demo app) does not work and I think it's the same reason as the reason why my own project does not work. The problem is made visible through SpeechKITT because I see the recording starting and stopping.What I'm trying in my own application:I am trying to get annyang (with SpeechKITT) added to my app. I use the following code:I expect, when I press the \"mic icon\" and start voice that I can talk and the voice commands get recorded and handled.What happens is that I click start and it immediately stops. If I add a  handler it immediately get's fired after the  handler.\nAnnyang get's configured only once, I can read out the .I've tried: , but to no avail.\nI am working in a svelte app. At first I thought it was because the app rerenders, but this wasn't it. I've tried the main index page and also a separate page without anything on it, the behavior is the same.I'm working on an app where voice commands would help people who have trouble typing, this component is an important part of the app.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/438",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ralphignacio13",
            "datetime": "Jun 20, 2021",
            "body": "When I am using requestFullscreen() function to make a video player fullscreen using voice commands, I am getting this \"(index):379 Failed to execute 'requestFullscreen' on 'Element': API can only be initiated by a user gesture.\"Video player should be fullscreenedReturning an error: (index):379 Failed to execute 'requestFullscreen' on 'Element': API can only be initiated by a user gesture.Maybe changing the height and width but I tried that it only changes the dimensions of the video the video controls are not included in the change sizeI am very thankful for the opportunity to be able to add voice recognition easily on web application, and I am currently using this for my college thesis.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/436",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "josephpramodh",
            "datetime": "Jun 4, 2021",
            "body": "How can I make my music button play or pause, using my voice commands, 'play music ' and 'pause music'I want to add submit option using the submit voice command, how can I write the code for the submit option\nHelp with these two issues ",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/435",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "josephpramodh",
            "datetime": "Jun 4, 2021",
            "body": "Sir, I'm trying to work on a voice based mail for blind using Annyang JS but I'm failing to implement it, so kindly help me to resolve it!!!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/393",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "scoy028",
            "datetime": "Mar 21, 2019",
            "body": "Repo branch with logs: \nRelevant files: client/components/recipe-step.js, client/annyangCommands.js\nBrowser we are using: ChromeMy team and I have been working on adding annyang with Web Speech API to recognize multiple commands, some of which trigger navigation. We are using the browser console to test. We speak a number of commands and get responses, but after a few commands, the console stops recognizing the speech event for the command. We have logged within the methods and the \"heard\" command and we are getting the correct logs, we just aren't hearing anything. We are also logging commands twice. The more commands we give, we start to eventually log the commands more than twice. Is this a garbage collection issue? We believe this is a bug with how we are integrating our annyang commands and the logic of how we trigger our command methods. Perhaps the same annyang event is persisting. Would love some help if possible!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/346",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ceides9497",
            "datetime": "Jun 11, 2018",
            "body": "I want to say pause and let annyang stop until I say resume and start listening againis it possible?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jrsarath",
            "datetime": "Jun 21, 2018",
            "body": "You might want to combine annyang with native speech-api..\ntell speech-api to wake up annyang on a specific word/hotword and shutdown native speech-api..\nand for pause function add a pause command to annyang execute pause function as start native speech-api again and stop annyang..",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/313",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "AdamMiltonBarker",
            "datetime": "Oct 20, 2017",
            "body": "Hi guys, an issue that is becoming a pretty big problem for us is that once a user has voice rec on, and then stops using abort(), if they restart again, it will continue hearing everything twice resulting in querying our NLU twice, displaying what it heard twice, and displaying the response twice, any suggestions to stop this happening?Here is the related code:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AdnanHafeez",
            "datetime": "Dec 18, 2017",
            "body": "Not sure if I am missing something here, but why are you using 'setTimeOut' to call Bootup every 4 seconds?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AdamMiltonBarker",
            "datetime": "Dec 18, 2017",
            "body": "I think you are missing something, setTimeOut executes only once, setInterval would execute multiple times. In the code setTimeOut makes the script wait 4 seconds before executing bootup.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AdnanHafeez",
            "datetime": "Dec 18, 2017",
            "body": "Yes, you are correct. My bad! I confused the two.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AdamMiltonBarker",
            "datetime": "Dec 18, 2017",
            "body": "No worries ;)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Adrianjewell91",
            "datetime": "Dec 31, 2017",
            "body": "Has this been resolved? I'm looking to start contributing to an open source project, and would like permission to tackle this one.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TalAter",
            "datetime": "Dec 31, 2017",
            "body": "Thank you \nIf you are able to replicate the issue please go ahead Have you been able to isolate the issue and understand what's the root cause?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Adrianjewell91",
            "datetime": "Jan 3, 2018",
            "body": "Thanks. , I was not able to replicate the bug. It looks like your code requires a certain HTML element called #TOA_Recognition_Window. If you can post a description on how to replicate the issue along with the complete code, I'm still happy to take a deep look.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/5641",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "SalahSoliman",
            "datetime": "Aug 1, 2022",
            "body": "\n            \n          ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Shreyas-dotcom",
            "datetime": "Aug 3, 2022",
            "body": "Hey Salah,No - Carla does not yet support Unreal Engine 5. It was discussed on discord by German Ros.He said:\nHope this Helps!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SalahSoliman",
            "datetime": "Aug 3, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "Shreyas-dotcom",
            "datetime": "Aug 3, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "SalahSoliman",
            "datetime": "Aug 3, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "glopezdiest",
            "datetime": "Sep 27, 2022",
            "body": "Closing this as it has been answered already",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "glopezdiest",
            "datetime": "Sep 27, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/TalAter/annyang/issues/288",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Hakim2014",
            "datetime": "Jun 4, 2017",
            "body": "Hi,I am working on a small project on raspberry pi, which have voice functionality. I am using electron + annyang. I have included annyang as  const annyang = require('annyang') in my index.html and provided google cloud speech api key , client key and secret key in main.js of electron. I have attached a call back function to annyang and it always give me network error on my windows machine and on my raspberry pi it gives me audio-capture error. I also tried to setup the keys in environment variable but no luck. May you help me out figuring the issue.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/4634",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "moro23",
            "datetime": "Jul 5, 2022",
            "body": "common_voice_train = load_dataset(\"common_voice\", \"ha\", split=\"train+validation\")",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "khushmeeet",
            "datetime": "Jul 15, 2022",
            "body": "Could you provide the error details. It is difficult to debug otherwise. Also try other config.  is not a valid.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Sep 13, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/4163",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "TristanThrush",
            "datetime": "Apr 13, 2022",
            "body": "\nA clear and concise description of what the problem is.We now have hate speech datasets on the hub, like this one: I'm wondering if there is an option to select a content warning message that appears before the dataset preview? Otherwise, people immediately see hate speech when clicking on this dataset.\nA clear and concise description of what you want to happen.Implementation of a content warning message that separates users from the dataset preview until they click out of the warning.\nA clear and concise description of any alternative solutions or features you've considered.Possibly just a way to remove the dataset preview completely? I think I like the content warning option better, though.\nAdd any other context about the feature request here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "May 2, 2022",
            "body": "Hi! You can use the  YAML field in a dataset card for displaying custom messages/warnings that the user must accept before gaining access to the actual dataset. This option also keeps the viewer hidden until the user agrees to terms.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HannahKirk",
            "datetime": "May 10, 2022",
            "body": "Hi , thanks for explaining how to add this feature.If the current dataset yaml is:Can you provide a minimal working example of how to added the gated prompt?Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "leondz",
            "datetime": "May 10, 2022",
            "body": "+ enable  under the Settings pane.There's a brief guide here  , and you can see the field in action here,  (you need to agree the terms in the Dataset Card pane to be able to access the files pane, so this comes up 403 at first).And a working example here!  :) Great to be able to mitigate harms in text.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "leondz",
            "datetime": "May 10, 2022",
            "body": "-- is there a way to gate content anonymously, i.e. without registering which users access it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Breakend",
            "datetime": "Jun 9, 2022",
            "body": "+1 to 's question. One scenario is if you don't want the dataset to be indexed by search engines or viewed in browser b/c of upstream conditions on data, but don't want to collect emails. Some ability to turn off the dataset viewer or add a gating mechanism without emails would be fantastic.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TristanThrush",
            "datetime": "Apr 13, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3978",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "kingabzpro",
            "datetime": "Mar 21, 2022",
            "body": " Am I the one who added this dataset ? Yes",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "severo",
            "datetime": "Mar 24, 2022",
            "body": "the dataset viewer is working on this dataset. I imagine the issue is that we would expect to be able to listen to the audio files in the  column, right?maybe  or  could help",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Mar 29, 2022",
            "body": "The structure of the dataset is not supported. Only the CSV file is parsed and the audio files are ignored.We're working on supporting audio datasets with a specific structure in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kingabzpro",
            "datetime": "Mar 29, 2022",
            "body": "Got it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kingabzpro",
            "datetime": "Mar 21, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "severo",
            "datetime": "Mar 21, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "kingabzpro",
            "datetime": "Mar 29, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "severo",
            "datetime": "Apr 4, 2022",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "severo",
            "datetime": "Apr 4, 2022",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1585",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "nsubiron",
            "datetime": "Apr 29, 2019",
            "body": "Make a Dockerfile that allows us building an image with a Windows 10 environment set up for building Carla.The main complication is having Unreal built without redistributing the code/binaries.See also .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cmpute",
            "datetime": "Jul 7, 2019",
            "body": "I guess this tool can help with dockerize UE4: I didn't have enough resources to build the image so I didn't try it. But I think it can be useful for building a docker image for CARLA under windows and linux ()",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cmpute",
            "datetime": "Jul 7, 2019",
            "body": "Also, make a docker image can make building on windows more easily... Definitely hope to have a docker image, and happy to help with it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jun 26, 2020",
            "body": "I would love to see this done too.. does anybody want to help?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TKeutgens",
            "datetime": "Sep 23, 2020",
            "body": "I'm no Docker expert but I'm currently trying to build the Docker image on Windows. I managed to install all pre-requesites. Unfortunately the docker build process takes very very long. Currently I'm already waiting for almost 24 h. I feel like the process is stuck during building the UE4 EditorDoes anyone has any suggestions?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Apr 29, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jun 26, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/4776",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "albertvillanova",
            "datetime": "Aug 1, 2022",
            "body": "Current version of  (0.12.0) raises a RuntimeError when trying to use  backend but non-Python dependency  is not installed:\nMaybe we should raise a more actionable error message so that the user knows how to fix it.UPDATE:TODO:Related to:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fxtentacle",
            "datetime": "Aug 8, 2022",
            "body": "Requiring torchaudio<0.12.0 isn't really a viable solution because that implies torch<0.12.0 which means no sm_86 CUDA support which means no RTX 3090 support in PyTorch.But in my case, the error only occurs if  resolves to  inside torchaudio 0.12.0 which is only the case if FFMPEG initialization failed:  That means the proper solution for torchaudio>=0.12.0 is to check  and if it is False, then we need to remind the user to install a dynamically linked ffmpeg 4.1.8 and then maybe call  to force a user-visible exception showing the missing ffmpeg dynamic library name.On my system, installingfrom ffmpeg 4.1.8 made HF datasets 2.3.2 work just fine with  torchaudio 0.12.1+cu116:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 24, 2022",
            "body": "Related: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Aug 1, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "huggingface/transformers#18379"
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Aug 2, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Aug 3, 2022",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 26, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "pytorch/audio#2652"
        },
        {
            "user_name": "polinaeterna",
            "datetime": "Aug 31, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#4923"
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Sep 21, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#3663"
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3720",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aasem",
            "datetime": "Feb 14, 2022",
            "body": "Missing language in Common Voice dataset I tried to call the Urdu dataset using  but couldn't due to builder configuration not found. I checked the source file here for the languages support:and Urdu isn't included there. I assume a quick update will fix the issue as Urdu speech is now available at the Common Voice dataset.Am I the one who added this dataset? No",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Feb 15, 2022",
            "body": "Hi , thanks for reporting.Please note that currently Commom Voice is hosted on our Hub as a community dataset by the Mozilla Foundation. See all Common Voice versions here: Maybe we should add an explaining note in our \"legacy\" Common Voice canonical script? What do you think   ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aasem",
            "datetime": "Feb 15, 2022",
            "body": "Thank you, , for the quick response. I am not sure about the exact flow but I guess adding the following lines under the  dictionary definition in  might resolve the issue. I guess the dataset is recently made available so the file needs updating.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Feb 15, 2022",
            "body": " for compliance reasons, we are no longer updating the  script.We agreed with Mozilla Foundation to use their community datasets instead, which will ask you to accept their terms of use:In order to use e.g. their Common Voice dataset version 8.0, please:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Feb 15, 2022",
            "body": "Yes, I agree we should have a deprecation notice in the canonical script to redirect users to the new script.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aasem",
            "datetime": "Feb 15, 2022",
            "body": ",\nI now get the following error after downloading my access token from the huggingface and passing it to  call:Any quick pointer on how it might be resolved?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Feb 15, 2022",
            "body": " What version of  are you using? We renamed that attribute from  to  fairly recently, so updating to the newest version should resolve the issue:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aasem",
            "datetime": "Feb 15, 2022",
            "body": "Thanks a lot, . That completely resolved the issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aasem",
            "datetime": "Feb 14, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Feb 15, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3909",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aliceinland",
            "datetime": "Mar 14, 2022",
            "body": "When loading the Common_Voice dataset, by downloading it directly from the Hugging Face hub, some files can not be opened.The common voice dataset downloaded and correctly loaded whit the use of the hugging face datasets library.The error is:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Mar 28, 2022",
            "body": "Hi ! It could an issue with torchaudio, which version of torchaudio are you using ? Can you also try updating  to 2.0.0 and see if it works ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zeboku",
            "datetime": "Mar 28, 2022",
            "body": "I  have a similar issue. I'm trying to use the librispeech_asr dataset and read it with soundfile.The code is taken directly from \"\".The short error code is \"RuntimeError: Error opening '6930-75918-0000.flac': System error.\" (it can't find the first file), and I agree, I can't find the file either. The dataset has downloaded correctly (it says), but on the location, there are only \".arrow\" files, no \".flac\" files.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Mar 29, 2022",
            "body": "Hi ! In  2.0 can access the audio array with  already, no need to use . See our documentation on  :)cc  we will need to update the readme at  as well as ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zeboku",
            "datetime": "Mar 29, 2022",
            "body": "Thanks!And sorry for posting this problem in what turned on to be an unrelated thread.I rewrote the code, and the model works. The WER is 0.137 however, so I'm not sure if I have missed a step. I will look further into that at a later point. The transcriptions look good through manual inspection.The rewritten code:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Mar 30, 2022",
            "body": "I think the issue comes from the fact that you set  while  still returns a list of strings for \"transcription\". You can fix it by adding  at the end of this line to get the string:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Apr 5, 2022",
            "body": "Updating as many model cards now as I can find",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Apr 5, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aliceinland",
            "datetime": "Mar 14, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/4089",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "HankYuan1101",
            "datetime": "Apr 14, 2021",
            "body": "Hi, there\nI follow the Linux build instruction and I got an error while executing make launch.\nThe error shown as follow:OS: ubuntu 18.04\ncarla version: 0.9.11\nunreal version: 4.24any suggestion is welcome, thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "onebitme",
            "datetime": "Apr 15, 2021",
            "body": "Can you please check the version of gcc,\nI am currently using v 7.5.0",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HankYuan1101",
            "datetime": "Apr 19, 2021",
            "body": "My gcc version is also 7.5.0\ngcc --version\ngcc (Ubuntu 7.5.0-3ubuntu118.04) 7.5.0Are those difference with your version?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Apr 20, 2021",
            "body": "Hi  could you attach the full output of any error you get after running",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HankYuan1101",
            "datetime": "Apr 23, 2021",
            "body": "Hi , thanks for your feedback.There is a lot of \"can't find file\" in the output. I'm not sure the connection between missing files and this error message.\nBut I export the UnrealUngine directory in the bashrc. Is there other directory setting?Here is my full output after running your suggestion",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Apr 23, 2021",
            "body": "When you ran the command from this part of the , did Unreal start up successfully?And you downloaded all the assets correctly and they were extracted to ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seuwcs",
            "datetime": "Jun 17, 2021",
            "body": "Hello, I have met a question that after run the code:\ncd ~/UnrealEngine_4.24/Engine/Binaries/Linux && ./UE4Editor\nin the terminal. The Unreal Engine could not start up, and the output is as below.Could you give me some suggestions on solving this question?  Thank you !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ryandoren",
            "datetime": "Jul 1, 2021",
            "body": "Hello , I ran into the same problem as yours, have you solved it yet?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 28, 2022",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HankYuan1101",
            "datetime": "Apr 14, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Apr 20, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Apr 20, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Apr 28, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3577",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "omarespejel",
            "datetime": "Jan 13, 2022",
            "body": "Instructions to add a new dataset can be found .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "omarespejel",
            "datetime": "Jan 13, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Jan 27, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3393",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "wiedymi",
            "datetime": "Dec 7, 2021",
            "body": "Instructions to add a new dataset can be found .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wiedymi",
            "datetime": "Dec 7, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Dec 9, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/2730",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yjernite",
            "datetime": "Jul 29, 2021",
            "body": "Instructions to add a new dataset can be found .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yjernite",
            "datetime": "Jul 29, 2021",
            "body": "cc ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 7, 2021",
            "body": "Does anybody know if there is a bundled link, which would allow direct data download instead of manual?\nSomething similar to:  ? cc ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 7, 2021",
            "body": "Also see: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yjernite",
            "datetime": "Jul 29, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 7, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#2771"
        },
        {
            "user_name": "DewiBrynJones",
            "datetime": "Sep 17, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "common-voice/common-voice#3262"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/166",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "stone-tong",
            "datetime": "Mar 10, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Mar 11, 2022",
            "body": "Can you use IDF version release/v4.2 and try again?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stone-tong",
            "datetime": "Mar 11, 2022",
            "body": "change esp-idf version to 4.2 and after compile, getting the following error\nso this function is not defined.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arunrkumaran",
            "datetime": "Jun 9, 2022",
            "body": "undefined reference to `xTaskNotify'\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/3663",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": "The path should be the complete absolute path to the downloaded audio file not some relative path.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": "Having talked to , I see that this feature is no longer supported.I really don't think this was a good idea. It is a major breaking change and one for which we don't even have a working solution at the moment, which is bad for PyTorch as we don't want to force people to have  decode audio files automatically, but  bad for Tensorflow and Flax where we  even use  to load  files - e.g.  doesn't work anymore in a TF training script. Note this worked perfectly fine before making the change (think it was done  no?)IMO, it's really important to think about a solution here and I strongly favor to make a difference here between loading a dataset in streaming mode and in non-streaming mode, so that in non-streaming mode the actual downloaded file is displayed. It's really crucial for people to be able to analyse the original files IMO when the dataset is not downloaded in streaming mode.There are the following reasons why it is paramount to have access to the  audio file in my opinion (in non-streaming mode):=> IMO, it's a  big priority to again have the correct absolute path in non-streaming mode. The other solution of providing a path-like object derived from the bytes stocked in the  file is not nearly as user-friendly, but better than nothing.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cahya-wirawan",
            "datetime": "Feb 1, 2022",
            "body": "Agree that we need to have access to the original sound files. Few days ago I was looking for these original files because I suspected there is bug in the audio resampling (confirmed in ) and I want to do my own resampling to workaround the bug, which is now not possible anymore due to the unavailability of the original files.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Feb 7, 2022",
            "body": "Just to clarify, here you describe the approach that uses the  attribute to access the underlying bytes?I'd assume this is because we use  as a backend for decoding. However, soon we should be able to use , which supports path-like objects, for MP3 ().Your concern is reasonable, but there are situations where we can only serve bytes (see  for instance). IMO it makes sense to fix the affected datasets for now, but I don't think we should care too much whether we rely on local paths or bytes after soundfile adds support for MP3 as long as our examples work (shouldn't be too hard to update the  functions) and we properly document how to access the underlying path/bytes for custom decoding (via ).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Feb 7, 2022",
            "body": "Related to this discussion: in  I propose how we could change  to work for streaming and also return local paths (as it used too !). I'd love your opinions on this",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 8, 2022",
            "body": "Yes!Yes this might be, but I highly doubt that  is the go-to library for audio then.  and I have tried out a bunch of different audio loading libraries (, , , pure , , ...). One thing that was pretty clear  to me is that there is just no \"de-facto standard\" library and they all have pros and cons. None of the libraries really supports \"batch\"-ed audio loading. Some depend on PyTorch.  is 100x faster (really!) than  fallback on MP3.  often has problems with multi-proessing, ... Also we should keep in mind that resampling is similarly not as simple as reading a text file. It's a pretty complex signal processing transform and people very well might want to use special filters, etc...at the moment we just hard-code  or  default filter when doing resampling.=> All this to say that we  care about whether we rely on local paths or bytes IMO. We don't want to loose all users that are forced to use  decoding or resampling or have to built a very much not intuitive way of loading bytes into a numpy array. It's much more intuitive to be able to inspect a local file. I feel pretty strongly about this and am happy to also jump on a call. Keeping libraries flexible and lean as well as exposing internals is very important IMO (this philosophy has worked quite well so far with Transformers).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mariosasko",
            "datetime": "Feb 8, 2022",
            "body": "Thanks a lot for the very detailed explanation. Now everything makes much more sense.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lhoestq",
            "datetime": "Feb 22, 2022",
            "body": "From  the Common Voice dataset now gives access to the local audio files as before",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertz",
            "datetime": "Apr 21, 2022",
            "body": "I understand the argument that it is bad to have a breaking change. How to deal with the introduction of breaking changes is a topic of its own and not sure how you want to deal with that (or is the policy this is never allowed, and there must be a  or so if you really want to introduce a breaking change?).Regardless of whether it is a breaking change, however, I don't see the other arguments.I don't exactly understand this. Why not?Why does the HF dataset on-the-fly decoding mechanism not work? Why is it anyway specific to PyTorch or TensorFlow? Isn't this independent?But even if you just provide the raw bytes to TF, on TF you could just use sth like  or  or ?I don't really understand the arguments (despite that it maybe breaks existing code). You anyway have the original audio files but it is just embedded in the dataset? I don't really know about any library which cannot also load the audio from memory (i.e. from the dataset).Btw, on librosa being slow for decoding audio files, I saw that as well, so we have this comment RETURNN:Resampling is also a separate aspect, which is also less straightforward and with different compromises between speed and quality. So there the different tradeoffs and different implementations can make a difference.However, I don't see how this is related to the question whether there should be the raw bytes inside the dataset or as separate local files.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Apr 21, 2022",
            "body": "Thanks for your comments here  - cool to get your input!Answering a bit here between the lines:The problem with decoding on the fly is that we currently rely on  for this now which relies on  which is not necessarily something people would like to install when using  or . Therefore we cannot just rely on people using the decoding on the fly method. We just didn't find a library that is ML framework independent and fast enough for all formats.  is currently in our opinion by far the best here.So for TF and Flax it's important that users can load audio files or bytes they way the want to - this might become less important if we find (or make) a good library with few dependencies that is fast for all kinds of platforms / use cases.Now the question is whether it's better to store audio data as a path to a file or as raw bytes I guess.\nMy main arguments for storing the audio data as a path to a file is pretty much all about users experience - I don't really expect our users to understand the inner workings of datasets:But the argument that the audio should be loadable directly from memory is good - haven't thought about this too much.\nI guess it's still very much possible for the user to do this:Guess the question is more a bit about what should be the default case?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertz",
            "datetime": "Apr 21, 2022",
            "body": "But how is this relevant for this issue here? I thought this issue here is about having the (correct) path in the dataset or having raw bytes in the dataset.How did TF users use it at all then? Or they just do not use on-the-fly decoding? I did not even notice this problem (maybe because I had  installed). But what do they use instead?But as I outlined before, they could just use  and co, where it would be more natural if you already provide the raw bytes.I was not really familiar with . It seems that they really don't provide an easy/direct API to operate on raw bytes. Which is very strange and unfortunate because as far as I can see, all the underlying backend libraries (e.g. soundfile) easily allow that. So I would say that this is the fault of  then. But despite, if you anyway use  with  backend, why not just use  directly. It's very simple to use and crossplatform.But ok, now we are just discussing how to handle the on-the-fly decoding. I still think this is a separate issue and having raw bytes in the dataset instead of local files should just be fine as well.I think nobody who writes code is scared by seeing the raw bytes content of a binary file. :)In , you said/proposed that this  is not needed anymore and  could do it automatically (maybe via some option)?Yea this is up to you. I'm happy as long as we can get it the way we want easily and this is a well supported use case. :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Apr 21, 2022",
            "body": "Yes! Should be super easy now see discussion here: Thanks for the super useful input :-)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DCNemesis",
            "datetime": "Jul 13, 2022",
            "body": "Despite the comments that this has been fixed, I am finding the exact same problem is occurring again (with datasets version 2.3.2)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DCNemesis",
            "datetime": "Jul 13, 2022",
            "body": "It appears downgrading to torchaudio 0.11.0 fixed this problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Aug 23, 2022",
            "body": ", sorry which problem exactly is occuring again? Also cc   here",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DCNemesis",
            "datetime": "Aug 23, 2022",
            "body": "   I was unable to load audio from Common Voice using  with the current version of torchaudio, but downgrading to torchaudio 0.11.0 fixed it. This is probably more of a torch problem than a Hugging Face problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "polinaeterna",
            "datetime": "Aug 24, 2022",
            "body": " that's interesting, could you please share the error message if you still can access it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DCNemesis",
            "datetime": "Aug 24, 2022",
            "body": " I believe it is the same exact error as above. It occurs on other .mp3 sources as well, but the problem is with torchaudio > 0.11.0. I've created a short colab notebook that reproduces the error, and the fix here: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Sep 21, 2022",
            "body": "Hi ,Your issue was slightly different from the original one in this issue page. Yours seems related to a change in the backend used by  ( instead of ). Refer to the issue page here:Normally, it should be circumvented with the patch made by  in:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Sep 21, 2022",
            "body": "I think the original issue reported here was already fixed by:Otherwise, feel free to reopen.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Feb 1, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#3662"
        },
        {
            "user_name": "anton-l",
            "datetime": "Feb 1, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3664"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#4184"
        },
        {
            "user_name": "polinaeterna",
            "datetime": "Aug 31, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#4923"
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Sep 21, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Sep 21, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#3736"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/164",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "chaitanya-smartrotamac",
            "datetime": "Dec 15, 2021",
            "body": "amazon_alexa with Set ALEXA_BT=1 enabled.\nBut with LyraT board i am getting this error.E (1955) spi_flash: Detected size(8192k) smaller than the size in the binary image header(16384k). Probe failed.\nassertion \"flash_ret == ESP_OK\" failed: file \"C:/Users/LENOVO/Desktop/esp-idf/components/esp32/cpu_start.c\", line 472, function: start_cpu0_defaultabort() was called at PC 0x40201500 on core 0\n0x40201500: __assert_func at /builds/idf/crosstool-NG/.build/HOST-x86_64-w64-mingw32/xtensa-esp32-elf/src/newlib/newlib/libc/stdlib/assert.c:62 (discriminator 8)I think we cant enable bluetooth with alexa in LyraT board is it true?? due to memory..Can I use memory card someway for this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 15, 2021",
            "body": "Hi \nIf you want to have OTA support as well, you might need a board with 16MB flash.\nIf not, you can remove ota_1 partition from partition.csv in examples directory and set flash size to 8MB via menuconfig->Serial flash config option.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Marcoz08",
            "datetime": "Dec 16, 2021",
            "body": "I have the same error. I did what you said, and removed ota_1 but now it doesn't compile the code.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 17, 2021",
            "body": "can you provide your updated partitions.csv and the error log?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chaitanya-smartrotamac",
            "datetime": "Dec 17, 2021",
            "body": "Thanks \nBut with this, I lost the Alexa wake word feature.\nBoard is not responding for wakeword. But responding to button.\nBluetooth working now.Log :\nentry 0x40080710\nI (29) boot: ESP-IDF v4.2.2-dirty 2nd stage bootloader\nI (29) boot: compile time 10:06:55\nI (29) boot: chip revision: 3\nI (32) qio_mode: Enabling default flash chip QIO\nI (38) boot.esp32: SPI Speed      : 80MHz\nI (42) boot.esp32: SPI Mode       : QIO\nI (47) boot.esp32: SPI Flash Size : 8MB\nI (51) boot: Enabling RNG early entropy source...\nI (57) boot: Partition Table:\nI (60) boot: ## Label            Usage          Type ST Offset   Length\nI (68) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (75) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (83) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (90) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (98) boot:  4 ota_0            OTA app          00 10 00020000 00480000\nI (105) boot: End of partition table\nI (109) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1cde7c (1891964) map\nI (676) esp_image: segment 1: paddr=0x001edea4 vaddr=0x3ffbdb60 size=0x02174 (  8564) load\nI (679) esp_image: segment 2: paddr=0x001f0020 vaddr=0x400d0020 size=0x141b88 (1317768) map\n0x400d0020: _stext at ??:?I (1070) esp_image: segment 3: paddr=0x00331bb0 vaddr=0x3ffbfcd4 size=0x036f0 ( 14064) load\nI (1075) esp_image: segment 4: paddr=0x003352a8 vaddr=0x40080000 size=0x18a50 (100944) load\n0x40080000: _WindowOverflow4 at C:/Users/LENOVO/Desktop/esp-idf/components/freertos/xtensa/xtensa_vectors.S:1730I (1126) boot: Loaded app from partition at offset 0x20000\nI (1127) boot: Disabling RNG early entropy source...\nI (1127) psram: This chip is ESP32-D0WD\nI (1132) spiram: Found 64MBit SPI RAM device\nI (1137) spiram: SPI RAM mode: flash 80m sram 80m\nI (1142) spiram: PSRAM initialized, cache is in low/high (2-core) mode.\nI (1149) cpu_start: Pro cpu up.\nI (1153) cpu_start: Application information:\nI (1158) cpu_start: Project name:     avs\nI (1163) cpu_start: App version:      1.0\nI (1167) cpu_start: Compile time:     Dec 17 2021 10:06:19\nI (1174) cpu_start: ELF file SHA256:  e9ffccbdbd4e6b33...\nI (1180) cpu_start: ESP-IDF:          v4.2.2-dirty\nI (1185) cpu_start: Starting app cpu, entry point is 0x40081dc8\n0x40081dc8: call_start_cpu1 at C:/Users/LENOVO/Desktop/esp-idf/components/esp32/cpu_start.c:287I (0) cpu_start: App cpu up.\nI (1683) spiram: SPI SRAM memory test OK\nI (1684) heap_init: Initializing. RAM available for dynamic allocation:\nI (1684) heap_init: At 3FFAFF10 len 000000F0 (0 KiB): DRAM\nI (1689) heap_init: At 3FFB6388 len 00001C78 (7 KiB): DRAM\nI (1696) heap_init: At 3FFB9A20 len 00004108 (16 KiB): DRAM\nI (1702) heap_init: At 3FFC7878 len 00018788 (97 KiB): DRAM\nI (1708) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\nI (1715) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\nI (1721) heap_init: At 40098A50 len 000075B0 (29 KiB): IRAM\nI (1727) cpu_start: Pro cpu start user code\nI (1732) spiram: Adding pool of 4077K of external SPI memory to heap allocator\nI (1753) spi_flash: detected chip: generic\nI (1753) spi_flash: flash io: qio\nI (1753) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (1762) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (1772) [app_main]: ==== Voice Assistant SDK version: v1.2.2 ====\nI (1782) [time_utils]: Initializing SNTP.\nI (1782) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (1822) wifi:wifi driver task: 3ffd5e34, prio:23, stack:6656, core=0\nI (1822) system_api: Base MAC address is not set\nI (1822) system_api: read default base MAC address from EFUSE\nI (1832) wifi:wifi firmware version: bb6888c\nI (1832) wifi:wifi certification version: v7.0\nI (1832) wifi:config NVS flash: enabled\nI (1832) wifi:config nano formating: disabled\nI (1842) wifi:Init data frame dynamic rx buffer num: 20\nI (1842) wifi:Init management frame dynamic rx buffer num: 20\nI (1852) wifi:Init management short buffer num: 32\nI (1852) wifi:Init static tx buffer num: 32\nI (1862) wifi:Init tx cache buffer num: 32\nI (1862) wifi:Init static rx buffer size: 1600\nI (1862) wifi:Init static rx buffer num: 12\nI (1872) wifi:Init dynamic rx buffer num: 20\nI (1872) wifi_init: rx ba win: 16\nI (1882) wifi_init: tcpip mbox: 32\nI (1882) wifi_init: udp mbox: 6\nI (1882) wifi_init: tcp mbox: 12\nI (1892) wifi_init: tcp tx win: 14360\nI (1892) wifi_init: tcp rx win: 14360\nI (1902) wifi_init: tcp mss: 1440\nI (1902) wifi_init: WiFi/LWIP prefer SPIRAM\nI (1912) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (1912) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (1922) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (1942) esp_codec_es8388: Initialising esp_codec\nI (1942) gpio: GPIO[21]| InputEn: 0| OutputEn: 1| OpenDrain: 0| Pullup: 0| Pulldown: 0| Intr:0\nI (2402) button_driver_gpio: Initialising button driver\nI (2402) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2402) gpio: GPIO[39]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2412) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2422) led_driver_esp_ledc: Initialising led driver\nI (2432) [scli]: Initialising UART on port 0\nI (2432) [diag_cli]: Registering command: up-time\nI (2432) uart: queue free spaces: 8I (4912) wifi<1,0>, old:<1,0>, ap:<255,255>, sta:<1,0>, prof:1\nI (4912) wifi:state: init -> auth (b0)\nI (4942) wifi:state: auth -> assoc (0)\nI (4942) wifi:state: assoc -> run (10)\nI (4962) wifi:connected with Office, aid = 5, channel 1, BW20, bssid = 5a:96:1d:f4:75:db\nI (4962) wifi:security: WPA2-PSK, phy: bgn, rssi: -43\nI (4972) wifi:pm start, type: 1I (5062) wifi:AP's beacon interval = 102400 us, DTIM period = 3\n[app_wifi]: Connected with IP Address: 192.168.137.196\nI (5812) esp_netif_handlers: sta ip: 192.168.137.196, mask: 255.255.255.0, gw: 192.168.137.1\nI (5812) [va_nvs_utils]: No value set for: friendlyname\nI (5822) SSDP: Network Interface List (1):\nI (5822) [time_utils]: SNTP already initialized.\nI (5822) SSDP: 1. wifi  : 192.168.137.196\nI (5832) LSSDP: create SSDP socket 57\nI (5852) [tls_certification]: Done setting global CA store\nI (5852) auth-delegate-config: Client ID or Refresh token found in NVS\nI (5862) [va_nvs_utils]: No value set for: clientSecret\nI (5862) auth-delegate-config: Returning auth delegate subsequent auth configuration\n[alexa]: Authentication done\nI (5872) [network_diagnostics]: Registering command: ping\nI (5882) [app_va_cb]: Dialog state is: 8\n[dialog]: Entering VA_IDLE\nI (5922) [alexa_playback_controller_cli]: Registering command: button\nI (5932) [alexa_bt]: Min. Ever Free Size        78316   1931664\nI (5942) [bluetooth]: Enabling BT host and controller\nI (5942) BTDM_INIT: BT controller compile version [ba56601]\nI (5952) phy_init: phy_version 4660,0162888,Dec 23 2020\nI (6312) [va_nvs_utils]: No value set for: locale\nI (6312) [va_nvs_utils]: No value set for: second_locale\nE (6312) BT_AV: a2dp invalid cb event: 4\n[speaker]: Volume changed to 59\nI (6352) [audio_codec]: Starting mp3_decoder codec\nI (6352) [alexa_player]: Init done\nI (6362) audio_stream: Starting http_stream stream\nI (6362) [audio_codec]: Starting mp3_decoder codec\nI (6362) [va_nvs_utils]: No value set for: dnd\nI (6372) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (20792) [time_utils]: The current time is: Fri Dec 17 04:56:25 2021 +0000[GMT], DST: No.\nI (21372) [time_utils]: The current time is: Fri Dec 17 04:56:25 2021 +0000[GMT], DST: No.\nI (21372) [va_nvs_utils]: No value set for: gateway\n[apigateway_handler]: Cannot find endpoint URL in NVS. Setting default: \nI (21382) [va_nvs_utils]: No value set for: endpoint\n[apigateway_handler]: AVS endpoint: \nI (21392) [http_transport]: Connecting to server: \nI (21402) [sh2lib]: [sh2-connect] Setting default tls_cfg parameter for alpn_proto.\nI (23422) [http_transport]: HTTP2 Connection done\nI (23422) [http_transport]: Set AVS connection packet priority to Voice\n[http_stream]: [stream_new]: Internal: 20464, External: 982728\n[auth-delegate]: Token will be refreshed after 3000 seconds.\n[http_stream]: [stream_get: 1]: /v20160207/directives\n[http_stream]: [sid: 1] Response code: 200\nI (23812) [http_stream]: [sid: 1] Response content type is multipart/related; boundary=------abcde123; type=application/json\nI (23812) [http_stream]: Final boundary:------abcde123\nI (23822) [http_stream]: Speculatively stopping capture\nI (23832) [http_stream]: Part begin\n[http_transport]: AVS level connction has now been established: \nI (23842) [http_transport]: Enqueueing SynchronizeState to the front\n[http_stream]: [stream_new]: Internal: 19896, External: 979140\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"Bluetooth\",\"name\":\"BluetoothState\"},\"payload\":{\"alexaDevice\":{\"friendlyName\":\"ESP32-0E0\"},\"pairedDevices\": [{\"uniqueDeviceId\":\"d246b1df-2144-42b1-a408-192eebe497b7\",\"friendlyName\":\"ChaituK20Pro\",\"supportedProfiles\":[{\"name\":\"A2DP-SOURCE\",\"version\":\"\"}]}]}},{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"FINISHED\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"System\",\"name\":\"SynchronizeState\",\"messageId\":\"261180f9-4e69-6a0b-d1bd-d237c8dda093\"},\"payload\":{}}}\n[http_stream]: [stream_post: 3]: /v20160207/events\n[http_stream]: [sid: 3] Response code: 204\nI (24522) [http_transport]: [sid: 3] stream close\n[22 seconds]: [http_stream]: [stream_delete: 3] Internal: 19324, External: 980496, min ever internal: 17880, largest free block: 17844\n[http_stream]: [stream_new]: Internal: 19484, External: 980704\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"Bluetooth\",\"name\":\"BluetoothState\"},\"payload\":{\"alexaDevice\":{\"friendlyName\":\"ESP32-0E0\"},\"pairedDevices\": [{\"uniqueDeviceId\":\"d246b1df-2144-42b1-a408-192eebe497b7\",\"friendlyName\":\"ChaituK20Pro\",\"supportedProfiles\":[{\"name\":\"A2DP-SOURCE\",\"version\":\"\"}]}]}},{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"FINISHED\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"Alexa.ApiGateway\",\"name\":\"VerifyGateway\",\"messageId\":\"e21a08ae-f65a-d521-e6ce-4c111c31c485\"},\"payload\":{}}}\n[http_stream]: [stream_post: 5]: /v20160207/events\n[http_stream]: [sid: 5] Response code: 204\n[alexa_discovery]: Capabilities unchanged\nI (25052) [esp_dsp]: Created I2S audio stream\nI (25052) audio_stream: Starting i2s_reader stream\nI (25062) audio_stream: Starting audio stream i2s_reader\nI (25062) audio_stream: Stream i2s_reader Event Started\nI (25072) [esp_dsp]: Reader stream event 2\nI (25082) audio_stream: Stopping audio stream i2s_reader\nI (25082) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (25102) audio_stream: Starting audio stream i2s_reader\nW (25152) wifi:m f nullI (25362) [va_nvs_utils]: No value set for: alert_0_data\nI (25362) [va_nvs_utils]: No value set for: alert_1_data\nI (25362) [va_nvs_utils]: No value set for: alert_2_data\nI (25362) [va_nvs_utils]: No value set for: alert_3_data\nI (25372) [alerts_nvs]: Alerts loaded from NVS\n############## Alexa is ready ##############\nI (26152) [http_transport]: [sid: 5] stream close\n[24 seconds]: [http_stream]: [stream_delete: 5] Internal: 7392, External: 867036, min ever internal: 92, largest free block: 7016\nI (61882) [network_diagnostics]: Network rssi: -41",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chaitanya-smartrotamac",
            "datetime": "Dec 17, 2021",
            "body": "How to use my own certificates for this example amazon_alexa ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Marcoz08",
            "datetime": "Dec 18, 2021",
            "body": "Thanks, I resolved the partitions.csv, code build and flash successfully but i can't connect to app.Log:`entry 0x40080710\nI (29) boot: ESP-IDF v4.2.2-dirty 2nd stage bootloader\nI (29) boot: compile time 17:53:31\nI (29) boot: chip revision: 3\nI (32) boot_comm: chip revision: 3, min. bootloader chip revision: 0\nI (40) qio_mode: Enabling default flash chip QIO\nI (45) boot.esp32: SPI Speed      : 80MHz\nI (49) boot.esp32: SPI Mode       : QIO\nI (54) boot.esp32: SPI Flash Size : 8MB\nI (59) boot: Enabling RNG early entropy source...\nI (64) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (82) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (90) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (97) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (105) boot:  4 ota_0            OTA app          00 10 00020000 00480000\nI (112) boot: End of partition table\nI (117) boot: No factory image, trying OTA 0\nI (121) boot_comm: chip revision: 3, min. application chip revision: 0\nI (129) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1cfb04 (1899268) map\nI (766) esp_image: segment 1: paddr=0x001efb2c vaddr=0x3ffbdb60 size=0x004ec (  1260) load\nI (767) esp_image: segment 2: paddr=0x001f0020 vaddr=0x400d0020 size=0x150f18 (1380120) map\n0x400d0020: _stext at ??:?I (1229) esp_image: segment 3: paddr=0x00340f40 vaddr=0x3ffbe04c size=0x0590c ( 22796) load\nI (1238) esp_image: segment 4: paddr=0x00346854 vaddr=0x40080000 size=0x1dde0 (122336) load\n0x40080000: _WindowOverflow4 at C:/Users/MARCOZ/Desktop/esp-idf/components/freertos/xtensa/xtensa_vectors.S:1730I (1304) boot: Loaded app from partition at offset 0x20000\nI (1304) boot: Set actual ota_seq=1 in otadata[0]\nI (1304) boot: Disabling RNG early entropy source...\nI (1310) psram: This chip is ESP32-D0WD\nI (1314) spiram: Found 64MBit SPI RAM device\nI (1319) spiram: SPI RAM mode: flash 80m sram 80m\nI (1324) spiram: PSRAM initialized, cache is in low/high (2-core) mode.\nI (1332) cpu_start: Pro cpu up.\nI (1336) cpu_start: Application information:\nI (1340) cpu_start: Project name:     avs\nI (1345) cpu_start: App version:      1.0\nI (1350) cpu_start: Compile time:     Dec 17 2021 17:53:00\nI (1356) cpu_start: ELF file SHA256:  da111424e87f7aac...\nI (1362) cpu_start: ESP-IDF:          v4.2.2-dirty\nI (1368) cpu_start: Starting app cpu, entry point is 0x40082010\n0x40082010: call_start_cpu1 at C:/Users/MARCOZ/Desktop/esp-idf/components/esp32/cpu_start.c:287I (0) cpu_start: App cpu up.\nI (1876) spiram: SPI SRAM memory test OK\nI (1878) heap_init: Initializing. RAM available for dynamic allocation:\nI (1878) heap_init: At 3FFAFF10 len 000000F0 (0 KiB): DRAM\nI (1883) heap_init: At 3FFB6388 len 00001C78 (7 KiB): DRAM\nI (1889) heap_init: At 3FFB9A20 len 00004108 (16 KiB): DRAM\nI (1896) heap_init: At 3FFC7E40 len 000181C0 (96 KiB): DRAM\nI (1902) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\nI (1908) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\nI (1915) heap_init: At 4009DDE0 len 00002220 (8 KiB): IRAM\nI (1921) cpu_start: Pro cpu start user code\nI (1926) spiram: Adding pool of 4077K of external SPI memory to heap allocator\nI (1946) spi_flash: detected chip: generic\nI (1947) spi_flash: flash io: qio\nI (1947) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (1955) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (1965) [app_main]: ==== Voice Assistant SDK version: v1.2.2 ====\nI (1975) [time_utils]: Initializing SNTP.\nI (1975) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (2025) wifi:wifi driver task: 3ffd63fc, prio:23, stack:6656, core=0\nI (2025) system_api: Base MAC address is not set\nI (2025) system_api: read default base MAC address from EFUSE\nI (2035) wifi:wifi firmware version: bb6888c\nI (2045) wifi:wifi certification version: v7.0\nI (2045) wifi:config NVS flash: enabled\nI (2045) wifi:config nano formating: disabled\nI (2045) wifi:Init data frame dynamic rx buffer num: 20\nI (2045) wifi:Init management frame dynamic rx buffer num: 20\nI (2055) wifi:Init management short buffer num: 32\nI (2065) wifi:Init static tx buffer num: 32\nI (2065) wifi:Init tx cache buffer num: 32\nI (2065) wifi:Init static rx buffer size: 1600\nI (2075) wifi:Init static rx buffer num: 12\nI (2075) wifi:Init dynamic rx buffer num: 20\nI (2085) wifi_init: rx ba win: 16\nI (2085) wifi_init: tcpip mbox: 32\nI (2085) wifi_init: udp mbox: 6\nI (2095) wifi_init: tcp mbox: 12\nI (2095) wifi_init: tcp tx win: 14360\nI (2095) wifi_init: tcp rx win: 14360\nI (2105) wifi_init: tcp mss: 1440\nI (2105) wifi_init: WiFi/LWIP prefer SPIRAM\nI (2115) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (2125) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (2125) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (2145) esp_codec_es8388: Initialising esp_codec\nI (2145) gpio: GPIO[21]| InputEn: 0| OutputEn: 1| OpenDrain: 0| Pullup: 0| Pulldown: 0| Intr:0\nI (2175) button_driver_gpio: Initialising button driver\nI (2175) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2175) gpio: GPIO[39]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2185) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2195) led_driver_esp_ledc: Initialising led driver\nI (2205) [scli]: Initialising UART on port 0\nI (2205) [diag_cli]: Registering command: up-time\nI (2205) uart: queue free spaces: 8[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\nI (32595) [va_nvs_utils]: No value set for: friendlyname\nI (32595) SSDP: Network Interface List (1):\nI (32595) SSDP: 1. wifi  : 0.0.0.0\nI (32595) [time_utils]: SNTP already initialized.\nI (32595) LSSDP: create SSDP socket 57\nI (32615) [tls_certification]: Done setting global CA store\nI (32625) [va_nvs_utils]: No value set for: clientId\nI (32625) [va_nvs_utils]: No value set for: refreshToken\nI (32625) [alexa]: Checking for companion app based authentication...[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36115) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36125) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201`",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-skainet/issues/48",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ghost",
            "datetime": "Mar 10, 2022",
            "body": "I am using a custom board with 2 I2S microphones. Havn't set up microphones yet, compiling the WakeWord example with ESP32-S3-EYE set as board in menuconfig. I have Disable MultiNet since I have only 2MB of RAM available, but this should be ample for WakeNet, correct? Then I don't underestand why I get \"Item psram alloc failed\" from model.  Am I running out of PSRAM?? I adjusted many settings in menuconfig, but still get same error.ELF file SHA256: 90725b566f0ff608Rebooting...\nxESP-ROM:esp32s3-20210327\nBuild:Mar 27 2021\nrst:0xc (RTC_SW_CPU_RST),boot:0x8 (SPI_FAST_FLASH_BOOT)\nSaved PC:0x403799e0\nSPIWP:0xee\nmode:DIO, clock div:1\nload:0x3fcd0108,len:0x17b4\nload:0x403b6000,len:0xe28\nload:0x403ba000,len:0x30d0\nentry 0x403b6274\nI (29) boot: ESP-IDF v4.4-348-gc45dee0a7a 2nd stage bootloader\nI (29) boot: chip revision: 0\nI (32) qio_mode: Enabling default flash chip QIO\nI (37) boot.esp32s3: Boot SPI Speed : 80MHz\nI (42) boot.esp32s3: SPI Mode       : QIO\nI (47) boot.esp32s3: SPI Flash Size : 8MB\nI (52) boot: Enabling RNG early entropy source...\nI (57) boot: Partition Table:\nI (61) boot: ## Label            Usage          Type ST Offset   Length\nI (68) boot:  0 factory          factory app      00 00 00010000 00271000\nI (75) boot:  1 model            Unknown data     01 82 00281000 0050c000\nI (83) boot: End of partition table\nI (87) esp_image: segment 0: paddr=00010020 vaddr=3c060020 size=1f420h (128032) map\nI (115) esp_image: segment 1: paddr=0002f448 vaddr=3fc9a800 size=00bd0h (  3024) load\nI (116) esp_image: segment 2: paddr=00030020 vaddr=42000020 size=5e6a8h (386728) map\nI (179) esp_image: segment 3: paddr=0008e6d0 vaddr=3fc9b3d0 size=01c4ch (  7244) load\nI (181) esp_image: segment 4: paddr=00090324 vaddr=40378000 size=127f4h ( 75764) load\nI (199) esp_image: segment 5: paddr=000a2b20 vaddr=50000000 size=00010h (    16) load\nI (207) boot: Loaded app from partition at offset 0x10000\nI (207) boot: Disabling RNG early entropy source...\nI (220) spiram: Found 16MBit SPI RAM device\nI (220) spiram: SPI RAM mode: sram 80m\nI (220) spiram: PSRAM initialized, cache is in normal (1-core) mode.\nI (225) cpu_start: Pro cpu up.\nI (228) cpu_start: Starting app cpu, entry point is 0x4037932c\nI (213) cpu_start: App cpu up.\nI (464) spiram: SPI SRAM memory test OK\nI (465) spiram: Instructions copied and mapped to SPIRAM\nI (510) spiram: Read only data copied and mapped to SPIRAM\nI (525) cache: SPIRAM wrap enabled, size = 32.\nI (534) cpu_start: Pro cpu start user code\nI (534) cpu_start: cpu freq: 160000000\nI (534) cpu_start: Application information:\nI (537) cpu_start: Project name:     wake_word_detection\nI (543) cpu_start: App version:      v0.3.0-105-g4847f52-dirty\nI (555) cpu_start: ELF file SHA256:  90725b566f0ff608...\nI (561) cpu_start: ESP-IDF:          v4.4-348-gc45dee0a7a\nI (568) heap_init: Initializing. RAM available for dynamic allocation:\nI (575) heap_init: At 3FC9F058 len 00040FA8 (259 KiB): D/IRAM\nI (581) heap_init: At 3FCE0000 len 0000EE34 (59 KiB): STACK/DRAM\nI (588) heap_init: At 3FCF0000 len 00008000 (32 KiB): DRAM\nI (594) heap_init: At 600FE000 len 00002000 (8 KiB): RTCRAM\nI (600) spiram: Adding pool of 1536K of external SPI memory to heap allocator\nI (608) spi_flash: detected chip: gd\nI (612) spi_flash: flash io: qio\nI (617) sleep: Configure to isolate all GPIO pins in sleep state\nI (623) sleep: Enable automatic switching of GPIO sleep configuration\nI (630) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (641) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nInitializing SPIFFS\nPartition size: total: 4857101, used: 874233\nI (1121) I2S: DMA Malloc info, datalen=blocksize=640, dma_buf_count=6\nmodel_name: hiesp8q8 model_data: /srmodel/hiesp8q8/wn8q8_data\nItem psram alloc failed. Size: 163860 = 81920 x 2 + 16 + 4\nitem buff malloc fail\nGuru Meditation Error: Core  0 panic'ed (StoreProhibited). Exception was unhandled.Core  0 register dump:\nPC      : 0x40056fcc  PS      : 0x00060f30  A0      : 0x82008934  A1      : 0x3fcf3f10\nA2      : 0x00000000  A3      : 0x3dede104  A4      : 0x00028000  A5      : 0x00000000\nA6      : 0x0101fb0f  A7      : 0x0a000101  A8      : 0xf90a060a  A9      : 0x3fcf3ec0\nA10     : 0x0000000a  A11     : 0x00000002  A12     : 0x00014000  A13     : 0x00000002\nA14     : 0x00000010  A15     : 0x00000004  SAR     : 0x00000010  EXCCAUSE: 0x0000001d\nEXCVADDR: 0x00000000  LBEG    : 0x40056fc5  LEND    : 0x40056fe7  LCOUNT  : 0x000027ffBacktrace:0x40056fc9:0x3fcf3f10 |<-CORRUPTED",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LiTongXue98",
            "datetime": "Mar 11, 2022",
            "body": "menuconfig 里面的 CONFIG_ESP32S3_SPIRAM_SUPPORT=y 有没有配置呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ghost",
            "datetime": "Mar 11, 2022",
            "body": "Yes, I have already done that:%\n% ESP32S3-Specific\n%\n% CONFIG_ESP32S3_DEFAULT_CPU_FREQ_80 is not set\nCONFIG_ESP32S3_DEFAULT_CPU_FREQ_160=y\n% CONFIG_ESP32S3_DEFAULT_CPU_FREQ_240 is not set\nCONFIG_ESP32S3_DEFAULT_CPU_FREQ_MHZ=160%\n% Cache config\n%\n% CONFIG_ESP32S3_INSTRUCTION_CACHE_16KB is not set\nCONFIG_ESP32S3_INSTRUCTION_CACHE_32KB=y\nCONFIG_ESP32S3_INSTRUCTION_CACHE_SIZE=0x8000\n% CONFIG_ESP32S3_INSTRUCTION_CACHE_4WAYS is not set\nCONFIG_ESP32S3_INSTRUCTION_CACHE_8WAYS=y\nCONFIG_ESP32S3_ICACHE_ASSOCIATED_WAYS=8\nCONFIG_ESP32S3_INSTRUCTION_CACHE_LINE_32B=y\nCONFIG_ESP32S3_INSTRUCTION_CACHE_LINE_SIZE=32\nCONFIG_ESP32S3_INSTRUCTION_CACHE_WRAP=y\n% CONFIG_ESP32S3_DATA_CACHE_16KB is not set\nCONFIG_ESP32S3_DATA_CACHE_32KB=y\n% CONFIG_ESP32S3_DATA_CACHE_64KB is not set\nCONFIG_ESP32S3_DATA_CACHE_SIZE=0x8000\n% CONFIG_ESP32S3_DATA_CACHE_4WAYS is not set\nCONFIG_ESP32S3_DATA_CACHE_8WAYS=y\nCONFIG_ESP32S3_DCACHE_ASSOCIATED_WAYS=8\n% CONFIG_ESP32S3_DATA_CACHE_LINE_16B is not set\nCONFIG_ESP32S3_DATA_CACHE_LINE_32B=y\n% CONFIG_ESP32S3_DATA_CACHE_LINE_64B is not set\nCONFIG_ESP32S3_DATA_CACHE_LINE_SIZE=32\nCONFIG_ESP32S3_DATA_CACHE_WRAP=y\n% end of Cache configCONFIG_ESP32S3_SPIRAM_SUPPORT=y%\n% SPI RAM config\n%\nCONFIG_SPIRAM_MODE_QUAD=y\n% CONFIG_SPIRAM_MODE_OCT is not set\nCONFIG_SPIRAM_TYPE_AUTO=y\n% CONFIG_SPIRAM_TYPE_ESPPSRAM16 is not set\n% CONFIG_SPIRAM_TYPE_ESPPSRAM32 is not set\n% CONFIG_SPIRAM_TYPE_ESPPSRAM64 is not set\nCONFIG_SPIRAM_SIZE=-1%\n% PSRAM Clock and CS IO for ESP32S3\n%\nCONFIG_DEFAULT_PSRAM_CLK_IO=30\nCONFIG_DEFAULT_PSRAM_CS_IO=26\n% end of PSRAM Clock and CS IO for ESP32S3CONFIG_SPIRAM_FETCH_INSTRUCTIONS=y\nCONFIG_SPIRAM_RODATA=y\n% CONFIG_SPIRAM_SPEED_120M is not set\nCONFIG_SPIRAM_SPEED_80M=y\n% CONFIG_SPIRAM_SPEED_40M is not set\nCONFIG_SPIRAM=y\nCONFIG_SPIRAM_BOOT_INIT=y\n% CONFIG_SPIRAM_IGNORE_NOTFOUND is not set\n% CONFIG_SPIRAM_USE_MEMMAP is not set\n% CONFIG_SPIRAM_USE_CAPS_ALLOC is not set\nCONFIG_SPIRAM_USE_MALLOC=y\nCONFIG_SPIRAM_MEMTEST=y\nCONFIG_SPIRAM_MALLOC_ALWAYSINTERNAL=131072\nCONFIG_SPIRAM_TRY_ALLOCATE_WIFI_LWIP=y\nCONFIG_SPIRAM_MALLOC_RESERVE_INTERNAL=32768\n% end of SPI RAM config% CONFIG_ESP32S3_TRAX is not set\nCONFIG_ESP32S3_TRACEMEM_RESERVE_DRAM=0x0\n% CONFIG_ESP32S3_ULP_COPROC_ENABLED is not set\nCONFIG_ESP32S3_ULP_COPROC_RESERVE_MEM=0\nCONFIG_ESP32S3_DEBUG_OCDAWARE=y\nCONFIG_ESP32S3_BROWNOUT_DET=y\nCONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_7=y\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_6 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_5 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_4 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_3 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_2 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_1 is not set\nCONFIG_ESP32S3_BROWNOUT_DET_LVL=7\nCONFIG_ESP32S3_TIME_SYSCALL_USE_RTC_FRC1=y\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_RTC is not set\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_FRC1 is not set\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_NONE is not set\nCONFIG_ESP32S3_RTC_CLK_SRC_INT_RC=y\n% CONFIG_ESP32S3_RTC_CLK_SRC_EXT_CRYS is not set\n% CONFIG_ESP32S3_RTC_CLK_SRC_EXT_OSC is not set\n% CONFIG_ESP32S3_RTC_CLK_SRC_INT_8MD256 is not set\nCONFIG_ESP32S3_RTC_CLK_CAL_CYCLES=1024\nCONFIG_ESP32S3_DEEP_SLEEP_WAKEUP_DELAY=2000\n% CONFIG_ESP32S3_NO_BLOBS is not set\n% CONFIG_ESP32S3_RTCDATA_IN_FAST_MEM is not set\n% CONFIG_ESP32S3_USE_FIXED_STATIC_RAM_SIZE is not set\n% end of ESP32S3-Specific",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Mar 14, 2022",
            "body": "8bit-wakenet8 takes about 850KB, Audio-Front-End module takes about 500 KB.\nWe haven't tested the R8N2, but it's possible to run out of memory.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Mar 14, 2022",
            "body": "Next month, we will release a wakenet9 in v2.0 which takes about 300 KB.\nIf you don't want to change ESP32-S3 module, you can wait for the latest wakenet9.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ghost",
            "datetime": "Mar 14, 2022",
            "body": "Thanks, I wish I knew about this, now I am left with custom boards that are no good , I made them specifically to use a single wake word (i.e. Alexa)\nI read in\n\nand\nthat RAM usage for WakeWord is <100KB, hence choosing N8R2. Anyways, please support \"Alexa\" and \"Hi, ESP\" in WakeNet9. Good to know that there is still some hope.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Mar 15, 2022",
            "body": "Hi eyewy，\nI think I need to explain the details about memory for different wakenet.\nwakenet5(ESP32): we read all parameters from flash and do not save a backup in PSRAM, so we only need RAM < 100KB.\nwakenet7&wakenet8(ESP32S3), When we initialize the model, parameters are read into PSRAM from flash(the parameter of wakenet8 is about 800KB), then we use the parameters in PSRAM to calculate.Why do we do that on ESP32-S3?\nThe main reason is that Octal PSRAM is faster than QIO flash. And Octal flash is too expensive.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ghost",
            "datetime": "Mar 16, 2022",
            "body": "Tested on ESP32-S3-DevKitC-1-N32R8V Development Board, and there is no RAM issue any more. This board says:\nI (729) spiram: Adding pool of 8192K of external SPI memory to heap allocator\nAfter running the program I check RAM usage by:\nprintf(\"RAM left %d\\n\", esp_get_free_heap_size());\nand I get:\nRAM left 6718923\nSo actual ram usage is: 1473KBThis confirms  calculation of 850KB + 500KB =1350KBNow here is my question: given ~1.5MB usage which is <2MB, how come I receive error on N8R2?? Is there a temperorary peak usage while loading the model? Is there something that can be done here to optimize instantaneous RAM usage?Also, I don't use Wifi and BT, is there a way to free up little bit more memory?If Answer to above questions is no, I guess I can only wait for WakeNet9 to become available, I would be happy to test it on N8R2 as soon as you have a beta version available.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Apr 8, 2022",
            "body": "Yes, when loading the model, it does require more memory.\nI am sorry to tell you wakenet9 will be delayed.  Colleagues in Shanghai can not upload new codes due to COVID-19.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Aug 11, 2022",
            "body": "Hi @eyewy ， wakenet9 has been updated in the master branch.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Mar 10, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Mar 10, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "feizi",
            "datetime": "Aug 23, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/160",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yusufk",
            "datetime": "Oct 19, 2021",
            "body": "Logged this on esp-idf and was asked to move it here ()Build fails for  with an error:Build succeededmake failed with exit code 2`",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Nov 12, 2021",
            "body": "The IDF branch you are using seems to be v4.4. But in the top level readme, it mentions to checkout the release/v4.2 branch.Can you check the development setup () and try again?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yusufk",
            "datetime": "Oct 19, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "espressif/esp-idf#7711"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/155",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 23, 2021",
            "body": "In readme they mentioned that this only support Lyra board..is it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 23, 2021",
            "body": "Which board do you have and which example are you trying to build?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 27, 2021",
            "body": "I'm trying to do Alexa example ( with music and playback support ) by using vaquita board)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 29, 2021",
            "body": "In the release yesterday, we have added the supported boards along with all the supported voice assistants on the master branch. The branches of other repos and the build commands have also been updated. Check the new READMEs for more info.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 29, 2021",
            "body": "Could u share the link for that exact branch plz?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 29, 2021",
            "body": "\nCheck the readme here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 29, 2021",
            "body": "Hi thank you so much..what are all the music app supported by this sdk",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 30, 2021",
            "body": "Hi.. this SDK  does not support Spotify ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 30, 2021",
            "body": "The default AVS product here does not support the music services which require approval. You can create your own AVS product, enable the music services that you want and use that AVS product instead of the default one. (You will also have to modify the phone app to use your AVS product and build it again.)Apart from Spotify, all the music services supported by Alexa should work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 30, 2021",
            "body": "Hi  ...thanks.. actually I flashed amazon_aia and it supported music except Spotify, pandora and some others",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Nalininalu",
            "datetime": "Sep 23, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/161",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yusufk",
            "datetime": "Oct 19, 2021",
            "body": "Getting the following error on console:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "s95i",
            "datetime": "Dec 14, 2021",
            "body": "I have the same problem. Did you find the solution?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yusufk",
            "datetime": "Dec 14, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 15, 2021",
            "body": "Hi  could you please tell which app are you trying and provide complete logs for the same?\nIt appears that we might need to update some certificate in our library.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yusufk",
            "datetime": "Dec 15, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "s95i",
            "datetime": "Dec 15, 2021",
            "body": "Hi , I had the same issue with the GVA app.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 16, 2021",
            "body": "I believe this is GVA app and not the Alexa app. We will check and get back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yusufk",
            "datetime": "Dec 16, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "s95i",
            "datetime": "Jan 11, 2022",
            "body": "Hi, , Is there any way to add a new certificate to solve this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "andrewwinter",
            "datetime": "Jan 18, 2022",
            "body": "I'm still having this issue does anyone have a solution? I believe that the problem is that the CA certifications are hardcoded into the lib files (found at esp-va-sdk/voice_assistant/*.a), I found an older google CA certification in them, however i believe that they're expired. Could you please recompile these libraries with the new CA certificates? \nThanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Feb 10, 2022",
            "body": "Hi \nSorry for the delay. We will provide updated library by next week.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "s95i",
            "datetime": "Mar 1, 2022",
            "body": "Hi  ,\nWhen can we expect the updated library?\nThanks",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/151",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 7, 2021",
            "body": "After about 5 minutes, Vaquita will crash and fall into gdb with the message \"Entering gdb stub now\".I've enabled RainMaker.  All code is the original, that is not modified.  Before the crash, everything seems to work well and is really quite impressive.\nWhen RainMaker is not enabled it also crashes but takes a little longer.Any ideas?The last printed messages from the serial port are:[0;32mI (2970557) opus_decoder: Total decoded frames = 240[0m\n[0;32mI (2970557) opus_decoder: Closed[0m\n[0;32mI (2970567) [audio_codec]: Codec opus_decoder Event Stopped[0m\n[2968 seconds]: [http_transport]: Event data: {\"events\":[{\"header\":{\"name\":\"SpeakerClosed\",\"messageId\":\"4377508f-c7be-66d2-a922-bbc1119a0733\"},\"payload\":{\"offset\":69280}}]}\n[0;32mI (2970587) [http_transport]: Sending to topic: $aws/alexa/ais/v1/517f70d2-c7f5-44af-8486-eab66f93135e/event[0m\n[0;32mI (2970597) [basic_player]: Waiting for decoder to stop line 116[0m\n[basic_player]: Codec event: Codec Stopped\n[0;32mI (2970607) opus_decoder: Closed[0m\n[0;32mI (2970617) [audio_codec]: Codec opus_decoder Event Stopped[0m\n[basic_player]: Codec event: Codec Stopped\n[0;32mI (2970627) [audio_codec]: Closing codec opus_decoder without decoding any data[0m\n[attachment]: Free ais_attachment_3\n[dialog]: Speech done\n[2970 seconds]: [http_transport]: Free Memory Internal: 51836, External: 2480240\n[directive_proc]: Json data: {\"directives\":[{\"header\":{\"name\":\"SetAttentionState\",\"messageId\":\"dd02e871-f5ca-4847-a105-952f0e738d45\"},\"payload\":{\"state\":\"IDLE\"}}]}\n[0;32mI (2972877) [directive_proc]: Name: SetAttentionState\n[0m\n[dialog]: Stream finished\n[dialog]: Event STREAM_CLOSED unsupported in SPEAKING state\n[0;32mI (2972897) [app_va_cb]: Dialog state is: 8[0m\n[dialog]: Speaking state timeout. Releasing focus\n[0;32mI (2975397) [focus_manager]: Release channel: Dialog SpeechSynthesizer[0m\n[0;32mI (2975397) [focus_manager]: Releasing SpeechSynthesizer[0m\n[0;32mI (2975397) [sys_playback]: Release[0m\n[dialog]: Entering VA_IDLE\n[auth-delegate]: Queue receive returned false for 3000 seconds. It timed out. Getting new credentials.\nGuru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.\nCore 0 register dump:\nPC      : 0x4015ff24  PS      : 0x00060e30  A0      : 0x80160897  A1      : 0x3f829410  \nA2      : 0x0000003c  A3      : 0x00000000  A4      : 0x3f983b1c  A5      : 0x3f829540  \nA6      : 0x3fff498c  A7      : 0x00000000  A8      : 0x00000004  A9      : 0x3f8293d0  \nA10     : 0x00000000  A11     : 0x00000fff  A12     : 0x00001005  A13     : 0x00000001  \nA14     : 0x00000008  A15     : 0x3f807574  SAR     : 0x00000018  EXCCAUSE: 0x0000001c  \nEXCVADDR: 0x00000004  LBEG    : 0x4000c2e0  LEND    : 0x4000c2f6  LCOUNT  : 0xffffffff  \n\nELF file SHA256: 5659eb0ee866b947\n\nBacktrace: 0x4015ff21:0x3f829410 0x40160894:0x3f8294e0 0x4011345c:0x3f829510 0x400daa6d:0x3f829530 0x400dab69:0x3f8295a0 0x400dae0c:0x3f8295c0\n\nEntering gdb stub now.\n$T0b#e6",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 8, 2021",
            "body": "Could you share the IDF branch/commit you are using?\nCould you also share another log of the device crashing?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 8, 2021",
            "body": "Thank you,\n\nI couldn't figure out how to tell the version number of IDF but I originally loaded it from the instructions:$ git clone --recursive \n$ cd esp-idf; git checkout release/v4.0; git submodule init; git submodule update --init --recursive;Here is the output of the  file:\nSetting IDF_PATH: C:\\Users\\howar\\Documents\\Alexa\\iot\\esp-vaquita\\esp-idfAdding ESP-IDF tools to PATH...\nC:\\Users\\howar.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\\bin\nC:\\Users\\howar.espressif\\tools\\esp32ulp-elf\\2.28.51.20170517\\esp32ulp-elf-binutils\\bin\nC:\\Users\\howar.espressif\\tools\\cmake\\3.13.4\\bin\nC:\\Users\\howar.espressif\\tools\\openocd-esp32\\v0.10.0-esp32-20210721\\openocd-esp32\\bin\nC:\\Users\\howar.espressif\\tools\\mconf\\v4.6.0.0-idf-20190628\nC:\\Users\\howar.espressif\\tools\\ninja\\1.9.0\nC:\\Users\\howar.espressif\\tools\\idf-exe\\1.0.1\nC:\\Users\\howar.espressif\\tools\\ccache\\3.7\nC:\\Users\\howar.espressif\\python_env\\idf4.0_py3.9_env\\Scripts\nC:\\Users\\howar\\Documents\\Alexa\\iot\\esp-vaquita\\esp-idf\\toolsand when I do a  from the esp-idf directory:\nv4.0.3-144-gd64bf0dfbeI could try updating to v4.1.12 if you think that will help.\nThis was captured 3x with the same result.  It is taking a little longer to crash today, about 1/2 hr or so.  It enters gdb.  When gdb is exited, it looks like vaquito tries to boot up but doesn't.  Hitting a reset does boot vaquito.\nAnd here is the log of the last crash. Note that the SSDP is my AV amp and I know that isn't supported:\n''\n40e4::urn:schemas-upnp-org:service:RenderingControl:1\nW (2486257) SSDP: --end--\nI (2487267) SSDP: ---SSDP List----\nI (2487267) SSDP: 1.id=, ip=, name=uuid:5f9ec1b3-ed59-1900-4530-00a0dea340e4::urn:schemas-upnp-org:service:ConnectionManager:1\nW (2487277) SSDP: --end--\nI (2487567) SSDP: ---SSDP List----\nI (2487567) SSDP: 1.id=, ip=, name=uuid:5f9ec1b3-ed59-1900-4530-00a0dea340e4::urn:schemas-upnp-org:service:AVTransport:1\nW (2487577) SSDP: --end--\nW (2538097) LSSDP: remove timeout SSDP neighbor:  () (50522ms)\nI (2538097) SSDP: ---SSDP List----\nW (2538097) SSDP: Empty\nW (2598387) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2598387) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2719017) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2719017) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2778647) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2778647) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2838877) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2838877) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\n[auth-delegate]: Queue receive returned false for 3000 seconds. It timed out. Getting new credentials.\nGuru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.\nCore 0 register dump:\nPC      : 0x4015ff24  PS      : 0x00060d30  A0      : 0x80160897  A1      : 0x3f829410\n0x4015ff24: esp_tcp_connect at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:163\n(inlined by) esp_tls_low_level_conn at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:616A2      : 0x0000003c  A3      : 0x00000000  A4      : 0x3f8519ec  A5      : 0x3f829540\nA6      : 0x3fff4984  A7      : 0x00000000  A8      : 0x00000004  A9      : 0x3f8293d0\nA10     : 0x00000000  A11     : 0x00000fff  A12     : 0x00001005  A13     : 0x00000001\nA14     : 0x00000008  A15     : 0x3f807574  SAR     : 0x00000018  EXCCAUSE: 0x0000001c\nEXCVADDR: 0x00000004  LBEG    : 0x4000c2e0  LEND    : 0x4000c2f6  LCOUNT  : 0xffffffffELF file SHA256: 5659eb0ee866b947Backtrace: 0x4015ff21:0x3f829410 0x40160894:0x3f8294e0 0x4011345c:0x3f829510 0x400daa6d:0x3f829530 0x400dab69:0x3f8295a0 0x400dae0c:0x3f8295c0\n0x4015ff21: esp_tcp_connect at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:163\n(inlined by) esp_tls_low_level_conn at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:6160x40160894: esp_tls_conn_new at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:7170x4011345c: http_connection_new at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-va-sdk/components/httpc/httpc.c:1560x400daa6d: get_auth_response_with_header at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:830x400dab69: get_auth_response at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:1410x400dae0c: refresh_auth_task at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:303Entering gdb stub now.\n$T0b#e6GNU gdb (crosstool-NG esp-2020r3) 8.1.0.20180627-git\nCopyright (C) 2018 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later \nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"--host=x86_64-host_w64-mingw32 --target=xtensa-esp32-elf\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n.\nFind the GDB manual and other documentation resources online at:\n.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from c:\\users\\howar\\documents\\alexa\\iot\\esp-vaquita\\esp-va-sdk\\examples\\amazon_aia\\build\\aia.elf...done.\nRemote debugging using COM7\nIgnoring packet error, continuing...\nwarning: unrecognized item \"timeout\" in \"qSupported\" response\nRemote replied unexpectedly to 'vMustReplyEmpty': vMustReplyEmpty\n(gdb)\n''",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 9, 2021",
            "body": "From your logs, it seems to crash after:The device connects to the server and gets new access tokens. And the crash seems to be when connecting. Let me check and get back. What I can think of, is it could be because of lack of memory or lack of sockets.Have you also checked with the default repository?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 9, 2021",
            "body": "Thanks again.  Not sure what you identify as the default repository.   Just tried building with the recent tools version v4.3.1 of the tools and got a linking error, undefined reference.  I did change the project.cmake file to add  statement.Got the below link error message.  Could not find where xTaskNofity is defined, except in a library in the recommended downloaded but not in v4.3.1 tools.  Did find a number of places in which the function is called.FAILED: aia.elf\ncmd.exe /C \"cd . && C:\\Users\\howar.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\\bin\\xtensa-esp32-elf-g++.exe  -mlongcalls -Wno-frame-address   @CMakeFiles\\aia.elf.rsp  -o aia.elf  && cd .\"\nc:/users/howar/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld.exe: C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-va-sdk/components/voice_assistant/lib/libaia.a(dialog.o):(.literal.dialog_sm_thread+0x30): undefined reference to dialog_sm_thread':\n/Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/dialog.c:324: undefined reference to `xTaskNotify'\ncollect2.exe: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 13, 2021",
            "body": "The esp-idf version or branch should be release/v4.0 for the amazon_aia example. It is not expected to work with other IDF versions since the libraries built are version specific. Can you try with the release/v4.0 branch for esp-idf? (From the logs in previous comments, I think you were already using this branch and getting the crashes on this branch. We have a planned release for this week with release/v4.2 version and it has a few stability fixes for AIA.)By 'default repository', I mean without any changes (apart from the patches required to compile) that you may have done.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 20, 2021",
            "body": "Thank you chiragatal.\nI deleted and reinstalled all the need support drivers, libraries, and packages.  Made sure it was version 4.0 of the esp-idf.  Still crashes in the same way. Did not try the \"future aia/beta\" version.\nLooked for version esp-va-sdk 4.2 and it is an older release so I'll wait for the \"planned release for this week of v4.2\".I also eliminated the SSDP sources in the local network, except for the other Alexa's which also sends out SSDP responses, still crashed.\nTried a couple of times to delete the build directory and did an \"idf.py fullclean\" to be thorough.What I don't understand is why I'm seeing this problem since this board is not new.  Doesn't anyone else have this same problem?  If not, perhaps my board is defective and I'm willing to buy another one to see if it is.  If not, I'll wait for the new release but need more information on which driver and support package is being released so I can  download and try it when ready.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Sep 21, 2021",
            "body": "The new release would be for esp-va-sdk. It will use esp-idf v4.2 and will not use esp-aws-iot at all. When released, just installing/updating the esp-idf drivers (running install.sh) should be enough.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 9, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "HowardHonig",
            "datetime": "Sep 9, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/143",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "KalashQK",
            "datetime": "Aug 19, 2021",
            "body": "Hello!\nPlease tell me if there are any methods of Wi-Fi authentication for Google Voice Assistant on LyraT board besides NVS input? For example, how is it implemented in the Alexa variant using Bluetooth, but only in the GVA? Perhaps not yet implemented, but planned?\nIf not planned, please tell me or advise where to start for development of such a solution?I also notice at GVA that if do not turn on the board and do not use it for more than a week, it stops responding to commands. After pronouncing WakeWord, the board reacts, there is a characteristic sound and light, but there is no response. The board need to be reflash again. How can fix this problem? I am using 2 LyraT boards, both have this problem.Thanks in advance!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/132",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "jassi00713",
            "datetime": "Mar 25, 2021",
            "body": "Trying to implement Alexa on ESP32 Vaquita DSPG and using branch feature/aia-betaFollowed instructions on the below link:-\nBuild is failing.\nError caused due to -> multiple definition of `set_xpd_sar'\nKindly help with the soltion to this.Below are logs on running idf.py build.\nLOGS-idf.py build\nChecking Python dependencies...\nPython requirements from /home/javivek/espaia2/esp-idf/requirements.txt are satisfied.\nExecuting action: all (aliases: build)\nRunning ninja in directory /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build\nExecuting \"ninja all\"...\n[211/1168] cd /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-...****************************************************************************\"\nPartition table binary generated. Contents:nvs,data,nvs,0x9000,24K,\nphy_init,data,phy,0xf000,4K,\nfctry_aia,data,nvs,0x10000,24K,\nfctry,data,nvs,0x16000,24K,\notadata,data,ota,0x1c000,8K,\nota_0,app,ota_0,0x20000,4032K,\nota_1,app,ota_1,0x410000,4032K,[304/1168] Performing configure step for 'bootloader'\n-- Component directory /home/javivek/espaia2/esp-idf/components/cbor does not contain a CMakeLists.txt file. No component will be added\n-- Component directory /home/javivek/espaia2/esp-idf/components/cmock does not contain a CMakeLists.txt file. No component will be added\n-- Component directory /home/javivek/espaia2/esp-idf/components/tinyusb does not contain a CMakeLists.txt file. No component will be added\n-- Project version: v4.0.2-442-g41efdb0b3\n-- Building ESP-IDF components for target esp32\n-- Project sdkconfig file /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/sdkconfig\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp32/ld/esp32.peripherals.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main/esp32.bootloader.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main/esp32.bootloader.rom.ld\n-- Components: bootloader bootloader_support efuse esp32 esp_common esp_rom esptool_py log main micro-ecc partition_table soc spi_flash xtensa\n-- Component paths: /home/javivek/espaia2/esp-idf/components/bootloader /home/javivek/espaia2/esp-idf/components/bootloader_support /home/javivek/espaia2/esp-idf/components/efuse /home/javivek/espaia2/esp-idf/components/esp32 /home/javivek/espaia2/esp-idf/components/esp_common /home/javivek/espaia2/esp-idf/components/esp_rom /home/javivek/espaia2/esp-idf/components/esptool_py /home/javivek/espaia2/esp-idf/components/log /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main /home/javivek/espaia2/esp-idf/components/bootloader/subproject/components/micro-ecc /home/javivek/espaia2/esp-idf/components/partition_table /home/javivek/espaia2/esp-idf/components/soc /home/javivek/espaia2/esp-idf/components/spi_flash /home/javivek/espaia2/esp-idf/components/xtensa\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/bootloader\n[331/1168] Performing build step for 'bootloader'\n[1/2] Linking C executable bootloader.elf\n[2/2] Generating binary image from built executable\nesptool.py v2.9-dev\nGenerated /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/bootloader/bootloader.bin\n[1067/1168] Building C object esp-idf/streams/CMakeFiles/__idf_streams.dir/i2s_stream/i2s_stream.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/streams/i2s_stream/i2s_stream.c: In function 'parse_i2s_config':\n/home/javivek/espaia2/esp-va-sdk/components/streams/i2s_stream/i2s_stream.c:45:15: warning: unused variable 'ret' [-Wunused-variable]\nesp_err_t ret = ESP_OK;\n^~~\n[1078/1168] Building C object esp-idf/httpc/CMakeFiles/__idf_httpc.dir/httpc.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c: In function 'http_connection_new':\n/home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c:156:5: warning: 'esp_tls_conn_new' is deprecated [-Wdeprecated-declarations]\ntls = esp_tls_conn_new(&url[u->field_data[UF_HOST].off], u->field_data[UF_HOST].len,\n^~~\nIn file included from /home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.h:18,\nfrom /home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c:20:\n/home/javivek/espaia2/esp-idf/components/esp-tls/esp_tls.h:271:12: note: declared here\nesp_tls_t )(void)' {aka 'int ()(uint8_t)' {aka 'int ()(int)' [-Wincompatible-pointer-types]\nmedia_hal->audio_codec_control_volume = es8311_set_volume;\n^\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/media_hal_codec_init.c:47:39: warning: assignment to 'esp_err_t ()(_Bool)'} from incompatible pointer type 'int (*)(int)' [-Wincompatible-pointer-types]\nmedia_hal->audio_codec_set_mute = es8311_set_mute;\n^\n[1146/1168] Building C object esp-idf/codec_es8311/CMakeFiles/__idf_codec_es8311.dir/es8311.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c: In function 'es8311_init':\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:321:9: warning: unused variable 'port_num' [-Wunused-variable]\nint port_num = media_hal_conf->port_num;\n^~~~~~~~\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:320:28: warning: unused variable 'es8388_dac_output' [-Wunused-variable]\nmedia_hal_dac_output_t es8388_dac_output = media_hal_conf->dac_output;\n^~~~~~~~~~~~~~~~~\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:319:27: warning: unused variable 'es8311_adc_input' [-Wunused-variable]\nmedia_hal_adc_input_t es8311_adc_input = media_hal_conf->adc_input;\n^~~~~~~~~~~~~~~~\n[1167/1168] Linking CXX executable aia.elf\nFAILED: aia.elf\n: && /home/javivek/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++  -mlongcalls -Wno-frame-address  -nostdlib CMakeFiles/aia.elf.dir/project_elf_src.c.obj  -o aia.elf  esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/driver/libdriver.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/efuse/libefuse.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/app_update/libapp_update.a esp-idf/spi_flash/libspi_flash.a esp-idf/nvs_flash/libnvs_flash.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/lwip/liblwip.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/esp_event/libesp_event.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/esp32/libesp32.a esp-idf/xtensa/libxtensa.a esp-idf/esp_common/libesp_common.a esp-idf/esp_rom/libesp_rom.a esp-idf/soc/libsoc.a esp-idf/log/liblog.a esp-idf/heap/libheap.a esp-idf/freertos/libfreertos.a esp-idf/vfs/libvfs.a esp-idf/newlib/libnewlib.a esp-idf/cxx/libcxx.a esp-idf/app_trace/libapp_trace.a esp-idf/asio/libasio.a esp-idf/bt/libbt.a esp-idf/coap/libcoap.a esp-idf/console/libconsole.a esp-idf/nghttp/libnghttp.a esp-idf/esp-tls/libesp-tls.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/esp_gdbstub/libesp_gdbstub.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/esp_https_ota/libesp_https_ota.a esp-idf/protobuf-c/libprotobuf-c.a esp-idf/protocomm/libprotocomm.a esp-idf/mdns/libmdns.a esp-idf/esp_local_ctrl/libesp_local_ctrl.a esp-idf/esp_websocket_client/libesp_websocket_client.a esp-idf/expat/libexpat.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/fatfs/libfatfs.a esp-idf/freemodbus/libfreemodbus.a esp-idf/jsmn/libjsmn.a esp-idf/json/libjson.a esp-idf/libsodium/liblibsodium.a esp-idf/mqtt/libmqtt.a esp-idf/openssl/libopenssl.a esp-idf/spiffs/libspiffs.a esp-idf/ulp/libulp.a esp-idf/unity/libunity.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/main/libmain.a esp-idf/json_parser/libjson_parser.a esp-idf/httpc/libhttpc.a esp-idf/streams/libstreams.a esp-idf/utils/libutils.a esp-idf/media_hal/libmedia_hal.a esp-idf/led_pattern/libled_pattern.a esp-idf/misc/libmisc.a esp-idf/audio_hal/libaudio_hal.a esp-idf/audio_pipeline/libaudio_pipeline.a esp-idf/sys_playback/libsys_playback.a esp-idf/basic_player/libbasic_player.a esp-idf/esp_httpd_ota/libesp_httpd_ota.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/qrcode/libqrcode.a esp-idf/sh2lib/libsh2lib.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/common/libcommon.a esp-idf/alexa_equalizer/libalexa_equalizer.a esp-idf/app_cloud/libapp_cloud.a esp-idf/app_smart_home/libapp_smart_home.a esp-idf/custom_player/libcustom_player.a esp-idf/esp-aws-iot/libesp-aws-iot.a esp-idf/audio_board_dspg_avs_kit_m/libaudio_board_dspg_avs_kit_m.a esp-idf/neo_pixel_led/libneo_pixel_led.a esp-idf/codec_es8311/libcodec_es8311.a -Wl,--cref -Wl,--Map=/home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/aia.map -fno-rtti -fno-lto /home/javivek/espaia2/esp-va-sdk/components/voice_assistant/lib/libaia.a /home/javivek/espaia2/esp-va-sdk/components/codecs/lib/libcodecs.a esp-idf/utils/libutils.a esp-idf/audio_hal/libaudio_hal.a esp-idf/media_hal/libmedia_hal.a esp-idf/streams/libstreams.a esp-idf/misc/libmisc.a esp-idf/led_pattern/libled_pattern.a esp-idf/sys_playback/libsys_playback.a /home/javivek/espaia2/esp-va-sdk/components/esp-ssdp/lib/libesp-ssdp.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/dbmd5-ipc/lib/libdspg-ipc.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/va_dsp/lib/libva_dsp.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/basic_player/libbasic_player.a esp-idf/main/libmain.a esp-idf/streams/libstreams.a esp-idf/utils/libutils.a esp-idf/media_hal/libmedia_hal.a esp-idf/led_pattern/libled_pattern.a esp-idf/misc/libmisc.a esp-idf/audio_hal/libaudio_hal.a esp-idf/audio_pipeline/libaudio_pipeline.a esp-idf/sys_playback/libsys_playback.a esp-idf/basic_player/libbasic_player.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/common/libcommon.a esp-idf/alexa_equalizer/libalexa_equalizer.a esp-idf/app_smart_home/libapp_smart_home.a esp-idf/custom_player/libcustom_player.a esp-idf/audio_board_dspg_avs_kit_m/libaudio_board_dspg_avs_kit_m.a esp-idf/neo_pixel_led/libneo_pixel_led.a esp-idf/codec_es8311/libcodec_es8311.a /home/javivek/espaia2/esp-va-sdk/components/voice_assistant/lib/libaia.a /home/javivek/espaia2/esp-va-sdk/components/codecs/lib/libcodecs.a esp-idf/utils/libutils.a esp-idf/audio_hal/libaudio_hal.a esp-idf/media_hal/libmedia_hal.a esp-idf/streams/libstreams.a esp-idf/misc/libmisc.a esp-idf/led_pattern/libled_pattern.a esp-idf/sys_playback/libsys_playback.a /home/javivek/espaia2/esp-va-sdk/components/esp-ssdp/lib/libesp-ssdp.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/dbmd5-ipc/lib/libdspg-ipc.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/va_dsp/lib/libva_dsp.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/basic_player/libbasic_player.a esp-idf/main/libmain.a esp-idf/qrcode/libqrcode.a esp-idf/spiffs/libspiffs.a esp-idf/sh2lib/libsh2lib.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/libsodium/liblibsodium.a esp-idf/fatfs/libfatfs.a esp-idf/httpc/libhttpc.a esp-idf/json_parser/libjson_parser.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/driver/libdriver.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/efuse/libefuse.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/app_update/libapp_update.a esp-idf/spi_flash/libspi_flash.a esp-idf/nvs_flash/libnvs_flash.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/lwip/liblwip.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/esp_event/libesp_event.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/esp32/libesp32.a esp-idf/xtensa/libxtensa.a esp-idf/esp_common/libesp_common.a esp-idf/esp_rom/libesp_rom.a esp-idf/soc/libsoc.a esp-idf/log/liblog.a esp-idf/heap/libheap.a esp-idf/freertos/libfreertos.a esp-idf/vfs/libvfs.a esp-idf/newlib/libnewlib.a esp-idf/cxx/libcxx.a esp-idf/app_trace/libapp_trace.a esp-idf/asio/libasio.a esp-idf/bt/libbt.a esp-idf/coap/libcoap.a esp-idf/console/libconsole.a esp-idf/nghttp/libnghttp.a esp-idf/esp-tls/libesp-tls.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/esp_gdbstub/libesp_gdbstub.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/esp_https_ota/libesp_https_ota.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/protobuf-c/libprotobuf-c.a esp-idf/protocomm/libprotocomm.a esp-idf/mdns/libmdns.a esp-idf/esp_local_ctrl/libesp_local_ctrl.a esp-idf/esp_websocket_client/libesp_websocket_client.a esp-idf/expat/libexpat.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/fatfs/libfatfs.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/freemodbus/libfreemodbus.a esp-idf/jsmn/libjsmn.a esp-idf/json/libjson.a esp-idf/libsodium/liblibsodium.a esp-idf/mqtt/libmqtt.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/openssl/libopenssl.a esp-idf/spiffs/libspiffs.a esp-idf/ulp/libulp.a esp-idf/unity/libunity.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/protocomm/libprotocomm.a esp-idf/bt/libbt.a -L/home/javivek/espaia2/esp-idf/components/bt/controller/lib -lbtdm_app esp-idf/protobuf-c/libprotobuf-c.a esp-idf/mdns/libmdns.a esp-idf/json/libjson.a esp-idf/json_parser/libjson_parser.a esp-idf/httpc/libhttpc.a /home/javivek/espaia2/esp-va-sdk/components/esp-downmix/lib/libesp-downmix.a esp-idf/esp_httpd_ota/libesp_httpd_ota.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/qrcode/libqrcode.a esp-idf/sh2lib/libsh2lib.a esp-idf/esp-tls/libesp-tls.a esp-idf/nghttp/libnghttp.a -L /home/javivek/espaia2/esp-va-sdk/components/speech_recog/lib -lesp_wwe -lspeech_recog -lc_speech_features -ldl_lib -lnn_model esp-idf/app_cloud/libapp_cloud.a esp-idf/console/libconsole.a esp-idf/esp-aws-iot/libesp-aws-iot.a esp-idf/jsmn/libjsmn.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a -u __cxa_guard_dummy -lstdc++ esp-idf/pthread/libpthread.a -u __cxx_fatal_exception esp-idf/newlib/libnewlib.a -u newlib_include_locks_impl -u newlib_include_heap_impl -u newlib_include_syscalls_impl -u newlib_include_pthread_impl -Wl,--undefined=uxTopUsedPriority -L /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld -T esp32.rom.ld -T esp32.rom.libgcc.ld -T esp32.rom.syscalls.ld -T esp32.rom.newlib-data.ld -Wl,--gc-sections /home/javivek/espaia2/esp-idf/components/xtensa/esp32/libhal.a -L /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-idf/esp32 -T esp32_out.ld -u app_main -L /home/javivek/espaia2/esp-idf/components/esp32/ld -T esp32.extram.bss.ld -L /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-idf/esp32/ld -T esp32.project.ld -T esp32.peripherals.ld -u call_user_start_cpu0 -u ld_include_panic_highint_hdl -mfix-esp32-psram-cache-issue -mfix-esp32-psram-cache-strategy=memw -u esp_app_desc -u vfs_include_syscalls_impl -L /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32 -lgcov -lc -lm -lgcc -u pthread_include_pthread_impl -u pthread_include_pthread_cond_impl -u pthread_include_pthread_local_storage_impl && :\n/home/javivek/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/esp_wifi/libesp_wifi.a(wifi_init.c.obj): in function set_xpd_sar'; esp-idf/common/libcommon.a(app_wifi.c.obj):/home/javivek/espaia2/esp-va-sdk/examples/common/app_wifi.c:185: first defined here\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Mar 25, 2021",
            "body": "I think this is because of a recent commit in esp-idf. Could you comment the  API in esp-va-sdk/examples/common/app_wifi.c:184 and try again?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jassi00713",
            "datetime": "Mar 25, 2021",
            "body": "Thanks a lot. It worked.If i could get one more help. Firmware flashes fine. I have generated the new ESP Alexa Credentials.\nmfg_config.csv file i have received in the mail has sme extra feilds as compared to the one already present in SDK.mfg_config.csv in SDK :-\nkey,type,encoding,value\ndevice,namespace,,\nclient_cert,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/device.crt\nclient_key,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/device.key\nserver_cert,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/server.crtmfg_config.csv Received in mail:-\nkey,type,encoding,value\ndevice,namespace,,\ndevice_id,file,binary,/path/to/device.info\naccount_id,file,binary,/path/to/account.info\nmqtt_host,file,binary,/path/to/endpoint.info\nclient_cert,file,binary,/path/to/device.cert\nclient_key,file,binary,/path/to/device.key\nserver_cert,file,binary,/path/to/server.certThanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Mar 25, 2021",
            "body": "All the fields in the downloaded one are required.\nThe one already present in the SDK only has some of those as the SDK provides other options (like menuconfig) to set the other fields.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jassi00713",
            "datetime": "Mar 28, 2021",
            "body": "cool ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#147"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/117",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "deepak141189",
            "datetime": "Oct 7, 2020",
            "body": "Connected ESp32 board with Android phone over bluetooth and played audio songs. But voice cracks on speaker.\nWhen asked alexa to play song via internet directly then songs plays ok without any crack in sound, so speaker is ok it seems.issue comes only when song is played over bluetooth.I have external speaker connected at \"Audio OUT\" of ESP32 board. Speaker is powered externally.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Oct 7, 2020",
            "body": "Hi Deepak,\nRelease here uses WW engine which runs on ESP32 board. Dedicated DSP is not used.To use dedicated DSP and go for production with lyratd-dspg, please contact Espressif.Thanks.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/115",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "deepak141189",
            "datetime": "Sep 23, 2020",
            "body": "I am using ESP32_LyraTD_DSPG_A_V3.1 & B_V1.1 .   Edit (using Window 10 + mingw32)Used these 6 commands to flash it.\n`1. cd esp-va-sdk/examples/amazon_alexaCode start building but in end get these error:esp_avrc_tg_init'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x54): undefined reference to esp_avrc_psth_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x5c): undefined reference to esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x64): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x141): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x175): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x18f): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x19c): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1aa): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1b7): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1c4): undefined reference to esp_avrc_psth_bit_mask_operation' follow\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_tg_set_psth_cmd_filter'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_tg_set_rn_evt_cap'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.literal.bt_app_av_notify_vol_change+0xc): undefined reference to esp_avrc_ct_send_get_rn_capabilities_cmd'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.text.bt_av_new_track+0x13): undefined reference to esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.text.bt_av_play_pos_changed+0xc): undefined reference to bt_app_av_notify_vol_change':\n(.text.bt_app_av_notify_vol_change+0x3e): undefined reference to bt_app_rc_ct_cb':\n(.text.bt_app_rc_ct_cb+0x2a): undefined reference to bt_app_rc_tg_cb':\n(.text.bt_app_rc_tg_cb+0x58): undefined reference to `esp_avrc_tg_send_rn_rsp'\ncollect2.exe: error: ld returned 1 exit status\nmake: *** [/C/msys32/home/bansal1/esp-idf/make/project.mk:461: /home/bansal1/esp-va-sdk/examples/amazon_alexa/build/alexa.elf] Error 1`",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/113",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Berosco",
            "datetime": "Sep 19, 2020",
            "body": "Hello!I'm having a problem using the example of integration with the dialogflow. I followed the steps correctly until the end. I performed the program build and flash, adding the necessary data, but when the program starts and returns the status \"Assistant is ready\", and then the\" REC \"button is pressed the following return is displayed, and after that nothing happens.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KalashQK",
            "datetime": "Mar 18, 2021",
            "body": "Hello! Please tell me if you have solved the problem? I had exactly the same situation, but with an Google Assistant, and not with a Dialogflow. Thank you in advance!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KalashQK",
            "datetime": "Mar 24, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#131"
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/107",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "klisevich",
            "datetime": "Jul 17, 2020",
            "body": "Hi!\nI evaluated voice assistant with Espressif AWS account and now want it run on my account. I created an AVS product and security profile but couldn't  get my device registered. I called   and set product ID to 'esp_vaq_va' and when I was pushing \"Allow\" button in amazon shopping app the following error floated up:\n\nAfter rolling back the Product ID to \"esp_alexa_open\" the console outputsand error wasn't floating up. What should I do to run with a custom product ID?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Jul 17, 2020",
            "body": "Hi  , As mentioned in the Readme(), if you want to use your custom AVS Product ID, you will have to generate the phone apps with your AVS Product.\nHowever you can use your own AWS Account without creating a new AVS Product and continue using the existing phone apps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "klisevich",
            "datetime": "Jul 17, 2020",
            "body": "Thank you for responding! For me, it is important to work with a custom AVS Product. I'm not an experienced Android developer, so could you kindly provide example files with changes(what and where) that I should make in the Android project.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/106",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "klisevich",
            "datetime": "Jul 15, 2020",
            "body": "Hi!\nWe want to use ESP-VA-SDK as a code reference for developing a demo for ESP-Vaquita board appliance. In our project we want to utilize  and  functionality to connect to AWS IoT Core. Are there possible ways for us to substitute ais_mqtt_init() with custom ais_mqtt_over ws_init() function while preserving voice assistant functionality?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chiragatal",
            "datetime": "Jul 15, 2020",
            "body": "Can you explain your use-case a bit more? Is it something that can't be supported with the existing implementation?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "klisevich",
            "datetime": "Jul 15, 2020",
            "body": "Sure! The point of this demo is to replace MQTT protocol with MQTT over WebSocket protocol. As AIA works over the MQTT protocol, it requires managing security certificates when used over TLS. This can create considerable friction in the provisioning procedure. MQTT can use the WebSocket protocol as a transport and it allows for a significantly simpler authentication procedure. We going to utilize pre-signed URL scheme used by AWS for opening the connection over WebSocket. It would be cool if you add to the SDK an option to work with MQTT over WebSocket protocol. Unfortunately, we couldn't find any information regarding MQTT over TCP support from AWS.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/102",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "tcpipchip",
            "datetime": "Jul 5, 2020",
            "body": "Working good!I loved!When will we have news about ESP-VA-SDK ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tcpipchip",
            "datetime": "Jul 6, 2020",
            "body": "Blog using your ESP-VA-SDK on LYRAT",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kroggen",
            "datetime": "Jul 7, 2020",
            "body": "*assistant",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Jul 15, 2020",
            "body": "Hi \nThat's great work. If you'd like to try any further, esp-va-sdk also offers solution for inbuilt Google voice assistant. Look forward to hear back from you. :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tcpipchip",
            "datetime": "Jul 15, 2020",
            "body": "Hi how are you ?But what i did different ?What means the INBUILT ?Do you mean ALEXA ?Thank you!!!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/93",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "skylli",
            "datetime": "May 9, 2020",
            "body": "I'm building a robot which can do someting with voice and answer some question like Alexa.\nI know i can build a alexa custom skill to do that, but i have to start with 'Alexa ask robot to do somthing ' which experience is very poor.\nCan Google Dialogflow and Alexa work at the same time? If it can, how i can make it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "May 11, 2020",
            "body": "Hi \nUnfortunately it's not possible to have multiple built-in voice assistants as of yet.\nBesides from the usecase perspective, how would you identify which query to be sent to the Alexa and which one to be sent to the DialogFlow?\nIt would either require multiple WW support (e.g 'OK google' and 'Alexa') and some way for DSPs to notify the host of which WW has been detected. Or it needs some sort of query/speech processing locally. Both of which would require significant time and effort. :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skylli",
            "datetime": "May 15, 2020",
            "body": "\nThanks for your answer.\nSo the multiple WW  is wake Aws-Alexa with 'Alexa', and wake DialogFlow with 'Ok Google' or 'Hey, rootbot'. How can we  to achieve that.\nCan you provide an example of the implementatio.\nI have connect Espressif, and they have send me an sdk of gitlab, but it oes not help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "May 15, 2020",
            "body": "Hi \nLike I said, it requires significant time and efforts. Multiple WW support (in fact we only have  WW support right now), and support for multiple voice assistant clouds is yet to be implemented, which is quite some work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skylli",
            "datetime": "May 16, 2020",
            "body": "Are there any plans to achieve this function？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skylli",
            "datetime": "May 16, 2020",
            "body": "\nI just read Google Assistant Doc, and i find we can \nDoes our esp-va-sdk can do that ?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/83",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "lxdemon",
            "datetime": "Jan 20, 2020",
            "body": "你好，我想通过Google Voice Assistant来控制开发板的硬件，我看了Google Assistant SDK的Device Actions的指南，在Actions Console中添加了一个Traits，但是sdk这边不知道怎么做，能指导一下吗？另外也想问一下Alexa如果也想实现相同的功能的话该怎么做，大致的流程是怎样的，感谢！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skylli",
            "datetime": "Jun 8, 2020",
            "body": "我也希望 sdk 能开发出 gva 的 build in 命令，这样我们就可以定制.\n目前我是通过 Google Dialogflow 实现，可是没有智能对话啊。",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/78",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "DuHeLong",
            "datetime": "Dec 31, 2019",
            "body": "Hello,\nI'm using the latest SDK. During the test, I found that during the wake-up and dialogue of ALEXA, only the right microphone works, and the left microphone does not affect the function even if I short-circuit it. Why is this? Isn't it a dual microphone?\n        I remember using the early SDK, shorting any microphone, ALEXA can work;",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DuHeLong",
            "datetime": "Dec 31, 2019",
            "body": "I tested it directly on the development board (esp32_lyrat).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Dec 31, 2019",
            "body": "For upload path, as per specification, data is down-channeled and down sampled to 16K sampling rate.\nHence, another channel simply is irrelevant and data for that is ignored.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DuHeLong",
            "datetime": "Jan 6, 2020",
            "body": "According to the specifications provided by your company, the function is a dual microphone function, and I remember that when testing in February last year, shielding any one microphone can work, can you tell us why? Did we test wrong before?Refer to the following specifications：\nKey Supported Features:\n•\tVarious mainstream, both lossy and lossless, compressed audio formats, including M4A, AAC, FLAC, OGG, OPUS, MP3, etc.\n•\tOne-key configuration and wake-up from the standby mode.\n•\tSoftAP and Station mode.\n•\tVarious wireless protocols：Wi-Fi 802.11b/g/n, Classic BT and Bluetooth LE.\n•\tA series of audio inputs, including Wi-Fi, BT-audio, DLNA, Line-in, etc.\n•\tBluetooth LE network configuration, and smart network configuration with apps, such as WeChat.\n•\tTwo microphones for the development of near-field and far-field voice recognition applications.\n•\tPeripherals for differentiated demands.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/facebookresearch/fairseq/issues/4045",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "patrickvonplaten",
            "datetime": "Nov 30, 2021",
            "body": "Hey Quintong, , ,Thanks a lot for open-sourcing the model weights of your recent paper !\nI can run the models well and the output seems coherent. E.g., on a LibriSpeech speech input with the transcription :The following models decode this input audio to the following phoneme sequence:Note that the models logits does not predict a word splitting character (the ) since it's also not in the dictionary. Now if I use the  phonemizer with the  library as follows:I get the output:  which is more or less the same as the prediction of the model - see:=> So I'm assuming that the phonemizer command:is correct here. However I couldn't find any file that confirms this. Could you take a look to see whether the phonemizer command is correct? This would make training such a model possible :-) Also if I now want to decode a French input sample, I would simple replace  with  no (I've tried it and it also gave me very good results)?Also I had to more questions:Thanks a lot!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xuqiantong",
            "datetime": "Nov 30, 2021",
            "body": "Hi Patrick. Yes, your command is correct, and changing  option is enough to phonemize different languages. We usually use  to separate phonemes, in order not to do any manual parsing afterwards.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Dec 8, 2021",
            "body": "Thanks a mille  - that's super useful! One last question regarding what was said above:Why was the \"|\" not added as a token to the dictionary and the token embeddings? Wouldn't it make sense to train the model with \"|\" so that it's easier to later map a phoneme sequence to a word sequence (since \"|\" would allow one to know what phonemes belong to the same word).Do you think leaving out \"|\" in the dictionary improves phoneme error rate?Thanks a lot!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "xuqiantong",
            "datetime": "Dec 10, 2021",
            "body": "Hi , this is a good question, inserting \"|\" between words will definitely help in decoding to words. We didn't do this in our work simply because we only focused on phonome recognition.Feel free to try all the other possibilities :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickvonplaten",
            "datetime": "Nov 30, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/espressif/esp-va-sdk/issues/68",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "jhpark555",
            "datetime": "Oct 29, 2019",
            "body": "Can we use bluetooth a2dp and alexa/google service at the same time?\nI tried to port a2dp sink demo code on alexa example, but there was an overflow error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Oct 31, 2019",
            "body": "Hi \nPlease wait for a few days, we are working on integrating A2DP sink with Alexa.\nNext release would have A2DP sink support.\nThanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 2, 2019",
            "body": "Is A2DP with Alexa still ongoing?\nThanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 9, 2019",
            "body": "Hi \nAlexa with A2DP sink support has been pushed in the latest release. You can try that out. May I know which evaluation board are you using? Since it would require the flash of size > 4MB.Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 9, 2019",
            "body": "Hello.  I'm using ESP32_LyraT board.  By the way I'm having a reboot problem.\nI enabled BT classic and a2dp sink on the menuconfig and make ALEXA_BT=1, the\nbelow reboot is happening. What am I missing?I (62) boot: Chip Revision: 1\nI (65) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 14:38:57\nI (43) boot: Enabling RNG early entropy source...\nI (45) qio_mode: Enabling QIO for flash chip GD\nI (50) boot: SPI Speed      : 80MHz\nI (54) boot: SPI Mode       : QIO\nI (58) boot: SPI Flash Size : 4MB\nI (62) boot: Partition Table:\nI (66) boot: ## Label            Usage          Type ST Offset   Length\nI (73) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (81) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (88) boot:  2 factory          factory app      00 00 00010000 00390000\nI (96) boot: End of partition table\nI (100) boot_comm: mismatch chip revision, expect 1, found 0\nI (106) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (684) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (691) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (692) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (703) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1082) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1116) esp_image: Image length 3887296 doesn't fit in partition length 3735552\nE (1116) boot: Factory app partition is not bootable\nE (1119) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 10, 2019",
            "body": "Hi \nAs mentioned in the changelog , A2DP sink requires flash size > 4MB. Since size of the Alexa binary increases substantially.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 10, 2019",
            "body": "Thanks. I understood.   Then, I need to buy esp32-lyratd-syna with 8MB flash board.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 16, 2019",
            "body": "Even if I replaced the flash to 8mb. Still it is crashing.\nPlease let me know how I can fix.I (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 14:29:37\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (57) boot: SPI Mode       : QIO\nI (61) boot: SPI Flash Size : 8MB\nI (65) boot: Partition Table:\nI (69) boot: ## Label            Usage          Type ST Offset   Length\nI (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (91) boot:  2 factory          factory app      00 00 00010000 007d0000\nI (98) boot: End of partition table\nI (102) boot_comm: mismatch chip revision, expect 1, found 0\nI (109) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (687) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (693) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (694) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (706) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1085) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 17, 2019",
            "body": "does the device crash with default partitions.csv as well?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 17, 2019",
            "body": "Yes, I think so.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 17, 2019",
            "body": "can you post a complete failure log with that? Since from the above logs, I am not really able to identify the actual error. Booting up seems to be happening normally.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 17, 2019",
            "body": "Okay. I'm at home now. I'll post full log message on tomorrow morning.   Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 17, 2019",
            "body": "I changed to use default partition table by blocking partitions_4mb_flash_csv in audio_board.mk.  Below is the full log data.     Thanks.MONITOR\n--- idf_monitor on /dev/ttyUSB0 115200 ---\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nets Jun  8 2016 00:22:57rst:0x1 (POWERON_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 10:18:03\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (56) boot: SPI Mode       : QIO\nI (60) boot: SPI Flash Size : 8MB\nI (65) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (83) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (90) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (98) boot:  3 factory          factory app      00 00 00010000 00100000\nI (105) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (113) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (120) boot: End of partition table\nI (125) boot: Defaulting to factory image\nI (129) boot_comm: mismatch chip revision, expect 1, found 0\nI (136) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (714) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (720) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (721) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (733) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1112) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1146) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1146) boot: Factory app partition is not bootable\nE (1148) esp_image: image at 0x110000 has invalid magic byte\nE (1155) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1161) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1168) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1175) boot: OTA app partition slot 0 is not bootable\nE (1180) esp_image: image at 0x210000 has invalid magic byte\nE (1187) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1193) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1200) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1207) esp_image: image at 0x210000 has invalid SPI size 11\nE (1213) boot: OTA app partition slot 1 is not bootable\nE (1219) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (91) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1151) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1185) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1185) boot: Factory app partition is not bootable\nE (1187) esp_image: image at 0x110000 has invalid magic byte\nE (1194) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1200) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1207) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1214) boot: OTA app partition slot 0 is not bootable\nE (1220) esp_image: image at 0x210000 has invalid magic byte\nE (1226) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1232) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1240) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1246) esp_image: image at 0x210000 has invalid SPI size 11\nE (1253) boot: OTA app partition slot 1 is not bootable\nE (1258) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (90) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1151) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1185) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1185) boot: Factory app partition is not bootable\nE (1187) esp_image: image at 0x110000 has invalid magic byte\nE (1194) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1200) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1207) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1214) boot: OTA app partition slot 0 is not bootable\nE (1220) esp_image: image at 0x210000 has invalid magic byte\nE (1226) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1232) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1240) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1246) esp_image: image at 0x210000 has invalid SPI size 11\nE (1253) boot: OTA app partition slot 1 is not bootable\nE (1258) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (90) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 19, 2019",
            "body": "Hi \nPartition size of ota0 and ota1 that you've set (0x100000) is insufficient for the image (as indicated by the error message \"E (1146) esp_image: Image length 3887296 doesn't fit in partition length 1048576\"). The default partition table, , sets this to 0x420000, since binary size is much larger.If it isn't fitting in 8MB flash, you may remove ota_1 partition for now (this would only affect OTA functionality).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Dec 20, 2019",
            "body": "Hello,\nWhen I used defalut partitions.csv and removed ota_1, the alexa worked fine but the bluetooth didn't work.     Can you tell me which board you used ? I need to prepare for demo for our customer in a month.   Thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Dec 23, 2019",
            "body": "It shouldn't be board specific.\nDid you compile with ALEXA_BT=1 ? It is mentioned ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 7, 2020",
            "body": "Even if I enabled ALEXA_BT=1 while compiling, I can't find bluetooth supported device from the PC or phone.  Only Amazon can works.rst:0x1 (POWERON_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (38) boot: compile time 10:46:40\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (56) boot: SPI Mode       : QIO\nI (60) boot: SPI Flash Size : 8MB\nI (64) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (90) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (98) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (105) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (113) boot: End of partition table\nI (117) boot: No factory image, trying OTA 0\nI (122) boot_comm: mismatch chip revision, expect 1, found 0\nI (128) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1fc714 (2082580) map\nI (662) esp_image: segment 1: paddr=0x0021c73c vaddr=0x3ffbdb60 size=0x038d4 ( 14548) load\nI (666) esp_image: segment 2: paddr=0x00220018 vaddr=0x400d0018 size=0x166bb0 (1469360) map\n0x400d0018: _stext at ??:?I (1038) esp_image: segment 3: paddr=0x00386bd0 vaddr=0x3ffc1434 size=0x017c8 (  6088) load\nI (1040) esp_image: segment 4: paddr=0x003883a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (1046) esp_image: segment 5: paddr=0x003887a8 vaddr=0x40080400 size=0x1da08 (121352) load\nI (1110) boot: Loaded app from partition at offset 0x20000\nI (1110) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1578420829\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: AVS endpoint: \nW (1994) I2S: I2S driver already installed\nW (2014) I2S: I2S driver already installed\n[http_stream]: [stream_new]: Internal: 54100, External: 1278256\n[auth-delegate]: Token will be refreshed after 3000 seconds.\n[http_stream]: [stream_get: 1]: /v20160207/directives\n[http_stream]: [sid: 1] Response code: 200\n[http_transport]: AVS level connction has now been established: \n[http_stream]: [stream_new]: Internal: 53052, External: 1270920\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":40,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechRecognizer\",\"name\":\"RecognizerState\"},\"payload\":{\"wakeword\":\"ALEXA\"}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"PLAYING\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"System\",\"name\":\"SynchronizeState\",\"messageId\":\"9bc366a8-091f-4ff9-8d36-60553287cb1d\"},\"payload\":{}}}\n[http_stream]: [stream_post: 3]: /v20160207/events\n[http_stream]: [sid: 3] Response code: 204\n############## Alexa is ready ##############\n[3 seconds]: [http_stream]: [stream_delete: 3] Internal: 52564, External: 1278240, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54480, External: 1278660\n[http_stream]: [stream_get: 5]: /ping\n[http_stream]: [sid: 5] Response code: 204\n[184 seconds]: [http_stream]: [stream_delete: 5] Internal: 52444, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 53220, External: 1278660\n[http_stream]: [stream_get: 7]: /ping\n[http_stream]: [sid: 7] Response code: 204\n[364 seconds]: [http_stream]: [stream_delete: 7] Internal: 54076, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54244, External: 1278660\n[http_stream]: [stream_get: 9]: /ping\n[http_stream]: [sid: 9] Response code: 204\n[544 seconds]: [http_stream]: [stream_delete: 9] Internal: 53960, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 52372, External: 1278496\n[http_stream]: [stream_get: 11]: /ping\n[http_stream]: [sid: 11] Response code: 204\n[725 seconds]: [http_stream]: [stream_delete: 11] Internal: 53844, External: 1278284, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54008, External: 1278496\n[http_stream]: [stream_get: 13]: /ping\n[http_stream]: [sid: 13] Response code: 204\n[905 seconds]: [http_stream]: [stream_delete: 13] Internal: 53728, External: 1278284, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_get: 15]: /ping: 52140, External: 1278496\n[http_stream]: [sid: 15] Response code: 204\n[1085 seconds]: [http_stream]: [stream_delete: 15] Internal: 53612, External: 1278284, min ever internal: 45080, largest free block: 25012",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 7, 2020",
            "body": "HelloNew issues happens to me. I erased flash in order to write again. At this time, I have a credential and token issues.  Please look at below log data.   Constantly repetitive message are appearing.[conn_mgr_prov]: Scan results:\nS.N. SSID                             RSSI\n[ 0] ESS-Sound-2.4G                    -34\n[ 1] ChromecastAudio7999.a             -47\n[ 2] ESSNETGR88                        -64\n[ 3] SOUND ROOM                        -71\n[ 4] EERO                              -83\n[ 5] ESSNETGR87                        -86\n[ 6] USR5461                           -87\n[ 7] AP5                               -92\n[avs_config]: APP Got: ANRjiVJkZIhlzUMekyvx, amzn1.application-oa2-client.e8c06545d9764a05b8b6d91eecfeb23a, amzn-com.espressif.provbleavs://?methodName=signin, abcd1234\n[auth-delegate]: Auth delegate with comp app: ANRjiVJkZIhlzUMekyvx, amzn1.application-oa2-client.e8c06545d9764a05b8b6d91eecfeb23a, amzn-com.espressif.provbleavs://?methodName=signin, abcd1234E (38681) esp-tls: couldn't get hostname for :api.amazon.com:\nE (38691) esp-tls: Failed to open new connection\nE (38701) httpc: Failed to create a new TLS connection\nE (38701) [auth-delegate]: Failed to connect to Auth URL \"\"\nE (38711) [auth-delegate]: Authentication attempt failed. Retrying. Please check connectivity and/or credentials. In case of authentication failure try resetting the device to factory mode and restart provisioning\n[conn_mgr_prov_handler]: WiFi Credentials Received:\nssid: ESS-Sound-2.4G\npassword: phoenixplus\nW (38921) wifi: alloc eb len=76 type=2 fail, heap:4143112W (38921) wifi: m f probe req l=0W (39041) wifi: alloc eb len=76 type=2 fail, heap:4143968W (39041) wifi: m f probe req l=0W (39161) wifi: alloc eb len=76 type=2 fail, heap:4143968W (39161) wifi: m f probe req l=0E (39231) esp-tls: couldn't get hostname for :api.amazon.com:\nE (39231) esp-tls: Failed to open new connection\nE (39231) httpc: Failed to create a new TLS connection\nE (39231) [auth-delegate]: Failed to connect to Auth URL \"\"\nE (39241) [auth-delegate]: Authentication attempt failed. Retrying. Please check connectivity and/or credentials. In case of authentication failure try resetting the device to factory mode and restart provisioning\nW (39281) wifi: alloc eb len=76 type=2 fail, heap:4143968",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Jan 7, 2020",
            "body": "For BT not working issue...If you had previously build without ALEXA_BT=1, you will need a clean build. i.e., you need to remove build/ sdkconfig and sdkconfig.old.\nAnd then do a build with ALEXA_BT=1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 7, 2020",
            "body": "I followed your instruction but I have reboot error like below. And continuously it reboots.Rebooting...\nets Jun  8 2016 00:22:57rst:0xc (SW_CPU_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 15:24:06\nI (52) boot: Enabling RNG early entropy source...\nI (52) boot: SPI Speed      : 80MHz\nI (52) boot: SPI Mode       : DIO\nI (56) boot: SPI Flash Size : 8MB\nI (60) boot: Partition Table:\nI (63) boot: ## Label            Usage          Type ST Offset   Length\nI (70) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (78) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (85) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (93) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (100) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (108) boot: End of partition table\nI (112) boot: No factory image, trying OTA 0\nI (117) boot_comm: mismatch chip revision, expect 1, found 0\nI (123) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (739) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (746) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (747) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (763) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1165) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1216) boot: Loaded app from partition at offset 0x20000\nI (1216) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1578440038\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (1952) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (2832) I2S: I2S driver already installed\nW (2862) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Jan 8, 2020",
            "body": "Can you try pulling latest IDF v3.2 branch?\nJust to confirm, you are trying with the default sdkconfig.bt.defaults right?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 8, 2020",
            "body": "I'm using sdkconfig. Below is this  configuration.  In the menuconfig, I selected bluetooth dual mode and enabled classic bluetooth. then I compiled with ALEXA_BT=1 option. Do I have wrong configuration?\nThanks.CONFIG_IDF_FIRMWARE_CHIP_ID=0x0000CONFIG_TOOLPREFIX=\"xtensa-esp32-elf-\"\nCONFIG_PYTHON=\"python\"\nCONFIG_MAKE_WARN_UNDEFINED_VARIABLES=yCONFIG_LOG_BOOTLOADER_LEVEL_NONE=\nCONFIG_LOG_BOOTLOADER_LEVEL_ERROR=\nCONFIG_LOG_BOOTLOADER_LEVEL_WARN=\nCONFIG_LOG_BOOTLOADER_LEVEL_INFO=y\nCONFIG_LOG_BOOTLOADER_LEVEL_DEBUG=\nCONFIG_LOG_BOOTLOADER_LEVEL_VERBOSE=\nCONFIG_LOG_BOOTLOADER_LEVEL=3\nCONFIG_BOOTLOADER_SPI_WP_PIN=7\nCONFIG_BOOTLOADER_VDDSDIO_BOOST_1_9V=y\nCONFIG_BOOTLOADER_FACTORY_RESET=\nCONFIG_BOOTLOADER_APP_TEST=\nCONFIG_BOOTLOADER_WDT_ENABLE=y\nCONFIG_BOOTLOADER_WDT_DISABLE_IN_USER_CODE=\nCONFIG_BOOTLOADER_WDT_TIME_MS=9000CONFIG_SECURE_SIGNED_APPS_NO_SECURE_BOOT=\nCONFIG_SECURE_BOOT_ENABLED=\nCONFIG_FLASH_ENCRYPTION_ENABLED=CONFIG_ESPTOOLPY_PORT=\"/dev/ttyUSB0\"\nCONFIG_ESPTOOLPY_BAUD_115200B=\nCONFIG_ESPTOOLPY_BAUD_230400B=\nCONFIG_ESPTOOLPY_BAUD_921600B=y\nCONFIG_ESPTOOLPY_BAUD_2MB=\nCONFIG_ESPTOOLPY_BAUD_OTHER=\nCONFIG_ESPTOOLPY_BAUD_OTHER_VAL=115200\nCONFIG_ESPTOOLPY_BAUD=921600\nCONFIG_ESPTOOLPY_COMPRESSED=y\nCONFIG_FLASHMODE_QIO=y\nCONFIG_FLASHMODE_QOUT=\nCONFIG_FLASHMODE_DIO=\nCONFIG_FLASHMODE_DOUT=\nCONFIG_ESPTOOLPY_FLASHMODE=\"dio\"\nCONFIG_ESPTOOLPY_FLASHFREQ_80M=y\nCONFIG_ESPTOOLPY_FLASHFREQ_40M=\nCONFIG_ESPTOOLPY_FLASHFREQ_26M=\nCONFIG_ESPTOOLPY_FLASHFREQ_20M=\nCONFIG_ESPTOOLPY_FLASHFREQ=\"80m\"\nCONFIG_ESPTOOLPY_FLASHSIZE_1MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_2MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_4MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_8MB=y\nCONFIG_ESPTOOLPY_FLASHSIZE_16MB=\nCONFIG_ESPTOOLPY_FLASHSIZE=\"8MB\"\nCONFIG_ESPTOOLPY_FLASHSIZE_DETECT=y\nCONFIG_ESPTOOLPY_BEFORE_RESET=y\nCONFIG_ESPTOOLPY_BEFORE_NORESET=\nCONFIG_ESPTOOLPY_BEFORE=\"default_reset\"\nCONFIG_ESPTOOLPY_AFTER_RESET=y\nCONFIG_ESPTOOLPY_AFTER_NORESET=\nCONFIG_ESPTOOLPY_AFTER=\"hard_reset\"\nCONFIG_MONITOR_BAUD_9600B=\nCONFIG_MONITOR_BAUD_57600B=\nCONFIG_MONITOR_BAUD_115200B=y\nCONFIG_MONITOR_BAUD_230400B=\nCONFIG_MONITOR_BAUD_921600B=\nCONFIG_MONITOR_BAUD_2MB=\nCONFIG_MONITOR_BAUD_OTHER=\nCONFIG_MONITOR_BAUD_OTHER_VAL=115200\nCONFIG_MONITOR_BAUD=115200CONFIG_ALEXA_PRODUCT_ID=\"esp_avs_open\"\nCONFIG_ALEXA_REMOVE_SHORT_ALERT_SUPPORT=\nCONFIG_ALEXA_ENABLE_OTA=\nCONFIG_ALEXA_ENABLE_LOCAL_PLAYER=\nCONFIG_ALEXA_ENABLE_EQUALIZER=\nCONFIG_ALEXA_ENABLE_AWS_IOT=CONFIG_PARTITION_TABLE_SINGLE_APP=\nCONFIG_PARTITION_TABLE_TWO_OTA=\nCONFIG_PARTITION_TABLE_CUSTOM=y\nCONFIG_PARTITION_TABLE_CUSTOM_FILENAME=\"partitions.csv\"\nCONFIG_PARTITION_TABLE_FILENAME=\"partitions.csv\"\nCONFIG_PARTITION_TABLE_OFFSET=0x8000\nCONFIG_PARTITION_TABLE_MD5=yCONFIG_OPTIMIZATION_LEVEL_DEBUG=\nCONFIG_OPTIMIZATION_LEVEL_RELEASE=y\nCONFIG_OPTIMIZATION_ASSERTIONS_ENABLED=y\nCONFIG_OPTIMIZATION_ASSERTIONS_SILENT=\nCONFIG_OPTIMIZATION_ASSERTIONS_DISABLED=\nCONFIG_CXX_EXCEPTIONS=\nCONFIG_STACK_CHECK_NONE=y\nCONFIG_STACK_CHECK_NORM=\nCONFIG_STACK_CHECK_STRONG=\nCONFIG_STACK_CHECK_ALL=\nCONFIG_STACK_CHECK=\nCONFIG_WARN_WRITE_STRINGS=\nCONFIG_DISABLE_GCC8_WARNINGS=CONFIG_ESP32_APPTRACE_DEST_TRAX=\nCONFIG_ESP32_APPTRACE_DEST_NONE=y\nCONFIG_ESP32_APPTRACE_ENABLE=\nCONFIG_ESP32_APPTRACE_LOCK_ENABLE=y\nCONFIG_AWS_IOT_SDK=y\nCONFIG_AWS_IOT_MQTT_HOST=\"\"\nCONFIG_AWS_IOT_MQTT_PORT=8883\nCONFIG_AWS_IOT_MQTT_TX_BUF_LEN=512\nCONFIG_AWS_IOT_MQTT_RX_BUF_LEN=512\nCONFIG_AWS_IOT_MQTT_NUM_SUBSCRIBE_HANDLERS=5\nCONFIG_AWS_IOT_MQTT_MIN_RECONNECT_WAIT_INTERVAL=1000\nCONFIG_AWS_IOT_MQTT_MAX_RECONNECT_WAIT_INTERVAL=128000CONFIG_AWS_IOT_OVERRIDE_THING_SHADOW_RX_BUFFER=\nCONFIG_AWS_IOT_SHADOW_MAX_SIZE_OF_UNIQUE_CLIENT_ID_BYTES=80\nCONFIG_AWS_IOT_SHADOW_MAX_SIMULTANEOUS_ACKS=10\nCONFIG_AWS_IOT_SHADOW_MAX_SIMULTANEOUS_THINGNAMES=10\nCONFIG_AWS_IOT_SHADOW_MAX_JSON_TOKEN_EXPECTED=120\nCONFIG_AWS_IOT_SHADOW_MAX_SHADOW_TOPIC_LENGTH_WITHOUT_THINGNAME=60\nCONFIG_AWS_IOT_SHADOW_MAX_SIZE_OF_THING_NAME=40CONFIG_BT_ENABLED=yCONFIG_BTDM_CONTROLLER_MODE_BLE_ONLY=\nCONFIG_BTDM_CONTROLLER_MODE_BR_EDR_ONLY=\nCONFIG_BTDM_CONTROLLER_MODE_BTDM=y\nCONFIG_BTDM_CONTROLLER_BLE_MAX_CONN=3\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_ACL_CONN=2\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_SYNC_CONN=0\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_HCI=\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_PCM=y\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_EFF=1\nCONFIG_BTDM_CONTROLLER_BLE_MAX_CONN_EFF=3\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_ACL_CONN_EFF=2\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_SYNC_CONN_EFF=0\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE_0=y\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE_1=\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE=0\nCONFIG_BTDM_CONTROLLER_HCI_MODE_VHCI=y\nCONFIG_BTDM_CONTROLLER_HCI_MODE_UART_H4=CONFIG_BTDM_CONTROLLER_MODEM_SLEEP=y\nCONFIG_BTDM_MODEM_SLEEP_MODE_ORIG=y\nCONFIG_BTDM_MODEM_SLEEP_MODE_EVED=\nCONFIG_BTDM_LPCLK_SEL_MAIN_XTAL=y\nCONFIG_BLE_SCAN_DUPLICATE=y\nCONFIG_SCAN_DUPLICATE_BY_DEVICE_ADDR=y\nCONFIG_SCAN_DUPLICATE_BY_ADV_DATA=\nCONFIG_SCAN_DUPLICATE_BY_ADV_DATA_AND_DEVICE_ADDR=\nCONFIG_SCAN_DUPLICATE_TYPE=0\nCONFIG_DUPLICATE_SCAN_CACHE_SIZE=200\nCONFIG_BLE_MESH_SCAN_DUPLICATE_EN=\nCONFIG_BLE_ADV_REPORT_FLOW_CONTROL_SUPPORTED=y\nCONFIG_BLE_ADV_REPORT_FLOW_CONTROL_NUM=100\nCONFIG_BLE_ADV_REPORT_DISCARD_THRSHOLD=20\nCONFIG_BLUEDROID_ENABLED=y\nCONFIG_BLUEDROID_PINNED_TO_CORE_0=y\nCONFIG_BLUEDROID_PINNED_TO_CORE_1=\nCONFIG_BLUEDROID_PINNED_TO_CORE=0\nCONFIG_BTC_TASK_STACK_SIZE=3072\nCONFIG_BLUEDROID_MEM_DEBUG=\nCONFIG_CLASSIC_BT_ENABLED=y\nCONFIG_A2DP_ENABLE=y\nCONFIG_A2DP_SINK_TASK_STACK_SIZE=2048\nCONFIG_A2DP_SOURCE_TASK_STACK_SIZE=2048\nCONFIG_BT_SPP_ENABLED=\nCONFIG_HFP_ENABLE=\nCONFIG_GATTS_ENABLE=y\nCONFIG_GATTS_SEND_SERVICE_CHANGE_MANUAL=\nCONFIG_GATTS_SEND_SERVICE_CHANGE_AUTO=y\nCONFIG_GATTS_SEND_SERVICE_CHANGE_MODE=0\nCONFIG_GATTC_ENABLE=y\nCONFIG_GATTC_CACHE_NVS_FLASH=\nCONFIG_BLE_SMP_ENABLE=y\nCONFIG_SMP_SLAVE_CON_PARAMS_UPD_ENABLE=\nCONFIG_BT_STACK_NO_LOG=CONFIG_HCI_TRACE_LEVEL_NONE=\nCONFIG_HCI_TRACE_LEVEL_ERROR=\nCONFIG_HCI_TRACE_LEVEL_WARNING=y\nCONFIG_HCI_TRACE_LEVEL_API=\nCONFIG_HCI_TRACE_LEVEL_EVENT=\nCONFIG_HCI_TRACE_LEVEL_DEBUG=\nCONFIG_HCI_TRACE_LEVEL_VERBOSE=\nCONFIG_HCI_INITIAL_TRACE_LEVEL=2\nCONFIG_BTM_TRACE_LEVEL_NONE=\nCONFIG_BTM_TRACE_LEVEL_ERROR=\nCONFIG_BTM_TRACE_LEVEL_WARNING=y\nCONFIG_BTM_TRACE_LEVEL_API=\nCONFIG_BTM_TRACE_LEVEL_EVENT=\nCONFIG_BTM_TRACE_LEVEL_DEBUG=\nCONFIG_BTM_TRACE_LEVEL_VERBOSE=\nCONFIG_BTM_INITIAL_TRACE_LEVEL=2\nCONFIG_L2CAP_TRACE_LEVEL_NONE=\nCONFIG_L2CAP_TRACE_LEVEL_ERROR=\nCONFIG_L2CAP_TRACE_LEVEL_WARNING=y\nCONFIG_L2CAP_TRACE_LEVEL_API=\nCONFIG_L2CAP_TRACE_LEVEL_EVENT=\nCONFIG_L2CAP_TRACE_LEVEL_DEBUG=\nCONFIG_L2CAP_TRACE_LEVEL_VERBOSE=\nCONFIG_L2CAP_INITIAL_TRACE_LEVEL=2\nCONFIG_RFCOMM_TRACE_LEVEL_NONE=\nCONFIG_RFCOMM_TRACE_LEVEL_ERROR=\nCONFIG_RFCOMM_TRACE_LEVEL_WARNING=y\nCONFIG_RFCOMM_TRACE_LEVEL_API=\nCONFIG_RFCOMM_TRACE_LEVEL_EVENT=\nCONFIG_RFCOMM_TRACE_LEVEL_DEBUG=\nCONFIG_RFCOMM_TRACE_LEVEL_VERBOSE=\nCONFIG_RFCOMM_INITIAL_TRACE_LEVEL=2\nCONFIG_SDP_TRACE_LEVEL_NONE=\nCONFIG_SDP_TRACE_LEVEL_ERROR=\nCONFIG_SDP_TRACE_LEVEL_WARNING=y\nCONFIG_SDP_TRACE_LEVEL_API=\nCONFIG_SDP_TRACE_LEVEL_EVENT=\nCONFIG_SDP_TRACE_LEVEL_DEBUG=\nCONFIG_SDP_TRACE_LEVEL_VERBOSE=\nCONFIG_SDP_INITIAL_TRACE_LEVEL=2\nCONFIG_GAP_TRACE_LEVEL_NONE=\nCONFIG_GAP_TRACE_LEVEL_ERROR=\nCONFIG_GAP_TRACE_LEVEL_WARNING=y\nCONFIG_GAP_TRACE_LEVEL_API=\nCONFIG_GAP_TRACE_LEVEL_EVENT=\nCONFIG_GAP_TRACE_LEVEL_DEBUG=\nCONFIG_GAP_TRACE_LEVEL_VERBOSE=\nCONFIG_GAP_INITIAL_TRACE_LEVEL=2\nCONFIG_BNEP_TRACE_LEVEL_NONE=\nCONFIG_BNEP_TRACE_LEVEL_ERROR=\nCONFIG_BNEP_TRACE_LEVEL_WARNING=y\nCONFIG_BNEP_TRACE_LEVEL_API=\nCONFIG_BNEP_TRACE_LEVEL_EVENT=\nCONFIG_BNEP_TRACE_LEVEL_DEBUG=\nCONFIG_BNEP_TRACE_LEVEL_VERBOSE=\nCONFIG_BNEP_INITIAL_TRACE_LEVEL=2\nCONFIG_PAN_TRACE_LEVEL_NONE=\nCONFIG_PAN_TRACE_LEVEL_ERROR=\nCONFIG_PAN_TRACE_LEVEL_WARNING=y\nCONFIG_PAN_TRACE_LEVEL_API=\nCONFIG_PAN_TRACE_LEVEL_EVENT=\nCONFIG_PAN_TRACE_LEVEL_DEBUG=\nCONFIG_PAN_TRACE_LEVEL_VERBOSE=\nCONFIG_PAN_INITIAL_TRACE_LEVEL=2\nCONFIG_A2D_TRACE_LEVEL_NONE=\nCONFIG_A2D_TRACE_LEVEL_ERROR=\nCONFIG_A2D_TRACE_LEVEL_WARNING=y\nCONFIG_A2D_TRACE_LEVEL_API=\nCONFIG_A2D_TRACE_LEVEL_EVENT=\nCONFIG_A2D_TRACE_LEVEL_DEBUG=\nCONFIG_A2D_TRACE_LEVEL_VERBOSE=\nCONFIG_A2D_INITIAL_TRACE_LEVEL=2\nCONFIG_AVDT_TRACE_LEVEL_NONE=\nCONFIG_AVDT_TRACE_LEVEL_ERROR=\nCONFIG_AVDT_TRACE_LEVEL_WARNING=y\nCONFIG_AVDT_TRACE_LEVEL_API=\nCONFIG_AVDT_TRACE_LEVEL_EVENT=\nCONFIG_AVDT_TRACE_LEVEL_DEBUG=\nCONFIG_AVDT_TRACE_LEVEL_VERBOSE=\nCONFIG_AVDT_INITIAL_TRACE_LEVEL=2\nCONFIG_AVCT_TRACE_LEVEL_NONE=\nCONFIG_AVCT_TRACE_LEVEL_ERROR=\nCONFIG_AVCT_TRACE_LEVEL_WARNING=y\nCONFIG_AVCT_TRACE_LEVEL_API=\nCONFIG_AVCT_TRACE_LEVEL_EVENT=\nCONFIG_AVCT_TRACE_LEVEL_DEBUG=\nCONFIG_AVCT_TRACE_LEVEL_VERBOSE=\nCONFIG_AVCT_INITIAL_TRACE_LEVEL=2\nCONFIG_AVRC_TRACE_LEVEL_NONE=\nCONFIG_AVRC_TRACE_LEVEL_ERROR=\nCONFIG_AVRC_TRACE_LEVEL_WARNING=y\nCONFIG_AVRC_TRACE_LEVEL_API=\nCONFIG_AVRC_TRACE_LEVEL_EVENT=\nCONFIG_AVRC_TRACE_LEVEL_DEBUG=\nCONFIG_AVRC_TRACE_LEVEL_VERBOSE=\nCONFIG_AVRC_INITIAL_TRACE_LEVEL=2\nCONFIG_MCA_TRACE_LEVEL_NONE=\nCONFIG_MCA_TRACE_LEVEL_ERROR=\nCONFIG_MCA_TRACE_LEVEL_WARNING=y\nCONFIG_MCA_TRACE_LEVEL_API=\nCONFIG_MCA_TRACE_LEVEL_EVENT=\nCONFIG_MCA_TRACE_LEVEL_DEBUG=\nCONFIG_MCA_TRACE_LEVEL_VERBOSE=\nCONFIG_MCA_INITIAL_TRACE_LEVEL=2\nCONFIG_HID_TRACE_LEVEL_NONE=\nCONFIG_HID_TRACE_LEVEL_ERROR=\nCONFIG_HID_TRACE_LEVEL_WARNING=y\nCONFIG_HID_TRACE_LEVEL_API=\nCONFIG_HID_TRACE_LEVEL_EVENT=\nCONFIG_HID_TRACE_LEVEL_DEBUG=\nCONFIG_HID_TRACE_LEVEL_VERBOSE=\nCONFIG_HID_INITIAL_TRACE_LEVEL=2\nCONFIG_APPL_TRACE_LEVEL_NONE=\nCONFIG_APPL_TRACE_LEVEL_ERROR=\nCONFIG_APPL_TRACE_LEVEL_WARNING=y\nCONFIG_APPL_TRACE_LEVEL_API=\nCONFIG_APPL_TRACE_LEVEL_EVENT=\nCONFIG_APPL_TRACE_LEVEL_DEBUG=\nCONFIG_APPL_TRACE_LEVEL_VERBOSE=\nCONFIG_APPL_INITIAL_TRACE_LEVEL=2\nCONFIG_GATT_TRACE_LEVEL_NONE=\nCONFIG_GATT_TRACE_LEVEL_ERROR=\nCONFIG_GATT_TRACE_LEVEL_WARNING=y\nCONFIG_GATT_TRACE_LEVEL_API=\nCONFIG_GATT_TRACE_LEVEL_EVENT=\nCONFIG_GATT_TRACE_LEVEL_DEBUG=\nCONFIG_GATT_TRACE_LEVEL_VERBOSE=\nCONFIG_GATT_INITIAL_TRACE_LEVEL=2\nCONFIG_SMP_TRACE_LEVEL_NONE=\nCONFIG_SMP_TRACE_LEVEL_ERROR=\nCONFIG_SMP_TRACE_LEVEL_WARNING=y\nCONFIG_SMP_TRACE_LEVEL_API=\nCONFIG_SMP_TRACE_LEVEL_EVENT=\nCONFIG_SMP_TRACE_LEVEL_DEBUG=\nCONFIG_SMP_TRACE_LEVEL_VERBOSE=\nCONFIG_SMP_INITIAL_TRACE_LEVEL=2\nCONFIG_BTIF_TRACE_LEVEL_NONE=\nCONFIG_BTIF_TRACE_LEVEL_ERROR=\nCONFIG_BTIF_TRACE_LEVEL_WARNING=y\nCONFIG_BTIF_TRACE_LEVEL_API=\nCONFIG_BTIF_TRACE_LEVEL_EVENT=\nCONFIG_BTIF_TRACE_LEVEL_DEBUG=\nCONFIG_BTIF_TRACE_LEVEL_VERBOSE=\nCONFIG_BTIF_INITIAL_TRACE_LEVEL=2\nCONFIG_BTC_TRACE_LEVEL_NONE=\nCONFIG_BTC_TRACE_LEVEL_ERROR=\nCONFIG_BTC_TRACE_LEVEL_WARNING=y\nCONFIG_BTC_TRACE_LEVEL_API=\nCONFIG_BTC_TRACE_LEVEL_EVENT=\nCONFIG_BTC_TRACE_LEVEL_DEBUG=\nCONFIG_BTC_TRACE_LEVEL_VERBOSE=\nCONFIG_BTC_INITIAL_TRACE_LEVEL=2\nCONFIG_OSI_TRACE_LEVEL_NONE=\nCONFIG_OSI_TRACE_LEVEL_ERROR=\nCONFIG_OSI_TRACE_LEVEL_WARNING=y\nCONFIG_OSI_TRACE_LEVEL_API=\nCONFIG_OSI_TRACE_LEVEL_EVENT=\nCONFIG_OSI_TRACE_LEVEL_DEBUG=\nCONFIG_OSI_TRACE_LEVEL_VERBOSE=\nCONFIG_OSI_INITIAL_TRACE_LEVEL=2\nCONFIG_BLUFI_TRACE_LEVEL_NONE=\nCONFIG_BLUFI_TRACE_LEVEL_ERROR=\nCONFIG_BLUFI_TRACE_LEVEL_WARNING=y\nCONFIG_BLUFI_TRACE_LEVEL_API=\nCONFIG_BLUFI_TRACE_LEVEL_EVENT=\nCONFIG_BLUFI_TRACE_LEVEL_DEBUG=\nCONFIG_BLUFI_TRACE_LEVEL_VERBOSE=\nCONFIG_BLUFI_INITIAL_TRACE_LEVEL=2\nCONFIG_BT_ACL_CONNECTIONS=4\nCONFIG_BT_ALLOCATION_FROM_SPIRAM_FIRST=\nCONFIG_BT_BLE_DYNAMIC_ENV_MEMORY=\nCONFIG_BLE_HOST_QUEUE_CONGESTION_CHECK=\nCONFIG_SMP_ENABLE=y\nCONFIG_BLE_ACTIVE_SCAN_REPORT_ADV_SCAN_RSP_INDIVIDUALLY=\nCONFIG_BLE_ESTABLISH_LINK_CONNECTION_TIMEOUT=30\nCONFIG_BT_RESERVE_DRAM=0xdb5cCONFIG_IO_GLITCH_FILTER_TIME_MS=50CONFIG_ADC_FORCE_XPD_FSM=\nCONFIG_ADC2_DISABLE_DAC=yCONFIG_SPI_MASTER_IN_IRAM=\nCONFIG_SPI_MASTER_ISR_IN_IRAM=y\nCONFIG_SPI_SLAVE_IN_IRAM=\nCONFIG_SPI_SLAVE_ISR_IN_IRAM=yCONFIG_ESP32_REV_MIN_0=y\nCONFIG_ESP32_REV_MIN_1=\nCONFIG_ESP32_REV_MIN_2=\nCONFIG_ESP32_REV_MIN_3=\nCONFIG_ESP32_REV_MIN=0\nCONFIG_ESP32_DPORT_WORKAROUND=y\nCONFIG_ESP32_DEFAULT_CPU_FREQ_80=\nCONFIG_ESP32_DEFAULT_CPU_FREQ_160=\nCONFIG_ESP32_DEFAULT_CPU_FREQ_240=y\nCONFIG_ESP32_DEFAULT_CPU_FREQ_MHZ=240\nCONFIG_SPIRAM_SUPPORT=yCONFIG_SPIRAM_BOOT_INIT=y\nCONFIG_SPIRAM_IGNORE_NOTFOUND=\nCONFIG_SPIRAM_USE_MEMMAP=\nCONFIG_SPIRAM_USE_CAPS_ALLOC=\nCONFIG_SPIRAM_USE_MALLOC=y\nCONFIG_SPIRAM_TYPE_AUTO=y\nCONFIG_SPIRAM_TYPE_ESPPSRAM32=\nCONFIG_SPIRAM_TYPE_ESPPSRAM64=\nCONFIG_SPIRAM_SIZE=-1\nCONFIG_SPIRAM_SPEED_40M=\nCONFIG_SPIRAM_SPEED_80M=y\nCONFIG_SPIRAM_MEMTEST=y\nCONFIG_SPIRAM_CACHE_WORKAROUND=y\nCONFIG_SPIRAM_BANKSWITCH_ENABLE=y\nCONFIG_SPIRAM_BANKSWITCH_RESERVE=8\nCONFIG_SPIRAM_MALLOC_ALWAYSINTERNAL=8190\nCONFIG_WIFI_LWIP_ALLOCATION_FROM_SPIRAM_FIRST=y\nCONFIG_SPIRAM_MALLOC_RESERVE_INTERNAL=32768\nCONFIG_SPIRAM_ALLOW_STACK_EXTERNAL_MEMORY=y\nCONFIG_SPIRAM_ALLOW_BSS_SEG_EXTERNAL_MEMORY=\nCONFIG_SPIRAM_OCCUPY_HSPI_HOST=\nCONFIG_SPIRAM_OCCUPY_VSPI_HOST=yCONFIG_D0WD_PSRAM_CLK_IO=17\nCONFIG_D0WD_PSRAM_CS_IO=16CONFIG_D2WD_PSRAM_CLK_IO=9\nCONFIG_D2WD_PSRAM_CS_IO=10CONFIG_PICO_PSRAM_CS_IO=10\nCONFIG_MEMMAP_TRACEMEM=\nCONFIG_MEMMAP_TRACEMEM_TWOBANKS=\nCONFIG_ESP32_TRAX=\nCONFIG_TRACEMEM_RESERVE_DRAM=0x0\nCONFIG_ESP32_ENABLE_COREDUMP_TO_FLASH=\nCONFIG_ESP32_ENABLE_COREDUMP_TO_UART=\nCONFIG_ESP32_ENABLE_COREDUMP_TO_NONE=y\nCONFIG_ESP32_ENABLE_COREDUMP=\nCONFIG_TWO_UNIVERSAL_MAC_ADDRESS=\nCONFIG_FOUR_UNIVERSAL_MAC_ADDRESS=y\nCONFIG_NUMBER_OF_UNIVERSAL_MAC_ADDRESS=4\nCONFIG_SYSTEM_EVENT_QUEUE_SIZE=32\nCONFIG_SYSTEM_EVENT_TASK_STACK_SIZE=4096\nCONFIG_MAIN_TASK_STACK_SIZE=6144\nCONFIG_IPC_TASK_STACK_SIZE=1024\nCONFIG_TIMER_TASK_STACK_SIZE=4096\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_CRLF=y\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_LF=\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_CR=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_CRLF=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_LF=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_CR=y\nCONFIG_NEWLIB_NANO_FORMAT=\nCONFIG_CONSOLE_UART_DEFAULT=y\nCONFIG_CONSOLE_UART_CUSTOM=\nCONFIG_CONSOLE_UART_NONE=\nCONFIG_CONSOLE_UART_NUM=0\nCONFIG_CONSOLE_UART_BAUDRATE=115200\nCONFIG_ULP_COPROC_ENABLED=\nCONFIG_ULP_COPROC_RESERVE_MEM=0\nCONFIG_ESP32_PANIC_PRINT_HALT=\nCONFIG_ESP32_PANIC_PRINT_REBOOT=y\nCONFIG_ESP32_PANIC_SILENT_REBOOT=\nCONFIG_ESP32_PANIC_GDBSTUB=\nCONFIG_ESP32_DEBUG_OCDAWARE=y\nCONFIG_ESP32_DEBUG_STUBS_ENABLE=\nCONFIG_INT_WDT=y\nCONFIG_INT_WDT_TIMEOUT_MS=800\nCONFIG_INT_WDT_CHECK_CPU1=y\nCONFIG_TASK_WDT=y\nCONFIG_TASK_WDT_PANIC=\nCONFIG_TASK_WDT_TIMEOUT_S=5\nCONFIG_TASK_WDT_CHECK_IDLE_TASK_CPU0=y\nCONFIG_TASK_WDT_CHECK_IDLE_TASK_CPU1=y\nCONFIG_BROWNOUT_DET=y\nCONFIG_BROWNOUT_DET_LVL_SEL_0=y\nCONFIG_BROWNOUT_DET_LVL_SEL_1=\nCONFIG_BROWNOUT_DET_LVL_SEL_2=\nCONFIG_BROWNOUT_DET_LVL_SEL_3=\nCONFIG_BROWNOUT_DET_LVL_SEL_4=\nCONFIG_BROWNOUT_DET_LVL_SEL_5=\nCONFIG_BROWNOUT_DET_LVL_SEL_6=\nCONFIG_BROWNOUT_DET_LVL_SEL_7=\nCONFIG_BROWNOUT_DET_LVL=0\nCONFIG_REDUCE_PHY_TX_POWER=y\nCONFIG_ESP32_TIME_SYSCALL_USE_RTC_FRC1=y\nCONFIG_ESP32_TIME_SYSCALL_USE_RTC=\nCONFIG_ESP32_TIME_SYSCALL_USE_FRC1=\nCONFIG_ESP32_TIME_SYSCALL_USE_NONE=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_INTERNAL_RC=y\nCONFIG_ESP32_RTC_CLOCK_SOURCE_EXTERNAL_CRYSTAL=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_EXTERNAL_OSC=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_INTERNAL_8MD256=\nCONFIG_ESP32_RTC_CLK_CAL_CYCLES=1024\nCONFIG_ESP32_DEEP_SLEEP_WAKEUP_DELAY=2000\nCONFIG_ESP32_XTAL_FREQ_40=y\nCONFIG_ESP32_XTAL_FREQ_26=\nCONFIG_ESP32_XTAL_FREQ_AUTO=\nCONFIG_ESP32_XTAL_FREQ=40\nCONFIG_DISABLE_BASIC_ROM_CONSOLE=\nCONFIG_ESP_TIMER_PROFILING=\nCONFIG_COMPATIBLE_PRE_V2_1_BOOTLOADERS=\nCONFIG_ESP_ERR_TO_NAME_LOOKUP=y\nCONFIG_ESP32_DPORT_DIS_INTERRUPT_LVL=5CONFIG_SW_COEXIST_ENABLE=y\nCONFIG_SW_COEXIST_PREFERENCE_WIFI=\nCONFIG_SW_COEXIST_PREFERENCE_BT=\nCONFIG_SW_COEXIST_PREFERENCE_BALANCE=y\nCONFIG_SW_COEXIST_PREFERENCE_VALUE=2\nCONFIG_ESP32_WIFI_STATIC_RX_BUFFER_NUM=10\nCONFIG_ESP32_WIFI_DYNAMIC_RX_BUFFER_NUM=32\nCONFIG_ESP32_WIFI_STATIC_TX_BUFFER=y\nCONFIG_ESP32_WIFI_TX_BUFFER_TYPE=0\nCONFIG_ESP32_WIFI_STATIC_TX_BUFFER_NUM=32\nCONFIG_ESP32_WIFI_CSI_ENABLED=\nCONFIG_ESP32_WIFI_AMPDU_TX_ENABLED=y\nCONFIG_ESP32_WIFI_TX_BA_WIN=6\nCONFIG_ESP32_WIFI_AMPDU_RX_ENABLED=y\nCONFIG_ESP32_WIFI_RX_BA_WIN=16\nCONFIG_ESP32_WIFI_NVS_ENABLED=y\nCONFIG_ESP32_WIFI_TASK_PINNED_TO_CORE_0=y\nCONFIG_ESP32_WIFI_TASK_PINNED_TO_CORE_1=\nCONFIG_ESP32_WIFI_SOFTAP_BEACON_MAX_LEN=752\nCONFIG_ESP32_WIFI_IRAM_OPT=y\nCONFIG_ESP32_WIFI_MGMT_SBUF_NUM=32CONFIG_ESP32_PHY_CALIBRATION_AND_DATA_STORAGE=y\nCONFIG_ESP32_PHY_INIT_DATA_IN_PARTITION=\nCONFIG_ESP32_PHY_MAX_WIFI_TX_POWER=20\nCONFIG_ESP32_PHY_MAX_TX_POWER=20CONFIG_PM_ENABLE=CONFIG_ADC_CAL_EFUSE_TP_ENABLE=y\nCONFIG_ADC_CAL_EFUSE_VREF_ENABLE=y\nCONFIG_ADC_CAL_LUT_ENABLE=yCONFIG_EVENT_LOOP_PROFILING=CONFIG_ESP_HTTP_CLIENT_ENABLE_HTTPS=yCONFIG_HTTPD_MAX_REQ_HDR_LEN=512\nCONFIG_HTTPD_MAX_URI_LEN=512\nCONFIG_HTTPD_PURGE_BUF_LEN=32\nCONFIG_HTTPD_LOG_PURGE_DATA=CONFIG_DMA_RX_BUF_NUM=10\nCONFIG_DMA_TX_BUF_NUM=10\nCONFIG_EMAC_L2_TO_L3_RX_BUF_MODE=y\nCONFIG_EMAC_CHECK_LINK_PERIOD_MS=2000\nCONFIG_EMAC_TASK_PRIORITY=20\nCONFIG_EMAC_TASK_STACK_SIZE=3072CONFIG_FATFS_CODEPAGE_DYNAMIC=\nCONFIG_FATFS_CODEPAGE_437=y\nCONFIG_FATFS_CODEPAGE_720=\nCONFIG_FATFS_CODEPAGE_737=\nCONFIG_FATFS_CODEPAGE_771=\nCONFIG_FATFS_CODEPAGE_775=\nCONFIG_FATFS_CODEPAGE_850=\nCONFIG_FATFS_CODEPAGE_852=\nCONFIG_FATFS_CODEPAGE_855=\nCONFIG_FATFS_CODEPAGE_857=\nCONFIG_FATFS_CODEPAGE_860=\nCONFIG_FATFS_CODEPAGE_861=\nCONFIG_FATFS_CODEPAGE_862=\nCONFIG_FATFS_CODEPAGE_863=\nCONFIG_FATFS_CODEPAGE_864=\nCONFIG_FATFS_CODEPAGE_865=\nCONFIG_FATFS_CODEPAGE_866=\nCONFIG_FATFS_CODEPAGE_869=\nCONFIG_FATFS_CODEPAGE_932=\nCONFIG_FATFS_CODEPAGE_936=\nCONFIG_FATFS_CODEPAGE_949=\nCONFIG_FATFS_CODEPAGE_950=\nCONFIG_FATFS_CODEPAGE=437\nCONFIG_FATFS_LFN_NONE=y\nCONFIG_FATFS_LFN_HEAP=\nCONFIG_FATFS_LFN_STACK=\nCONFIG_FATFS_FS_LOCK=0\nCONFIG_FATFS_TIMEOUT_MS=10000\nCONFIG_FATFS_PER_FILE_CACHE=yCONFIG_MB_QUEUE_LENGTH=20\nCONFIG_MB_SERIAL_TASK_STACK_SIZE=2048\nCONFIG_MB_SERIAL_BUF_SIZE=256\nCONFIG_MB_SERIAL_TASK_PRIO=10\nCONFIG_MB_CONTROLLER_SLAVE_ID_SUPPORT=\nCONFIG_MB_CONTROLLER_NOTIFY_TIMEOUT=20\nCONFIG_MB_CONTROLLER_NOTIFY_QUEUE_SIZE=20\nCONFIG_MB_CONTROLLER_STACK_SIZE=4096\nCONFIG_MB_EVENT_QUEUE_TIMEOUT=20\nCONFIG_MB_TIMER_PORT_ENABLED=y\nCONFIG_MB_TIMER_GROUP=0\nCONFIG_MB_TIMER_INDEX=0CONFIG_FREERTOS_UNICORE=\nCONFIG_FREERTOS_NO_AFFINITY=0x7FFFFFFF\nCONFIG_FREERTOS_CORETIMER_0=y\nCONFIG_FREERTOS_CORETIMER_1=\nCONFIG_FREERTOS_HZ=100\nCONFIG_FREERTOS_ASSERT_ON_UNTESTED_FUNCTION=y\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_NONE=\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_PTRVAL=\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_CANARY=y\nCONFIG_FREERTOS_WATCHPOINT_END_OF_STACK=\nCONFIG_FREERTOS_INTERRUPT_BACKTRACE=y\nCONFIG_FREERTOS_THREAD_LOCAL_STORAGE_POINTERS=1\nCONFIG_FREERTOS_ASSERT_FAIL_ABORT=y\nCONFIG_FREERTOS_ASSERT_FAIL_PRINT_CONTINUE=\nCONFIG_FREERTOS_ASSERT_DISABLE=\nCONFIG_FREERTOS_IDLE_TASK_STACKSIZE=1024\nCONFIG_FREERTOS_ISR_STACKSIZE=1536\nCONFIG_FREERTOS_LEGACY_HOOKS=\nCONFIG_FREERTOS_MAX_TASK_NAME_LEN=16\nCONFIG_SUPPORT_STATIC_ALLOCATION=y\nCONFIG_ENABLE_STATIC_TASK_CLEAN_UP_HOOK=\nCONFIG_TIMER_TASK_PRIORITY=1\nCONFIG_TIMER_TASK_STACK_DEPTH=6144\nCONFIG_TIMER_QUEUE_LENGTH=10\nCONFIG_FREERTOS_QUEUE_REGISTRY_SIZE=0\nCONFIG_FREERTOS_USE_TRACE_FACILITY=y\nCONFIG_FREERTOS_USE_STATS_FORMATTING_FUNCTIONS=\nCONFIG_FREERTOS_GENERATE_RUN_TIME_STATS=\nCONFIG_FREERTOS_DEBUG_INTERNALS=CONFIG_HEAP_POISONING_DISABLED=y\nCONFIG_HEAP_POISONING_LIGHT=\nCONFIG_HEAP_POISONING_COMPREHENSIVE=\nCONFIG_HEAP_TRACING=CONFIG_HTTP_CLIENT_MAX_HDR_VAL_LEN=50CONFIG_STATUS_LED_QUICK_BLINK_FREQ=5\nCONFIG_STATUS_LED_SLOW_BLINK_FREQ=1\nCONFIG_USE_LEDC_HIGHSPEED_MODE=y\nCONFIG_USE_LEDC_LOWSPEED_MODE=\nCONFIG_STATUS_LED_SPEED_MODE=0\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_0=y\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_1=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_2=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_3=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_4=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_5=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_6=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_7=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_DEF=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL=0\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_0=y\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_1=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_2=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_3=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_DEF=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER=0\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_0=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_1=y\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_2=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_3=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_4=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_5=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_6=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_7=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_DEF=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL=1\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_0=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_1=y\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_2=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_3=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_DEF=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER=1\nCONFIG_STATUS_LED_NIGHT_MODE_ENABLE=y\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_0=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_1=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_2=y\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_3=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_4=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_5=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_6=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_7=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_DEF=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL=2\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_0=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_1=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_2=y\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_3=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_DEF=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER=2CONFIG_LIBSODIUM_USE_MBEDTLS_SHA=yCONFIG_LOG_DEFAULT_LEVEL_NONE=\nCONFIG_LOG_DEFAULT_LEVEL_ERROR=\nCONFIG_LOG_DEFAULT_LEVEL_WARN=y\nCONFIG_LOG_DEFAULT_LEVEL_INFO=\nCONFIG_LOG_DEFAULT_LEVEL_DEBUG=\nCONFIG_LOG_DEFAULT_LEVEL_VERBOSE=\nCONFIG_LOG_DEFAULT_LEVEL=2\nCONFIG_LOG_COLORS=yCONFIG_L2_TO_L3_COPY=\nCONFIG_LWIP_IRAM_OPTIMIZATION=\nCONFIG_LWIP_MAX_SOCKETS=9\nCONFIG_USE_ONLY_LWIP_SELECT=y\nCONFIG_LWIP_SO_REUSE=y\nCONFIG_LWIP_SO_REUSE_RXTOALL=y\nCONFIG_LWIP_SO_RCVBUF=\nCONFIG_LWIP_DHCP_MAX_NTP_SERVERS=3\nCONFIG_LWIP_IP_FRAG=\nCONFIG_LWIP_IP_REASSEMBLY=\nCONFIG_LWIP_STATS=\nCONFIG_LWIP_ETHARP_TRUST_IP_MAC=y\nCONFIG_ESP_GRATUITOUS_ARP=y\nCONFIG_GARP_TMR_INTERVAL=60\nCONFIG_TCPIP_RECVMBOX_SIZE=32\nCONFIG_LWIP_DHCP_DOES_ARP_CHECK=y\nCONFIG_LWIP_DHCP_RESTORE_LAST_IP=CONFIG_LWIP_DHCPS_LEASE_UNIT=60\nCONFIG_LWIP_DHCPS_MAX_STATION_NUM=8\nCONFIG_LWIP_AUTOIP=\nCONFIG_LWIP_NETIF_LOOPBACK=y\nCONFIG_LWIP_LOOPBACK_MAX_PBUFS=8CONFIG_LWIP_MAX_ACTIVE_TCP=8\nCONFIG_LWIP_MAX_LISTENING_TCP=8\nCONFIG_TCP_MAXRTX=12\nCONFIG_TCP_SYNMAXRTX=6\nCONFIG_TCP_MSS=1436\nCONFIG_TCP_MSL=60000\nCONFIG_TCP_SND_BUF_DEFAULT=14360\nCONFIG_TCP_WND_DEFAULT=14360\nCONFIG_TCP_RECVMBOX_SIZE=12\nCONFIG_TCP_QUEUE_OOSEQ=y\nCONFIG_ESP_TCP_KEEP_CONNECTION_WHEN_IP_CHANGES=\nCONFIG_TCP_OVERSIZE_MSS=y\nCONFIG_TCP_OVERSIZE_QUARTER_MSS=\nCONFIG_TCP_OVERSIZE_DISABLE=\nCONFIG_LWIP_WND_SCALE=CONFIG_LWIP_MAX_UDP_PCBS=16\nCONFIG_UDP_RECVMBOX_SIZE=6\nCONFIG_TCPIP_TASK_STACK_SIZE=3072\nCONFIG_TCPIP_TASK_AFFINITY_NO_AFFINITY=y\nCONFIG_TCPIP_TASK_AFFINITY_CPU0=\nCONFIG_TCPIP_TASK_AFFINITY_CPU1=\nCONFIG_TCPIP_TASK_AFFINITY=0x7FFFFFFF\nCONFIG_PPP_SUPPORT=CONFIG_LWIP_MULTICAST_PING=\nCONFIG_LWIP_BROADCAST_PING=CONFIG_LWIP_MAX_RAW_PCBS=16CONFIG_MBEDTLS_INTERNAL_MEM_ALLOC=y\nCONFIG_MBEDTLS_EXTERNAL_MEM_ALLOC=\nCONFIG_MBEDTLS_DEFAULT_MEM_ALLOC=\nCONFIG_MBEDTLS_CUSTOM_MEM_ALLOC=\nCONFIG_MBEDTLS_SSL_MAX_CONTENT_LEN=16384\nCONFIG_MBEDTLS_ASYMMETRIC_CONTENT_LEN=\nCONFIG_MBEDTLS_DEBUG=\nCONFIG_MBEDTLS_HARDWARE_AES=y\nCONFIG_MBEDTLS_HARDWARE_MPI=\nCONFIG_MBEDTLS_HARDWARE_SHA=\nCONFIG_MBEDTLS_HAVE_TIME=y\nCONFIG_MBEDTLS_HAVE_TIME_DATE=y\nCONFIG_MBEDTLS_TLS_SERVER_AND_CLIENT=y\nCONFIG_MBEDTLS_TLS_SERVER_ONLY=\nCONFIG_MBEDTLS_TLS_CLIENT_ONLY=\nCONFIG_MBEDTLS_TLS_DISABLED=\nCONFIG_MBEDTLS_TLS_SERVER=y\nCONFIG_MBEDTLS_TLS_CLIENT=y\nCONFIG_MBEDTLS_TLS_ENABLED=yCONFIG_MBEDTLS_PSK_MODES=\nCONFIG_MBEDTLS_KEY_EXCHANGE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_DHE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ELLIPTIC_CURVE=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDHE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDH_RSA=y\nCONFIG_MBEDTLS_SSL_RENEGOTIATION=y\nCONFIG_MBEDTLS_SSL_PROTO_SSL3=\nCONFIG_MBEDTLS_SSL_PROTO_TLS1=y\nCONFIG_MBEDTLS_SSL_PROTO_TLS1_1=y\nCONFIG_MBEDTLS_SSL_PROTO_TLS1_2=y\nCONFIG_MBEDTLS_SSL_PROTO_DTLS=\nCONFIG_MBEDTLS_SSL_ALPN=y\nCONFIG_MBEDTLS_SSL_SESSION_TICKETS=yCONFIG_MBEDTLS_AES_C=y\nCONFIG_MBEDTLS_CAMELLIA_C=\nCONFIG_MBEDTLS_DES_C=\nCONFIG_MBEDTLS_RC4_DISABLED=y\nCONFIG_MBEDTLS_RC4_ENABLED_NO_DEFAULT=\nCONFIG_MBEDTLS_RC4_ENABLED=\nCONFIG_MBEDTLS_BLOWFISH_C=\nCONFIG_MBEDTLS_XTEA_C=\nCONFIG_MBEDTLS_CCM_C=y\nCONFIG_MBEDTLS_GCM_C=y\nCONFIG_MBEDTLS_RIPEMD160_C=CONFIG_MBEDTLS_PEM_PARSE_C=y\nCONFIG_MBEDTLS_PEM_WRITE_C=y\nCONFIG_MBEDTLS_X509_CRL_PARSE_C=y\nCONFIG_MBEDTLS_X509_CSR_PARSE_C=y\nCONFIG_MBEDTLS_ECP_C=y\nCONFIG_MBEDTLS_ECDH_C=y\nCONFIG_MBEDTLS_ECDSA_C=y\nCONFIG_MBEDTLS_ECP_DP_SECP192R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP224R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP256R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP384R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP521R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP192K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP224K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP256K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP256R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP384R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP512R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_CURVE25519_ENABLED=y\nCONFIG_MBEDTLS_ECP_NIST_OPTIM=yCONFIG_MDNS_MAX_SERVICES=10CONFIG_MQTT_PROTOCOL_311=y\nCONFIG_MQTT_TRANSPORT_SSL=y\nCONFIG_MQTT_TRANSPORT_WEBSOCKET=y\nCONFIG_MQTT_TRANSPORT_WEBSOCKET_SECURE=y\nCONFIG_MQTT_USE_CUSTOM_CONFIG=\nCONFIG_MQTT_TASK_CORE_SELECTION_ENABLED=\nCONFIG_MQTT_CUSTOM_OUTBOX=CONFIG_OPENSSL_DEBUG=\nCONFIG_OPENSSL_ASSERT_DO_NOTHING=y\nCONFIG_OPENSSL_ASSERT_EXIT=CONFIG_ESP32_PTHREAD_TASK_PRIO_DEFAULT=4\nCONFIG_ESP32_PTHREAD_TASK_STACK_SIZE_DEFAULT=16000\nCONFIG_PTHREAD_STACK_MIN=768CONFIG_SPI_FLASH_VERIFY_WRITE=\nCONFIG_SPI_FLASH_ENABLE_COUNTERS=\nCONFIG_SPI_FLASH_ROM_DRIVER_PATCH=y\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_ABORTS=y\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_FAILS=\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_ALLOWED=\nCONFIG_SPI_FLASH_YIELD_DURING_ERASE=CONFIG_SPIFFS_MAX_PARTITIONS=3CONFIG_SPIFFS_CACHE=y\nCONFIG_SPIFFS_CACHE_WR=y\nCONFIG_SPIFFS_CACHE_STATS=\nCONFIG_SPIFFS_PAGE_CHECK=y\nCONFIG_SPIFFS_GC_MAX_RUNS=10\nCONFIG_SPIFFS_GC_STATS=\nCONFIG_SPIFFS_PAGE_SIZE=256\nCONFIG_SPIFFS_OBJ_NAME_LEN=32\nCONFIG_SPIFFS_USE_MAGIC=y\nCONFIG_SPIFFS_USE_MAGIC_LENGTH=y\nCONFIG_SPIFFS_META_LENGTH=4\nCONFIG_SPIFFS_USE_MTIME=yCONFIG_SPIFFS_DBG=\nCONFIG_SPIFFS_API_DBG=\nCONFIG_SPIFFS_GC_DBG=\nCONFIG_SPIFFS_CACHE_DBG=\nCONFIG_SPIFFS_CHECK_DBG=\nCONFIG_SPIFFS_TEST_VISUALISATION=CONFIG_IP_LOST_TIMER_INTERVAL=120\nCONFIG_TCPIP_LWIP=yCONFIG_SUPPRESS_SELECT_DEBUG_OUTPUT=y\nCONFIG_SUPPORT_TERMIOS=yCONFIG_WL_SECTOR_SIZE_512=\nCONFIG_WL_SECTOR_SIZE_4096=y\nCONFIG_WL_SECTOR_SIZE=4096",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Jan 8, 2020",
            "body": "I see quite a few differences when in terms of the configuration. (pinned core for BT task, Debug logs for BT enabled etc.).\nI suggest please try removing all older artifacts (build, sdkconfig, sdkconfig.old) and try once with . See if it works and gradually work up with other changes.\nRight now, it's really tight in terms of internal memory availability when BT and Alexa are ran simultaneously.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 14, 2020",
            "body": "I followed your recommendation and at this time I'm having a error and reboot again continuously.E (2440) [bluetooth-internal]: Error reading paired device list from NVSIs that something to do with flash size? I used 8mb.  Can you let me know what the error means?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Jan 14, 2020",
            "body": "The error just indicates that you don't have any paired devices yet.Just go ahead and pair a device.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 14, 2020",
            "body": "Before I can try paring, the system already went crash. only one time , when the wifi has failure, the bluetooth seems to be started pairing.   Do I need 16mb memory instead of 8mb?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Jan 14, 2020",
            "body": "Hi ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 15, 2020",
            "body": "Yes.  here is.--- idf_monitor on /dev/ttyUSB0 115200 ---\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nets Jun  8 2016 00:22:57rst:0x1 (POWERON_RESET),boot:0x3e (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (63) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (38) boot: compile time 11:49:06\nI (51) boot: Enabling RNG early entropy source...\nI (51) boot: SPI Speed      : 80MHz\nI (51) boot: SPI Mode       : DIO\nI (55) boot: SPI Flash Size : 8MB\nI (59) boot: Partition Table:\nI (62) boot: ## Label            Usage          Type ST Offset   Length\nI (69) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (77) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (84) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (92) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (99) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (107) boot: End of partition table\nI (111) boot: No factory image, trying OTA 0\nI (116) boot_comm: mismatch chip revision, expect 1, found 0\nI (122) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (738) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (745) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (746) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (762) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1164) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1215) boot: Loaded app from partition at offset 0x20000\nI (1215) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1579044379\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (2461) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (3361) I2S: I2S driver already installed\nW (3381) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506Rebooting...\nets Jun  8 2016 00:22:57rst:0xc (SW_CPU_RESET),boot:0x3e (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 11:49:06\nI (52) boot: Enabling RNG early entropy source...\nI (52) boot: SPI Speed      : 80MHz\nI (52) boot: SPI Mode       : DIO\nI (56) boot: SPI Flash Size : 8MB\nI (60) boot: Partition Table:\nI (63) boot: ## Label            Usage          Type ST Offset   Length\nI (70) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (78) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (85) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (93) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (100) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (108) boot: End of partition table\nI (112) boot: No factory image, trying OTA 0\nI (117) boot_comm: mismatch chip revision, expect 1, found 0\nI (123) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (739) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (746) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (747) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (763) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1165) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1216) boot: Loaded app from partition at offset 0x20000\nI (1216) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1579044383\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (1932) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (2802) I2S: I2S driver already installed\nW (2822) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avsheth",
            "datetime": "Jan 15, 2020",
            "body": "Hi \nCan you try with below changes:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jhpark555",
            "datetime": "Jan 15, 2020",
            "body": "Hi Amit and Vikram,It worked as I followed your last instruction. Many Thanks.   I can demo now for my customer.\nBy the way I have one more question.\nI want to use AAC decoder on Bluetooth a2dp sink. Does esp32 can support it?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vikramdattu",
            "datetime": "Jan 15, 2020",
            "body": "Unfortunately, ESP32 bluetooth stack support only SBC for now.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1908",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Aug 10, 2020",
            "body": "To reproduce:It shouldn't match at all, but it is also broken when it matches. The problematic code is here: I don't understand what  is supposed to be about, and it actually breaks things. I think I was trying to be clever, but the cleverness was never used.Maybe the entire logic needs to be removed.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Simpcyclassy",
            "datetime": "Aug 11, 2020",
            "body": "I was about to raise an issue on the page name routine being broken as well as I just explained to you and saw this. Looks like it's a related issue affecting ",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1900",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "awallin",
            "datetime": "Aug 10, 2020",
            "body": "Immediately after installing the extension the onboarding screens should be displayed for users to make choices about telemetry and be educated on how to use Firefox Voice.Currently there appears to be a bug where new installations do not see the onboarding screens until after they press the microphone icon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Aug 10, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1907",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Aug 10, 2020",
            "body": "Frequently (usually?) the browser won't identify any preferred/default voice, and so we'll have to pick one. Right now we pick the alphabetically first voice.We should choose a voice more intelligently. For instance my alphabetically first one is Agnes, which is not a good voice. Samantha however is a good voice.To see the available voices:My \"default\" is Alex, which I've never chosen. My system default is Samantha. Note that Samantha has the string \"premium\" in the .There may be heuristics on Windows and Linux, but we'll have to look more closely to determine them.Note that internally we should use null as the preference, and then determine the voice dynamically – and if the user sets an explicit preference we respect that.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/PCCoE-Hacktoberfest-21/FRIDAY/issues/3",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 1, 2021",
            "body": "If the command is \"Where is Italy?\" then the Voice Assistant must open Google maps and display Italy",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaiNarra8",
            "datetime": "Oct 2, 2021",
            "body": "Hey I would like to work on this issue!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 2, 2021",
            "body": "Hello  . I am assigning this to you",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yashdabhade1",
            "datetime": "Oct 23, 2021",
            "body": "Hello can you please assign this to me?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 23, 2021",
            "body": "Go ahead ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ChetansMittal",
            "datetime": "Oct 26, 2021",
            "body": "IS the work going on, I want to work on these issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yashdabhade1",
            "datetime": "Oct 26, 2021",
            "body": "I am pushing my commit by the EOD",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 1, 2021",
            "body": [],
            "type": "issue",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 2, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 5, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#31"
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 23, 2021",
            "body": [],
            "type": "unassigned",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 23, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#58"
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 23, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/PCCoE-Hacktoberfest-21/FRIDAY/issues/4",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 1, 2021",
            "body": "User: What's 5+5 ?\nFRIDAY (Voice-assistant) : It's 10",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaiNarra8",
            "datetime": "Oct 2, 2021",
            "body": "Hi can I work on this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Abhishek-Rath",
            "datetime": "Oct 2, 2021",
            "body": "Can I work on this ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 2, 2021",
            "body": "Hey  I have already assigned you with another issue.\nWill it be fine if i assign this issue to  ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "harshadbhere",
            "datetime": "Oct 2, 2021",
            "body": " can I work on this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SaiNarra8",
            "datetime": "Oct 2, 2021",
            "body": "Sure sure",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 3, 2021",
            "body": "Alright  I am assigning this issue to you\nGo ahead",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ChetansMittal",
            "datetime": "Oct 16, 2021",
            "body": "Hey I want to work on it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 1, 2021",
            "body": [],
            "type": "issue",
            "related_issue": null
        },
        {
            "user_name": "rohitjoshi6",
            "datetime": "Oct 3, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1898",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "leo-lb",
            "datetime": "Aug 9, 2020",
            "body": "Hello!The initiative that Mozilla has started is interesting, but I am a bit troubled that this requires \"cloud\" services. I think Mozilla's initiative is especially interesting if it's offline and can operate locally, with the same performance as if it was running in a \"cloud\". I am personally not at ease sending my voice over the Internet even with promises of it not being saved.I am curious what are blockers to this and propose tracking them here.Probably those could be around:How does this exactly work on the \"cloud\" side? What would it take moving the \"cloud\" side entirely on the client side?I would imagine it's possible to train a model using \"cloud\" resources and later execute that model locally.Thank you!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1913",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "julienreszka",
            "datetime": "Sep 16, 2020",
            "body": "\n            \n          ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Extarys",
            "datetime": "Sep 22, 2020",
            "body": "Yeah I discovered this like 5 minutes ago on the . Damn.I'll start donating to Mozilla. Can't believe I never donated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "grahamperrin",
            "datetime": "Jan 30, 2021",
            "body": "I was not aware until I began seeking information in the absence of release notes for : does not exist.Re:  for a different (but related) extension, I assume that the release note for   will be similar to the release note for  :Please: where, exactly? (undated) is quite vague:– but there's no explanation for the decommissioning. I can guess a reason, but I'd prefer the explanation to come from Mozilla.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "grahamperrin",
            "datetime": "Feb 5, 2021",
            "body": "Cross reference:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "andrenatal",
            "datetime": "Feb 9, 2021",
            "body": "Hi . I'm a Mozilla employee and have created Voice Fill, Firefox Voice, our speech-proxy server, and numerous other voice projects here and was the one responsible for decommissioning the addons.The reason is simple and transparent: we are decommissioning the addons purely because the use of Google's STT backend won't scale for us, and we never managed to train production ready models through projects like Deep Speech and Common Voice with the quality required for VF and FxVoice.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Extarys",
            "datetime": "Feb 9, 2021",
            "body": " I really appreciate you take the time to provide this explanation and it's totally understandable.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "grahamperrin",
            "datetime": "Feb 22, 2021",
            "body": "Likewise,  much appreciated.(After shifting the discussion to Discourse, I found some key posts on the subject; linked/quoted there. It's most useful to have your comment here as well.)",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/facebookresearch/fairseq/issues/3275",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "PhilipMay",
            "datetime": "Feb 24, 2021",
            "body": "Hi,\nI want to translate german voice to text and use the XLSR-53 model.\nThat model is mentioned here: But the usage example from here:\n\nIs not realy helpful.Can you please help me how to lead and use the XLSR-53 model to convert german .wav file to text?Thanks\nPhilip",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "olafthiele",
            "datetime": "Feb 24, 2021",
            "body": "Hi Philip,\nnice to see you here, you will need to finetune the model with good German material before you can use it for sth useful.  has a good overview and scripts how to do that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "guillefix",
            "datetime": "Feb 26, 2021",
            "body": "I imagine the recommended amounts of data mentioned in that repo won't appy for XLSR-53, as they used wav2vec_small? With XLS3-53, we could bypass all the language-specific pretraining no?\nAlso not sure why they use a language model for. I think in the ASR implementations I've seen using wav2vec, they just use a simple linear classifier on top of the wav2vec represenation, trained on the small supervised dataset. I could be wrong, but it seems like it should be much simpler than what that repo does, given XLSR-53",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "PhilipMay",
            "datetime": "Mar 5, 2021",
            "body": "Hi  - nice to meet you here. :-)Do you have any experience with these models for german language?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "olafthiele",
            "datetime": "Mar 5, 2021",
            "body": "Yep, just trained a couple of models with different params. Have written you an email. For all the others and reference, use the repo mentioned above (thanks ) to begin with and adapt params to your needs. And as always: gold in, gold out or s...t in, s...t out :-)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Gorodecki",
            "datetime": "Mar 10, 2021",
            "body": ", please, share which model was used as a base. What parameters were set for fine-tuning? How many hours of audio? Whether augmentation was used?\nI want to train a model for the Russian language. I have 50 hrs labeled noise audio.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "olafthiele",
            "datetime": "Mar 10, 2021",
            "body": ", we used the published XSLR-53 model together with the 100h base config values for first tests with fewer steps. But that depends on your data. We are testing anything between 10 mins and 500 hours with different configs. No augmentation. Our data is not very noisy, maybe clean it in advance? And if you do Russian, have you tried  ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Gorodecki",
            "datetime": "Mar 10, 2021",
            "body": " thank you!  This  it is dataset, but not models.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "olafthiele",
            "datetime": "Mar 10, 2021",
            "body": "Sorry, not writing a book here :-) Check him, there are models somewhere. Not wav2vec, but Russian and easy to start with last time I checked. And maybe the data helps you too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "PhilipMay",
            "datetime": "Mar 19, 2021",
            "body": "Hey \nare you aware of this: They claim to have a WER (word error rate) of 18.5 %\non the german commen voice corpus (test set).But I do not know how they trained it...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "olafthiele",
            "datetime": "Mar 19, 2021",
            "body": "Common Voice is a good start, but if possible, find better material for finetuning or maybe use a a more suitable language model.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexeib",
            "datetime": "Mar 29, 2021",
            "body": "trying out the hugging face implementation would be a great start. their demo is super nice. in terms of \"how they trained it\" - they actually just took our published model.one downside is i dont think they use a language model for decoding but that can be added on top after you get the argmax decoding working",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 28, 2021",
            "body": "This issue has been automatically marked as stale.  (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "PhilipMay",
            "datetime": "Feb 24, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "lematt1991",
            "datetime": "Mar 1, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "lematt1991",
            "datetime": "Mar 1, 2021",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 28, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1897",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "novellac",
            "datetime": "Aug 7, 2020",
            "body": "\nOn the  and  pages, the button element in the header marked \"Install Now\" doesn't do anything.\nNot quite sure!  - If an actual action isn't required here, perhaps use the same link which is used on the  on the .",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1896",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "novellac",
            "datetime": "Aug 7, 2020",
            "body": "\nThe Privacy Policy and Lexicon links at the top of the Lexicon page currently lead to 404s.\nEach link should lead to a valid page.\nIn partials/pageHeader, the links should be amended:",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1893",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Aug 6, 2020",
            "body": "If you are in the options page and select a new voice, then we should immediately play a sample using that voice. It could be as simple as saying \"this is a test\" in the new voice.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1876",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "lisa-wolfgang",
            "datetime": "Aug 4, 2020",
            "body": "Firefox Voice's default keyboard shortcut is also the keyboard shortcut for opening the Firefox Multi-Account Containers extension.Furthermore, this cannot be changed during the onboarding process, which may turn away Containers users.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GeraldNDA",
            "datetime": "Aug 5, 2020",
            "body": "Probs a dupe of  ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "strorozhsergeich",
            "datetime": "Aug 5, 2020",
            "body": "The same issue and more. I am not able to change the default keyboard shortcut. Input box does not register my input. If, however, I am starting the new shortcut with a Key instead of MOD character ( e.g. A ) it detects my input and displays a message suggesting to start with one of three valid MOD buttons Ctrl, Alt or Shift. Should I report it as a new issue, perhaps?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lisa-wolfgang",
            "datetime": "Aug 5, 2020",
            "body": "You have to type out the modifier keys with your keyboard: I believe actual key detection is a feature being worked on.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1869",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Aug 3, 2020",
            "body": "When the extension is updated, users should get a \"NEW\"  on the toolbar icon.When the person next activates Firefox Voice, a new tab should open that shows the updates. This will be a page within the extension (design to be done in a different issue).The latest version of the feature list should be kept in browser storage. Not every release will necessarily have updates (e.g., a bugfix-only release won't need a proactive release page), so we should explicitly say what version last had an update and test against that. (The current version can be fetched via getManifest().)We may want to use a different icon instead of a badge.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Aug 3, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#1870"
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1843",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "12people",
            "datetime": "Jul 23, 2020",
            "body": "It'd be great if Firefox Voice could run  runs.\nMyCroft's skills all appear to be open-source.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 24, 2020",
            "body": "I believe MyCroft is running everything on a device with Python. So that's two barriers: a browser is... well, it's a device of its own, different than Linux. And it doesn't run Python (without a lot of work).I think our emphasis is on routing things to web-based services. So if we could send an utterance to a MyCroft device, then that would be quite sensible. You'd need to configure your personal MyCroft device. But if MyCroft has a local web page where you can type in some text (or put it in the query string) then the configuration could be as simple as saying \"this is my MyCroft page\" with the appropriate page open.That would open a second question: which phrases do we forward to MyCroft? The easy thing is to give the device a name and require the user to say \"tell mycroft to [something]\". We could also consider a configurable fallback service (Google is essentially our fallback service), but I doubt MyCroft would be useful that way – it doesn't handle random queries particularly well. The fanciest would be for MyCroft to either publish some phrases, or dynamically tell us if it thinks it can parse a phrase well. There's some danger there in MyCroft being greedy about handling phrases, but it would be an interesting question to try to figure out.Lastly, we just take some of those skills and replicate them here. Whenever possible we'd like to forward things to a web-based service, though in some cases implementing something natively is helpful, for instance there's reasons for us to have native timers.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lb803",
            "datetime": "Aug 24, 2020",
            "body": "Someone on Mycroft forum asked how to submit utterances via api requests. Would this work for the present case?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1828",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Jul 22, 2020",
            "body": "This is an incomplete concept and will require some discussion.The cards we get for many search results are quite valuable, but limited to Google search results. What if you could teach Firefox Voice to display other cards? While this is connected to the popup UI, it's also a start for generating voice, and for operating in the background and extracting the most relevant information.For instance, let's say I want to make a card from the . It looks like:But the card might look like this:This one is a little bit of a pain, as there's no good element that contains the card, I'm hoping usually there is.This card might be a static response to a particular phrase, like \"check irobot\". Maybe to create this you'd open the page, say \"create card check irobot\" and then select the card by dragging or hovering.Or, if we know  how to load these cards, then we can respond to \"search yahoo finance for irobot\" by loading that page in the background, and showing the card if it's available (focusing the page if not). We'd have to know that these pages should be loaded in the background.We don't have any way to create a simple alias for something like \"search yahoo finance for irobot\" as \"check stocks for irobot\", especially since it includes a slot. That might be nice, especially since Yahoo Finance could have a bang search but accessing other sites may be harder.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1826",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Jul 21, 2020",
            "body": "One reason you might open the popup and say nothing is because Firefox Voice is listening to the wrong microphone. If we don't detect any speech we should display the microphone name so the user can understand if that's the problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 27, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1822",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "shelbyKiraM",
            "datetime": "Jul 21, 2020",
            "body": "Pretty straight forward? I don't want to give them ANY data I don't have to, that's a big part of why I use Firefox! When I do a thing like say \"Open DuckDuckGo\", it redirects through  for some reason!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shelbyKiraM",
            "datetime": "Jul 21, 2020",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 27, 2020",
            "body": "@marv3lls Firefox Voice should respect the default search engine you've selected in the browsers preference. Do you have DuckDuckGo set as your default search engine?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Vinnl",
            "datetime": "Jul 28, 2020",
            "body": " I have the same issue and yes, it's my default search engine:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shelbyKiraM",
            "datetime": "Jul 30, 2020",
            "body": "Yes  ,  's comment shows basically the same as my setup.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 30, 2020",
            "body": "@marv3lls  to clarify when you do a voice search such as  the results you see are from your default search engine (DuckDuckGo) but when you navigate using a phrase such as  Google is acting as the referral and you'd prefer your default search engine be used in that instance?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Vinnl",
            "datetime": "Jul 30, 2020",
            "body": "That phrase goes to Google as well, but doesn't execute a search:(Sorry, my computer was struggling while I recorded that.)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Duckbilled",
            "datetime": "Aug 14, 2020",
            "body": "Yes I have the same issue as Vinnl.Also, when I say \"use duckduckgo to search for Chickens\" It opens one empty google page and one google page that searches for the whole sentence.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 30, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1819",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "awallin",
            "datetime": "Jul 17, 2020",
            "body": "Audio output creates a more hands free experience which we've seen as a compelling use case for voice interactions on desktop. To ensure the features isn't overlooked because it's buried in the preferences it should be enabled by default with an option to turn off from the popup window.Changes to make for audio output",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 17, 2020",
            "body": "Attaching speaker icons for enabled/disabled states\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 17, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1808",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Jul 13, 2020",
            "body": "\"This is a test\" gives this:The background of the checkmark isn't great.Also the popup should probably stay open slightly longer than it does currently.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "awallin",
            "datetime": "Jul 13, 2020",
            "body": "Can we use the existing checkmark image that's typically seen when an intent is completed?Seen here: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 24, 2020",
            "body": "Yes, I think we could put that in, that would require using the Zap component instead of an image.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1752",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "Jun 18, 2020",
            "body": "We should add a boolean about whether there was voice output, and another about whether the pref is on (since it may be on, but the intent didn't resolve into any audio).I'm thinking maybe both should be text fields, NULL for none, and the  of audio generation indicates their was audio (\"card\" being the first kind). And maybe also for the pref: what, if anything, indicates that we should have audio output? That could be a pref, a wakeword, or a phrase modifier.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jun 29, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "PascalUlor",
            "datetime": "Jul 7, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#1794"
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 29, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1704",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "awallin",
            "datetime": "May 29, 2020",
            "body": "Users want to skip forward and back while listening to an article in reader mode. We should support the following commands in reader mode:Bonus points if we can supportSee There's also the question of context as 'back' and 'forward' triggered page navigation in the above case.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jun 18, 2020",
            "body": "As a first pass, let's just let \"forward/backword\" execute a click on these controls that are already available in reader mode:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AbhiVaidya95",
            "datetime": "Jul 14, 2020",
            "body": " As per your suggestion I have added Intents for Foreword and Backward in reader mode.\nCan you please check it once? Thanks.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jun 18, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "danielamormocea",
            "datetime": "Jun 23, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "AbhiVaidya95",
            "datetime": "Jul 14, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "AbhiVaidya95",
            "datetime": "Jul 15, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 15, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "danielamormocea",
            "datetime": "Aug 4, 2020",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1681",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "May 27, 2020",
            "body": "Watching Firefox close down, I notice a lot of error messages about message handlers coming from Firefox Voice. I think this might be cases where sendMessage is called, and the promise hangs or doesn't return.If we put in place some more structured message handling, we could also put in warnings about slow message responses. These might be cases where we simply never resolve a message, and it waits indefinitely for the response.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jun 18, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "Jul 9, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1663",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": "This doesn't technically involve \"voice\", but it's relatively easy to imagine configuring new hotkeys to run a voice command (which could itself be a routine).Inspired some by , and ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1645",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "alexandra-martin",
            "datetime": "May 13, 2020",
            "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA Google Drive account is logged in and a Google Docs or Google Slides file is opened and made the default notes taking tab with the \"Make notes here\" command.Note is written in the notes taking tab.Nothing happens.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-12).\nNot reproduced if Google Sheets is made the default notes taking tab.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Ishakikani9117",
            "datetime": "Jun 14, 2020",
            "body": " can I work on this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Ishakikani9117",
            "datetime": "Jun 21, 2020",
            "body": " I worked through this issue. For adding notes we are focusing on \"document.activeElement\" and this element is either iframe or body. We also cannot access the content of the iframe due to the\nsame origin policy.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 13, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1642",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "alexandra-martin",
            "datetime": "May 13, 2020",
            "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA tab in narration mode is made active, like a New York Times article where the \"Read\" command is made.If tab is not one of the available music services, a corresponding message is displayed.\"Internal error: TypeError: service.mute is not a function\" is displayed in the doorhanger.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-12).\n\"Internal error: TypeError: service.unmute is not a function\" is displayed in the doorhanger, if the \"Unmute audio\" or \"Unmute music\" command is made (error appears either if tab is manually muted or not).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AbhiVaidya95",
            "datetime": "Jul 14, 2020",
            "body": " The right utterance is mute which refer to Muting.mute intent, when we are using Mute music its going to music intent so we need to do error checking in Music mute intent is this the right approach?I have done some changes let me know if the approach is correct so I will commit the code.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 13, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1626",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nYoutube is made active and a video is playing.Muted video player is unmuted.Muted video player is not unmuted.Reproduced on Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nNot reproduced if player audio icon is manually muted and the \"Unmute music\" command is made.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Ishakikani9117",
            "datetime": "Jun 16, 2020",
            "body": "  I think my pull request   can solve this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 8, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#1625"
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1624",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA non music related tab is made active.If no music is playing, display a message in the doorhanger that volume can't be decreased or increased.A new tab opens with the most used music service or the default music service chosen in the \"Preferences\" page.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nIf most used or default music tab is already opened, the commands make the tab active.\nDon't know if it's possible to make the commands act more generally like the \"Mute\" command, where they act upon a playing tab, even if the command is made from another tab.Reproduced also for the \"Mute music\" and \"Unmute music\" commands when music tabs are closed. On Soundcloud the \"Mute music\" command opens the tab but it also performs the action. If most used or default music tab is already opened, the commands act as expected. (2nd gif)Spotify gives various errors which will be filed separately.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1623",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.Youtube opens in a new tab and plays the first playlist found corresponding to \"rock playlist\".\"Internal error: TypeError: service.playPlaylist is not a function\" is displayed in the doorhanger.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nFiled because it can work similar to commands like \"Play rock album on Youtube\" which are working to open the first playlist from the Youtube search. Also the term \"Playlist\" is frequently used on Youtube.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "melvin2016",
            "datetime": "May 9, 2020",
            "body": "let me look into it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "alexandra-martin",
            "datetime": "May 7, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "ianb",
            "datetime": "May 20, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/iNavFlight/inav/issues/3902",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "avgorbi",
            "datetime": "Oct 4, 2018",
            "body": "It`s look like no action on battery discharge.Add autolanding when battery lowWhen battery capacity bellow \"Critical Capacity (%)\", activate autolanding procedure.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "teckel12",
            "datetime": "Oct 4, 2018",
            "body": "In my opinion, I'm not so sure this would be helpful. First, you'd really need a current sensor due to voltage sag. Secondly, RTH is way slower than I can fly home. It also couldn't be tied to only critical capacity as distance from home would also make a huge difference.On my Mavic the auto RTH mostly works well for two reasons.What I've found that works very well with INAV is just knowing what the battery capacity is while flying. This can easily be accomplished from the transmitter via telemetry. You can then use a Lua script or functions to give voice announcements of your battery level and warnings.Basically, even if this feature existed, I don't think is turn it on as it wouldn't really be as useful as it seems. And cause more problems (like flying till the battery is at 10% critical level from 1km away, causing a power failure half way back).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "avgorbi",
            "datetime": "Oct 25, 2018",
            "body": "Of course, any function will work well only if it is properly configured.But it will be better for mine if the copter will land softly, even though far from home, than it will fall like a stone when cut off at the speed regulator.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Dec 24, 2018",
            "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Hennyr",
            "datetime": "May 13, 2019",
            "body": "I think this feature helps to prevent accidents",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "teckel12",
            "datetime": "May 13, 2019",
            "body": " If it was realistically possible.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shellixyz",
            "datetime": "May 15, 2019",
            "body": "I guess it can be done with the sag compensated battery voltage as source. I probably won't use this feature but if people want to...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "teckel12",
            "datetime": "May 16, 2019",
            "body": " There's a lot of variables, calculations, and total \"fudge factor\" to consider. Different kinds of RTH, different RTH speeds (not just flying, but also up and down), distance to home, wind speed, etc.  I just don't see it as being even remotely accurate.I feel the only way it can work is to trigger RTH at an unrealistic voltage, like 4.0 volts, just to be sure. And in that case, no one would use it.If we had known hardware like DJI it would be possible, but I think it would need to overestimate so much it would be useless. But I'd love to be proved wrong.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "shellixyz",
            "datetime": "May 16, 2019",
            "body": " Why are you talking about RTH ? The OP only asked for landing in case the battery is almost empty to at least try to get it back in one piece. But I'm not convinced this is very useful either.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "teckel12",
            "datetime": "May 16, 2019",
            "body": " Sorry, serves me right for browsing quickly and assuming.  There were previous requests for RTH for low battery, and I just incorrectly assumed.  \"auto land\" is never an option for me, as my primary flying locations are near/over water, so \"land\" guarantees a lost/destroyed model for me.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SeanTheITGuy",
            "datetime": "Sep 13, 2019",
            "body": "This is stale, but to add my 2c.  Either having the option to trigger failsafe or even just scaling the throttle harshly when the critical voltage level is hit would be a huge benefit.  I can't find reference to any specific feature or code in Betaflight that actions this, but I have the feeling that when I get to near the end of a pack in Betaflight, it seems to scale the throttle to where even at full, the quad is still descending.  (Though this could just be the result of voltage drop of my smaller mah packs) This would be far preferable to a sudden out of control tumble when you over discharge a pack accidentally.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "michalelektryk",
            "datetime": "Apr 5, 2020",
            "body": "I'm using this option but implemented as hack / workaround in taranis, where one channel is sending back battery voltage and FC is using it to trigger landing. Not really well done but it's the only way",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Dec 24, 2018",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "shellixyz",
            "datetime": "Jan 1, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/heartexlabs/label-studio/issues/2641",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aisensiy",
            "datetime": "Jul 9, 2022",
            "body": "\nA clear and concise description of what the bug is.I am trying to use label-studio with the feature of  in a template of . But I found that the same prediction from my ML backend will be triggered again and again.I am not sure if this is the LabelStudio's responsibility or my ML backend's responsibility to avoid this behavior.\nSteps to reproduce the behavior:\nA clear and concise description of what you expected to happen.I think LabelStudio show also know that the current task is already get annoated by the ML backend so that the same annotation will not be added twice.\nIf applicable, add screenshots to help explain your problem.Here is a screen record to show the behavior (no voice).In the video you can see no matter how many times I annotated the text, the same prediction result from ML backend will be added to the text.\nAdd any other context about the problem here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KonstantinKorotaev",
            "datetime": "Jul 12, 2022",
            "body": "Hi \nCould you please check what your ML service returned in last calls?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aisensiy",
            "datetime": "Jul 12, 2022",
            "body": "Do you mean the predict result? The result looks like this:And here is the  in my label-studio-ml-backend:It is based on the . I just use the first three letters to generate a dummy result and return.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KonstantinKorotaev",
            "datetime": "Jul 12, 2022",
            "body": "Yes, do you have a sequence of calls that lead to duplicated results?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aisensiy",
            "datetime": "Jul 12, 2022",
            "body": "Yes. I do have a sequence of actions to manually add some more annotations not covered by the ml backend. But I think it is not necessary to return the prediction multi times. Every time when I do some actions (even when I delete some annotations) the same prediction result will be added.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aisensiy",
            "datetime": "Jul 18, 2022",
            "body": "Any process about this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "KonstantinKorotaev",
            "datetime": "Jul 19, 2022",
            "body": "Hi \nI have created a new feature request for it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aisensiy",
            "datetime": "Jul 9, 2022",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "makseq",
            "datetime": "Jul 19, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/leon-ai/leon/issues/344",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "superflewis",
            "datetime": "Feb 2, 2022",
            "body": "pi@alchemyvoice:~/.leon $ npm run check.: CHECKING :. node --version\n v17.1.0 npm --version\n 8.1.2 pipenv --version\n pipenv, version 2022.1.8 pipenv --where\n /home/pi/.leon/bridges/python pipenv run python --version\n Python 3.7.3 pipenv run python bridges/python/main.py scripts/assets/query-object.json\n {\"package\": \"leon\", \"module\": \"randomnumber\", \"action\": \"run\", \"lang\": \"en\", \"input\": \"Give me a random number\", \"entities\": [], \"output\": {\"type\": \"end\", \"codes\": [\"success\"], \"speech\": 23, \"options\": {}}} NLP model state\n Found and valid Amazon Polly TTS\n Configured Google Cloud TTS/STT\n Configured Watson TTS\n Watson TTS is not yet configured Offline TTS\n Cannot find bin/flite/flite. You can setup the offline TTS by running: \"npm run setup:offline-tts\" Watson STT\n Watson STT is not yet configured Offline STT\n Cannot find bin/deepspeech/lm.binary. You can setup the offline STT by running: \"npm run setup:offline-stt\".: REPORT :. Here is the diagnosis about your current setup\n Run\n Run modules\n Reply you by texting\n Amazon Polly text-to-speech\n Google Cloud text-to-speech\n Watson text-to-speech\n Offline text-to-speech\n Google Cloud speech-to-text\n Watson speech-to-text\n Offline speech-to-text Hooray! Leon can run correctlyI typed a greeting, Leon responded with text. But did not produce an audio response.My setup is a RaspberryPi with an Adafruit Voice Bonnet",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Feb 2, 2022",
            "body": "Hi  ,Thanks for opening this issue.Now to answer your questions:Yes, if it is correctly configured you should expect an audio response.As long as you allow your browser to access your audio input, it should work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "superflewis",
            "datetime": "Feb 2, 2022",
            "body": "Hi , thanks for your fast reply!Browser console was showing:\nMediaDevices.getUserMedia() is not supported on your browser.\nI've fixed that. Currently...1.) No audio from Pi or browser when this shows:\n.: LEON :.\n Talking...2.) The mic icon activates. When it stops, only this is showing:\n.: ASR :.\n Encoding WebM file to WAVE file...\n Encoding done\n Parsing WAVE file...No errors in console, tested browsers in Win/Linux.\nMic tested at mic-test.comJust to confirm, is this not a correct expected behavior?:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Feb 3, 2022",
            "body": "Thanks for sharing more details!This should work. Can you please try to install Leon on your PC with the current configuration and see if it works by using the PC only? Maybe there is an issue about the audio source/output when streaming from a Pi.Also, can you please share the content of your  file?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "superflewis",
            "datetime": "Feb 2, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/leon-ai/leon/issues/361",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "nxturistic",
            "datetime": "Mar 23, 2022",
            "body": "Hey Louis, an excellent project by the way. Can you please help me with this Azure TTS implementation?\nI'm having some issues with the  file. I followed the steps mentioned in .But whenever I pass the string input in the browser, I get the following error when leon starts talking.Can you please look into it and let me know what's the issue, I want to contribute to your project in this way .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Mar 25, 2022",
            "body": "Hi ,Thanks for willing to contribute! Sure I can check, would you mind to open a draft PR? ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Mar 26, 2022",
            "body": " ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nxturistic",
            "datetime": "Mar 23, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#362"
        }
    ]
},
{
    "issue_url": "https://github.com/leon-ai/leon/issues/206",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "dhequex",
            "datetime": "Nov 20, 2020",
            "body": "I would like to help with translations for Japanese language.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "arlyxiao",
            "datetime": "Mar 28, 2021",
            "body": "TTS and STT both only support English right now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Becker-Asano",
            "datetime": "Aug 6, 2021",
            "body": "German language support would also be nice.\nTTS from Google-Services also supports German and Japanese: \nSST is also supported by Google-Cloud-Services in German and Japanese: Anyone here interestes in teaming up to get these to languages running?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Dec 28, 2021",
            "body": " ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Dec 28, 2021",
            "body": "Hi ,Thanks all for willing to contribute with more languages support. I know this is a late answer, but I'm back on working on the core of Leon.I plan to improve a better support of languages scaling. Also, I took note so that we can work together if you are still interested.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "blokhin",
            "datetime": "Feb 7, 2022",
            "body": "I'd be definitely interested in digging into the Turkish or Russian language support, even without the TTS/STT, but just the basic NLP/NER.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Feb 8, 2022",
            "body": "Thanks for willing to support! I will let you know once more languages will be added. First I need to rework some architecture to make Leon more scalable in terms of language.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dhequex",
            "datetime": "Nov 20, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/leon-ai/leon/issues/198",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "andresdev16",
            "datetime": "Sep 10, 2020",
            "body": "Hi, I've been working in the code and i added a new free TTS engine for windows, i used  like alternative, but in the code i had many questions that i realized in the all TTS engine's code and it's the need of save the audio in a .mp3 on the /tmp/ folder, Why we need that? and i can work with that TTS engine without save that file?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Becker-Asano",
            "datetime": "Aug 6, 2021",
            "body": " , why would gTTS be necessary? Isn't it just using Google-Voices in its backend, which is already possible to configure in Leon anyways?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "andresdev16",
            "datetime": "Sep 10, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "Divlo",
            "datetime": "Apr 24, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 25, 2021",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/363",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aaronchantrill",
            "datetime": "Jun 21, 2022",
            "body": "Like most Voice Assistants, Naomi's intents serve multiple purposes. First, they are used by the speech to text system to prepare a dictionary of words to recognize. Next they are converted into a language model to help the speech to text system guess what it is most likely hearing given the likelihoods of different arrangements of words. Finally, it is used by the text to intent system to figure out which intent to trigger.When developing the format for creating grammars for Naomi speechhandler plugins, I created a structure format where the grammar is split into keywords and phrases, with keywords providing a list of options in a phrase. This was similar to the way grammars are constructed for intent parsing systems I have looked at and was a simple way to move Naomi from simply spotting keywords to reacting to more complex utterances, but has a few big problems:There are a few grammar formats out there; JSGF, Nuance, ANTLR, SRGS, etc. SRGS seems to be a W3C specification, but I see very little support for it, . JSGF has been around a long time and there is a pyJSGF library on PyPI which could be helpful. DeepSpeech/Coqui can use JSGF files directly, so I propose that we use JSGF grammar format for building Naomi intents, unless someone has a reason to prefer a different format.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Jun 26, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/267",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aaronchantrill",
            "datetime": "May 3, 2020",
            "body": "Naomi should be able to respond differently to different users. If a family member asks \"do I have any emails\" it should not be necessary for Naomi to ask \"who are you?\" This would allow the user's voice to act as a sort of authorization. As part of the speech to text training, ultimately I would like to train a different acoustic model for each member of the family. Being able to identify the speaker by voice before selecting the acoustic model would make it possible to use an acoustic model optimized for the speaker, which should lead to better recognition overall.This could start allowing a database to be built around the user, and also help improve speech recognition",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Nov 27, 2020",
            "body": "I'm looking at using this project for an initial test: I already have had the NaomiSTTTrainer.py allowing you to enter a name for a while, so I have a database with a bunch of recordings labeled with my own name and just a few with other people's names. It would be interesting to see how many recordings are needed to differentiate between two individuals, and also how much audio is required to do a check.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Aug 30, 2021",
            "body": "I've been working with  and have a test project at . This package is easy to install on x86_64 systems (pip install speaker-verification-toolkit) but a pain on ARM (Raspberry Pi). To install it on ARM, you need to install version 11 of llvm first, which isn't really obvious from the error messages. Also, when building the package from source it is import to build it as type=Release or else you will run out of memory during the linking step.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 3, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 3, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Jul 11, 2020",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Jul 12, 2020",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/jovotech/jovo-framework/issues/1237",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JRMeyer",
            "datetime": "Feb 16, 2022",
            "body": " hi there!Would be great to be able to test and debug a voice bot  an internet connection. Offline STT and TTS (from ) would make this possible using the existing UX from the new Currently there's no offline STT or TTS",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jankoenig",
            "datetime": "Feb 16, 2022",
            "body": "Hi there. Thank you.This is not on our immediate roadmap, but would be a great community contribution.Coqui STT could be implemented as .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JRMeyer",
            "datetime": "Feb 17, 2022",
            "body": "Hi  -- just looked into the integration with Lex, and it would be considerably different with Coqui because the user would have their own server running. For example, the user might be running a simple server on their local desktop or they might have spun up a server on their AWS cloud, and using endpoints there. In either case, the API syntax and integration would be identical, but there would be an expectation that the user spins up the server themselves. Not too difficult, but I'm not sure if that's something the Jovo crowd would be interested in.I think the biggest value add for Jovo users would be to be able to test out their voicebots locally, without having an ASR backend running on one of the providers (like Lex).Thoughts?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jankoenig",
            "datetime": "Feb 18, 2022",
            "body": "This could work similar to our  where people also have to run their own servers.An integration like this would also be useful for our web starters:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JRMeyer",
            "datetime": "Feb 18, 2022",
            "body": "Yeah, I think a general setup mirroring the Snips approach would work nicely. You know of anyone in your community who might like to hack on this? We're happy to offer support/guidance for using the Coqui tools.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rubenaeg",
            "datetime": "Feb 18, 2022",
            "body": "I think I could give this a spin :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rmtuckerphx",
            "datetime": "Aug 26, 2022",
            "body": " Are there any developer docs on the Coqui APIs for STT and TTS using Node.js or REST?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jankoenig",
            "datetime": "Feb 16, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "jankoenig",
            "datetime": "Feb 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/leon-ai/leon/issues/234",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": "Support for modern TTS models for various languages without the need for external TTS APIs.Consider giving a go to Silero TTS models. These are published under an open license assuming non-commercial / personal usage. Please see our TTS models here -  (corresponding article ).What is most important our TTS models can run on one CPU thread / core decently and depend mostly only on PyTorch.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": "Also please note that this is just a  release, models will be much faster in future",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 2, 2021",
            "body": "Hello  ,Thanks for suggesting, it looks promising!May I know if you have any Node.js binding? As Leon's core is built on the top of Node.js.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": "We just base our models off PyTorch and / or ONNX\nAs far as I know there are no actively maintained node-js bindings for PyTorch\nThere are though for ONNX, but we could not yet port our TTS models to ONNX",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": "Internally in such cases (where the controlling app and the inference engine are not the same) we just use rabbit-mq communication with a model in a separate container",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 2, 2021",
            "body": "I see. For the moment Leon does not rely on a broker for such operation but directly on Node.js binding. However, it can be a good path to explore. I'll add it to the roadmap.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 2, 2021",
            "body": " ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 2, 2021",
            "body": "For reference, do you have any online demo of the output that you can share?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": "Please see this article - it has plenty of audios - \nOr just use the colab - Since we do not have web developers, we do not active develop fully online web demos\nColab can be considered \"online\" since it works in real-time in a notebook",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jankapunkt",
            "datetime": "May 4, 2021",
            "body": " is it possible to have these voices being rendered into ? Then there would no issue which backend had been used to create the voices, right?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "May 4, 2021",
            "body": "Hi,I could not really understand from their example where / how the actual speech synthesis is run / storedThere is an example code here -  - but I do not see any models here",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "snakers4",
            "datetime": "Apr 2, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "louistiti",
            "datetime": "Apr 2, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/265",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "fracpete",
            "datetime": "Apr 27, 2020",
            "body": "So far, I've only come across readily available language models etc for the various STT/TTS plugins.\nMy question is, what steps are necessary in order to add a completely new language, e.g., an indigenous one, to Naomi? Code and/or configuration changes? What would be necessary to use DeepSpeech in such a scenario (I presume some form of training on a new audio corpus)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Apr 28, 2020",
            "body": "Hey Peter. That's a good question. There is a sort of answer here:  with regard to adding Esperanto specifically, although nobody has yet gone through the whole process of adding a whole new language.As far as changes to the core naomi code, you would have to add your language as an option to the get_language method in naomi/commandline.py (this really isn't the right place for this function, so I imagine it will move eventually, and it should also be modified to key off of the list of .po files in naomi/data/locale instead of a hard-coded list) if you want people to be able to select it during the initial configuration (once a language is selected, Naomi should switch to communicating in that language, but the translation files for French and German currently need to be updated - see pull request ).The process of adding a new language could be broken down into three projects: STT (speech to text), TTI (text to intent), and TTS (text to speech).With Speech to Text, you first have to decide what engine you want to work with. Pocketsphinx and Deepspeech are both good choices for a more or less complete solution, and Kaldi is good for a solution once you are more comfortable with the concepts used in STT. They all have tutorials where building a new speech recognition model is discussed:Speech to text is generally broken down into three main concepts of acoustic model, phoneme to grapheme, and language model. The language model flows directly into the next project, Text to Intent, since it is using expectations to determine what it most likely heard.For Text to intent, you currently have to modify the intents() methods in each of the speechhandler plugins, and also generate new .po gettext translation files for the core and plugins.The intents() methods return a list of things the user might say to activate the plugin, which then get fed into whichever Text to Intent plugin you are using. Since there are different numbers of ways to say things in different languages, it just didn't work to use literal translations here. This gives an intent author better control of how an intent is constructed in a specific language, but does require someone who is adding a new language to do a lot more work, and modify every speechhandler plugin. There are instructions for writing intents here: I have considered defining a JSON format file for holding intents, so that someone adding a new translation would be adding new files, not modifying the intents() method of the plugin itself. One benefit of using that kind of file structure is that it could provide a fairly easy method of identifying plugins in the Naomi Plugin Exchange by the locales they are configured to work with, and possibly even allow people to attach additional translations to remotely hosted plugins without having to modify the plugin itself. Currently there is no indication on the Naomi Plugin Exchange of which languages a plugin supports.Generating the .po files is simply a matter of running the \"update_languages.sh -l <locale_identifier>\" with the locale identifier. If there is no locale identifier for the language you want to add, you can just make it up. It only has to be consistent within Naomi. This generates a bunch of files called <local_identifier>.po. Unfortunately, you have to go in and manually translate all the phrases in those .po files. This allows Naomi to translate its responses into another language to either display on the screen or say to the user.Last, you need Naomi to be able to say the response, so you need a Text to Speech system which is trained to speak your language so is able to pronounce the words being fed to it. If you have a mismatch between the locale and the voice, it can be difficult to understand (as an analog, I have been told by a Swiss friend that pronouncing Maori words correctly is much easier if you try to pronounce them in German than in English). Voice building can be a pretty complex task. Here are instructions for building a new voice for the Mary TTS system:  and Festival: So it is certainly not easy, but it can be done. If someone would be interested in doing this and documenting their progress, I think that would be incredibly helpful to others. The whole process is a lot easier if you can find a ready made STT model and TTS voice. If you are generating a new language from scratch then you will need a lot of labeled recordings. Naomi is able to help with that, especially if you can find a cloud provider that already provides STT and TTS in the target language. Then, once you have customized the intents and built translation files, you could use Naomi normally with audiolog enabled to build up a collection of labeled samples which could be used to build both the STT models and TTS voice.Please let me know if you have more questions or if you see any mistakes above. I hope that's helpful and not overly dense. I could go into a lot more detail, and would be willing to work directly with someone attempting to do this, especially if they would be willing to help document the process. Do you have a specific use case in mind?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fracpete",
            "datetime": "Apr 30, 2020",
            "body": "Thanks for the detailed reply, I will have to mull over that a bit, as there is quite a bit of work involved. A possible use case would have been to add Maori as a language.\nBTW What do you think of meta-STT and meta-TTS wrappers?\nFor example, you could take the output of your base STT and push that through Google translate (from Maori to English) to avoid having to update all your plugins for handling this language. Any English text that would have to be spoken could be translated again, e.g., through Google translate again, before pushing it out through a Maori TTS. I could imagine that this kind of translation approach might work relatively well, as long as the incoming and outgoing text is relatively simple and short.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 1, 2020",
            "body": "I agree, it's a lot of work. I don't know that google translate could help all that much. My experience with google translate is that it works well enough to get the intent across, but rarely beyond pidgin level language.My biggest concern is with regard to 3rd party speechhandler plugins. I'd like to have an easy way for someone to add a translation \"pack\" to some else's plugin consisting of a .po translation file and an intent file, and for another user to then download the plugin with the added translation files.I have thought about expanding the \"update_translations.py\" program to use Google translate to generate the translation files, which should work fine in our current state without leaking usage data to Google. That way a user could generate all the translation files needed for their whole system to get Naomi up and running in a new language quickly and later go and fix the translations.For the time being, I see Naomi as more of a development kit than a finished product. My hope is that through being able to experiment with different technologies, we will eventually be able to come up with an effective platform that runs locally rather than in the cloud. Once something really seems to be working, Naomi could be used as a template to write a system optimized for the specific plugins.Have you ever used a piece of software called Simon ()? It used to be part of the KDE desktop, but the lead developer went to work for Apple's Siri division a few years ago and the project sort of fell apart after that. The idea was to have a simple means of controlling the desktop using voice, sort of like the scene in Bladerunner where Harrison Ford is zooming in on an image on his computer. One thing it included was the ability to train STT systems, especially the Julius STT engine. One of my visions for Naomi is to provide that same ability to train speech recognition engines which can then be re-applied to your own projects.You would still need both Speech to Text and Text to Speech systems with acoustic models optimized for the specific language, which I consider to be the most difficult part. Doing the actual translations is pretty straightforward.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 4, 2020",
            "body": " can you recommend what form you would like this project to take? Do you want some specific documentation around it? This is kind of a big question, and given that Naomi uses plugins, it's impossible to give a specific set of instructions that cover every use case. At the same time, there are some definite steps that would always have to be done that can be documented along with some vague information about generating new STT and TTS models for those who need to. I'm just not sure how to resolve this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fracpete",
            "datetime": "May 4, 2020",
            "body": " had a discussion around this today. For the time being, we will concentrate on getting a handle on STT (DeepSpeech) and TTS (MaryTTS), with building up a speech corpus as the first step. Once model performance is satisfactory, we will look into a tighter integration into Naomi.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 4, 2020",
            "body": "Wow, that's awesome. Definitely keep me informed.What I have learned from working with Naomi, though, is that you don't need perfect recognition to get good comprehension at the intent level, especially if you are willing to accept a pretty simple model where only one intent is triggered at a time.Adding some humorous responses can be a good strategy for generating some good will with users and keeping them engaged when the computer is having some trouble understanding them, as long as they don't happen too often.The point is that speech recognition will never be perfect, since even you are processing language at multiple levels to make sense of it. I have some hearing loss, so often what I actually hear is garbled, but I can usually work out the speaker's intent from context.A good illustration of the process of of the process our brains engage in for listening would be the \"Mares eat oats\" song which can sound like gibberish until you get to the \"wouldn't you?\" part of the song and realize that the whole thing has been in english the whole time.Since I started verifying/correcting transcriptions with the NaomiSTTTrainer.py software, I have gained a lot of understanding of how exactly the computer hears things and the process of matching the sounds up with a meaningful sentence. Often a little nudging in the language model is enough to get much better comprehension, and that is why edit distances and soundex type matching can be a huge help when dealing with spoken language.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 4, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Aug 30, 2020",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 20, 2021",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/643",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "xiaoguazh",
            "datetime": "Jul 12, 2022",
            "body": "1)Use miven's model:\n code: 20212)Got exception\nsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).3)NOT ABLE to have Chinese voice, but only  noise.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jul 16, 2022",
            "body": "请仔细看模型的适用代码版本，并搜索issue区解决",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/212",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 17, 2019",
            "body": "This is an issue having to do with the new asynchronous mic.say_thread() method. Now that Naomi can talk and listen at the same time, Naomi can trigger itself by saying its own wakeword. For instance, this is what it looks like when Naomi first starts up now:So it is hearing \"Naomi\" and recognizing the wake word using the keyword vocabulary, then totally borking the \"How can I be of service?\" because the default vocabulary doesn't actually contain any of those words. This only showed up after I switched to using the SeeedStudio respeaker microphone with an external speaker. I have used several usb audio cards with Naomi without having this issue, so I assume most of them have some hardware echo cancelling built in. In this case, though, the respeaker is unaware of the audio out stream.Naomi should either recognize that it is hearing itself talk by recognizing its own voice, or just not hear itself at all, which would reflect a sort of selective attention. This could be accomplished by monitoring the audio out and applying a negative waveform to audio in. The goal is for Naomi to continue to hear what other people are saying as it is talking without really hearing itself. I want Naomi to react the same as when it is speaking to me through headphones, or when using the conference phone, which had noise cancellation built in.Naomi hears itself talking, and can trigger itself with wake words.Supposedly the jack audio library has some built in tools for this. Jack also has some other advantages that might be helpful. I'm going to attempt to write a jack audio engine.I was trying to use my current Naomi setup. The only real effect so far is that Naomi immediately declares that it doesn't understand a command upon starting up. After that, Naomi doesn't say Naomi again, so no longer reacts to its own voice, however, it does affect its ability to hear what I am saying when it is talking.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Mar 11, 2020",
            "body": "I have tried using the pulseaudio module-echo-cancel, but it seems to be more about noise cancellation than echo cancellation. I'll keep playing with it, but in the meantime I will modify Naomi to block listening while talking by default and provide an option to allow listening while talking which can be switched on by user either on the command line or in the yaml settings.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Mar 11, 2020",
            "body": "I have been having this issue with the Google AIY Voice kit, which I have been using by default for a while now. The device that worked well for me was a cheap little conference phone usb device and also a cheap little external usb sound card with 3.5mm input and output jacks. Neither of them were hearing themselves, and were quite capable of reacting to \"Naomi, stop\". The conference phone makes sense to have hardware correction, but I am impressed that the little usb sound card was able to work without knowing how loud the output would actually be.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 22, 2020",
            "body": "There are numerous solutions out there. Pulseaudio has \"module-echo-cancel\" which needs to be configured with an echo-cancelling engine like \"webrtc-audio-processing\", so it's not a simple setup and requires that the user be using pulseaudio.\nProbably the best thing to \"bake into\" naomi would be .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Jan 31, 2022",
            "body": "After a lot of playing with ec, I have discovered that the pulseaudio module-echo-cancel is much easier to work with and does a good job of providing echo cancellation with default parameters. To get this to work, Naomi needs to be set up to use the pyaudio audio engine with the \"pulse\" device for both the input and output device.The module can be loaded in the Naomi function created by the Noami.sh script by adding the following:where the sink_master can be selected from  and the source_master can be selected from ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Aug 1, 2022",
            "body": "This is basically solved. Naomi speaks synchronously by default, which prevents it from hearing anything when speaking. By setting the listen_while_speaking setting to \"true\", Naomi can listen while speaking, which allows the user to interrupt.There are several solutions to keeping Naomi from hearing what it is saying. The best is to use a speaker/microphone with integrated hardware echo cancellation - like a USB conference phone. The second best is to use the Pulseaudio module_echo_cancel module with an aec_method of \"webrtc\". If that fails to work with minimal effort, it would probably be better to purchase different hardware than to try to get echo cancellation working.If you come up with a better method or a method that works with specific hardware, please let us know.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 21, 2019",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 21, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Oct 4, 2019",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Oct 4, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Mar 11, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Mar 11, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Aug 30, 2020",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 22, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 22, 2020",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 22, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Sep 23, 2020",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/160",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "sbernhard",
            "datetime": "Jan 27, 2019",
            "body": "HiWould it be possible to add snowboy stt? See I would love to have a offline voice detection only and it looksike snowboy is one of the best.Best regards\nBernhard",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Jan 28, 2019",
            "body": "Snowboy support was added in 2016 but since then nobody has picked up the ball to support it. Currently the snowboy plugin contains a precompiled binary shared object built for arm on the raspberry pi. The project needs to be updated, and someone needs to also update the documentation so the module can be applied to other platforms. I don't see the plugin listed in the documentation, probably because it simply no longer works. If you are interested in picking up support for this plugin, we would definitely appreciate the help.What are the advantages of snowboy? What is it that makes it \"one of the best?\" I haven't ever used the system myself, so don't really understand their business model or what the benefits are.That being said, have you tried pocketsphinx? It is lightweight, fast, fairly straightforward to configure and good for simple tasks like keyword detection. It is offline, open source, and what I generally use for keyword detection myself.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TSRBerry",
            "datetime": "Oct 15, 2019",
            "body": "I want to work on this, but it seems like it is already implemented as a stt plugin. What is the issue here?\nIs it outdated and it needs to be updated or is it just about the doc for other OS?Snowboy seems to do a really good job at recording a keyword and then detecting it, so no internet connection is required and the user trains the predownloaded model from  to recognize the new keyword. It should only consume a little amount of the cpu which makes it a great stt for rpi.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TSRBerry",
            "datetime": "Oct 15, 2019",
            "body": "Okay I read a bit more on snowboy and it seems like the devs are inactive and issues won't get any answers. I think we can still update the current version of it but since this version is already outdated we can't expect any updates or improvements in the future.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Oct 15, 2019",
            "body": "I think the issue is basically just that the plugin is undocumented. I imagine that is because it doesn't currently work, but I don't know because I've never tried it. I know there is a binary file in there that is pre-compiled for raspberry pi. It would be good to try to locate and link to the source code or official download page for this file so users can use this plugin on other platforms.If you find that the plugin works in its current state, then just update the STT documentation to cover it. If it doesn't currently work, then it would be necessary to fix the issues before creating the documentation.Naomi currently has a few suspiciously old STT plugins that really need to be tested and verified.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "redragonx",
            "datetime": "Jan 28, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Jan 29, 2019",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Feb 7, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Oct 4, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Nov 3, 2019",
            "body": [],
            "type": "removed  the",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Aug 30, 2020",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Aug 30, 2020",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mindslab-ai/voicefilter/issues/30",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "nnbtam99",
            "datetime": "Jan 27, 2022",
            "body": "Hello, I have two questions about the implementation.To obtain data from the Librispeech 360h + 100h, I generate the mixed audios for 360h and 100h separately, then add them together in another folder. Is this the right way when I want to use more data to train the voice filter module?Theoretically, I expect the voice filter module will benefit from the embedder trained on more data, but the results got even worse. Can you share how you train this embedder?Thank you in advance!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/mindslab-ai/voicefilter/issues/20",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "weedwind",
            "datetime": "Sep 29, 2019",
            "body": "Hi, seungwonpark,I was trying to use Google's posted  for LibriSpeech to reproduce their results. But I can not even get their initial mean SDR (10.1 dB in their paper). I got only 1.5 dB. I am wondering have you tried their list and got around 10.1 dB for mean SDR before applying voice filter?Thank you so much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seungwonpark",
            "datetime": "Sep 29, 2019",
            "body": "Hi, \nThank you for letting me know! Yes, I was aware of that test list, but haven’t tried to measure the actual performance with that.Considering the followings, I think the experimental result (1.5dB, which turned out to be far worser than Google’s) is not really wrong:Shall we leave this issue open, since this is somewhat critical issue? Thanks a lot!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seungwonpark",
            "datetime": "Sep 29, 2019",
            "body": "TL; DR: (to the title of this issue)\nNo, I haven’t tried yet but I don’t think I can.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "weedwind",
            "datetime": "Sep 29, 2019",
            "body": "Hi, Thank you for your reply. I mean the SDR before applying the voice filter, not after. In Table 4 of their paper, this is the mean SDR in the first row, which is 10.1 dB. But I only got 1.5 dB. I used the same bss_eval python function as you did, just feed the function with the clean target utterance and the mixed utterance to compute the SDR before applying the voice filter. Do you have a clue why this SDR is so low?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seungwonpark",
            "datetime": "Sep 29, 2019",
            "body": "Oh, looks like I had misunderstood your question. Sorry for that.\n10.1dB is relatively high SDR for the mixed audios to have. The authors of VoiceFilter mentioned that the SDR before VoiceFilter got high due to silent part of utterances being sampled and mixed. (Note that fixed length of audio segments are sampled here)\nBut I’m not sure why you’re not getting 10.1dB. Perhaps we should review the preprocessing part and the SDR calculation code in bss_eval.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "weedwind",
            "datetime": "Sep 29, 2019",
            "body": "I noticed that your code used the first 3 sec and threw away the rest. I did not use fixed length. I used the entire length of the target clean signal, and truncate or zero pad the interference signal to the same length. Then I computed the SDR. Did you ever compute the mean SDR for your test set?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "weedwind",
            "datetime": "Sep 29, 2019",
            "body": "Hi, I read your generator again. In your code, both w1 and w2 need to be at least 3 sec long. Then, you take the first 3 sec from them and add. So the resulting target utterance is fully interfered by the other utterance. Since they have the same volume, the SDR should be nearly 0 dB in this case. Why did you get a median SDR of 1.9 dB?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "seungwonpark",
            "datetime": "Oct 1, 2019",
            "body": "Not yet.Actually the value 1.9dB was not calculated from all datasets -- it was from a single dataset. I should fix the table in README accordingly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matnshrn",
            "datetime": "Mar 15, 2022",
            "body": " I'm getting the same results as you (1.5dB SDR over the google LibriSpeech test list), have you managed to solve this problem?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matnshrn",
            "datetime": "Mar 16, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "weedwind/CTC-speech-recognition#1"
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/691",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ms903x1",
            "datetime": "Jul 31, 2022",
            "body": "\n原文件似乎是为linux环境创建的，没有说明如何在windows下修改地址，请教一下如何在windows下修改ppg2mel.yaml文件里的地址，有好多文件我并没有在预处理后文件夹中找到\n\n环境：windows11，anaconda：python3.9.12数据集文件夹：C:\\test\\test8\\aidatatang_200zh预处理生成文件夹：C:\\test\\test8\\PPGVC\\ppg2mel我只能在预训练生成文件夹里找到原文件4,5,6行的同名的文件，这么修改对不对？剩下的7-14行怎么修改？\n\n直接运行报错\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/NaomiProject/Naomi/issues/67",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "aaronchantrill",
            "datetime": "Jul 29, 2018",
            "body": "It has always bothered me that when the user first types \"./Jasper.py\" it just errors out with a message that the configuration file does not exist. The documentation around creating a configuration file has always been awful, and successfully creating a configuration file at this point requires reading the code itself looking for options. There is a program called \"populate.py'\" in the jasper directory that starts the job of configuring jasper, but it is fairly useless being that it is missing a lot of options and doesn't produce an actual working configuration anyway.I would like to alter the beginning of \"application.py\" to ask if you would like to create a configuration file, then alter \"populate.py\" to at least get you part of the way there.This project also touches on issue 15 (if you select \"pocketsphinx\" as your engine, it should download and configure pocketsphinx automagically, if you select deepspeech as your engine, it should download and install deepspeech automagically), issue 16 (gmail password stored in plain text), and issue 57 (support for email services other than gmail).I will not try to solve these issues in this fix, but will at least try to get it to the point where I can successfully use it to create a configuration file given that I already have pocketsphinx and festival installed. I am also doing a little cleanup in making the language selection the first question that appears so that (given that someone has gone through the trouble to write a translation file) the rest of the configuration can be done in the user's chosen tongue.Eventually, the goal would be to get this all working in such a way that the configuration can be done primarily verbally, with the text interface only being a backup for when you need to enter a complex email password.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": "Considering there are 3 options to get this job done: Ask the user in  if he want to create his profile (only if the profile doesn't exist)  and if yes, switch to  Add an argument to the jasper execution  to create your profile using  too.\nSee  and  in the  function Both  and  but  would be to recreate your profile if you missed/want to change somethingI'm currently working on a User Interface with The idea is to maintain your Naomi in your device from this interface only, meaning you'll be able to create, delete, and modify your profile from it (stt engine, tts engine, email adress, weather credentials ...), way more friendly-user than the current wayYou'll also able to see properties of your devices (device cpu type, memory/cpu usages, uptime) system related informations ...Once this UI will be mature enough, we'll be able to remove the program , becoming obsolete and incompleteTheses points are for the I have a lot of ideas for the  ...I'll push it  soonWhen the UI is out with , i'll create a Raspberry Pi image with all the dependencies installed, pocketsphinx included, and with the user interface.Users will just have to boot the raspberry pi with the burned micro sd card, go to the raspberry pi IP adress and setup their profile quickly and be able to use it, not more.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "May 18, 2019",
            "body": "We need to add the ability to specify passive, active and special mode speech to text engines separately. Also, right now plugin settings are only picked up for SpeechHandler plugins, but need to be expanded to the other speechhandler types (audioengine, stt, tts, vad)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Aug 27, 2019",
            "body": "I am having a strange situation right now where somehow on my Buster install I have an incomplete profile.yml that is causing Naomi to crash withso somehow I managed to create a profile without specifying an output device. I think this is because it saves a copy of the profile.yml immediately after setting the language at this point, so exiting the application after starting but not finishing populate.py (say you suddenly realize that you don't have a tts module installed) leads to this unstable situation. I think that adding a \"settings\" check to the main routine that kicks off populate.py if any of the basic fields (first_name, last_name, etc) before initializing the audio devices is the way to handle this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 7, 2019",
            "body": "Okay, I figured out what is causing the crash. Basically, each time I am running the pluginstore.detect_plugins() function, if there are any new plugins then I am saving that list to the profile. I can fix this by setting a profile arg for \"save profile\" set to false at the beginning of application.init() and then check it at the end (or at least after populate runs) to see if I need to save it. This is only a problem when the user kills populate before it finishes the first time.I'll put that in my next pull request.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 8, 2019",
            "body": "By the way, a while back I added some code for playing a test phrase if the user selected a flite voice. I'd love to see that expanded to all voices if anyone has time to work on that.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Feb 5, 2021",
            "body": "In the audio device setup, we start by choosing an output device. Then Naomi plays a tone to the device and asks if we can hear it. In my last few setups, both on Raspberry Pi and VirtualBox, I have had to adjust the output_chunksize parameter to at least 2048 to avoid buffer underrun errors. I'm going to change that to the default, and also the ability for the user to modify output_chunksize, output_padding, and output_pause while getting the beep to work in the validate_output_device() method.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 30, 2018",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "TuxSeb",
            "datetime": "Jul 31, 2018",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#73"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 17, 2018",
            "body": [],
            "type": "pull",
            "related_issue": "#101"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Oct 3, 2018",
            "body": [],
            "type": "pull",
            "related_issue": "#107"
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Dec 19, 2018",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Dec 20, 2018",
            "body": [],
            "type": "modified the milestones:",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Dec 31, 2018",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "Dec 31, 2018",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Apr 6, 2019",
            "body": [],
            "type": "pull",
            "related_issue": "#171"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Apr 22, 2019",
            "body": [],
            "type": "pull",
            "related_issue": "#183"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Sep 8, 2019",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#230"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Apr 11, 2020",
            "body": [],
            "type": "pull",
            "related_issue": "#253"
        },
        {
            "user_name": "AustinCasteel",
            "datetime": "May 26, 2020",
            "body": [],
            "type": "pull",
            "related_issue": "#274"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Feb 18, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#327"
        },
        {
            "user_name": "aaronchantrill",
            "datetime": "Aug 18, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#344"
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/639",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Summerxu86",
            "datetime": "Jul 11, 2022",
            "body": "在gen_voice.py最后合成音频时报了这个错误:\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n是单纯的显存不够吗？还是和我在训练生成器时修改了batchsize有关？请问大家应该如何解决？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jul 16, 2022",
            "body": "训练和推理配置应该无关。",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/537",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "SchweitzerGAO",
            "datetime": "May 5, 2022",
            "body": "\n我在实验室的机器上训练，使用的是我自己收集的，并且用aishell3数据集格式标注的数据，总是报这个错误（见截图）\n\nOS: CentOS 7\nPython: Anaconda+python 3.8\npytorch:1.11.0\nCUDA:11.4\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z",
            "datetime": "May 15, 2022",
            "body": "Look in the original repo RTVC and search ValueError in issues I saw the same problem being answered before.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z",
            "datetime": "May 15, 2022",
            "body": "Send your full notebook for better debug if you wish.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SchweitzerGAO",
            "datetime": "May 15, 2022",
            "body": "Many thanks if that's convenient for you. By the way, could you please give me the link of the 'original repo of RTVC' please?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z",
            "datetime": "May 15, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SchweitzerGAO",
            "datetime": "May 15, 2022",
            "body": "Thanks  but which issue exactly did you refer to? I found many similar issues...",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/459",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ganggang233",
            "datetime": "Mar 16, 2022",
            "body": "In ppg2mel.yaml，there is no details  about （train_fid_list：）which is in second line. hope your reply",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 17, 2022",
            "body": "you need to replace it with the file in result folder of preprocessing",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ganggang233",
            "datetime": "Mar 18, 2022",
            "body": "  Thanks for your reply, I will try.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/436",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": "将 synthesizer部分换为 tacotron2 详见 @\n我发现合成器在达到一定数量的训练后，对质量影响更大的反而是声码器，hifigan能用更好的效果但始终有电噪音，现在有些新的项目采样lpcnet而非wavernn，据说是复杂度要远低于wavernn，但质量优于wavernn，可以达到类似hifigan的效果而不带电噪音，请问有考虑过引入lpcnet之类的新声码器的计划呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MaxMax2016",
            "datetime": "Mar 8, 2022",
            "body": "声码器可以参考一下：\nSingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation\n标题：SingGan：高保真歌声生成的生成性对抗性网络（声码器中加入F0输入，字节也这样做）\n链接：\n演示：\n作者：Feiyang Chen,Rongjie Huang,Chenye Cui,Yi Ren,Jinglin Liu,Zhou Zhao,Nicholas Yuan,Baoxing Huai\n机构：Zhejiang University, Huawei Cloud\n备注：vocoder, generative adversarial network, singing voice synthesis\n摘要：由于超长的连续发音、高采样率和强的表现力，高保真歌唱语音合成对神经声码器来说是一项挑战。现有的用于文本到语音的神经声码器不能直接应用于歌唱语音合成，因为它们会导致生成的频谱图出现小故障，并且高频重建效果不佳。为了解决歌唱建模的困难，本文提出了一种具有生成对抗网络的歌唱声码器SingGAN。具体来说，\n1）SingGAN使用源激发来缓解谱图中的小故障问题；（字节跳动也这么弄）\n2）SingGAN采用多频带鉴别器，引入频域损耗和子带特征匹配损耗来监督高频重构。\n据我们所知，SingGAN是第一个设计用于高保真多扬声器歌唱语音合成的声码器。实验结果表明，与以前的方法相比，SingGAN合成的人声质量要高得多（0.41MOS增益）。进一步的实验表明，结合FastSpeech~2作为声学模型，SingGAN在歌唱语音合成管道中实现了很高的鲁棒性，并且在语音合成中表现良好。Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus\n标题：Multi-Singer：基于大规模语料的多发音人歌声声码器\n作者：Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao\n代码：\n演示：\n摘要：高保真度多歌手歌唱语音合成由于歌唱语音数据不足、歌手泛化能力有限、计算量大等问题，对神经声码器来说是一个挑战。现有的开放语料库由于规模和质量的不足，无法满足高保真声乐合成的要求。以前的声码器在多歌手建模方面有困难，并且在进行看不见的歌手歌唱的声音生成时出现了明显的退化。为了加快社区对歌唱嗓音的研究，我们发布了一个大规模的、多歌手的中文歌唱嗓音数据集OpenSinger。为了解决隐形歌唱者建模的困难，我们提出了一种基于生成对抗网络的快速多歌唱者声码器Multi-Singer。\n具体来说，\n1)Multi-Singer使用Mulit Band genertor来加速训练和推理过程。\n2) Multi-Singer采用singer条件判别器和条件对抗训练目标，从声学特征(即mell -谱图)中获取并重建歌唱者身份。（字节跳动也是这样做的，必备模块）\n3)为了监督在频域频谱包络中歌唱者身份的重建，我们提出了一种辅助的歌唱者感知损失；联合训练方法是一种有效的多歌唱者语音建模方法。（声纹联合训练）\n实验结果验证了OpenSinger算法的有效性，表明Multi-Singer算法在速度和质量上都比以前的算法得到了提高。进一步的实验证明，Multi-Singer结合FastSpeech 2作为声学模型，在多singer歌唱语音合成流水线中具有较强的鲁棒性。通用声码必备技术：1，F0转换为激励，解决持续发音的断音；\n2，判别器加入speaker embedding;\n3，声纹损失约束；",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JerryZRF",
            "datetime": "Mar 9, 2022",
            "body": "虽然我不太了解深度学习，但是我看到一些相关的文章。\n这个训练的optimizer本来是Adam，不知道换成NAdam或者AdamW或者Adamax有没有什么帮助",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "issue",
            "related_issue": "#420"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#588"
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/427",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "AyahaShirane",
            "datetime": "Mar 6, 2022",
            "body": "我做了一个利用aliyun tts批量生成数据集的软件，大家可以尝试使用现成的tts制作更多纯净的语音数据集来反哺自己的模型，暂时只有CLI，没有GUI：\n\n希望大家支持，也希望各位大佬帮我继续补全，谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": "这里的说话人会集中在几个人吗？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AyahaShirane",
            "datetime": "Mar 7, 2022",
            "body": "将voice参数设置为random可以随机生成28个发音人的内容，再算上语调和语速上的改变，基本上够用",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": "比较适合把vocoder部分训练好一点，其他的会影响模型泛化能力把",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/530",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "siao888",
            "datetime": "May 4, 2022",
            "body": "\n您好\n在訓練合成器步驟\n\"注意在上一步先下載好ppg2mel.yaml, 修改裡面的地址指向預訓練好的文件夾： python ppg2mel_train.py --config .\\ppg2mel\\saved_models\\ppg2mel.yaml --oneshotvc\"不知道該怎麼修改地址.自己試了好久都失敗.\n\nIf applicable, add screenshots to help\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "May 4, 2022",
            "body": "/FakeVC/\n这样的路径应该不符合windows风格， 你要使用 D:\\xxx\\xx 或者 相对路径",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "May 4, 2022",
            "body": "謝謝.\n我也試了.好像不行主要是.文件中應該修改第幾行?\n另外命令是\n這樣 python ppg2mel_train.py\n還是 python ppg2mel_train.py --config .\\ppg2mel\\saved_models\\ppg2mel.yaml --oneshotvc\n抱歉.很白癡的問題\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "May 4, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "May 4, 2022",
            "body": "好.謝謝.我再試試",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "May 7, 2022",
            "body": "請問.我用自己的音頻做數據集\n一直出現錯誤訊息.\n我知道是音頻格式的問題.\n但是.轉換了很久.都轉不到他要的格式.\n有什麼方法可以做到呢?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "May 9, 2022",
            "body": "好了.大致上搞定了.\n現在應該是正常訓練了.\n感恩.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "May 11, 2022",
            "body": "vc这部分我搞了一半觉得模型不是特别好，在用espnet练新的，有兴趣可以邮件我一起搞",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yxwudi",
            "datetime": "Jul 1, 2022",
            "body": " 能给我一个改好的样本截图吗？我也是改了好久没弄好。谢谢！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "Jul 5, 2022",
            "body": "不好意思.一陣子沒用了.那個文件被我刪除了.\n我剛剛找到了一個.不知道對不對.\n您試試看\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yxwudi",
            "datetime": "Jul 5, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "siao888",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/440",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": "\n\n有足够的样本的前提下我想自己训练出来一个日语模型，不知道行不行\n答：就像把中文句子例如 “你好” 变成 “ni2 hao3”，只需要找到一个tts前端处理一下日语为phenomenon",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 8, 2022",
            "body": "感觉这样做出来会很强大，还自带翻译功能，会更复杂",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emiya0415",
            "datetime": "Mar 19, 2022",
            "body": "关于日语方面，我已经找到了对应的tts前端，配合自己写的java脚本对日语处理为phenomenon，可以将一个wav文件夹下所有的语音和文本生成对应的alignment格式。一个是将日语的所有汉字等转换为片假名（类似于中文的拼音），另一个是输入对应的语音和txt文本，输出对应的每个拼音的时间。最后用java脚本生成alignment.txt文件。\ntts前端：链接:  提取码: 6f5s 复制这段内容后打开百度网盘手机App，操作更方便哦\n包括两个文件japankana和segmentation-kit2\n第一个japankana使用比较简单，打开后输入就行了，可以将包括汉字的日语转换为片假名\n\n第二个操作方法请参考里面的readme.txt我已经开始尝试训练了50k步，但日语效果并不是很好。基本只有前面的两个词左右能够识别输出，剩下的都是语音和输入文本对应不上。目前数据集是单独一个人的日语语音10个小时左右，可能是因为数据集太小的原因所以效果不好。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 20, 2022",
            "body": "赞",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "584761274",
            "datetime": "May 15, 2022",
            "body": "链接：\n提取码：1111\n我不知道gui如何上传音频文件 所以使用百度网盘上传\n这是我训练的效果，我相信相同的步骤可以应用于其他语言\n第一步 修改MockingBird-main\\synthesizer\\utils\\symbols.py 中的参数\n\n这里我是想训练日语tts 所以加入了所有的片假名\n第二步 修改一个日语数据集，使其符合aidatatang_200zh, magicdata, aishell3, data_aishell的格式\n\n第三步用数据集训练合成器（我认为只训练合成器就可以达到较好的效果，如果训练声码器和编码器效果应该会更好？）\n\n（时间原因 我只训练了20k步 更长时间的训练应该能获得更好的效果）",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emiya0415",
            "datetime": "May 16, 2022",
            "body": "我们用的是同一个数据集，都是英伟达的common voice。你直接用片假名训练的，我则是把片假名再进一步转换为罗马音进行训练。现在是90k step，效果只能说一般，部分文字还是识别不出来。可能是数据集大小还不够",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "584761274",
            "datetime": "May 16, 2022",
            "body": "我对日语并不熟悉，但或许片假/平假对tts来说更易拟合？ 我对目前训练的效果还是满意的 毕竟日语的大型数据集太难找了\n我接下来会用jsut 和 jvs混合训练试一试",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emiya0415",
            "datetime": "May 16, 2022",
            "body": "那我今天根据片假名再重新开始训练好了。日语的数据集还是挺多的比如LaboroTVSpeech和Corpus of Spontaneous Japanese等，都有几百个小时。但前者申请需要国内大学的老师或者日本当地大学的学生、后者一个数据集2000rmb，对于个人兴趣负担还是太大了。方便的话可以分享一下你训练的模型吗，我已经把我训练的上传到issue了。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "584761274",
            "datetime": "May 16, 2022",
            "body": "我训练到90k的时候会分享的，感觉loss还有下降的空间",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emiya0415",
            "datetime": "May 16, 2022",
            "body": "你训练时有输入音频每个单词对应的时间吗，这部分是怎么处理的。我是用了segment_julius可以同时将片假名转换为罗马音并获取对应单词的时间，但如果只输入片假名如何获取其对应单词的时间呢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "584761274",
            "datetime": "May 16, 2022",
            "body": "\n神经网络似乎可以自己对单词的时间进行划分 所以我没有划分单词时间\n链接：\n提取码：1111",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Emiya0415",
            "datetime": "May 16, 2022",
            "body": "感谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "issue",
            "related_issue": "#356"
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#214"
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#588"
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/388",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "SeedKunY",
            "datetime": "Feb 14, 2022",
            "body": "\n            \n          ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "luan78zaoha",
            "datetime": "Feb 17, 2022",
            "body": "可以试试这个项目\n",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/382",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ZJ-CAI",
            "datetime": "Feb 11, 2022",
            "body": "\n            \n          ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "luan78zaoha",
            "datetime": "Feb 17, 2022",
            "body": "可以试试这个项目\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Feb 17, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/423",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "yrsn509",
            "datetime": "Mar 5, 2022",
            "body": "请问最新的ppg两个模型可以在哪里下载？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 6, 2022",
            "body": "已补充到readme",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 8, 2022",
            "body": "好像没有下载链接。。要自己寻找吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 8, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 8, 2022",
            "body": "oh shit，好像一直没有成功，我这里先贴一下：\n链接:   密码: bt9m\n--来自百度网盘超级会员V4的分享",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 8, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 10, 2022",
            "body": "这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 10, 2022",
            "body": "不用，大概率是你没选对模型，除了encoder基本上都要替换成新的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 10, 2022",
            "body": "这样子吗，有没有demo参照一下呢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 10, 2022",
            "body": "readme中文版里面的截图就是一个可行的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 12, 2022",
            "body": "请问如果要提高特定人语音的相似度，训练哪一个模型是最好的，Convertor 或者Encoder还是其他的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 12, 2022",
            "body": "目前看 convertor 是瓶颈，但我继续用aidatatang数据训练效果收益一般",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 14, 2022",
            "body": "这个图谱显示了全是电流\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 14, 2022",
            "body": "训练ppg2mel的时候出现了三种错误，用了合成synthesizer时用的数据集，起初显示第一种错误经过我本人手动转采样率后，又出现了第二种错误然后本人调低了n的值，最终出现第三种错误，至今无法解决",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 14, 2022",
            "body": "你做了什么手动转码？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 14, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 14, 2022",
            "body": "感觉你触发了不曾见过的bug，先不转，你试着preprocess一下",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 14, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "xiaoyeye1117",
            "datetime": "Mar 18, 2022",
            "body": "应该把作者提供的声码器模型hifigan_24k.pt重命名为g_hifigan_24k.pt~ 再选择~  电流音解决~但感觉效果不咋样捏",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 20, 2022",
            "body": "确实 而且再播放合成好的声音还会变化成低音",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 21, 2022",
            "body": "所以为啥训练合成器的时候能去掉时长较短的音频，训练ppg2mel的时候没有这一步？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 29, 2022",
            "body": "我感觉也不怎么样，怀疑数据集问题，还是相信paper本身的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Chopin68",
            "datetime": "Mar 29, 2022",
            "body": "有了解过他们的技术吗 看起来已经能和变声器一样实时了，不知道是不是真的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Quadcore1010",
            "datetime": "Jul 7, 2022",
            "body": "大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jul 10, 2022",
            "body": "数据集的原因，这个模型对数据集要求高一些",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Quadcore1010",
            "datetime": "Jul 10, 2022",
            "body": "请问具体是哪方面要求呢？有没有适合训练的中文数据集推荐呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jul 16, 2022",
            "body": "开源的很多都不行，最好是有商用的",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Quadcore1010",
            "datetime": "Jul 18, 2022",
            "body": "是采样率的问题吗？还有其他方面会有影响吗？我好按照具体方向去找相应数据集",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tylqbq",
            "datetime": "Sep 22, 2022",
            "body": "求一个最新的 两个ppg模型 百度网盘链接",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tylqbq",
            "datetime": "Sep 23, 2022",
            "body": "求最新的 两个ppg模型 百度网盘链接",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yrsn509",
            "datetime": "Mar 14, 2022",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/328",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "absc",
            "datetime": "Jan 9, 2022",
            "body": "Traceback (most recent call last):\nFile \"/Users/huyouwangzecai/Documents/项目/个人项目/声音项目/MockingBird-main/toolbox/ui.py\", line 160, in setup_audio_devices\nsd.check_output_settings(device=device_namecheck, samplerate=sample_rate)\nFile \"/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py\", line 691, in check_output_settings\n_check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate))\nFile \"/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py\", line 2736, in _check\nraise PortAudioError(errormsg, err, hosterror_info)\nsounddevice.PortAudioError: \n这是我的报错内容，sounddevice check_output_settings方法运行不成功有了解的么",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mr-m0nst3r",
            "datetime": "Jan 12, 2022",
            "body": "一样遇到这个问题。解决了吗？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "toimc",
            "datetime": "Jan 30, 2022",
            "body": "一样的问题！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hxqbeyond",
            "datetime": "Feb 27, 2022",
            "body": "一样的问题 求解；打开就闪退",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hxqbeyond",
            "datetime": "Feb 28, 2022",
            "body": "后面解决了吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "absc",
            "datetime": "Apr 1, 2022",
            "body": "全部找m1专用的arm架构包， 不要用x86架构包，就可以正常运行",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/314",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "\n打开项目文件后，发现源音频部分是黑色不能使用，合成出来的声音也有问题，而且会报错——之前那个兼容性错误，目前用的是新手教程里面给的模型",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "Yes, thank you for your kindness, I ran into some problems while running",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "Excuse me, now I cannot synthesize the sound I want with the loaded model",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "I performed the operation and sound synthesis according to the project's tutorial, and the sound that came out was distorted, not normal sound, only current sound and noise",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "Yes, I checked the dependency package and the version of pytorch according to the requirements before running the project, and proceeded without errors.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "Let me try to restart my computer",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "It can be seen that the synthesized sound is problematic, and the Mel spectrum image is abnormal.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "I think it should be the problem of the model I imported. I directly used my own voice, which is not ideal. It should be tested with the given model. My voice needs to be trained with a certain sample set. I think it’s me. I made a mistake",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mr-MoNET",
            "datetime": "Jan 4, 2022",
            "body": "Thank you for your enthusiastic help, I have found the problem so far",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/244",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Rita-ritally",
            "datetime": "Dec 2, 2021",
            "body": "由于之前一直关注Real-Time-Voice-Cloning() 这个项目，这次Mocking Bird项目没有使用Tacotron2太令人可惜了。所以自己斗胆将Tacotron2迁到这个系统中，比较粗糙但是可以成功训练和推理。\n模型还在训练中，有效果及时来这里更新！\n代码肯定漏洞百出.....求各位大佬指点！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Dec 2, 2021",
            "body": "需要更多协助可以随时联系我",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Rita-ritally",
            "datetime": "Dec 2, 2021",
            "body": "\n 大佬您好，现在一直在这个阶段，loss下降特别慢。。。。\n我的代码在 这个仓库中，希望能得到您的指导～",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Rita-ritally",
            "datetime": "Dec 3, 2021",
            "body": "现在训练了46000步，平均loss在0.74，从测试结果中可以看出已经收敛，且合成的mel谱纹路清晰。\n\n\n但是正式测试的时候发现使用aishell+aidatadang_200zh训练46000步的合成器无法正确合成语音，mel谱很模糊。\n还没有只使用aishell训练合成器8w步合成的效果好\n\n\n现在还在继续训练，不知道是不是训练 步数少的问题。。。。。。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Dec 26, 2021",
            "body": "现在效果会好一些了吗？我本来也打算fork个分支试试的，你这边可以fork试一下吗，成为contributor？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Rita-ritally",
            "datetime": "Dec 26, 2021",
            "body": "在我的实验中使用aishell3数据集训练的效果比aidatatang效果好，由于aishell3男性说话人语料较小，发现无法正确合成男性说话人的声音。所以我将train和test的所有语料整合在一起训练，可以缓解这个问题。在我的仓库中只有synthesizer的code是有变化的，vocoder中加入了melgan和waveglow声码器，但是效果还不是很好。如果可以的话，非常荣幸能成为这个项目的contributor！！！！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "facenl",
            "datetime": "Feb 26, 2022",
            "body": "大佬牛啊！！！",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Feb 27, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/230",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "mithisha",
            "datetime": "Nov 23, 2021",
            "body": "I do want to try this  with a vocoder which has trained for the english speakers.. how can i do that? can i have a little guidance on that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Olivia-Ye",
            "datetime": "Nov 23, 2021",
            "body": "This repository is forked from Real-Time-Voice-Cloning which only support English.\nYou'd better see this ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Olivia-Ye",
            "datetime": "Nov 23, 2021",
            "body": "or maybe you can train synthesizer with English dataset, and keep the vocoder and encoder remain",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Nov 24, 2021",
            "body": "what vocoder model do you want to use? The one of WaveRNN in this repo is originally trained with English.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Nov 27, 2021",
            "body": "For the question you asked in issue title, answer is yes, and it works event better than the wavernn",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mithisha",
            "datetime": "Nov 28, 2021",
            "body": "highly appreciated your reply. I  have used the toolbox provided by the with the . Then I have used your work to change the vocoder into the hifigan. but in the end, there is no difference in the outputs . It gives the same outputs for the bother wave rnn and hifigan vocoder.  I wanted to know whether this scenario is normal to not? ## I have not trained hifigan model, instead of that I have used ur weight files.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mithisha",
            "datetime": "Nov 28, 2021",
            "body": "yeah I have referred it . thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Nov 28, 2021",
            "body": "The toolbox from  won't work with our HifiGan in this repo for sure.  Please try the toolbox in this repo and see how different it works with this vocoder.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mithisha",
            "datetime": "Nov 28, 2021",
            "body": "I want to have the results for the English speakers, can you please give help with that?can I do it by replacing the pretrained.pt file of the synthesizer in this repo with the pretrained.pt  file of the  \"\" repo?sorry for the inconvenience !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Nov 28, 2021",
            "body": "This won't work as well. Now these two models has too much difference on states to be compatible.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mithisha",
            "datetime": "Nov 28, 2021",
            "body": "Then I have to train the synthesizer in this repo using an English speaker data set. Since I don't have enough resources to train a model , cant I use a pre-trained model (trained with English speakers) for the synthesizer? if it is so where can i find a compatible pretrained model ? do you have any guidance for that ?thank you in advance!!!!!!",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/olivia-ai/olivia/issues/166",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "A-Yamout",
            "datetime": "Nov 14, 2021",
            "body": "You can switch between Male or Female, it will default to Female witch is Olivia but you can switch it to male witch it Oscar.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hugolgst",
            "datetime": "Nov 14, 2021",
            "body": "Is it a question? A suggestion? I do not get it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "A-Yamout",
            "datetime": "Nov 16, 2021",
            "body": " This is a suggestion",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "A-Yamout",
            "datetime": "Nov 14, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/RasaHQ/rasa/issues/10457",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "NareshDen",
            "datetime": "Dec 5, 2021",
            "body": "2.8.0 ( 2.7.0 )3.6LinuxRasa 1.0 form were working, and suddenly it stopped working. The version of rasa was 2.8.2 and was tried on 2.8.0 and even 2.7.0 version also. Code was on production and it worked before November 10th. The temp solution was to remove forms 1.0 definition. The question is why the same code worked before. Here is the log for working and non working forms. ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "z-bodikova",
            "datetime": "Dec 6, 2021",
            "body": "Exalate commented:z-bodikova commented: - this is the issue we talked about in #on-point - I think we should time-box it and was hoping you could give me an idea what an appropriate time box for this investigation (I was thinking maybe 4-6 hrs) would be and when you can spend that time on this - especially since you are not in the firefighting crew this week... Let me know.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NareshDen",
            "datetime": "Dec 5, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "z-bodikova",
            "datetime": "Dec 6, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 15, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot",
            "datetime": "Mar 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/olivia-ai/olivia/issues/136",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "PiperMp3",
            "datetime": "Oct 18, 2020",
            "body": "Hello,I've currently only gotten so far to have her installed, running as CLI and tested Web App a little. I've yet to see the training of the neural network during a new training.json occur. What I've read is when new intents or new datasets are being added Olivia needs a quick nap (restart) . How could we get her to never need a restart and can detect file changes / new datasets.. new intents, during runtime and she will automatically decide to start training. She could still process anything asked during this training that she already knows, but she will announce to the user that she has detected new data and is currently learning. When she's done she will also state the completion and at this point the new information she's learned is available to be requested/answered with.Would such a feature be possible?The other method that would also be interactive is that the user could ask Olivia, \"Why don't you do a little homework?\"(or something) Whereas Olivia remains active, running, but she rewrites her training.json using the files she has at the time.Mighty impressed by your work so far!Side question, as I didn't really see it mentioned, or perhaps I missed it.. 1. Does Olivia have a limit to how many intents / datasets she can handle? If I copied Wikipedia into intents and sets, giving her 800gb of data, how would she handle it?\n2. During runtime,  is Olivia's \"brain\" running in RAM or is she pulling information to make answers from file system on demand?Many thanks, and keep at it. Good work !!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hugolgst",
            "datetime": "Oct 19, 2020",
            "body": "Hello,It is definitely possible to have a such feature, I only did the current one to save time.\nWe could inject some features and then just re-train the model a bit to include it.Technically she could handle it but I think that it will take a long time, during runtime there is a save of the neural network loaded into the RAM. There is also a cache if you ask multiple times the same question.Thanks a lot for that, I appreciate it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "PiperMp3",
            "datetime": "Oct 19, 2020",
            "body": "Good morning,Many thanks for coming back to me - I saw mentioned that a Discord server was started?\nI have some further questions and I don't want to spam the GitHub issues page :)I understand Olivia does not store any data, that may come from the Swiss culture.\nWould be cool to have an optional setting where Olivia could track an unique identifier to see whether its \"me\" or someone else, and store data accordingly.An example; I talk to Olivia as ME, I share secrets (perhaps) and then my partner connects as her (different name, perhaps different device ID that Olivia now sees) she will say \"Hi [NAME]!\" and Olivia now knows not to talk about me, but she now uses all data she knows about the other person. Should I ask Olivia to share secrets that my partner has told her, Olivia would refuse etc.How could one implement such option? Ever had a similar thought?\nCurrently I got Olivia via CLI, but depending on how she's evolving and how I can teach her more I would most likely look to eventually talk to her, to extend her speech - How could wouldn't it be that she recognizes different voices? I have one voice, my partner has another and Olivia answers us accordingly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hugolgst",
            "datetime": "Oct 19, 2020",
            "body": "I think to do such a feature Olivia would need to share the ID of the other user and make the client replace it in its local storage.\nOf course it can be done but I'm not sure that it is in the spirit of Olivia, you could however implement it in a fork :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "A-Yamout",
            "datetime": "Nov 4, 2020",
            "body": "I think that is great that we value the privacy of others and keep expanding Olivia.   you have done a great job with this and I would love to help if there is any way that I could help with the new 3.00 release just ask.BTW I am great with logos graphics and art more then go (I am horrible at go) so want a new logo or a new UI for the web app I can help you with that, and thanks for all your hard work.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hugolgst",
            "datetime": "Nov 4, 2020",
            "body": "Thanks a lot for that! However, it is hard for me to keep maintaining Olivia thus I am pretty busy.\nI am very open to new ideas so you can create issues or work on some PR if you want so!\nHuge thanks! :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "A-Yamout",
            "datetime": "Nov 4, 2020",
            "body": "No Problem. Thanks for making this open source and open to the world.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hugolgst",
            "datetime": "Oct 19, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/10",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "babysor",
            "datetime": "Aug 16, 2021",
            "body": "已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)\n需要更多请在这里提建议，并+1投票，将为大家补充支持",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yfq512",
            "datetime": "Aug 16, 2021",
            "body": "朋友，你是怎么跑起来的，我运行python synthesizer_preprocess_audio.py <datasets_root> 就迷惑了，这个datasets_root是指什么呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 16, 2021",
            "body": "假如你下载的 aidatatang_200zh文件放在D盘，train文件路径为  , 你的就是 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hertz-pj",
            "datetime": "Aug 17, 2021",
            "body": "推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是而不是",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 17, 2021",
            "body": "是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hertz-pj",
            "datetime": "Aug 17, 2021",
            "body": "那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 17, 2021",
            "body": "程序逻辑还没实现 囧 目前我是手动分开跑",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XiuChen-Liu",
            "datetime": "Aug 17, 2021",
            "body": "推薦標貝數據集",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "0we",
            "datetime": "Aug 23, 2021",
            "body": "aishell3 和 Mozilla Common Voice 数据集",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "loilih",
            "datetime": "Aug 27, 2021",
            "body": "aidatatang_200zh在哪里下载呢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XiuChen-Liu",
            "datetime": "Aug 27, 2021",
            "body": "這裡 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "loilih",
            "datetime": "Aug 27, 2021",
            "body": "谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 28, 2021",
            "body": " 已支持aishell3，不过训练效果没增强",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XiuChen-Liu",
            "datetime": "Aug 28, 2021",
            "body": "大佬，現在你提供的版本還需要使用原項目的 encoder 和 vocoder 嗎",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 28, 2021",
            "body": "已经不再需要下载了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XiuChen-Liu",
            "datetime": "Aug 28, 2021",
            "body": "好的，謝謝大佬的回覆，另外推薦 zhvoice數據集: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "FawenYo",
            "datetime": "Sep 2, 2021",
            "body": "請問有沒有大佬能提供 Mozilla Common Voice 架構的支援... 原 repo 中有人也發過類似的 issue 可以參考 \n檔案結構大致如圖\n\n其中  資料夾內容包含各項 \n希望日後能支持，謝謝",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zhiqing-Xu",
            "datetime": "Sep 5, 2021",
            "body": "老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 6, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Zhiqing-Xu",
            "datetime": "Sep 6, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 7, 2021",
            "body": " 这里有新的二维码",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ShouNichi",
            "datetime": "Sep 26, 2021",
            "body": "\n这个看着很厉害的样子\n这边在研究改代码跑跑看\n不过都是mp3的很麻烦",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 26, 2021",
            "body": "等你好消息，不过我访问不了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ShouNichi",
            "datetime": "Sep 27, 2021",
            "body": "是指数据集不能访问吗？如果是百度云不能访问的话我这边可以转mega或者GD\n链接:  提取码: dwet-----------------更新---------------------访问不了是链接不知怎的最后多了个z，删掉就行了\n\n写了貌似可以直接用于zhrtvc，同一个分支出来的\n\n突然发现上面已经有人推荐过了...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Charlottecuc",
            "datetime": "Sep 27, 2021",
            "body": " 二维码过期了，求重发一个～",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 27, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LxKxC",
            "datetime": "Oct 7, 2021",
            "body": "群满了，加不进去，求作者微信号",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kslz",
            "datetime": "Oct 11, 2021",
            "body": "有没有可能提取游戏里的音频素材，或者关闭背景音乐后用软件录制",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Oct 11, 2021",
            "body": "理论可行，可以联系我讨论",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kslz",
            "datetime": "Oct 11, 2021",
            "body": "你好 已发到你的gmail里  上面的二维码过期了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lcp580",
            "datetime": "Oct 14, 2021",
            "body": "新增标贝数据支持BZNSYP\nMozillaCommonVoice\n以上两个数据集支持在最新的主版本中没有看到？难道还没能合并进来吗？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Oct 14, 2021",
            "body": "都在分支上， ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "woct0rdho",
            "datetime": "Nov 7, 2021",
            "body": "给个思路，那些galgame都是几十万字的文本与语音对应的数据集",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ycMia",
            "datetime": "Nov 24, 2021",
            "body": "牵涉版权问题哦 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "joshua54321",
            "datetime": "Dec 23, 2021",
            "body": "作者您好，请问现在支持自动跑多个数据集了吗？如果手动分开跑，是如何操作的呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Dec 26, 2021",
            "body": "可以，  分开跑比较麻烦，因为混合起来就不太能区分了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "l-i-p-f",
            "datetime": "Jan 30, 2022",
            "body": "请问有粤语数据集吗？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ZeroAurora",
            "datetime": "Aug 17, 2022",
            "body": "\nTHCHS-30 数据集，体量较小，想拿来练手用",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 10, 2022",
            "body": "最好确保有100hrs级别的语音",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 27, 2021",
            "body": [],
            "type": "pinned this issue",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": [],
            "body": [],
            "type": "",
            "related_issue": "#7"
        },
        {
            "user_name": "babysor",
            "datetime": "Mar 7, 2022",
            "body": [],
            "type": "unpinned this issue",
            "related_issue": null
        },
        {
            "user_name": "18klove",
            "datetime": "Jun 19, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#625"
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/214",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "CoolPiotr",
            "datetime": "Apr 24, 2020",
            "body": "If one is using your Pi (for example) as a music player system as well as running Rhasspy for voice control, it would be nice to mute (or substatially lower) the volume on all the other audio sources after wake word detection while listening, and also possibly during text-to-speech playback, much like Alexa or Google Home does.\n(This would require an audio loopback / separate channel for Rhasppy's audio playback of speech or alert noises, I would expect, so one could differentiate its volume from the muted ones.)\nIs anybody working on something like that?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "litinoveweedle",
            "datetime": "Apr 25, 2020",
            "body": "Hello, we discussed this on the forum, it is quite easy to do with external program (muting) if you have information about key word received. For satellite systems this could be done when using Hermes MQTT by subscribing for given topics. I did also asked for possibility to get Rhasspy status, or even better to get programmable hooks (i.e. executing of external command on given Rhasspy action) as a . I can even try to prepare PR for that, the only problem is I am not yet migrated to 2.5, so I would wait for it first.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/RasaHQ/rasa/issues/9952",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Fares-Ayed",
            "datetime": "Oct 21, 2021",
            "body": "Hi, Actually I'm developing a bot based on Rasa. I have integrated it into Telegram. I find a problem that Telegram.py doesn't support the Speech to Text conversion. My main goal is when the user send a voice message via Telegram My bot can understand this messageI suppose that a class can be added into the custom channel (Telegram.py) to convert this audio file to text using Deepspeech models (for examples) then send it to Rasa in Text Form",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Oct 21, 2021",
            "body": "Exalate commented:rgstephens commented:See  for more details on this request.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Fares-Ayed",
            "datetime": "Oct 21, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 15, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot",
            "datetime": "Mar 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/691",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "colinboice",
            "datetime": "Nov 11, 2017",
            "body": "I needs to control the light by voice with GPIO and relay. How can I do ??",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 15, 2017",
            "body": "Take a look at the lights plugin, that's a good place to get started. You can use  for GPIO control on the pi.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 24, 2017",
            "body": "how goes it ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 24, 2017",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/242",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "michelepanegrossi",
            "datetime": "Jun 19, 2020",
            "body": "HelloI am experiencing an issue where it is really hard for rhasspy to recognize the word  especially when pronounced by a female voice (sometimes 0% rate over 10 utterances).My sentences.ini file: and  work a lot better even with female voicesMy setup is as follow: rhasspy is running as a service on my raspberry PI4. I have another python3 script which is controlling rhasspy via the HTTP API, receives the transcript and forwards that over to another machine. This other script also runs at startup as a service.I send messages to that script and ask it to query Rhasspy over HTTP.This is the log of one of my requestsWhat can I do to solve this issue?At the moment I am using . I noticed that Rhasspy 2.5 also now supports . Would I get a better result switching to a different recogniser such as Kaldi or Deepspeech?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "keith721",
            "datetime": "Jun 28, 2020",
            "body": "When I previously had word recognition problems in Rhasspy 2.4.x, I switched from pocketSphinx to kaldi, and things got much better. At the time, Rhasspy was having trouble differentiating between 'on' and 'off', quite a bother.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "michelepanegrossi",
            "datetime": "Jun 28, 2020",
            "body": "Yes I had the same experience! I switched to Rhasspy 2.5 too. I started with Deepspeech as I was hoping that would be the answer, but after trying Kaldi I now think that engine is the best solution so far. The issue with 'yes' and 'no' seems solved.However, at the moment I am looking for a way to avoid triggering an intent when a random word is spoken. In my experience Rhasspy tries to force an intent when any word is spoken, not just words that appear in the sentences.ini file. The result is that intents are triggering with random words.Do you know if there is a way of having some kind of 'default' or 'fallback' intent to handle this kind of situation?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Jul 7, 2020",
            "body": "See  for some ideas",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/RasaHQ/rasa/issues/9737",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "rgstephens",
            "datetime": "Sep 28, 2021",
            "body": "2.8.23.7LinuxResults generating a model from the same training data can be significantly different when built on a GPU from one build to the next.Details can be seen .Slack conversations regarding this issue can be viewed  and .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tttthomasssss",
            "datetime": "Sep 29, 2021",
            "body": "Exalate commented:tttthomasssss commented: can you provide some more context for reproducing this issue? I will start an investigation but to speed things up I'd need:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tttthomasssss",
            "datetime": "Sep 29, 2021",
            "body": "Exalate commented:tttthomasssss commented: there are still a number of unaddressed sources of non-determinism with TF GPU training (see ) - for example with  that we're using in DIET, the Response Selector & TED.The available documentation suggests 2 things to try out (though my reading is, this doesn't get rid of the non-determinism but can make it  ):  As Akela points out in the comment below, Rasa doesn't support TF 2.4 yet, so upgrading is not an option.Its noted that the fix provided with TF 2.4 only works for the dtype , which  mean it works with our codebase (i.e. by default we use  throughout the code as far as I have seen). Additional information is added below.For further context, there is an active  to make TF with GPU training deterministic, but its still an ongoing effort (i.e.  appears to be one of the last unaddressed things). The problem seems to be known for a quite a while now (looks as if  is the original issue). There is currently movement in several different ways in making  deterministic, one way forward was providing a fix for TF 2.4 (and later), subject to the  switch being enabled, that supports  (i.e. should work with our codebase), but not e.g.  (see discussion on a TF PR ).For TF 2.6 onwards the following behaviour is implemented:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "akelad",
            "datetime": "Sep 29, 2021",
            "body": "Exalate commented:akelad commented:Rasa OSS 2.8 doesn't have 2.4 as a requirement though, as far as I know you can't just upgrade to tf 2.4 just like that - ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "koernerfelicia",
            "datetime": "Oct 4, 2021",
            "body": "Exalate commented:koernerfelicia commented:  we've actually run into this exception being thrown when  is set with TF 2.6 . It turns out that the deterministic implementation (for any data dtype, including float32) is  available for standard TF. If users/customers want determinism, they will have to use the NVIDIA container  onwards.As Thomas noted above, we never had determinism for this particular op. As far as I understand, it was briefly possible for TF 2.4-2.5, but no longer in 2.6. Is this something we want to support? If so, we'd need to look into the NVIDIA container further. I'd create a followup issue and try to get the relevant eyes on it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Oct 11, 2021",
            "body": "Exalate commented:rgstephens commented:GPU determinism issue is an issue with the tensorflow library itself. It’s a bit better in versions 2.4+, unfortunately we haven’t upgraded to a higher version yet due to some performance issues (more info ). Rasa  upgrades to TensorFlow 2.6.Even with the upgrade, it doesn’t sound like tensorflow has fully solved this problem. Aa a result, for the best stability users should train on CPU for now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "koernerfelicia",
            "datetime": "Oct 11, 2021",
            "body": "Exalate commented:koernerfelicia commented:In fact, in the process of upgrading I discovered there are even more problematic ops, including , and . There's a whole list .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JEM-Mosig",
            "datetime": "Oct 15, 2021",
            "body": "Exalate commented:JEM-Mosig commented: Are we ok with the workaround (use CPU), or should we invest more time on this? From the TF side there is no near-term solution, so it might be  hard for us to come up with a solution. It seems like the problem here is more that the training data isn't good enough / one would need a validation set, etc.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Oct 15, 2021",
            "body": "Exalate commented:rgstephens commented: Can you explain further about the  that would be needed? This  is a well tested production bot which runs cross validation in their ci/cd pipeline as we recommend.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "b-quachtran",
            "datetime": "Oct 15, 2021",
            "body": "Exalate commented:b-quachtran commented: Many customers are training their large bots in a GPU environment, so I don't think recommending CPU training as a workaround is a viable alternative.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JEM-Mosig",
            "datetime": "Oct 18, 2021",
            "body": "Exalate commented:JEM-Mosig commented: Cross-validation should also do it. But if a model generates wildly different outputs when trained with different random seeds, then either the training data isn't good or it hasn't converged (or both). But that should show up in cross-validation. Or is this mostly about confidence values?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Oct 18, 2021",
            "body": "Exalate commented:rgstephens commented:Their random seed is .The customers evaluation details is posted privately .The priority setting for this issue was last discussed  and I think it should stay as .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "akelad",
            "datetime": "Nov 1, 2021",
            "body": "Exalate commented:akelad commented: what's happening with this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TyDunn",
            "datetime": "Nov 1, 2021",
            "body": "Exalate commented:TyDunn commented: Followed up in Slack ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Dec 2, 2021",
            "body": "Exalate commented:rgstephens commented:Any updates on this issue? I may be running into this with another bot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dakshvar22",
            "datetime": "Feb 7, 2022",
            "body": "Exalate commented:dakshvar22 commented:Fluctuations in performance of models in terms of F1 score on test set was investigated in this . Results were documented  and in the  as well. (Sneak peek: Extremely small fluctuations observed, the customer dataset was also included in the investigation.)What hasn't been investigated:As this  mentions, the problem might be one level deeper in the values of confidences that are predicted by the models. Since the former issue did not investigate this, we should look into that as part of this issue.One hunch that I have: The config used in the dataset has  included. The featurizer uses batch inference which might result in  since  might be using  operation. So, the real problem might be with the featurizer. If you have time, can you re-run your analysis with two different configs:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "rgstephens",
            "datetime": "Sep 28, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "akelad",
            "datetime": "Sep 29, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "tttthomasssss",
            "datetime": "Sep 29, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "tttthomasssss",
            "datetime": "Sep 30, 2021",
            "body": [],
            "type": "removed their assignment",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 15, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot",
            "datetime": "Mar 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "rasabot-exalate",
            "datetime": "Mar 17, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/evancohen/smart-mirror/issues/386",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "decentralgabe",
            "datetime": "Aug 31, 2016",
            "body": "Continuing what I was working on in .I have read when you (Evan) have written about TTS in , and agree that having a speech response to most commands would be useless; however, when asking \"what time is it?\" I think it would be a nice feature if the mirror spoke the time at the user, though the \"what time is it\" feature should be fixed to display the time on the mirror too if any other services are open. There are other cases too where TTS could be useful down the line – speaking results from the Wolfram Alpha API, announcing sports scores, timed reminders and so on.I had been looking into the ; however, it appears that Google has dropped support for the API within embedded/dev Chromium environments as of . The problem isn't with the SpeechSynthesis commands itself (they are available within the mirror), but there are no voices available for the API to work with (evident by a call to speechSynthesis.getVoices()). .I had looked into a few other user's projects that make use of the Speech API (they too do not work in the mirror), including , and .What I will attempt next is to work with a few of the Linux-specific TTS solutions outlined .: Might be worth noting that SpeechSynthesisUtterances work when I run the mirror code on my Mac, but does not work on the RPi.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Sep 6, 2016",
            "body": "I've been working with getting Festival to speak commands, which work via the terminal, and would work through ; however, ALSA does not allow multiple sound sources (the mirror and festival) to go at the same time.I've been playing around with the .asoundrc file to see if I can get  to work, but I don't understand this stuff too well yet.By the way,  I've been trying to figure out.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 13, 2016",
            "body": "Sorry for the delay in response here! You've answered your own question, DMIX is the way to go. It would be helpful to know what issues you've been having trying to figure it out.Just taking a stab in the dark here, but if you are using the provided  file then you should just be able to update the  pcm device, for instance:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Oct 8, 2016",
            "body": "Working asoundrc...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Oct 22, 2016",
            "body": "With the updated speech recognition work that I just completed, my recommendation would be to use Say: Troublesome audio configuration should be a thing of the past (I still have to verify this though).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Oct 31, 2016",
            "body": "I had tried say back when I started down this rabbit hole. Just tried it again with the updated mirror code, and it has no output sound, no error messages. I'm guessing there's still a dmix issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Nov 1, 2016",
            "body": "@glcohen have you followed the updated documentation? I tried Say with Sonus a few weeks ago and it worked without a hitch.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Nov 2, 2016",
            "body": "Updated asoundrc worked!Should I submit a pull request?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Nov 27, 2016",
            "body": "what's the status on this one... @glcohen were you able to submit a pull request to the dev branch can this and  be closed? also i'm concerned with the asoundrc info above leading people down the wrong troubleshooting path... can you please update the asoundrc file that is working for you?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Nov 27, 2016",
            "body": "Just submitted pull request .  can be closed.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 6, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "evancohen",
            "datetime": "Sep 23, 2016",
            "body": [],
            "type": "issue",
            "related_issue": "#378"
        },
        {
            "user_name": "decentralgabe",
            "datetime": "Nov 27, 2016",
            "body": [],
            "type": "pull",
            "related_issue": "#455"
        },
        {
            "user_name": "justbill2020",
            "datetime": "Dec 2, 2016",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Jan 3, 2017",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "justbill2020",
            "datetime": "Jan 3, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#280"
        },
        {
            "user_name": "justbill2020",
            "datetime": "Mar 17, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#609"
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/217",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "patrickjane",
            "datetime": "May 1, 2020",
            "body": "I want to use rhasspy to build a voice assistant which (for now) contains the following functionality:As you can see, not all of those tasks are handled by home assistant. In fact, I have existing implementations (in python) for Snips.ai for all those tasks.\nTo reuse them, I am developing some kind of plugin-based skill server, where I can hook on my existing code from snips (with, of course, slight modifications).Now, regarding rhasspy, I understand that it can have several endpoints for intent handling:From the documentation I understand that rhasspy will HTTP POST any intent which was detected to my server. This works. I can see the intent JSON coming in at my skill server. , it is unclear to me what kind of HTTP response is expected. From the documentation I can see it must be JSON, however I fail to find a detailed description of how this JSON should look like.If I return an empty JSON, rhasspy complains:  (I'll get this error as popup in the rhasspy browser, not in the rhasspy log files)\nThe log looks like:If I return , rhasspy complains:  (I'll get this error as popup in the rhasspy browser, not in the rhasspy log files)\nThe log looks like:From the documentation I can see that in case of outputting speech, this should be given:and in case of forwarding something (what?) to home assistant, this should be given:(and in this case: what is 'rest of input JSON'?)So, , what do I need to send back to rhasspy after my remote server has successfully handled some intent which was detected by rhasspy and send to my remote server?And what is the idea of the \"forward to home assistant\" feature? I mean if my remote server shall handle the intent, why forward anything else to home assistant? Is this meant to be some kind of light-wrapper for the HA-API in order to enable the remote server to easily generate HA events in addition to its very own intent handling?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "May 1, 2020",
            "body": "If your intents are already handled via Snips MQTT Hermes protocol then you won’t have to do much using the next version of Rhasspy (2.5) as it is completely compatible with the Hermes protocol already. It is even going to propose Snips NLU.For the last few months, Rhasspy has gone through an intense restructuring of its services for improved modularity based on the MQTT Hermes protocol.I think the official release of the 2.5 version is approaching rapidly.For more info, maybe this can help:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 1, 2020",
            "body": "Okay so youre saying that the \"remote server\" / HTTP based variant is going to be deprecated soon?\nWhen reading the docs, I rather had the impression that the snips/hermes interface is merely \"something rhasspy also does, but preferred way is home assistant/node red\".",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "May 1, 2020",
            "body": "I think the remote HTTP handler will go on via a separated Rhasspy service.As the Hermes MQTT protocol will be used as the underlying glue between all services, it might be simpler to interface directly with it instead of relying on an additional service just to forward intents and dialogue handling messages.If your intents are already handling Snips topics then they should be completely compatible with Rhasspy next version (2.5).How did you handle your skills with Snips? We’re you using snips-skill-server?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 1, 2020",
            "body": "Okay I see. I wanted to implement both connectors anyway (HTTP + hermes), since its not so much of an effort.Yes I was using the snips-skill-server previously, so basically I am trying to make a replacement for it since snips is dead.In the current version (2.4.x) I can see there is options for MQTT/snips/hermes already. Is there going to be a bigger change regarding this interface in the upcoming 2.5 release?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "May 2, 2020",
            "body": "Hi , the short answer is that the same intent JSON should be returned (in 2.4). My original idea was that an intent handler could alter the intent before it got passed to Home Assistant (maybe add some extra information).Going forward in 2.5, remote HTTP handling is fully supported. Like any other Hermes-compatible service,  listens for intents via MQTT and POSTs them to some HTTP endpoint. It only expects a JSON object back with an optional \"speech\" property (with a \"text\" sub-property).If you're using NodeRED, you have many choices in 2.5 to handle intents: directly via MQTT (Hermes protocol), via WebSocket ( or through the  bridge), or with the \"remote\" HTTP intent handling system. These can all be used simultaneously as well :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 2, 2020",
            "body": "Okay I see. Meanwhile I've switched to MQTT, and I am receiving the intents from rhasspy via . So I've dropped the HTTP approach.I have used node-red before with home assistant to do automations, however at some point I have dropped node-red and decided to just do everything in home assistant.\nI am a programmer for quite a long time, and I would probably always favor coding something fancy instead of clicking things together in node-red. I would probably never try to implement the dark-sky API within node-red. But that might be just me.Also I liked the way snips did it, in that you could pull existing skills from their store and just plug them into your system without much effort. This is why I started working on a skill-server replacement.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 3, 2020",
            "body": "So in 2.4 rhasspy does not yet listen on , right? So it currently only pushes intents to  but no further MQTT interaction I guess.\nSo for now I have my existing skills working with said skill server, which might look like this:If you think this project might be useful for rhasspy I'd be happy to put it on github, so far I guess its satisfying my own personal needs for the voice assistant.Idea of the skill server would be:I might add a little cli tool for this to handle skill installation & setup (same as we had with snips).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 7, 2020",
            "body": "\nOkay so meanwhile I actually came up with this:\nI'd be happy to work on some kind of skill-platform/marketplace thingy, if you guys be up for it.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "May 7, 2020",
            "body": "This is pretty neat I’d be even better if languages outside of python could be used for skills. Like executing a command line and using stdin/stdout to communicate over a simple JSON protocol. A simple JSON/YAML at skill root level with skill properties (name, description, author, intents to handle, command to execute, etc) maybe ?Just thinking ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 7, 2020",
            "body": "I think we have that simple JSON protocol already, which is hermes over MQTT. Introducing a similar transport-/language-agnostic protocol on top of it might not be  useful, since you could achieve that with something like node-red already I guess.I think it boils down a little on how the overall workflow of \"skills\" shall be, and if it is meant to be more for like developers and hackers, or more for the average user.\nWhat I mean is, that in Snips/Alexa/.../ one would usually not install the assistant and then start programming. The average user would merely just install the assistant, and pick up existing skills from some kind of skill marketplace. Still, developers/hackers are the ones who can/will provide/develop a plethora of skills, and from my point of view, it would be okay (for a , that is), to keep it in one technology stack.[edit] By the way, when it comes to sharing skills and installing skills from other developers, one half of it right now cannot be easily shared, which is the sentences & slots. Is there any idea/concept for this to enable easy sharing in the future of rhasspy?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "May 7, 2020",
            "body": "Fair enough ;)It boils down to how the sentences/slots are registered and forwarded to the ASR and NLU services. Maybe  could provide more insights on how Rhasspy 2.5 will handle the dataset.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "philtweir",
            "datetime": "May 29, 2020",
            "body": "Just to feed back,  (and others), I've been experimenting with your  and rhasspy 2.5, and have managed to proof-of-concept (very roughly) a dialogue-based countdown timer skill, and a skill for adding reactions to RocketChat's most recent Gnome notification (happy to share once tidied a little) - for keeping those modular, I find your hss-skill pattern works quite well, and can see a couple of other itches I'll likely to use it to scratch. However, keen to know if you or others have had any more thoughts on trajectory.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 30, 2020",
            "body": "I am using the  now for a while with rhasspy 2.4, and indeed it  work, at least as much as to replace my existing snips-based skills.\nI have a couple of skills on my account, which could serve as example implementations (, ,  and ).Yet,  is just a first draft, and I would be happy to augment and improve it further. Two things I would like to implement, but will need help from rhasspy developers:Some thoughts on 2):\nCurrently, this  be possible, since rhasspy's HTTP API contains endpoints for a) updating sentences and b) an endpoint to trigger training. However, I don't feel exactly good about having the  push sentencens into rhasspy like this. It would be any idea, however I'd like to hear rhasspy developer's opinion first on this, maybe there's a smarter way.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "koenvervloesem",
            "datetime": "May 30, 2020",
            "body": "Hi , impressive work! I have been thinking about the same functionality and implementing part of it too.Some remarks:Are you active ? I have discussed about these and other topics here:The result of my thoughts in the first forum post is a helper library for Rhasspy apps, . This is just a wrapper library around  to make it easy as possible to create Rhasspy apps. It's still a proof of concept, but already quite usable.It seems to me that your hss-server and hss-skill are tightly intercoupled. I'm not sure that's the best way to go forward. Ultimately a skill server should be able to install skills developed in various languages, as  . So that's why I'm not too fond of the idea to couple a skill server to an app library.But even with Python alone it would be better to make the architecture more flexible. There are a couple of initiatives to create libraries to develop Rhasspy apps ( is also working on a proof of concept for Rhasspy Hermes apps in AppDaemon) and it would probably better if we could share some parts of the API. Because one of Rhasspy's strong points is its flexibility in which services you can use with it, I think we should try to keep our options open for the creation and distribution of Rhasspy apps, so it's nice that there are various app platform implementations. But it would be good if we could share some resources.Another idea I have created a proof of concept for is running each Rhasspy app in a Docker container. You can see my thoughts about it in the third forum link mentioned above. Coupled with Mosquitto's access control list and a username and password for each app, we can precisely limit what an app is able to do. My goal is to work on this idea further, because I don't like the idea of apps being able to do what they want on my machine or my network. So an alternative \"skill server\" could just install Docker containers this way to add Rhasspy apps.These are just some ideas :-) Don't let this dissuade you from working on hss-server and hss-skill, I think there's still a lot to explore in this domain and having multiple implementations for Rhasspy is good.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 30, 2020",
            "body": "Well mainly rhasspy would need to support the following workflow:As I am still on rhasspy 2.4, I have no clue whether or not this already works.Will do.Agreed. All which I have written on this thread should be considered as support for rhasspy, and while I've already implemented a working skillserver, it should be merely a first draft, and subject to change. I would be happy to contribute, and agree to share resources.When I first started using rhasspy, I found that there is no real intent handling in place, other than publishing intents via MQTT, HTTP or to home assistant, all of which were not suitable for me.Nope, didn't even know a forum exists :DWell, I think both approaches have their pros and cons, and as I have stated earlier, it pretty much boils down to how rhasspy is meant to do intent handling. There is the option already to publish intents over MQTT, HTTP and send to home assistant. Lets ignore the HTTP stuff, then MQTT alone is  the language agnostic decoupling, since it would allow anyone to just hook on the MQTT message using their favourite language. It would still work with a running , since the server would just drop unknown intents.When I've started working on  (and the efforts made so far are really trivial, so starting from scratch again is not an issue for me) I had the idea of resembling the , which pretty much acted in the very same way.Some of the reasons for using a server-approach over standalone app runtimes were:So basically what I want to say is, if rhasspy decides to offer intent handling, but at the same time make it possible to use , then it would get a bit hard to bring all this together. As I said, you've got the decoupling via MQTT already, I see no real benefit of  decoupling within the skill-server, only to enable developers to implement skills in other programming languages.Especially when we're talking about installing skills from other developers, skill marketplace/ecosystem, I think all this might get really complicated, when it shall be possible to install skills written in arbitrary programming languages. Just think about all the stuff that might need to be set up, like dependencies, tools, libraries. Right now,  is going to create python venvs for each skill and then install the skill's dependencies upon installation. Similar tasks would be necessary for other programming languages as well. So unless you want to decouple completely - that is, provide no rhasspy tool/script to install skills, and just have the user/developer install their skills manually - this might get overcomplicated.\nI think other voice assistants also don't support arbitrary programming languages for their skill development.Then, you would need some sort of protocol between the skill server and the docker container. And this would essentially mean you're back to zero, as hermes  already. So either your docker containers which contain the skills just need to implement the hermes protocol , or we're talking about a second, non-standard protocol.However maybe the skill server  support two kinds of skills; docker based and python based, and docker based skills would have  with the skill server at all.But even then, the user would need to configure MQTT connection parameters for every docker container, which is pretty much what I wanted to avoid in the first place.No worries, all I want is to contribute to rhasspy's functionality. Although it is named , for me its more like a rhasspy-skill-server.\nI am a professional c++/nodejs/python developer, if you want me to contribute and just tell me what to implement, I'll go for it ;)BTW: you're using the term \"app\" for what I consider \"skill\", so in the above just read \"skill\" as \"app\" :)[edit] So maybe as some kind of rough requirements list for intent handling:(to be continued)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "koenvervloesem",
            "datetime": "May 30, 2020",
            "body": "This should already work on the Rhasspy 2.5 pre-release :-) Have a look at . That's why I was a bit puzzled why you would need a skill server for this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "philtweir",
            "datetime": "May 30, 2020",
            "body": "I can confirm that this flow seems to work for me with hss-server and Rhasspy Voltron - my understanding (from up the issue) was that that was where you were targetting  ?I'm not sure if this is quite what you mean, but I have added a couple of small tweaks to my local version of hss-server and hss-skill to add a  with same args as  but sends a  instead of the , with an intent-filter of only the current intent. That then comes back into the Skill's  method, and an  switch on the text gives separate flows for original and follow-up commands.Working example:Rhasspy seems to implement that fine, and afaiu the dialogue state handling parameters from the Hermes protocol are implemented (but haven't tried) - I have tested the \"no-matching-intent\" dialogue event too, and that can be picked up, for conversational-response misses. The main downside is that, as it still has to match the intent (even if filtered), the possible responses must be sentences for that intent in Rhasspy, just as the original command is.I do recall seeing suggestion in the forums of modally switching the STT for follow-up, which would be nice, but at least if there was some way of making an intent, or certain sentences that trigger it, only matchable on follow-up dialogue (so words like \"no\" and \"yes\" wouldn't technically be valid opening commands), the next step, of switching speech-to-text from e.g. PocketSphinx to DeepSpeech in follow-up to give greater freedom, would be a bonus.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "philtweir",
            "datetime": "May 30, 2020",
            "body": "On the language-independence as  , I was thinking that too - I can see your reservations  but if  is negotiable, there's one or two RPC options.IMO a simple option from a skill-maker's perspective (which should be an almost drop-in replacement from skill-maker-flow perspective) would be WAMP with Autobahn - I have used this on a number of projects for near-transparent RPC between languages in a Python-native-feeling way (it also has the bonus of supporting event subscription ootb). Happy to PoC that, if it would be a potential option. That said, having MQTT already there, there's maybe an argument for RPC over MQTT, but it those options don't seem nearly as mature as either RPyC or Autobahn.A second benefit of this is that it'll work fine with venv or dockerized processes (Python or otherwise), and not increase the code a skill-creator would write.To  's question about where a skill-server would fit - I think  's point about abstracting MQTT protocol interaction away is important. I probably wouldn't have bothered getting started with those if it wasn't just a case of \"fill in this \" and away you go, only a small papercut, but hss-server (or seemingly Rhasspy Hermes App) does address it.And of course the bullets  mentioned sound like things that, given the modular nature of Rhasspy, it would want to defer to a handler such as  or Rhasspy Hermes App (which I hadn't seen and haven't yet looked at properly!)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "philtweir",
            "datetime": "May 30, 2020",
            "body": "(and a language-independent RPC framework would avoid every language having to have a Hermes implementation as a library for skill-makers)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 30, 2020",
            "body": "Yeah, thats exactly the idea. Although I would have named it  :)I havent worked on this since 2.5 is not yet released.Thats what I mean with \"rhasspy needs to support this\". Doing full/plain new intent recognition for a follow up question is probably not more than a workaround I guess.\nMost likely, the follow up responses should also go into sentences.ini, maybe even a separate NLU model could be used for those purposes.See the above. I'm gonna check it out when 2.5 is released.I think my point was not so much about the protocol between skill-server and skill (which  with no issues be language agnostic, e.g. HTTP/JSON), but more about the dependencies and different handling for different languages. For example, node.js based skills would require , which in turn needs to find  on your system. C++ based skills might depend on some c libraries, would you fire off some  upon skill installation?\nUnless you call some generic  upon installation and pretty much leave all those issues to the skill-developers, I can't think of a viable solution that involves arbitrary technologies.So while its perfectly possible and fine to me to use a non-python RPC protocol, you would still have issues when installing the skill via  and later running the skill process.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 30, 2020",
            "body": "Maybe we should close this issue, and move the discussion to the forums? I think we have some really good ideas, and we should continue to discuss?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "philtweir",
            "datetime": "May 30, 2020",
            "body": "Fair! Like I say, tidying required :)Indeed - given an intent filter is part of the conversation response API (which Rhasspy implements, afaict, and is a start), it does seems like that approach is not inconsistent with Hermes, at least. However, it would make sense for Rhasspy to do some minimal implementation here - even just to allow marking sentences as ineligible for initial intent matching. Conversely, a potential use-case for full intent recognition (by specifying more than one, or no, intents in the filter) would be to ask a question that could switch path to a different skill.True, but perhaps its a question of level-of-abstraction - if the decision is not made at the protocol level, but potential skill-family helper classes could be made, then language-specific-functionality is not quite so baked in and encapsulated to installation/provisioning functionality (a simple Python install-class for JS might use nodeenv, for instance).Yes, I think this touches on some broader questions that would be great to get input from the Rhasspy architects on (as you'd suggested).Agreed - I think it's safe to say this has turned into solution development rather than issue resolution! If you want to post a link, we can jump over - would be keen to  in that loop - have to say, from my brief look, I like the decorator skill syntax of Rhasspy Hermes App - wondering how hard it might be to use both hss-server and Rhasspy Hermes App together ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 31, 2020",
            "body": "Okay so I have posted here: Currently I am working on a proper hermes dialog implementation, and I also had some idea for a low-cost marketplace-thingy, I'll see if I can get this up and running until tomorrow.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/5",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "JarbasAl",
            "datetime": "Nov 1, 2018",
            "body": "Hi, i think MycroftOS is a great idea!Since it is aimed at raspi3 for now, it would make sense to support some common mic arrays,Maybe also support some other hats that make sense for mycroftSuggestions:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Nov 1, 2018",
            "body": "You are definitely right! Buildroot should take care of all the drivers and OS configuration. The visuals should be handled by a skill I think.I already have a ReSpeaker 4-Mic array at hand, so that one is indeed on  but agree that at this point in time AIY and Matrix voice are the next two important ones.The new ReSpeaker arrays are on this list as well, but I believe they use the exact same driver, just some other OS configuration files. The only \"worry\" I have at the moment is to properly figure out at boot which one the user has. However that are future concerns.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Nov 3, 2018",
            "body": "ReSpeaker kernel drivers and OS configuration is merged. At this point we do not have configuration wizards yet, but later on we will have some sort of cornfiguration wizard where you could select it and it will be used. Similar as the picroft, but I do not want that to be done over the cli.In the future I would like to do that webbased. A small local webserver and easy setup of hardware and such.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Nov 1, 2018",
            "body": [],
            "type": "added",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/20",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "j1nx",
            "datetime": "Jan 30, 2019",
            "body": "MycroftOS is a voice controlled system, however some settings are way easier to configure via a webbased system. Hence Mycroft A.I. even has exactly this with; home.mycroft.aiI strongly believe MycroftOS should also have some sort of webbased backend where you can change stuff. Not only pairing and skill settings, but also hooks into the new to be released GUI part. So a bullit list of what I think in the end should be available via a browser connecting to MycroftOS.I believe most of the appraoches I have seen and linked above use npm/flask/etc. So believe this should be the way forward to minimise dependencies and extra packages. (Wifi-Wizard also uses it, so npm should be there rather sooner then later)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JarbasAl",
            "datetime": "Nov 18, 2020",
            "body": "quick note, i stopped maintaining personal-backend when selene was released, some more updated have meanwhile been made at i also package this as a skill, anyone can simply install it and not require backend i feel the settings etc should be essentially handled by the GUI a local frontend might also be served for headless devices if needed, this could happen at/integrate with mock-backendif there is interest i can move mock-backend under OVOS organization",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Jan 30, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Jan 30, 2019",
            "body": [],
            "type": "added this to the",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Jan 30, 2019",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Jan 30, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "j1nx",
            "datetime": "Oct 2, 2021",
            "body": [],
            "type": "moved this from",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/211",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "patrickjane",
            "datetime": "Apr 23, 2020",
            "body": "Hello,I am on a fresh install of 2.4.20 on a fresh install of raspbian buster, and I've just added credentials and configuration for the wavenet tts. However it always falls back to espeak.This is what I get in the logs:Not sure what could be wrong here?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Deca",
            "datetime": "Apr 25, 2020",
            "body": "Same error, I too I'm working on 2.4.20, seems that the tts.py can't import google tts client libraries",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "linuxlurak",
            "datetime": "Apr 25, 2020",
            "body": "Same here... seems google tts module is missing?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "Apr 25, 2020",
            "body": "Well at least when trying to install it by hand it says its already installed:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "linuxlurak",
            "datetime": "Apr 25, 2020",
            "body": "forgot to clarify, i'am on the rhasspy docker image.  if you are running rhasspy in a docker container too: did you log into this container?perhaps  can help? ;)Edit: Ah I see now, your created a python venv",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "Apr 25, 2020",
            "body": " nope I'm on a venv installation.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Romkabouter",
            "datetime": "Apr 25, 2020",
            "body": "I have got the same issue, I'll see if I can fix it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Romkabouter",
            "datetime": "Apr 25, 2020",
            "body": "I had not noticed it, because playing from cache works fine",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 8, 2020",
            "body": "Seems like there was a missing dependency. After  it still doesnt work, however the error is different in the logs:Continuing to investigate ...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 8, 2020",
            "body": "Okay, found the issue. I was giving  as voice (this is shown at the google website), but rhasspy adds an additional  in front of it, making an invalid argument. I've fixed it, and now wavenet TTS works. I didn't follow the docs correctly on this parts.This is the relevant part in profile.json:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Romkabouter",
            "datetime": "May 8, 2020",
            "body": "Good find, so I think the docker should be updated",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 8, 2020",
            "body": "I am not using docker, I did the venv installation.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Deca",
            "datetime": "May 8, 2020",
            "body": "I logged into the docker image, installed pip, the google-cloud-texttospeech library and then restarted rhasspy but I still have the same error\n\nIs there a proper way to install that python libraries in the docker image?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 8, 2020",
            "body": "Okay so I remember that two weeks ago I already tried to fix this, and installed some google package, but still it didnt work, so I stopped investigating. Maybe theres a second package missing.Lets compare:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Deca",
            "datetime": "May 8, 2020",
            "body": "google-api-core           1.17.0\ngoogle-auth               1.14.2\ngoogle-cloud-speech       1.3.2\ngoogle-cloud-texttospeech 1.0.1\ngoogleapis-common-protos  1.51.0Pretty the same for my docker image.\nThe google-cloud-speech library was missing and I manually installed it but isn't relevant with the issue",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "patrickjane",
            "datetime": "May 13, 2020",
            "body": "Okay so I just did a complete new install, and for me it was fixed after manually installing .So after the initial installation I had:In this,  was missing ( ).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "erikcoin",
            "datetime": "May 28, 2020",
            "body": "I am using the hassio addon and have the same problems with wavenet tts.GoogleWaveNetSentenceSpeaker: Falling back to EspeakSentenceSpeaker\n[ERROR:87465816] GoogleWaveNetSentenceSpeaker: speak\nTraceback (most recent call last):\nFile \"/usr/share/rhasspy/rhasspy/tts.py\", line 643, in in_ready\nself.wav_data = self.speak(message.sentence, voice, language_code)\nFile \"/usr/share/rhasspy/rhasspy/tts.py\", line 742, in speak\nfrom google.cloud import texttospeech\nImportError: cannot import name 'texttospeech' from 'google.cloud' (unknown location)Is there a solution for the addon too?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Romkabouter",
            "datetime": "May 29, 2020",
            "body": "If have found the cause of this issue.\nBack on 7th of december this commit:\nIt sets google from true to false, causing to skip the install of the Google TTS.\nI have created a PR for this: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Romkabouter",
            "datetime": "Jun 5, 2020",
            "body": "PR is merged, can you retry? I do not know if the docker image is already released however.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/209",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "litinoveweedle",
            "datetime": "Apr 21, 2020",
            "body": "When one Rhasspy (client) is using another Rhasspy (server) via HTTP API for STT conversion and no sentence is pronounced after wake word, so webrtcvad will record empty audio, then API error 400 is returned by Rhasspy server (which is probably OK), but it is also TTS by client Rhasspy back to user (which is probably not OK as it is just internal error):It would be nice to handle this and similar exception and for example play just error sounds, when anything else than HTTP 200 is returned by remote STT API. Rhasspy administrator could still see returned error codes in log, if troubleshooting is required.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/199",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "vabene1111",
            "datetime": "Apr 2, 2020",
            "body": "I dont know if this is a bug or just some kind of usage error but i have tried both pocketsphyx and prorcupine as wake word providers. Both work and have good recognition rates but the recognition after the wake word is very bad compared to the recognition when clicking \"hold to record\".When using hold to record i feel like the recognition rate is somewhere around 98%, with the wake word and a lot of trial and error for the right pronunciation and timing between waking and speaking its somwhere around 20-30%. I have been trying  and  as commands.I am using v2.4.19 with the docker install and a ReSpeaker 2-Mic board as a microphone (but i dont thing thats the cause since the manual record works great).The settings are all default (except of course that the wake word detection was turned on). I have trained multiple times, restarted, and cleared the training chache and retrained.Any ideas why this difference could occur ?here the settings from the advanced tab (should be only the non default ones if i understand correctly)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Apr 10, 2020",
            "body": "There should be almost no difference unless the feedback sounds are bleeding over into the recorded voice command. If you record a command and click the play back button in the web UI, do you hear the beeps?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vabene1111",
            "datetime": "Apr 10, 2020",
            "body": "so until now i did not have an output device configured. I attatched a headset for testing.When recording, stopping recording or playing the voice command no sounds play. When saying the wake word it does make two sounds, one when starting recording and one when ending (at least that what i think since the icon on the top left changes as well between the beeps).Still the detection is basically useless when using the wake word and almost perfect when triggering the recording manually.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markusappel",
            "datetime": "Apr 15, 2020",
            "body": "Started playing today and had the same problem. When listening to the last command in the web UI after the wake word (thanks for the hint ), I realized that the first split second of the command was cut off and the first word could not be recognized. Seems like webrtcvad needed some tuning ... adding this to the profile fixed it for me:(Although something was weird about the  setting: somewhere around 2-3 the behaviour jumped from \"cutting the first command word\" to \"keep all the silence between wake word and first command word\")",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vabene1111",
            "datetime": "Apr 16, 2020",
            "body": "Ok that is definitely a huge improvement to how it was before! It feels like everything is a little slower now but that might be something else.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mic92",
            "datetime": "May 26, 2020",
            "body": "I can confirm that  is an improvement:Before when I would say:  it would only recognize . Now it recognizes the whole sentence.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mic92",
            "datetime": "May 26, 2020",
            "body": "Should I make a PR to change the defaults?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Mic92",
            "datetime": "Jun 7, 2020",
            "body": "I don't see this problem anymore with rhasspy 2.5 from here: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Apr 10, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/82",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "limitless-code",
            "datetime": "Aug 26, 2021",
            "body": "Hardware: RPi4 2GB, 32GB sd card, HDMI out to TV for sound, PS3 eye USB for mic.\nWake word: Hey JarvisNot sure if this is the correct place (mycroft.ai forum maybe a better place?) or not but didn't have this behaviour before on picroft.Firstly it doesn't respond to the spoken wake word \"hey jarvis\", until text input is processed via mycroft-cli-client or home assistant sends a message via the notification integration for mycroft, but separate to this it also goes into a wake word loop.I enter 'play new retro wave' in the mycroft-cli-client and it finds some good tunes and starts playing, then as below it keeps detecting the wake word even though nothing verbal is being said, just music.I'd say the mic is overly sensitive perhaps, but it doesn't respond to when I say 'hey jarvis', it only \"detects the wake word\" when there is continuous loud sound playing from it's self. It will also detect the wake word when there is silence.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 26, 2021",
            "body": "In voice.log it shows below. Could not found find model for hey jarvis on precise. and load pocketsphinx instead which is not as accurate?  i shall change the wake word to 'hey mycroft' and test again but I was having similar issues with that wake word too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ChanceNCounter",
            "datetime": "Aug 26, 2021",
            "body": "Indeed, inappropriate PocketSphinx activations are no surprise. Sometimes I drive with PocketSphinx running on my phone, and it goes off when I hit a bump!I think that's probably a config problem, but I don't mess with Precise. ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JarbasAl",
            "datetime": "Aug 26, 2021",
            "body": "can you share your config? my guess is that its using the old plugins pre migration to OVOS, the fallback to pocketsphinx is hardcoded from mycroft-core, and only works for \"hey mycroft\" since the phonemes etc are undefined for other words (our plugin should accout for this, but core wont use the plugin in fallback)slightly more concerning and weird is the logs saying STT failed to load before the wake word failures, it def looks like a bad config (prolly my fault with bad docs) can you share your .conf ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 26, 2021",
            "body": "Attached, have also supplied logs. I have redacted some sensitive bits in the logs.\nI did try to swap Hey Jarvis to Hey Mycroft but my sd card got corrupted after  'sudo reboot'\nTried a different sd card but the exact same thing happens after 'sudo reboot'It tries to start then RPI4 green led flashes 4 times and fails to boot. I don't think both sd cards could have a problem, seems too consistent?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JarbasAl",
            "datetime": "Aug 26, 2021",
            "body": "does it work if you replace \"jarbas_precise_ww_plug\" with \"ovos-ww-plugin-precise\" ?i dont see stt changed anywhere, did you previously select the local backend (it messes with user config)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 26, 2021",
            "body": "I tried the replace suggestion but doesn't improve things. I clicked on mycroft backend and paired via setup screen (with hey jarvis wake word) on first boot after setting up Wifi, nothing else was touched. Still does not respond to \"Hey Jarvis\" but also now does not do TTS. I did have to do 'sudo systemctl stop mycroft' and 'sudo systemctl start mycroft', not sure if this is the correct way, as cannot reboot the pi without it corrupting the sd card currently.Same looping wake word detection happening.It still plays the video and audio of the video though after typing in cli \"play new retrowave\"latest logs and configs attached\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 27, 2021",
            "body": "Attached are logs and configs from another fresh install with the Mycroft Backend but paired with 'Hey Mycroft' instead of 'Hey Jarvis'  for comparison. Hey Mycroft behaves much better. When music/ video is playing wake word detection happens every few minutes compared to Hey Jarvis which was every few seconds. It still doesn't respond to spoken \"hey Mycroft\" as it did on picroft but I suspect that's the Mic settings.re: rpi4 bricking itself after 'sudo reboot' that seems to be OK now after pressing the SD card holder on the RPI4 in more so the all the PINs are in contact with the SD card.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 27, 2021",
            "body": "A quick note/ query re: video playback. When playing a video with music with a still image everything works as it should. If it plays a full moving video, the audio and video stutters. It is running via a HDMI port on a big screen. Not sure if VLC (I presume it's using VLC) has hardware acceleration enabled or not. I know in the past the 64 bit kernel (in general and not specific to OVOS) did not have this feature configured/ implemented for VLC. I see the kernel for OVOS is aarch64.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "limitless-code",
            "datetime": "Aug 26, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/193",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "jwillmer",
            "datetime": "Mar 12, 2020",
            "body": "It would be most useful if we can train the system to differentiate who said something. Depending on the person we could then start or ignore a command. For instance:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Mar 28, 2020",
            "body": "Kaldi apparently supports this through something called \"x-vectors\". I'd be interested to add this, but I haven't had time to look into how to do a basic \"WAV files + labels\" training for classification.BTW, the kids activating Rhasspy are why I can't really use it at home much :/",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "Mar 28, 2020",
            "body": "I’ve tested Kaldi « i-vectors » for speaker identification but it needs a LOT of training data to approach a satisfactory error rate (a few hundred short WAVs per user is apparently the minimum).The best I got with around 5 samples per user was a 24% error rate following this :\nThe « x-vectors » add some improvements but they still needs like hundreds of samples per user to perform correctly (like 7-8% ER)It would be pretty awesome to achieve speaker identification though ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Mar 28, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/synesthesiam/rhasspy/issues/181",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "basnijholt",
            "datetime": "Feb 16, 2020",
            "body": "I am using a HassIO installed Rhasspy 2.4.18 on a RPi 4.I am getting the error consistently:I have removed my entire  and reinstalled the add-on, however, the error returns. All of my relevant config files are .I have found  where  suggested , however, that didn't solve it for me.Another person  the same problem. There,  removed the  folder and the problem was solved. This also didn't help in my case. the same problem. There \"deleting the settings\" is suggested, however, it's unclear to me which settings should be removed.Several others seem to have the problem, judging from .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Feb 24, 2020",
            "body": "There must be an error during training then, since the HCLG.fst is not being produced. This could also be caused if you have \"open transcription\" checked in your settings.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "basnijholt",
            "datetime": "Feb 27, 2020",
            "body": "Thanks for your reply,  is turned off.Considering many others experience this problem, I am not sure whether it's due to my sentences.Would you know how to debug this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mathquis",
            "datetime": "Feb 27, 2020",
            "body": "I often had this error when Rhasspy had to generate pronunciations for unknown words when using Kaldi.\nWhat I did to fix it is download the profile files again and re-train.\nIf it doesn't work, try looking at your  file to check if everything is ok (does it have a new line at the end, etc.).\nHope this helps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "basnijholt",
            "datetime": "Feb 27, 2020",
            "body": ", indeed, thanks a lot. The culprit was my  (see below). I removed them and now it works again., shouldn't there be a better error message that points people in the right direction>?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Apr 10, 2020",
            "body": "The custom words look fine to me. Was there anything in the log from Kaldi about them?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "basnijholt",
            "datetime": "Feb 16, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#172"
        },
        {
            "user_name": "synesthesiam",
            "datetime": "Apr 10, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/80",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "russchidy",
            "datetime": "May 13, 2022",
            "body": "Hi guys can anyone help with this deploy error in the second stage;Waiting for app to start...\nFAILED\nTIP: use 'cf logs watson-voice-bot-20220513154240582 --recent' for more information\nJob execution ended at Fri May 13 15:53:47 UTC 2022full text is as per below:Preparing to start the job...\nRunning on public worker: au-syd-tok05-backup\nJob execution started at Fri May 13 15:44:33 UTC 2022\nPipeline image: latest\nPreparing the build artifacts...\ncf login -a \"\" -u apikey -p \"****\" -o \"\" -s \"dev\"\nAPI endpoint: \nAuthenticating...\nOKTargeted org Targeted space devAPI endpoint:    (API version: 2.180.0)\nUser:           \nOrg:            \nSpace:          dev\nCreating service instance wvb-watson-assistant in org  / space dev as ...\nOKService wvb-watson-assistant already exists\nCreating service instance wvb-text-to-speech in org  / space dev as ...\nOKService wvb-text-to-speech already exists\nCreating service instance wvb-speech-to-text in org  / space dev as ...\nOKService wvb-speech-to-text already exists\nPushing from manifest to org  / space dev as ...\nUsing manifest file /workspace/4816dbc4-b309-4a7e-a63e-06b97f68932b/manifest.yml\nGetting app info...\nCreating app with these attributes...Creating app watson-voice-bot-20220513154240582...\nMapping routes...\nBinding services...\nComparing local files to remote cache...\nPackaging files to upload...\nUploading files...0 B / 1.60 MiB    0.00%\n192.00 KiB / 1.60 MiB   11.70%\n512.00 KiB / 1.60 MiB   31.20%\n1.12 MiB / 1.60 MiB   70.19%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00% 2sWaiting for API to complete processing files...Staging app and tracing logs...\nDownloading python_buildpack...\nDownloaded python_buildpack (4.5M)\nCell 33a42869-b23a-465a-afc6-9f59595e2625 creating container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nCell 33a42869-b23a-465a-afc6-9f59595e2625 successfully created container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nDownloading app package...\nDownloaded app package (1.6M)\n-----> Python Buildpack version 1.7.53\n-----> Supplying Python\n-----> Installing python 3.10.4\nDownload [https://buildpacks.cloudfoundry.org/dependencies/python/python_3.10.4_linux_x64_cflinuxfs3_e053ca78.tgz]\nUsing python's pip module\n-----> Running Pip Install\nCollecting ibm-watson==5.2.3\nDownloading ibm-watson-5.2.3.tar.gz (406 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 406.9/406.9 KB 6.8 MB/s eta 0:00:00\nInstalling build dependencies: started\nInstalling build dependencies: finished with status 'done'\nGetting requirements to build wheel: started\nGetting requirements to build wheel: finished with status 'done'\nPreparing metadata (pyproject.toml): started\nPreparing metadata (pyproject.toml): finished with status 'done'\nCollecting Flask==1.1.1\nDownloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.5/94.5 KB 4.8 MB/s eta 0:00:00\nCollecting python-dotenv==0.10.3\nDownloading python_dotenv-0.10.3-py2.py3-none-any.whl (16 kB)\nCollecting flask-cors==3.0.9\nDownloading Flask_Cors-3.0.9-py2.py3-none-any.whl (14 kB)\nCollecting flask-socketio==4.2.1\nDownloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\nCollecting requests<3.0,>=2.0\nDownloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 KB 301.0 kB/s eta 0:00:00\nCollecting websocket-client==1.1.0\nDownloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.7/68.7 KB 1.8 MB/s eta 0:00:00\nCollecting ibm-cloud-sdk-core==3.*,>=3.3.6\nDownloading ibm-cloud-sdk-core-3.15.1.tar.gz (50 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 KB 667.7 kB/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'done'\nCollecting python-dateutil>=2.5.3\nDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 5.5 MB/s eta 0:00:00\nCollecting Werkzeug>=0.15\nDownloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.9/224.9 KB 5.7 MB/s eta 0:00:00\nCollecting click>=5.1\nDownloading click-8.1.3-py3-none-any.whl (96 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 3.1 MB/s eta 0:00:00\nCollecting Jinja2>=2.10.1\nDownloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 6.1 MB/s eta 0:00:00\nCollecting itsdangerous>=0.24\nDownloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting Six\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting python-socketio>=4.3.0\nDownloading python_socketio-5.6.0-py3-none-any.whl (56 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 KB 3.6 MB/s eta 0:00:00\nCollecting urllib3<2.0.0,>=1.26.0\nDownloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.0/139.0 KB 7.6 MB/s eta 0:00:00\nCollecting PyJWT<3.0.0,>=2.0.1\nDownloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\nCollecting MarkupSafe>=2.0\nDownloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nCollecting bidict>=0.21.0\nDownloading bidict-0.22.0-py3-none-any.whl (36 kB)\nCollecting python-engineio>=4.3.0\nDownloading python_engineio-4.3.2-py3-none-any.whl (52 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 KB 1.7 MB/s eta 0:00:00\nCollecting charset-normalizer~=2.0.0\nDownloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting certifi>=2017.4.17\nDownloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.2/149.2 KB 3.7 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\nDownloading idna-3.3-py3-none-any.whl (61 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 4.4 MB/s eta 0:00:00\nUsing legacy 'setup.py install' for ibm-cloud-sdk-core, since package 'wheel' is not installed.\nBuilding wheels for collected packages: ibm-watson\nBuilding wheel for ibm-watson (pyproject.toml): started\nBuilding wheel for ibm-watson (pyproject.toml): finished with status 'done'\nCreated wheel for ibm-watson: filename=ibm_watson-5.2.3-py3-none-any.whl size=403321 sha256=6293495beb9cb2f15a3c774847508825066fe14cc5bea6220677e49da197392f\nStored in directory: /tmp/cache/final/pip_cache/pip/wheels/56/24/2f/1622dfa6e36d96d4f3df7b335822b80fa26ef2b8e219dab52f\nSuccessfully built ibm-watson\nInstalling collected packages: python-dotenv, certifi, Werkzeug, websocket-client, urllib3, Six, python-engineio, PyJWT, MarkupSafe, itsdangerous, idna, click, charset-normalizer, bidict, requests, python-socketio, python-dateutil, Jinja2, ibm-cloud-sdk-core, Flask, ibm-watson, flask-socketio, flask-cors\nWARNING: The script dotenv is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: The script normalizer is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nRunning setup.py install for ibm-cloud-sdk-core: started\nRunning setup.py install for ibm-cloud-sdk-core: finished with status 'done'\nWARNING: The script flask is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed Flask-1.1.1 Jinja2-3.1.2 MarkupSafe-2.1.1 PyJWT-2.4.0 Six-1.16.0 Werkzeug-2.1.2 bidict-0.22.0 certifi-2021.10.8 charset-normalizer-2.0.12 click-8.1.3 flask-cors-3.0.9 flask-socketio-4.2.1 ibm-cloud-sdk-core-3.15.1 ibm-watson-5.2.3 idna-3.3 itsdangerous-2.1.2 python-dateutil-2.8.2 python-dotenv-0.10.3 python-engineio-4.3.2 python-socketio-5.6.0 requests-2.27.1 urllib3-1.26.9 websocket-client-1.1.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (65M)\nUploaded droplet (66.8M)\nUploading complete\nCell 33a42869-b23a-465a-afc6-9f59595e2625 stopping instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nCell 33a42869-b23a-465a-afc6-9f59595e2625 destroying container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6Waiting for app to start...\nFAILED\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-20220513154240582 --recent' for more information\nJob execution ended at Fri May 13 15:53:47 UTC 2022Finished: FAILED",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/babysor/MockingBird/issues/20",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 19, 2021",
            "body": "（作者借楼编辑ing\n社区视频教程：\n奶糖 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 19, 2021",
            "body": "再分享 ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 19, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 19, 2021",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 19, 2021",
            "body": "这。。看起来你都没train起来synthesizer啊",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhangji261",
            "datetime": "Aug 19, 2021",
            "body": "同求，比如数据集在哪里下载",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XiuChen-Liu",
            "datetime": "Aug 19, 2021",
            "body": " closed裡面有同樣的問題，有放下載連結",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 20, 2021",
            "body": "E:\\data\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train   数据集解压路径       这一步synthesizer_preprocess_audio.py有问题吗\nC:\\Users\\Administrator\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main>\npython synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh\\aidatatang_200zh\nArguments:\ndatasets_root:   E:\\data\\aidatatang_200zh\\aidatatang_200zh\nout_dir:         E:\\data\\aidatatang_200zh\\aidatatang_200zh\\SV2TTS\\synthesizer\nn_processes:     None\nskip_existing:   False\nhparams:\nno_alignments:   False\ndataset:         aidatatang_200zhUsing data from:\nE:\\data\\aidatatang_200zh\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"synthesizer_preprocess_audio.py\", line 63, in \npreprocess_dataset(**vars(args))\nFile \"C:\\Users\\Administrator\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main\\synthesizer\\preprocess.py\", line 32, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 20, 2021",
            "body": " 不用多一层",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 20, 2021",
            "body": "python synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh 不用多一层\n解决了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 20, 2021",
            "body": "python synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh 不用多一层\n@解决了大佬",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 22, 2021",
            "body": "这步也太慢了。。。\nC:\\Users\\lxd\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main>python synthesizer_train.py mandarin D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\nArguments:\nrun_id:          mandarin\nsyn_dir:         D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\nmodels_dir:      synthesizer/saved_models/\nsave_every:      1000\nbackup_every:    25000\nforce_restart:   False\nhparams:Checkpoint path: synthesizer\\saved_models\\mandarin\\mandarin.pt\nLoading training data from: D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\train.txt\nUsing model: Tacotron\nUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 30.872MLoading weights at synthesizer\\saved_models\\mandarin\\mandarin.pt\nTacotron weights loaded from step 0\nUsing inputs from:\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\train.txt\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\mels\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\embeds\nFound 122482 samples\n+----------------+------------+---------------+------------------+\n| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n+----------------+------------+---------------+------------------+\n|   20k Steps    |     12     |     0.001     |        2         |\n+----------------+------------+---------------+------------------+{| Epoch: 1/2 (500/10207) | Loss: 0.9025 | 0.065 steps/s | Step: 0k | }Input at step 500: wo3 yao4 gei3 wang2 ming2 da3 dian4 hua4~__________________________________________________________\n{| Epoch: 1/2 (1000/10207) | Loss: 0.8266 | 0.071 steps/s | Step: 1k | }Input at step 1000: na4 me wo3 jiu4 chong2 xin1 ren4 shi2 ni3~______________________________________________________________\n{| Epoch: 1/2 (1500/10207) | Loss: 0.7602 | 0.074 steps/s | Step: 1k | }Input at step 1500: mei3 tian1 dou1 na4 me wan3 shui4 jiao4~___________________________________\n{| Epoch: 1/2 (2000/10207) | Loss: 0.7415 | 0.075 steps/s | Step: 2k | }Input at step 2000: da3 dian4 hua4 gei3 deng4 han4 ling2~_________________________________________________________________________\n{| Epoch: 1/2 (2500/10207) | Loss: 0.6921 | 0.068 steps/s | Step: 2k | }Input at step 2500: zhen1 xiang4 yong3 yuan3 zhi3 you3 yi2 ge4~___________________________________\n{| Epoch: 1/2 (3000/10207) | Loss: 0.6741 | 0.072 steps/s | Step: 3k | }Input at step 3000: xia4 men2 wai4 guo2 yu3 xue2 xiao4 chu1 er4 nian2 ji2 chen2 xiao3 qi2 jia1 de zhu4 zhi3~\n{| Epoch: 1/2 (3500/10207) | Loss: 0.6499 | 0.070 steps/s | Step: 3k | }Input at step 3500: ru2 guo3 wo3 he2 ni3 zai4 yi4 qi3~_______________________________________________________\n{| Epoch: 1/2 (4000/10207) | Loss: 0.6679 | 0.073 steps/s | Step: 4k | }Input at step 4000: fu4 jin4 de ping2 an1 yin2 hang2~_________________________________\n{| Epoch: 1/2 (4500/10207) | Loss: 0.6349 | 0.069 steps/s | Step: 4k | }Input at step 4500: ming2 zi4 shi4 hui3 guo4 cheng2 nuo4 shu1~_____________________________________________________\n{| Epoch: 1/2 (5000/10207) | Loss: 0.6392 | 0.073 steps/s | Step: 5k | }Input at step 5000: wo3 shen2 me shi2 hou4 cai2 neng2 chong1 man3 dian4~_______________________\n{| Epoch: 1/2 (5500/10207) | Loss: 0.6293 | 0.073 steps/s | Step: 5k | }Input at step 5500: wo3 da3 ni3 hao3 bu4 hao3 ma ge2 shi4 chong2 fu4~___________\n{| Epoch: 1/2 (6000/10207) | Loss: 0.6715 | 0.077 steps/s | Step: 6k | }Input at step 6000: ci3 ji4 hao3 wu2 liao2 da3 yi1 dian4 ying3 ming2~___________________________________\n{| Epoch: 1/2 (6500/10207) | Loss: 0.6446 | 0.075 steps/s | Step: 6k | }Input at step 6500: wo3 gei3 ni3 fa1 de ni3 shou1 dao4 le ma~___________________________________________________________\n{| Epoch: 1/2 (7000/10207) | Loss: 0.6022 | 0.068 steps/s | Step: 7k | }Input at step 7000: ning4 que1 wu2 lan4 zhi3 wei4 yi3 hou4 de du2 yi1 wu2 er4~________________________\n{| Epoch: 1/2 (7500/10207) | Loss: 0.6178 | 0.067 steps/s | Step: 7k | }Input at step 7500: mei2 you3 wang3 luo4 ni3 hai2 hui4 liao2 tian1 ma~______________________\n{| Epoch: 1/2 (8000/10207) | Loss: 0.6041 | 0.068 steps/s | Step: 8k | }Input at step 8000: wo3 bu4 fa1 le wo3 yao4 shui4 jiao4 le~____________________________________________________________________________________________\n{| Epoch: 1/2 (8500/10207) | Loss: 0.6078 | 0.072 steps/s | Step: 8k | }Input at step 8500: ni3 cai1 lai2 cai1 qu4 ye3 cai1 bu4 ming2 bai2~____________________________________________________\n{| Epoch: 1/2 (9000/10207) | Loss: 0.6055 | 0.072 steps/s | Step: 9k | }Input at step 9000: ni3 wen4 le wo3 tou2 dou1 da4 le~_______________________________________\n{| Epoch: 1/2 (9500/10207) | Loss: 0.5816 | 0.069 steps/s | Step: 9k | }Input at step 9500: xia4 ban1 mei2 you3 mei2 chu1 qu4 guang4~_______________________________________\n{| Epoch: 1/2 (10000/10207) | Loss: 0.5664 | 0.068 steps/s | Step: 10k | }Input at step 10000: ni3 jin1 tian1 bu2 shi4 bu4 shang4 ban1 ma~__________________________________________________\n{| Epoch: 1/2 (10207/10207) | Loss: 0.5879 | 0.071 steps/s | Step: 10k | }\n{| Epoch: 2/2 (293/10207) | Loss: 0.5840 | 0.070 steps/s | Step: 10k | }Input at step 10500: ai4 qing2 xiao3 shuo1 ma2 que4 gao3 ding4 hua1 mei3 nan2~_________________________________\n{| Epoch: 2/2 (322/10207) | Loss: 0.5892 | 0.070 steps/s | Step: 10k | }",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "miven",
            "datetime": "Aug 23, 2021",
            "body": " 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 23, 2021",
            "body": "...GPU，怎么配置显卡全局配置了呀。。",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dukechain2333",
            "datetime": "Aug 23, 2021",
            "body": "Arguments:\ndatasets_root:   D:\\Data\\aidatatang_200zh\nout_dir:         D:\\Data\\aidatatang_200zh\\SV2TTS\\synthesizer\nn_processes:     None\nskip_existing:   False\nhparams:\nno_alignments:   False\ndataset:         aidatatang_200zhUsing data from:\nD:\\Data\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"synthesizer_preprocess_audio.py\", line 63, in \npreprocess_dataset(**vars(args))\nFile \"D:\\Realtime-Voice-Clone-Chinese\\synthesizer\\preprocess.py\", line 32, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dukechain2333",
            "datetime": "Aug 23, 2021",
            "body": "路径好像没啥问题啊",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dukechain2333",
            "datetime": "Aug 23, 2021",
            "body": "\ntrian里是长这样的嘛",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yuzd",
            "datetime": "Aug 24, 2021",
            "body": "有群吗 一起交流下怎么跑",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 24, 2021",
            "body": "\n7天有效",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hexiosr",
            "datetime": "Aug 31, 2021",
            "body": "二维码过期了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "oceanarium",
            "datetime": "Aug 31, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "josh-zhu",
            "datetime": "Sep 8, 2021",
            "body": "群二维码过期了，求更",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JackChow6",
            "datetime": "Sep 12, 2021",
            "body": "二维码过期了，求更",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 12, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 12, 2021",
            "body": "见上",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JackChow6",
            "datetime": "Sep 12, 2021",
            "body": "谢谢你",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chloe5685",
            "datetime": "Sep 19, 2021",
            "body": "二维码失效了呜呜呜",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 19, 2021",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jan 24, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pjun463",
            "datetime": "Feb 4, 2022",
            "body": "更新一下二维码谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pjun463",
            "datetime": "Feb 4, 2022",
            "body": "D:\\M\\MockingBird-main>python pre.py D:\\数据集 -d aidatatang_200zh -n 7\nUsing data from:\nD:\\数据集\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"D:\\M\\MockingBird-main\\pre.py\", line 74, in \npreprocess_dataset(**vars(args))\nFile \"D:\\M\\MockingBird-main\\synthesizer\\preprocess.py\", line 45, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError\n有大佬帮帮我吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pjun463",
            "datetime": "Feb 5, 2022",
            "body": "更新一下二维码谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fengxiangyun",
            "datetime": "Feb 7, 2022",
            "body": "+1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hentai-hf",
            "datetime": "Feb 10, 2022",
            "body": "在输入音频后出现了这个报错……Feel free to add your own. You can still use the toolbox by recording samples yourself.\nTraceback (most recent call last):\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\toolbox__.py\", line 103, in \nfunc = lambda: self.load_from_browser(self.ui.browse_file())\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\toolbox__.py\", line 170, in load_from_browser\nwav = Synthesizer.load_preprocess_wav(fpath)\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\synthesizer\\inference.py\", line 146, in load_preprocess_wav\nwav = librosa.load(str(fpath), hparams.sample_rate)[0]\nTypeError: load() takes 1 positional argument but 2 were given",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hentai-hf",
            "datetime": "Feb 10, 2022",
            "body": "已解决\n在命令的命令输入pip install librosa==0.8.1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ptilopsisG",
            "datetime": "Feb 11, 2022",
            "body": "请问可以更新一下二维码吗，谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gdfshzh",
            "datetime": "Feb 12, 2022",
            "body": "各位大佬！！帮我看看这是什么错误啊，，我已经把模板放入相应的文件夹里了可是仍然不行额D:\\迅雷下载\\MockingBird-main>python demo_toolbox.py\nArguments:\ndatasets_root: None\nenc_models_dir: encoder\\saved_models\nsyn_models_dir: synthesizer\\saved_models\nvoc_models_dir: vocoder\\saved_models\ncpu: False\nseed: None\nno_mp3_support: FalseWarning: you did not pass a root directory for datasets as argument.\nThe recognized datasets are:\nLibriSpeech/dev-clean\nLibriSpeech/dev-other\nLibriSpeech/test-clean\nLibriSpeech/test-other\nLibriSpeech/train-clean-100\nLibriSpeech/train-clean-360\nLibriSpeech/train-other-500\nLibriTTS/dev-clean\nLibriTTS/dev-other\nLibriTTS/test-clean\nLibriTTS/test-other\nLibriTTS/train-clean-100\nLibriTTS/train-clean-360\nLibriTTS/train-other-500\nLJSpeech-1.1\nVoxCeleb1/wav\nVoxCeleb1/test_wav\nVoxCeleb2/dev/aac\nVoxCeleb2/test/aac\nVCTK-Corpus/wav48\naidatatang_200zh/corpus/dev\naidatatang_200zh/corpus/test\naishell3/test/wav\nmagicdata/train\nFeel free to add your own. You can still use the toolbox by recording samples yourself.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SeedKunY",
            "datetime": "Feb 12, 2022",
            "body": "二维码需要更新",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "luoyudong",
            "datetime": "Mar 2, 2022",
            "body": "求个新的群二维码",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "QuellaMC",
            "datetime": "Mar 6, 2022",
            "body": "大佬，我requirements安装没报错，但是运行程序时会报错:\nTraceback (most recent call last):\nFile \"E:\\MockingBird\\demo_toolbox.py\", line 2, in \nfrom toolbox import Toolbox\nFile \"E:\\MockingBird\\toolbox__.py\", line 6, in \nimport ppg_extractor as extractor\nFile \"E:\\MockingBird\\ppg_extractor__.py\", line 6, in \nfrom .frontend import DefaultFrontend\nFile \"E:\\MockingBird\\ppg_extractor\\frontend.py\", line 5, in \nfrom torch_complex.tensor import ComplexTensor\nModuleNotFoundError: No module named 'torch_complex'",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "luoyudong",
            "datetime": "Mar 7, 2022",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "1135126802",
            "datetime": "Mar 26, 2022",
            "body": "大佬们  问个问题  我自己的数据太小了  中途更换别的数据集进行训练  但是出现了这样的错误代码\nwarnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\nTraceback (most recent call last):\nFile \"E:\\数据集制作\\MockingBird-main\\synthesizer_train.py\", line 37, in \ntrain(**vars(args))\nFile \"E:\\数据集制作\\MockingBird-main\\synthesizer\\train.py\", line 208, in train\noptimizer.step()\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 88, in wrapper\nreturn func(*args, **kwargs)\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 28, in decorate_context\nreturn func(*args, **kwargs)\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\", line 133, in step\nF.adam(params_with_grad,\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim_functional.py\", line 86, in adam\nexp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\nRuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3\n想问一下有大佬遇见过么   这种应该怎么办啊",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Apr 2, 2022",
            "body": "海外的同学我新建了一个长期tg channel  ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MuFannnn",
            "datetime": "Apr 2, 2022",
            "body": "请问我训练过程中发现已经符合要求了，怎么保存当前的进度呢？",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Apr 2, 2022",
            "body": "会按一定步数自动保存得，直接退出就好了",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MuFannnn",
            "datetime": "Apr 2, 2022",
            "body": "好的，谢谢",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Louis24",
            "datetime": "May 11, 2022",
            "body": "can i join the wechat group?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "feee1ng",
            "datetime": "May 17, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "skyflym",
            "datetime": "May 25, 2022",
            "body": "大佬们 我来求群二维码了 在线等",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Super-Badmen-Viper",
            "datetime": "Jun 6, 2022",
            "body": "大佬们，求群二维码[doge]",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "MichaelChen1989",
            "datetime": "Jul 12, 2022",
            "body": "求个群哈，想进行些业务交流",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Jul 16, 2022",
            "body": "email联系把，",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gwegwegewtg",
            "datetime": "Aug 15, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gwegwegewtg",
            "datetime": "Aug 15, 2022",
            "body": "群过期了  还有新群吗",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pzhyyd",
            "datetime": "Aug 18, 2022",
            "body": "求更新群二维码",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TaoTaaaa",
            "datetime": "Sep 2, 2022",
            "body": "Initialising Tacotron Model...Traceback (most recent call last):\nFile \"synthesizer_train.py\", line 37, in \ntrain(**vars(args))\nFile \"D:\\mockingbird\\MockingBird-main\\synthesizer\\train.py\", line 74, in train\nloaded_shape = torch.load(str(weights_fpath), map_location=device)[\"model_state\"][\"encoder.embedding.weight\"].shape\nKeyError: 'encoder.embedding.weight'",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "TaoTaaaa",
            "datetime": "Sep 2, 2022",
            "body": "求更新",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "zhuzaileiting",
            "datetime": "Aug 19, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Aug 19, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "babysor",
            "datetime": "Sep 7, 2021",
            "body": [],
            "type": "issue",
            "related_issue": "#10"
        },
        {
            "user_name": "gwegwegewtg",
            "datetime": "Aug 15, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#709"
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/78",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ogbeh",
            "datetime": "Jan 6, 2022",
            "body": "I have already copied and paste the api key and url, it still does not work, I have restarted the entire setup and it still throws the same error, what am i doing wrong? Thanks in advance",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/77",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "sfmishra",
            "datetime": "Dec 21, 2021",
            "body": "Trying the steps mentioned but getting stuck at the deploying step, everything was fine till build stage but it keep getting failed on Deploy Stage with below logs===================================================================\n4 certifi-2021.10.8 charset-normalizer-2.0.9 click-8.0.3 flask-cors-3.0.8 flask-socketio-4.2.1 ibm-cloud-sdk-core-3.13.2 ibm-watson-5.2.3 idna-3.3 itsdangerous-2.0.1 python-dateutil-2.8.2 python-dotenv-0.10.3 python-engineio-4.3.0 python-socketio-5.5.0 requests-2.26.0 urllib3-1.26.7 websocket-client-1.1.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (59.5M)\nUploaded droplet (61.3M)\nUploading complete\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 stopping instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 destroying container for instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 successfully destroyed container for instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2Waiting for app to start...\nFAILED\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-001 --recent' for more information\nJob execution ended at Tue Dec 21 06:07:34 UTC 2021reached out to so many people even posted it issue with Watson group but no one care to respond, can anyone help me resolve this issue, I am very new to Watson so have very minimal knowledge.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/76",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "charlie-2201",
            "datetime": "Nov 12, 2021",
            "body": "We want to implement Speech-to-Text and Text-to-Speech functionality over IBM Watson Assistant.\nPlease assist with the process for the same.We visited one of your repositories for the solution\n\nUsing this method, we are facing problems while executing the app.py file\nError: The chatbot speech icon is not functioning and is unable to record any voice input.\nI'm attaching the screenshot of the problem.",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/72",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ather143",
            "datetime": "Jun 18, 2021",
            "body": "Preparing to start the job...\nRunning on public worker: jp-tokpublicworker02-2\nPipeline image: latest\nPreparing the build artifacts...\ncf login -a \"\" -u apikey -p \"****\" -o \"\" -s \"dev\"\nAPI endpoint: \nAuthenticating...\nOKTargeted org Targeted space devAPI endpoint:    (API version: 2.164.0)\nUser:           \nOrg:            \nSpace:          dev\nCreating service instance wvb-watson-assistant in org  / space dev as ...\nOKCreating service instance wvb-text-to-speech in org  / space dev as ...\nOKCreating service instance wvb-speech-to-text in org  / space dev as ...\nOKPushing from manifest to org  / space dev as ...\nUsing manifest file /workspace/a8653fd8-66b9-480b-a4e2-75557073a1cb/manifest.yml\nGetting app info...\nCreating app with these attributes...Creating app watson-voice-bot-20210618015210255...\nMapping routes...\nBinding services...\nComparing local files to remote cache...\nPackaging files to upload...\nUploading files...0 B / 1.59 MiB    0.00%\n128.00 KiB / 1.59 MiB    7.84%\n160.00 KiB / 1.59 MiB    9.80%\n576.00 KiB / 1.59 MiB   35.29%\n1.09 MiB / 1.59 MiB   68.62%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00% 2sWaiting for API to complete processing files...Staging app and tracing logs...\nDownloading python_buildpack...\nDownloaded python_buildpack\nCell 879700a2-564c-421c-9025-d97173c35f0b creating container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b successfully created container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nDownloading app package...\nDownloaded app package (1.6M)\n-----> Python Buildpack version 1.7.37\n-----> Supplying Python\n-----> Installing python 3.8.9\nDownload [https://buildpacks.cloudfoundry.org/dependencies/python/python_3.8.9_linux_x64_cflinuxfs3_e9cbc67f.tgz]\n-----> Installing pip-pop 0.1.5\nDownload [https://buildpacks.cloudfoundry.org/dependencies/manual-binaries/pip-pop/pip-pop-0.1.5-b32efe86.tar.gz]\n-----> Running Pip Install\nCollecting ibm-watson==4.0.1\nDownloading ibm-watson-4.0.1.tar.gz (297 kB)\nCollecting Flask==1.1.1\nDownloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\nCollecting python-dotenv==0.10.3\nDownloading python_dotenv-0.10.3-py2.py3-none-any.whl (16 kB)\nCollecting flask-cors==3.0.8\nDownloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\nCollecting flask-socketio==4.2.1\nDownloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\nCollecting requests<3.0,>=2.0\nDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\nCollecting python_dateutil>=2.5.3\nDownloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\nCollecting websocket-client==0.48.0\nDownloading websocket_client-0.48.0-py2.py3-none-any.whl (198 kB)\nCollecting ibm_cloud_sdk_core==1.0.0\nDownloading ibm-cloud-sdk-core-1.0.0.tar.gz (19 kB)\nCollecting itsdangerous>=0.24\nDownloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\nCollecting click>=5.1\nDownloading click-8.0.1-py3-none-any.whl (97 kB)\nCollecting Werkzeug>=0.15\nDownloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\nCollecting Jinja2>=2.10.1\nDownloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\nCollecting Six\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting python-socketio>=4.3.0\nDownloading python_socketio-5.3.0-py2.py3-none-any.whl (53 kB)\nCollecting idna<3,>=2.5\nDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\nCollecting certifi>=2017.4.17\nDownloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\nCollecting urllib3<1.27,>=1.21.1\nDownloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\nCollecting chardet<5,>=3.0.2\nDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\nCollecting PyJWT>=1.7.1\nDownloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\nCollecting MarkupSafe>=2.0\nDownloading MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl (30 kB)\nCollecting bidict>=0.21.0\nDownloading bidict-0.21.2-py2.py3-none-any.whl (37 kB)\nCollecting python-engineio>=4.1.0\nDownloading python_engineio-4.2.0-py2.py3-none-any.whl (51 kB)\nUsing legacy 'setup.py install' for ibm-watson, since package 'wheel' is not installed.\nUsing legacy 'setup.py install' for ibm-cloud-sdk-core, since package 'wheel' is not installed.\nInstalling collected packages: idna, certifi, urllib3, chardet, requests, Six, python-dateutil, websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson, itsdangerous, click, Werkzeug, MarkupSafe, Jinja2, Flask, python-dotenv, flask-cors, bidict, python-engineio, python-socketio, flask-socketio\nWARNING: The script chardetect is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nRunning setup.py install for ibm-cloud-sdk-core: started\nRunning setup.py install for ibm-cloud-sdk-core: finished with status 'done'\nRunning setup.py install for ibm-watson: started\nRunning setup.py install for ibm-watson: finished with status 'done'\nWARNING: The script flask is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: The script dotenv is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed Flask-1.1.1 Jinja2-3.0.1 MarkupSafe-2.0.1 PyJWT-2.1.0 Six-1.16.0 Werkzeug-2.0.1 bidict-0.21.2 certifi-2021.5.30 chardet-4.0.0 click-8.0.1 flask-cors-3.0.8 flask-socketio-4.2.1 ibm-cloud-sdk-core-1.0.0 ibm-watson-4.0.1 idna-2.10 itsdangerous-2.0.1 python-dateutil-2.8.1 python-dotenv-0.10.3 python-engineio-4.2.0 python-socketio-5.3.0 requests-2.25.1 urllib3-1.26.5 websocket-client-0.48.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (58.1M)\nUploaded droplet (61.2M)\nUploading complete\nCell 879700a2-564c-421c-9025-d97173c35f0b stopping instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b destroying container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b successfully destroyed container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4Waiting for app to start...\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-20210618015210255 --recent' for more information\nFAILEDFinished: FAILED",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sfmishra",
            "datetime": "Dec 21, 2021",
            "body": "were you able to resolve this, facing same issue",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "russchidy",
            "datetime": "May 13, 2022",
            "body": "having same issue...any luck on a solution?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/64",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "markstur",
            "datetime": "Sep 25, 2020",
            "body": "There is still a problem with the credentials.  There is a known problem with the assistant and speech-to-text creds roles.  There is a workaround that works for both.Go to the  Cloud Foundry App -> Runtime -> Environment Variables and add the Assistant and STT creds (APIKey and URL) using the same key/values described for use in the local runtime .env.  We'll need to document this if we don't come up with an alternative.Adding a screenshot here for now:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Sep 25, 2020",
            "body": "Note:  For assistant make sure you are using the APIKEY from the \"Service\" and not the \"Cloud Foundry Service\".Same for STT.TTS seems to be working fine from the CF provided VCAP_SERVICES runtime variable.  You should not need to add these as shown in the image.  Just ASSISTANT_APIKEY and URL and SPEECH_TO_TEXT_APIKEY and URL",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Jan 12, 2022",
            "body": "Update:  Seems to work better if you also specify runtime env for TEXT_TO_SPEECH_APIKEY and TEXT_TO_SPEECH_URL.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Sep 25, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Sep 25, 2020",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/IBM/watson-voice-bot/issues/62",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "Saraswathi24",
            "datetime": "Sep 17, 2020",
            "body": "\nI have followed all the steps mentioned  in Readme.nd and tried to deploy the code locally. I haven't installed any web socket yet.\nWith the default one, I am getting output as follows:\nINFO:engineio.server:Server initialized for threading.\nINFO:assistant_setup:Using WORKSPACE_ID=cd761ded-c1a9-476a-89c4-c6de53e00ce2\nWARNING:werkzeug:WebSocket transport not available. Install eventlet or gevent and gevent-websocket for improved performance.The program isn't working when the mic button is clicked it is just enabling the listening mode, after that webpage isn't responding anything. Kindly help me with this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Sep 17, 2020",
            "body": " Can you help with this?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sanjeevghimire",
            "datetime": "Sep 25, 2020",
            "body": " the code is working mostly in firefox and chrome. I tested it on mine and the mic works. Can you make sure the version chrome and firefox you use has support for  and is not deprecated?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Saraswathi24",
            "datetime": "Sep 28, 2020",
            "body": "  The   is not deprecated in Chrome, even then I find difficulty in recording the audio.It is still not working as expected.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "SapnaChouta",
            "datetime": "Oct 12, 2020",
            "body": " - We have similar issues.  Voice bot shows as \"listening\" when we talk but does not respond back.. We have tried this from multiple Mac laptops and multiple browsers like Firefox and Chrome. navigator.getUserMedia() is not deprecated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "edffrench",
            "datetime": "Jan 7, 2021",
            "body": "  I'm having the same issue running off a ThinkPad, I've tried chrome and firefox. I've attached the respective DevTools consoles for the browsers.Chrome:\nFirefox:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yeshapatel356",
            "datetime": "Feb 22, 2021",
            "body": "Having same issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "erum007",
            "datetime": "Aug 17, 2021",
            "body": "It worked on my PC but when I did the exact same thing from scratch on laptop, I faced this issue and could not figure it out.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "erum007",
            "datetime": "Aug 18, 2021",
            "body": "OK so I found a solution for those who still need it. First of all, use this repository: [deleted]\nAlso, after you configure it on one device, do not use the same credentials on another device.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Oct 1, 2021",
            "body": "I edited the above comment from  suggesting to use another repository because I don't see any updates there.  It is just a copy of this one.  Please clarify if there is an actual fix.  Maybe there is a point-in-time or commit in this repo you are referring to?  Please don't just redirect folks to another repository.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Oct 1, 2021",
            "body": "The comment about not using multiple devices is a good tip though.  I think this bot gets confused easily and having multiple connections from wherever probablky doesn't help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "erum007",
            "datetime": "Oct 1, 2021",
            "body": "I don’t know man, I was just playing around. Found that one and it worked while this didn’t. That one is outdated while this one is relatively updated so there must be a problem with any update made. I didn’t dig deep as to what specifically was the problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Oct 1, 2021",
            "body": "Right, and thanks for the info.  I think this issue is mostly about browser compatibility, but people are also getting caught in some other problems (e.g. multiple simultaneous connections?) that have not been well reproduced.  With the latest version I've seen some \"works for me\" responses.  I'll put the commit which was used in that \"other repo\" in this comment.  It's from 2018, but if anyone wants to compare an old version w/ the latest I don't want to lose that info. The latest commit used above was:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ShuwaiGhz",
            "datetime": "Feb 21, 2022",
            "body": "Has anyone fix this issue? I am currently having the same issue. Hope anyone can give me pointer on this. It works fine at first, but now it cannot listen what I speak. I tried using firefox , ie, edge as well. but nothing works",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "markstur",
            "datetime": "Sep 17, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/wechaty/wechaty/issues/2343",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "su-chang",
            "datetime": "Jan 10, 2022",
            "body": "\nIf the bot has receive  or , I think receive an event notification is better than a message.\nAdd any other context or screenshots about the feature request here.[enhancement]Related issue: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Mar 24, 2022",
            "body": "I think the  event should have another parameter, for example: , so that we can \"pick it up\".And another design need to be done is that after we have picked up the call, how can we receive the streaming of the voice, and how can we send the streaming of the voice.I have no voice-over IP experience so I have no idea about what they should look like.Any suggestions would be welcome.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "su-chang",
            "datetime": "Jan 10, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "wechaty/puppet-whatsapp#23"
        },
        {
            "user_name": "huan",
            "datetime": "Jan 11, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/13624",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "eldernewborn",
            "datetime": "Jan 12, 2021",
            "body": ":\n2.0.0 (use ):\nN/A:\nN/A:\nThe UI tooltip is misleading and confuses the user.\nTooltip says \" use this toggle to pause the dag\" which implies that if the toggle is set to  the flow is paused, but in fact it's the reverse of that.\nEither the logic should be reversed so that if the toggle is on, the DAG is paused, or the wording should be changed to explicitly state the actual functionality of the \"on state\" of the toggle.\nsomething like \"When this toggle is ON, the DAG will be executed at scheduled times, turn this toggle off to pause executions of this dag \".:\nUI tooltip should be honest and clear about its function.:\nopen DAGs window of the airflow webserver in a supported browser, hold mouse over the (i) on the second cell from left on the top row.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Jan 12, 2021",
            "body": "Are you willing to submit a PR?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": "That is not entirely mis-leading, as a DAG can be paused/unpaused at creation based on what is set in . just makes it clearer that the toggle is used for pausing and unpausing -- but I still think it was not required",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eldernewborn",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#13642"
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "pull",
            "related_issue": null
        },
        {
            "user_name": "kaxil",
            "datetime": "Jan 12, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "vikramkoka",
            "datetime": "Jan 18, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "VBhojawala",
            "datetime": "Jan 19, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lmaczulajtys",
            "datetime": "Feb 22, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Sep 17, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Sep 23, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Nov 27, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Mar 10, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Jun 4, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "kosteev",
            "datetime": "Jul 9, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "leahecole",
            "datetime": "Aug 27, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/23733",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "aspain",
            "datetime": "May 16, 2022",
            "body": "2.3.0 (latest released)See recorded screencap - in the task instance pop-up menu, sometimes the top menu options aren't clickable until you move the mouse around a bit and find an area where it will allow you to clickThis only seems to affect the , , and  options - but not , or The entire 'bubble' for the options such as 'XCom' should always be clickable, without having to find a 'sweet spot'I am using Astro Runtime 5.0.0 in a localhost environmentmacOS 11.5.2apache-airflow-providers-amazon==3.3.0\napache-airflow-providers-celery==2.1.4\napache-airflow-providers-cncf-kubernetes==4.0.1\napache-airflow-providers-databricks==2.6.0\napache-airflow-providers-elasticsearch==3.0.3\napache-airflow-providers-ftp==2.1.2\napache-airflow-providers-google==6.8.0\napache-airflow-providers-http==2.1.2\napache-airflow-providers-imap==2.2.3\napache-airflow-providers-microsoft-azure==3.8.0\napache-airflow-providers-postgres==4.1.0\napache-airflow-providers-redis==2.0.4\napache-airflow-providers-slack==4.2.3\napache-airflow-providers-snowflake==2.6.0\napache-airflow-providers-sqlite==2.1.3AstronomerI experience this in an Astro deployment as well (not just localhost) using the same runtime 5.0.0 image",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aspain",
            "datetime": "May 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "uranusjr",
            "datetime": "May 16, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "May 16, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#23736"
        },
        {
            "user_name": "bbovenzi",
            "datetime": "May 17, 2022",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/22036",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "NirmalSharmaRaj",
            "datetime": "Mar 7, 2022",
            "body": "2.2.2On the home page, when the window size is reduced, a bottom scrollbar appears. But this scrollbar is unclickable, so I am not able to side-scroll using it. The page navigation buttons seems to be overlapping it.\nI expected bottom scrollbar to be moved using click and hold with mouse, to scroll the page.Reduce the size of the browser window for the bottom scrollbar to appear. Then try to click and hold on the scrollbar.Ubuntu 20 LTSVirtualenv installationBrowsers used: Chrome, Edge, Firefox",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "josh-fell",
            "datetime": "Mar 7, 2022",
            "body": " I'm not able to reproduce this behavior on 2.2.2 with both side-scrolling via trackpad nor click and drag of the scrollbar.  Are you able to reproduce and/or is there other information that would be helpful to know if this is a bug? We can always convert to a Discussion too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Mar 7, 2022",
            "body": "Yeah I am not sure, but the css for the dags page was changed in  So we won't have anything on top of the scrollbar anymore.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Jul 2, 2022",
            "body": "This issue has been automatically marked as stale because it has been open for 30 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Jul 5, 2022",
            "body": "I've checked a few browsers and OSes. This issue no longer exists.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "NirmalSharmaRaj",
            "datetime": "Mar 7, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "josh-fell",
            "datetime": "Mar 7, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "eladkal",
            "datetime": "Jun 1, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Jul 2, 2022",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Jul 5, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/wechaty/wechaty/issues/2186",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "ghost",
            "datetime": "May 7, 2021",
            "body": "As shown in the error report, my silk voice file is correct and can be sent through other social software, but I can’t send it in Wechaty. What is the specific problem?",
            "type": "commented",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/21428",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "potiuk",
            "datetime": "Feb 8, 2022",
            "body": "main (development)There is a weird effect when you click the task in the new Grid view to get to task details. The whole grid view seems to shift a little when the pop-up window is displayed and it has a very disturbing effect for the user because if you did not move your mouse, it looks like you clicked wrong DagRun (the shift in my case is such that it looks like you clicked the \"previous DagRun\" as the mouse remains on top of the previous DagRun.This is pretty disturbing - when I saw it for the first time I literally thought I missed the right box and pressed escape and tried again and again - all the time the same and I thought something is wrong with my mouse. Only after a moment I realized that this is the \"grid\" that shifts a bit.This is best seen with the video:No shift of the grid - mouse should stay on top of the clicked DagRunClick on the DagRunLinux Mint 20.3Not relevantOther in BreezeThis was run in Chrome: Version 98.0.4758.80 (Official Build) (64-bit)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Feb 8, 2022",
            "body": "cc:  - > I wonder if this is just me or is it same for everyone :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Feb 8, 2022",
            "body": "On the main tree view your browser thinks there is vertical/horizontal scrolling, but that isn't the case with the modal overlay. It's especially not good that the scrollbar width is basically the same as the task instance width.I assume this happens in a number of browsers/OSs. So I'll work on a fix.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Feb 8, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Feb 8, 2022",
            "body": "Ah yeah. I see it now with the scrollbar :). Great eye!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Feb 8, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Feb 8, 2022",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Feb 8, 2022",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Mar 10, 2022",
            "body": [],
            "type": "pull",
            "related_issue": "#22123"
        },
        {
            "user_name": "bbovenzi",
            "datetime": "Mar 31, 2022",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/apache/airflow/issues/15416",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "pateash",
            "datetime": "Apr 17, 2021",
            "body": "Currently, when we run doesn't load local tmux configuration file  and we get default tmux configuration inside the containers.Breeze must load local  in to the containers and developers should be able to use their local configurations.\nYES\nNone",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "mik-laj",
            "datetime": "Apr 19, 2021",
            "body": "SGTM. Can you submit a PR?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "uranusjr",
            "datetime": "Apr 19, 2021",
            "body": "I wondered about this a while ago (mostly because I don’t want to repeatedly ). The problem is any slightly sophisticated local tmux setup would contain a lot of dependencies and it’s not realistic to pull them all into the container. So maybe a better approach would be have a directory in the repository; if the user puts a (git-ignored)  file in it, the file is mounted to the container’s .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 20, 2021",
            "body": "Yes I will.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "potiuk",
            "datetime": "Apr 20, 2021",
            "body": "We already have similar mechanism for reading user-supplied env variables placed in  folder.\nIt could be done in a very similar way .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 20, 2021",
            "body": "thanks , let me check.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 20, 2021",
            "body": "PR ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 17, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 17, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "pateash",
            "datetime": "Apr 20, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#15454"
        },
        {
            "user_name": "potiuk",
            "datetime": "Apr 21, 2021",
            "body": [],
            "type": "pull",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/wechaty/wechaty/issues/371",
    "issue_status": " Open\n",
    "issue_list": [
        {
            "user_name": "lijiarui",
            "datetime": "Mar 29, 2017",
            "body": " is a fantastic bot analytics platform, it can provide abundant metrics to help you better monitor your data. You can increase user engagement, acquisition, and monetization through analytics, bot specific metrics, funnel analysis and live transcripts and other functions.Now, I'm trying to communicate with their founder DennisYang and trying to integrate with wechaty.We can send data with generic type, but we want to integrate more.Here is the raw message Json example to see whether we can store more.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 29, 2017",
            "body": "Now I'm using dashbot with generic like the following function:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 29, 2017",
            "body": "FYI I've chatted with Dennis (as in Tennis :) quite a lot. I had problems with dashbot being blocked from our (china/AWS) servers. One idea is that maybe they were hosted on digitalOcean (I'm not sure though), which sometimes the whole IP block gets censored.Other than that it's a great service!So you don't have any problem now sending events to dashbot?FYI analytics for Bots is a big topic in the US. Another company is:\nbotanalytics.co",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 29, 2017",
            "body": "Because my bot is hosted on digitalOcean too, so I don't have too many problems when sending events to dashbot. And I just begin to use dashbot these days, so I'm not very sure.By the way, I have several bots, some on aliyun and some on digitalOcean. For now, there isn't any difference between them.And I found  is also good too.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 29, 2017",
            "body": "About the userId problem and getting a unique IDinstead of:it looks like Contact class has access to the wechatID - can't we use that?\nis this the real wechat ID, or is it an openID that changes with each session?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "About the uniqueID, my solution on my bot is to set alias for each contactand use alias(we called remark before, see  ) to find the contact.all of the contact on my phone like this:\nBtw, wechaty do has function  to get contact's wechatID, because sometimes we can get wechatID in contact's rawObj from webwx, but cannot always get some person's wechatID each time for some reason, I haven't found the real reason, so using  is not a good solution.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 30, 2017",
            "body": "so alias remains consistent between sessions and devices, since it is stored server side by wechat, correct? ie you set it thru the wechaty/web client but then it shows up on your phone too?the wechat ID is not available about 10% of the time? I think its only for users that joined wechat directly from QQ in the old days, when this was possible.otherwise this is a bit ugly and it makes it hard to know who you are talking to, if a real human was to take over the chat.we could do something likethat would create a user alias  the wechat ID doesn't exist?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 30, 2017",
            "body": "in our app at least we often have a live teacher take over from the bot and chat to peoplewe also want to be able to see the userName so we can associate users from the official account into the bot chat.So in this way we would want to keep the original name visible. Maybe you dont need this for a fully automated bot ...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 30, 2017",
            "body": "125428 unread messages   \nyou have some catching up to do do you find this slows down wechaty on login?\nI know on native clients it fetches all those messages, even in rooms you have muted.I had a bot and found that the \"get list of rooms\" function was taking a long time - I think because it was syncing a lot of unread messages to the web client even in chromedriver. But not sure...\nDoes the web client only request messages when you want to view them?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "Yes, alias remains consistent between sessions and devices, even if you reload wechat or relog in wechat on the different phone.And I set alias through wechaty/web client and then it shows up on my phone.The wechat ID is not available all the time, but I don't know the exact number, I guess only the new users can get wechat ID, because I can get my wechat ID by wechaty, but  cannot get his wechat ID by wechaty.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "I really agree with this, and it confused me a lot when take over. So I suggest whether dashbot can show  both uniqueId and username, then it will easy to recognize.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "But I do not entirely agree with your solution:because we cannot set contacts's alias by wechaty by default. It should let developer to decide, or if someone uses wechaty and dashbot, and then he saw his contact  has changed to , he will shouted......maybebut I'm not sure it should be obj.id(somethin like ) or obj.name (something like )As you have said, usually names are almost unique.... Although my bot has two contact with the same name....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "About a great amount of message, it never slows down wechaty on login. But it indeed slow down my android device... Receiving message on server is quite faster then the device, so if I can connect dashbot, I can leave out my device completely.Yes,  does take some time, but not that long. I have more than 500 hundred groups and it won't take me too long time to get all. Actually, server always faster than web or phone.... I thought the time is cost on render front-end show, maybe.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "Here is the contact rawObj data, maybe you can find something interesting",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dcsan",
            "datetime": "Mar 30, 2017",
            "body": "yes, agreed. that's why I almost think using name is ok... it would have very few collisions, but would be very usable.The code comment saysbut i think this does not  for the same user, right?\nits only when the user has a from QQ type of account? in which case it is always hidden for that user?\nso maybe just in those cases we overwrite the alias with the nickname, and use that.\nIt would mean even less chance that nickname will have a collision (but still not zero of course...)Also using names is risky since the user can change it. We would lose their info, in future this does expose the fact someone could change their name, and impersonate someone to get at content inside our bot app. So, not a good way forward.So the only reliable way is really ugly then...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 30, 2017",
            "body": "yes, maybe this is the most important reason why we cannot user name.....\nbut it is better than contactId?So maybe the best implement as follows?And we should add log.warn() if user doesn't set user's alias?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lijiarui",
            "datetime": "Mar 31, 2017",
            "body": "Here is the rawObj message for Dennis YangGeneral data as follows:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Aug 18, 2017",
            "body": "Is there any progress/milestones update for the dashbot.io integration?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sinned",
            "datetime": "Aug 19, 2017",
            "body": "Yes! We added conversationId to our Generic implementation now:\nWhen we launch the wechat integration, it will use the same JSON payload as Generic, with platform=wechat in the API call instead.Does that make sense?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Aug 19, 2017",
            "body": "Awesome! Do you have any examples on dashbot website? I'd like to have a try.Cheers!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Oct 8, 2017",
            "body": "See: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Mar 31, 2017",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "Jan 2, 2019",
            "body": [],
            "type": "pinned this issue",
            "related_issue": null
        },
        {
            "user_name": "huan",
            "datetime": "May 12, 2019",
            "body": [],
            "type": "unpinned this issue",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/4281",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "aib",
            "datetime": "Jan 12, 2022",
            "body": "Upon launching LinuxNoEditor/Blocks.sh or LinuxNoEditor/Blocks/Binaries/Linux/Blocks I get a black screen with my mouse cursor visible. From the alt-tab dialog I can surmise that a \"Choose Vehicle\" window and a simulation window have been created. At this point, the desktop the simulation is running at is blocked, I cannot see or visibly switch to any windows on the desktop, including the console that ran the program. If I focus the Choose Vehicle window and close it, the simulation window becomes normal and I am able to use everything normally, including the simulation.Is the Choose Vehicle window a modal dialog over a fullscreen application which takes over the screen?Where is the windowed mode setting?  produces nothing and the documentation only refers to simulation configuration JSON files.No settings. I would play with them if I could find them.Did a search for \"fullscreen\" and converted a 1 to 0 and a True to False and the only change is that I see a quarter-screen window for a split second before I get the working fullscreen simulation window, after fumblingly closing Choose Vehicle.N/AWell, this is my first introduction to this program, or any of its kind. This is a computer/setup that runs hundreds of games through Steam and Proton.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aib",
            "datetime": "Jan 12, 2022",
            "body": "stdout:(this is where I close Choose Vehicle)stderr:(this is where I close Choose Vehicle)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aib",
            "datetime": "Jan 12, 2022",
            "body": "Just found out about  thorough an external site. Fixes the problem as expected.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 31, 2022",
            "body": "Thanks, , for solving by yourself, and keeping us informed!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 31, 2022",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Jan 31, 2022",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/2660",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "avivazran",
            "datetime": "May 5, 2020",
            "body": "I'm trying to use the python API to enable drone control with keyboard or xbox controller.\nis there a script which implements it? or plans to write one?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 27, 2020",
            "body": "Have you read thid doc:  ?\nDo you need to enable the rc on runtime?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 27, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "jonyMarino",
            "datetime": "Aug 27, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#2671"
        },
        {
            "user_name": "avivazran",
            "datetime": "Sep 9, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/219",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Murplugg",
            "datetime": "May 15, 2017",
            "body": "Proposed concept: Have the PIP views follow the drone while in FPV. The PIPs can look around by using a PS3 / Xbox controller (think typical First Person Shooter mouse-look).Issues: AirSim seems to support just one active camera, it's not possible to control the drone in FPV and still have the PIPs follow the drone (they switch position with the external camera).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sytelus",
            "datetime": "May 16, 2017",
            "body": "Currently we have kept things simple. You have FPV camera and external follower camera. You can swap them in PIP or main view. This design decision is not the most flexible one, of course, but it makes few things simple, for example, no complications to select which camera is PIP or main.Having said that there are no technical limitations on how many camera you may have and which one is PIP or main. However, as you add more cameras, FPS might start to drop dramatically.In very near future, the plan is this: We allow to place N PIPCamera objects wherever user like. We can then offer following APIs:getCameraCount()\ngetCameraImage(index, imageType)\nsetCameraToPIPView(index)\nsetCameraToMainView(index)Lot of code that existed in Blueprint in now moved to C++ so above is now relatively easier.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 16, 2017",
            "body": "Thanks. So FPV and the PIPs share the same defined camera, the PIPs are just three different render targets? To have the PIPs steerable by a controller basically means adding new dedicated camera and C++ code to take control over orientation of that cam?Do you have a rough idea of when your plan / API can be implemented? (Days, weeks?): Got it working. In short:\nBP_FlyingPawn holds two cameras: LeftPIPCamera and RightPIPCamera. AirSim uses LeftPIPCam by default so I expanded CameraDirector::getCamera() to point to one of BP_FlyingPawn's cameras depending on the index argument (int) (just a switch statement, 0 = TargetPawn.getFpvCamera(), 1 = TargetPawn.getGimbalFpvCamera (a new method)).As seen above, the Scene PIP is on a 2nd camera Actor called GimbalPIPCamera in UE4, placed underneath the drone model, it's controlled from the default CameraDirector instance in AirSim (SimModeWorldMultiRotor::setupVehiclesAndCamera() ). Then PIPCamera::setToPIPView calls: EPIPCameraType pip_state = getCamera(1)->toggleEnableCameraTypes(EPIPCameraType::PIP_CAMERA_TYPE_SCENE);That is, it uses the 2nd cam by calling getCamera(1).A new FRotator variable was made in CameraDirector, similar to \"camera_rotation_manual_\", to hold the 2nd camera's orientation separate from External cam view and so on. Both LeftPIPCamera and our new cam follow the drone model by default, all that's needed for manual control of 2nd cam is (example for negative pitch): In the new pitch down event:\ngimbal_camera_rotation_manual_.Add(-val, 0, 0);\ngetCamera(1)->SetActorRelativeRotation(gimbal_camera_rotation_manual_);",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sytelus",
            "datetime": "Feb 21, 2018",
            "body": "I just added Gimbal APIs in AirSim. .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 24, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 26, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#237"
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 26, 2017",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 26, 2017",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 29, 2017",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "Murplugg",
            "datetime": "May 29, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/allenai/allennlp/issues/1782",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "nitishgupta",
            "datetime": "Sep 18, 2018",
            "body": "I have Tensor T = [B, T, D]  of contextual word embeddings for a given piece of text. Alongside, I have a tensor S = [B, M, 2] of M-spans in this text and their representations, i.e. a tensor R = [B, M, D]. For any given span (i, j), I want to add it's representation to all the tokens in span [i : j].Currently the solution I have is to loop over dim-1 in S (and R), and for each span, repeat the span representation to the length of the span, and add it to the corresponding slice in T.I was wondering if somebody has a better solution in mind.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 18, 2018",
            "body": "Just making sure I understand the problem:Let's say I have the sentence \"The cat ate the mouse\", and I have four spans: \"the cat\", \"the cat ate\", \"the mouse\", and \"the cat ate the mouse\".  So each word will get either two or three span representations added to it.  Yes?  Something like this:Right?  What this suggests is that you want to construct a  shape binary tensor, which you can then multiply by your  tensor to get something of shape  that you can add to your token representations.  You should be able to construct that  tensor using a range vector (for the token indices) and some greater than / less than operations.Does this make sense?  (I'll add that doing this seems a  bit odd to me, but hey, maybe the model can segregate the span features into one part of the feature space, and have them be additive...)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nitishgupta",
            "datetime": "Sep 18, 2018",
            "body": "Yes, you understood the problem correctly and the solution seems correct. Thanks!Sidenote: The actual modeling in mind is different from this, but I simplified it for the sake of easy explanation. Still, is having additive features a bad idea?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 18, 2018",
            "body": "I'm glad it helped!  I'm closing this issue now.And additive features aren't necessarily bad - the model can just partition the feature space so that each feature gets its own bucket, and things can work out.  I've been surprised at how well it works in a number of different occasions (e.g., positional embeddings).  I wouldn't trust my intuitions on this point very much - much easier to just try something and see empirically if it's a good idea.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nitishgupta",
            "datetime": "Sep 18, 2018",
            "body": "I implemented a test version of this and it works. Thanks again for the idea.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 18, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/28",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "morrisonbrett",
            "datetime": "Feb 20, 2017",
            "body": "Is it possible to use AirSim without an external controller? Keyboard / Mouse / Even XBox Controller.Or, must you have an external device?Is there a way to have a \"software emulator\" version of what's setup in the settings.json file?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "morrisonbrett",
            "datetime": "Feb 20, 2017",
            "body": "I see that this has indeed been addressed in the \"alternatives\" doc.Closing this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "morrisonbrett",
            "datetime": "Feb 20, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/allenai/allennlp/issues/5462",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 9, 2021",
            "body": "Hi all,\nIs it possible to train AllenNLP's coreference resolver with custom data? What format does the custom data have to be in? And finally, if it is possible how do I accomplish it? Thank you very much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 12, 2021",
            "body": "Yes, you can train coref with custom data. The training instructions are at . The hard part is that the original data is not freely available, so it's hard to look at.You probably don't need all of that stuff. It'll be easier to modify the dataset reader for coref to read some other format that you have available. Dataset readers are quite easy.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 14, 2021",
            "body": "Thank you very much. I assume that allennlp can train custom data for relation extraction as well?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 15, 2021",
            "body": "Hi, I'm facing an issue with training. I am running this command:I get the following error:I installed allennlp-2.8.0 allennlp-models-2.8.0 and my coref_spanbert_large.jsonnet file is the same as the link provided except I change these three lines:I'm using v4_gold_conll files (which are basically the same as Conll-2012 files), is that acceptable? I am using these because neuralcoref uses these as well.Also, why do we need ? Is that for accessing online files? Thank you very much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 17, 2021",
            "body": "Hi, I have nearly successfully trained the coreference resolver. I did this on a Linux machine. I ran this command in coref/training_config in allennlp-models (I installed both allennlp and allennlp-models from source):I have this file (train.demo.v4_gold_conll) in train/, test/ and dev/ folders (just for a test train) in the same folder:I get this error:Do you know where I might be going wrong? Could it be #begin document (demo); part 000, should it be #begin document (bc/cctv/00/cctv_0000); part 000?This is the version of Linux I am running:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 18, 2021",
            "body": "We use that to read environment variables. You can say  in your shell, and then read out the value in the config with .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 18, 2021",
            "body": "There is likely something wrong with your input file, maybe a special character that's not encoded correctly? Either way, it sounds like you got past that problem?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 18, 2021",
            "body": "I don't know exactly what's going wrong with your input file, but I have two observations:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 19, 2021",
            "body": "I used the Inception tool to create the ConLL-2012 files (), but the file that you showed above is slightly different from what I got using the Inception tool. I do not get the same columns with the asterisk. Do you think adding 2-3 columns using a script with  an asterisk can solve the issue? Thank you very much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 19, 2021",
            "body": "I don't know what the asterisk is used for in the reader. Probably for constructing parse trees, which you don't need for coref. I think it might be better to make a copy of the reader and modify it. You could get rid of a lot of code in there and only keep the bits you need for coref.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 22, 2021",
            "body": "I tried to add 2 more columns with an asterisk, I ran into the same error as before:The conll file I am using is as follows (note I change it to a .conll file not a .txt file for training):I then tried to train the sample file you gave, except with #end document at the end (it is as follows):I got this error:What do you think is causing the error? It looks like your code needs the entire Ontonotes dataset to do custom training but that makes custom training very difficult. Can you recommend a course of action? Thank you very much",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Nov 23, 2021",
            "body": "AllenNLP does not need the entire Ontonotes dataset, just a dataset in the right format. But the Ontonotes format is complicated, because it contains a lot of stuff that's unnecessary for training a coref model. I recommend writing your own  that produces data in the right format for the model, but reads a different input format.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 24, 2021",
            "body": "I will attempt to write my own DatasetReader. However, in order to do so I need to start with data which trains successfully as a base for my code otherwise I cannot proceed. I humbly request a source for this sample data as the sample data you provided me is not working. If that is not possible, could you guide me for references for finding the data in the right format for the model. Thank you very much.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Dec 2, 2021",
            "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dirkgr",
            "datetime": "Dec 2, 2021",
            "body": "Sorry, we are legally not allowed to give out the source data. It's stupid, but that's what it is. It came from . I think you can go to that website, sign up, and then you get a link to download.This information is at .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "AlanQuille",
            "datetime": "Nov 9, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "github-actions",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/116",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 17, 2017",
            "body": "Hello. I used the sample assembly from this video . The following errors occurred during the build (debug x64):Error LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (client.obj)\nError LNK2038 detected a discrepancy for \"RuntimeLibrary\": the value of \"MD_DynamicRelease\" does not match the value of \"MDd_DynamicDebug\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (client.obj) 1\nError LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (rpc_error.obj) 1\nError LNK2038 detected discrepancy for \"RuntimeLibrary\": MD_DynamicRelease value does not match MDd_DynamicDebug value in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (rpc_error.obj) 1\nError LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (response.obj) 1\nError LNK2038 detected a discrepancy for \"RuntimeLibrary\": the value of \"MD_DynamicRelease\" does not match the value of \"MDd_DynamicDebug\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (response.obj) 1\nWarning LNK4098 default library \"MSVCRT\" is inconsistent with the use of other libraries; Use the / NODEFAULTLIB parameter: library DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ LINK 1\nIf you comment out the lines in build.cmd:REM msbuild /p:Platform=x64 /p:Configuration=Debug AirSim.sln\nREM if ERRORLEVEL 1 goto :buildfailedAssembly is performed without errors.\nAfter completing the steps to create a project, unreal gives an error:The following modules are missing or build with a different engine version:\nUE4Editor-AirSim.dll\nWould you like to rebuild them now?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 17, 2017",
            "body": "Hi , have you tried cleaning (Build->Clean Solution) before building in Visual Studio? I sometimes have build errors too when importing newly built versions of AirSim, cleaning beforehand gets rid of those.If not, did you choose to rebuild the UE4-Editor-AirSim.dll?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 17, 2017",
            "body": ",No, I have not tried it.Yes, the project is going. In the unreal there is no directory containing the plugin AirSim. When you run the project in unreal, nothing happens",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 17, 2017",
            "body": "So did cleaning help you build the project or not? I don't understand how you can get to the Unreal project without having built the Visual Studio project. Does this mean you got post the build error?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 17, 2017",
            "body": " , Yes, instead of the file I used AirSim.sln in the directory and collected Debug after cleaning.\nBut now nothing is still happening when I press to play in the project. Maybe there are no scene cameras installed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 17, 2017",
            "body": "Ok, so you got the project built and are now in the Unreal environment, check. It's absolutely possible that there is no starting position defined in your current map. You can check this out by typing 'start' in the top right input box, it should show you all the currently defined starting positions. See this cropped screenshot:The  (the link starts at the relevant part) also clearly shows how you should position this starting point for optimal result.Don't forget to set your  to , otherwise the drone won't spawn.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 17, 2017",
            "body": ", Yes, I have a starting position. And after clicking on the play, I see:\n\nThe camera does not display segmentation, depth, etc.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 17, 2017",
            "body": "So, what's exactly the problem you're having now? Did you check the instructional video and  configuration?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 17, 2017",
            "body": "The fact is that I did not find the description of the . I want to achieve the same result as on the video (flight, segmentation cameras, depth, etc)\nIt's normal that I can not find BP_Camera Director?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sytelus",
            "datetime": "Mar 18, 2017",
            "body": "Hi Folks, this seems to be because you are using 4.14. We just upgraded everything to 4.15 couple of days ago. This required some changes in BP_CameraDirector but unfortunately it turned out that if you modify these in 4.15 then they become invisible (and unusable!) in 4.14. That is, written BPs in 4.15 are no longer compatible to 4.14. Even worse was that there are no error messages and you will basically see nothing happening when you press Play button. This was disappointing however there is no known way to go back and we don't want to get in the hassle of of maintaining two versions. So we suggest that every one upgrade to 4.15. I've also put in some code to display warning message in latest version. Upgrading to 4.15 is easy: .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 18, 2017",
            "body": ",  thanks. Really, I have 4.14.3 version.\nI try to reinstall unreal.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 18, 2017",
            "body": ", thanks. Reinstallation helped in my case. Now I got the following:Can I control the drone with a keyboard or mouse? Without specialized tools?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 20, 2017",
            "body": "As far as I know, there is no way to control the drone with mouse or keyboard at this instant. You'll have to either use an RC controller wirelessly connected through a Pixhawk device or directly through USB, or some kind of .It would also be very hard to actually control a drone using a keyboard: the control values would vary from 0 to max, nothing in between. While I do agree it would be a nice-to-have feature to test if your setup is correctly running, it will in no way come close to actually flying a drone using a dedicated controller. You need very accurate control to be able to fly a drone correctly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sytelus",
            "datetime": "Mar 18, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#114"
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 18, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 18, 2017",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Mar 20, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#121"
        },
        {
            "user_name": "Maxfashko",
            "datetime": "Mar 20, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/111",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Alejolas",
            "datetime": "Mar 16, 2017",
            "body": "Hi all,Thanks a lot for this project. So I just bought a new build to run this properly, Intel I7 6800K, 32GB DDR4 RAM, GTX 1080. Installed everything following the documentation and videos, my problem is: every time I hit play, Unreal Editor crashes. When I debug it, here's what I get:\nAny help would be appreciated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 16, 2017",
            "body": "Can you post the full call stack ?  Could be a setup issue.  Can you tell us more about your setup? which OS, Pixhawk hardware versus SITL mode, etc, thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Alejolas",
            "datetime": "Mar 16, 2017",
            "body": "Thanks for the reply Chris!I still don't have any Pixhawk hardware or drone... I just ordered the stuff from Amazon, I just got the computer, sorry if there some things in Spanish:\n\n\n\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 16, 2017",
            "body": "Thanks, looks like it is crashing in the implementation of delete of a MavLinkNode which is not good.  But since you don't have pixhawk hardware, can you change your ~/Documents/AriSim/settings.json so that it has \"Serial\" set to \"false\".  Then I'd expect you would be looking at the  so you can get a flying drone.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 16, 2017",
            "body": "Note also that the master branch was just changed to switch to Unreal 4.15.  So if you have 4.14 you will need to upgrade.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Alejolas",
            "datetime": "Mar 16, 2017",
            "body": "Oh, I don't have any controller of any kind, just keyboard and mouse. Should that be the problem?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 16, 2017",
            "body": "The  show you how to get a drone that flies.  But we don't yet have support for keyboard/mouse flying.  But you could do some autonomous flying as showing in the .  Depends on what your goals are here.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Alejolas",
            "datetime": "Mar 16, 2017",
            "body": "Yes, for the moment I just want to do autonomous fly while I get the other hardware, but first let's see if I can get pass through that MavLinkNode error that I'm having and crashing my Unreal Editor. I'm going to try again following all the steps. Thank you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 16, 2017",
            "body": "Ok, let me know how it goes.  if you still run into trouble after following all the steps, then it might also help to enable the debugger to \"break\" on all C++ exception and see what you can find that way.  I'm guessing the delete is crashing because other setup code was skipped.  We have a bug there obviously, but we'll need to do some more debugging to track it down.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "imoon",
            "datetime": "Mar 19, 2017",
            "body": "I'm having this issue also, only I do have HIL but get this stacktrace after running:\nMavLinkTest.exe works fine:I'm at a complete loss. I've stepped through until RpcLibServer.cpp line 116. Executing this crashes Unreal as described above with the stacktrace above.Any assistance would be appreciated. I have followed all instructions and cannot get it to run.Additional Info:I've tried a number of settings.json settings and this is my latest:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 20, 2017",
            "body": "I am getting the same issue but with HIL. I have my pixhawk connected to my PC in HIL mode . RC receiver is connected to the pixhawk.  In the SITL mode, I get an error saying \" Computer load temporarily too high for real-time simulation\". My Hardware looks something like this. 16gb RAM, Nvidia GTX 1060- 6GB GPU Memory, 128 GB SSD,. I am running the latest version of AirSim on Unreal 4.15 . Do you think 16GB RAM is the problem ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 22, 2017",
            "body": "I am able to run smaller worlds with 16gb no problem.  What is the CPU ?  I've seen this message in SITL also \"Computer load temporarily too high for real-time simulation\" but I'm still able to fly, so I doubt this is the root issue.One thing with HIL mode is you must set this Airframe using QGroundControl then click \"Apply and Restart\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 22, 2017",
            "body": "And for HIL mode your settings should look like this:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 22, 2017",
            "body": "Thanks a lot for the reply. I am using Intel i7 6th gen processor. Also, pixhawk is on HIL mode. I haven't loaded any environment into unreal engine as I thought CPU load might get too high and my settings are the same as yours. And although I connect my Taranis Fr Sky to PC through USB. When I launch the project I am getting RC Controller not detected on USB",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 22, 2017",
            "body": "Ok, great, i7 is fine.  That message is coming from AFlyingPawn::detectUsbRc.  Perhaps there are addition steps required to get your RC Controller to appear as a device that Unreal is happy with. There are some remote control instructions at the .I believe we currently only support \"xinput\" devices (as per SimJoyStick.cpp) and there may be a missing step to convert your USB joystick into something that emulates xinput.  For example  might be what you need there.I also added .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 23, 2017",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 23, 2017",
            "body": "hang on, why can't you connect your RC to your Pixhawk then ?  You do not need to use RC through USB to PC.  AirSim can get the RC controls via pixhawk as I show in .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 23, 2017",
            "body": "",
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 23, 2017",
            "body": "Ah, I see now, so the message about RC and USB is something you have to ignore in your case.  I agree it is confusing.  I will try and fix that so you don't get the confusing message.But now I realize you haven't actually yet specified on this thread what your problem actually is.  Can you describe the symptoms you are seeing and the goal you are trying to reach ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 23, 2017",
            "body": "I am getting the same error as seen in the snapshots of the 1st post of this thread. The GUI crashes after reading settings.json from the documents. The debugger says the crash occurs at MavLinkNodeImpl.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 23, 2017",
            "body": "I see thanks for clarifying.  Ok, I just made this  and followed every step there and it works fine, so this is indeed a mystery.  It would help me if you could paste the text of each C++ exception call stack you see before the crash while running the game in Visual Studio debugger.  Is that possible?Also, to ensure I never have any \"stale bits\" involved in my unreal projects I run this little script:then I update AirSim bits, and then I \"regenerate the visual studio project\", load it in VS and hit F5.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lkumar93",
            "datetime": "Mar 23, 2017",
            "body": "Surprisingly it seems to be working right now. Previously I had launched the project file manually from unreal engine and that didn't work. This time I opened the solution file and put it in debug mode on Visual studio and then hit F5 , which launched the engine and then I was able to run it. However the drone is not flying as I increase the throttle after arming it. The gui says takeoff detected but It doesn't really move. The propellers are spinning though. Btw , the pixhawk is on altitude mode.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Mar 23, 2017",
            "body": "Great, that's progress then.  Better to take off without Altitude mode - Altitude mode tries to hold the current altitude which is probably fighting the takeoff.  You also need to ensure Pixhawk is in HIL mode as I showed in the QGroundControl screen shot earlier in this thread.  Sometimes \"rebooting\" the pixhawk is necessary to reset any residual state it had from previous crashes or weird behavior.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CaoYongshengcys",
            "datetime": "Jan 12, 2019",
            "body": "I came across the same problem,  takeoff detected but It doesn't really move. How do you fix it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "sytelus",
            "datetime": "Mar 16, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "imoon",
            "datetime": "Mar 22, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#126"
        }
    ]
},
{
    "issue_url": "https://github.com/allenai/allennlp/issues/1727",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "aleSuglia",
            "datetime": "Sep 7, 2018",
            "body": "Hi there,I was wondering if you have some recommendation in terms of how to implement a copy mechanism for a seq2seq model following AllenNLP best practices. I was thinking to modify the DataIterator in order to return, for each batch, an extended vocabulary mapping the OOV tokens to the original tokens in the sequence. This change will help me to both compute the loss function and the predictions generated by the model. Do you have any suggestions?Thank you in advance for your reply!Alessandro",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 7, 2018",
            "body": "This is a good question, and I had to think a bit to come up with some ideas for how to do this.  Here's my initial take; there are probably ways to improve it.As you say, there are two places where a copy mechanism needs special handling: in computing the loss function (should I have copied this word?) and when showing the output of decoding to a person.  These happen in very different places in the code, and should be handled separately.First, for decoding, we typically  about what tokens were in the input, so that it can be displayed nicely in a demo.  You should be able to do the same thing during decoding in a copy model - you just keep track of whether you sampled from the vocab or from the copy mechanism during decoding, and display the result to the user accordingly, using that metadata field.  Doing it this way, instead of fudging with the vocabulary or input data, means that you can still use an arbitrary encoder to do your input embedding (including, e.g., ELMo), without resorting to any crazy hackery.Second, the harder part, is how you train a model with a copy mechanism.  I  the right thing to do here is to add a new  that tells you token overlap on the target outputs.  For example, say I have the input sentence \"the dog ate the food .\" and the target sentence \"the cat ate the mouse .\".  What I need during training is to know which target tokens overlap with which input tokens, so my loss function can include the correct copy probabilities.  This field would output a tensor like this for those two inputs:where  is used for padding here.Then, in my loss computation, I'd do something like this:Does this make sense?  Any thoughts?  , what do you think?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 7, 2018",
            "body": "Also, we would  to have a nice implementation of a seq2seq + copy model in AllenNLP.  If you get this working, please consider contributing back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aleSuglia",
            "datetime": "Sep 12, 2018",
            "body": ": thanks for your feedback. At the moment I was checking the official implementation of See et al. () and the copy mechanism implementation in OpenMT-py (). Apparently over there the idea is to treat the attention scores as a sort of probability score associated to the UNK tokens. Ultimately, they minimise the negative log-likelihood associated to the token indexes that compose the sequence plus additional special tokens associated to the unknowns tokens in the sentence.I'm going to work on this from now on. I'll let you know when I'll have a basic implementation. Are you thinking to integrate it in the current seq2seq implementation?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 12, 2018",
            "body": "Abigail's implementation makes one larger probability distribution from the vocab distribution and the attention distribution, seen here: This accomplishes the same math as what I suggested, but requires a single distribution of size  for each timestep.  Actually constructing this distribution is unnecessary, as you really just need to compute the loss.The part that will be very tricky to make work with our data code is this line: Abigail's code has a separate input that includes token ids for the inputs with no OOVs computed.  You can do that with our data code - just have a separate  for the input that uses the same vocabulary namespace as your target .  The trouble is we compute the vocabulary once, so if you ever want to run this on data that wasn't seen before (like in a demo), this could very easily break, because the tokens you want to copy aren't in the vocab.So, these are the two options, I think:It feels like there are a lot more details that are easy to get wrong in the second way, especially as you think about putting up a demo for the model or using a trained model on new test data.As far as integrating something with our current seq2seq implementation - that implementation was just an example, seeing if we could get any kind of seq2seq model implemented before starting some much more complex semantic parsing stuff.  I wouldn't suggest going out of your way to merge what you have with that, if you are starting from something else.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Oct 21, 2018",
            "body": ",  just mentioned in  that he has a copynet implementation available here: .  He says it's only almost implemented, and I don't want to overload him with a support burden he didn't ask for, but that code may be a good starting place for you.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "aleSuglia",
            "datetime": "Oct 25, 2018",
            "body": ": thank you for mentioning it. I'll have a look asap :)I think we can close this issue!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "schmmd",
            "datetime": "Oct 15, 2018",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Oct 21, 2018",
            "body": [],
            "type": "pull",
            "related_issue": "#1928"
        },
        {
            "user_name": "aleSuglia",
            "datetime": "Oct 25, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/allenai/allennlp/issues/1300",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "matt-gardner",
            "datetime": "May 29, 2018",
            "body": "I'm pretty sure you used to get span highlighting on mouse-over for all of the constituents, but that's not currently happening.  Not sure what happened.More generally, it'd be really nice if we had some tests of our frontend JS code.  I have no idea how to write or set those up, though.  If we had a frontend engineer...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DeNeutoy",
            "datetime": "May 29, 2018",
            "body": "Yeah, I knew about this already:\nIt's pretty tricky to get it to line up correctly, and it's less important for a constituency parse where the tree is more explicit.I'll probably get around to fixing this eventually when i'm bored, so leave it open.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "May 29, 2018",
            "body": "Oh, I thought the earlier demo I saw did this, which is why I opened the issue.  I didn't realize you already knew about this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 14, 2018",
            "body": "Moving to allenai/allennlp-demo.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "schmmd",
            "datetime": "Jun 22, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 14, 2018",
            "body": [],
            "type": "issue",
            "related_issue": "allenai/allennlp-demo#58"
        },
        {
            "user_name": "matt-gardner",
            "datetime": "Sep 14, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/5685",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "astopo",
            "datetime": "Nov 11, 2021",
            "body": "\nIn Studio, when adding a second slot, the first slot on the line above is duplicated. Afterwards, any kind of input (mouse click, key down) triggers more duplicated words on the lines above.\nSteps to reproduce the behavior:\nSuccessfully adding a second slot.\nScreen recording available here: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Nov 11, 2021",
            "body": "What  \nThanks for reporting the issue. We'll prioritize this quickly ! has this been fixed in next branch already ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "franklevasseur",
            "datetime": "Nov 11, 2021",
            "body": "Hey , thx for the report!Can you check if this fixes your issue: Feel free to do some QA and try to break it",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "justis18",
            "datetime": "Nov 29, 2021",
            "body": "Hi, is the problem fixed?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "franklevasseur",
            "datetime": "Nov 29, 2021",
            "body": "Hi , it's not as botpress still uses studio It will be fixed once, botpress upgrades to studio v.0.0.43",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Gordon-BP",
            "datetime": "Dec 2, 2021",
            "body": "Fixed in ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "astopo",
            "datetime": "Nov 11, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "Michael-N-M",
            "datetime": "Nov 21, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "Michael-N-M",
            "datetime": "Nov 21, 2021",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "Gordon-BP",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "Gordon-BP",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Dec 2, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/microsoft/AirSim/issues/138",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "exploke",
            "datetime": "Mar 31, 2017",
            "body": "I have gotten the Airsim environment up and running on linux, but I am unable to make the quadcopter take off. I have followed every step on the Airsim tutorial, and all my software for the Unreal Engine, Airsim, Pixhawk, and Ubunutu is up to date to the tutorial specifications. First, I attempted to use a controller to fly the quadcopter; the controller would be able to arm the quadcopter, but when I thrusted up, nothing would happen. Upon checking the RC inputs on QGroundControl and calibrating the settings multiple times, there was no change in the simulation. With the controller not working, I attempted to use DroneShell to test out different commands, but I ran into the same issues. I was able to arm the quadcopter, but taking off did not work. I either got   or  when I prompted the \"takeoff\" command. Using the \"getimage\" command did save pictures to the Airsim folder (which I verified), so I know the Unreal Simulation is able to connect with the DroneShell. I also observed that using the \"disarm\" command did not make the propellers stop spinning. Does anybody know if I am doing something wrong, and how to fix the errors?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Timebutt",
            "datetime": "Apr 3, 2017",
            "body": "You seem to be experiencing the exact same problems I reported in issue . Just pulled and built the latest AirSim version to test this again, and I can confirm the issues persist.  works fine,  throws the same error you mention. I too wait for a solution ;)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 4, 2017",
            "body": "I added some  under  before trying to takeoff, can you check if that is the issue you are running into here?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 11, 2017",
            "body": "I am getting the same error as above using the actual PixHawk Hardware. Could it be an issue with how it is set up with QGroundControl? I followed all the steps in the tutorial  and its firmware is up to date so I do not know what could be causing this issue.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 11, 2017",
            "body": "When you run DroneShell.exe does it get past the \"waiting for GPS location\" step ?  if not I'm guessing your forgot to put your PX4 in HIL mode by selecting \"HIL Quadrocopter X\" in QGroundControl.  Also, please try the latest PX4 firmware (ignore my video that says version 1.4.4).  Use 1.6.0 instead.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 11, 2017",
            "body": "I was looking at the qgroundcontrol and it kept saying the accel, mag, and gyro calibration failed. I looked it up online and it's not showing me any solutions, so that may be our problem. I ran unreal and drone shell and saw that it was getting past the step of getting the gps location. Additionally I am running the latest PX4 firmware (1.6.0 px4fmu-v2_default.px4).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 12, 2017",
            "body": "Sounds like you need to select the simulation AirFrame as shown below:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 12, 2017",
            "body": " we already selected the HIL Quadcopter X",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 12, 2017",
            "body": "Can you paste a screen shot of the Unreal simulator, you should see some text in the top left that looks like this:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 12, 2017",
            "body": "It says \"taking off Detected\" but the Quad never takes off. We get the same error when using Drone Shell in addition to Drone Shell spitting out (\"drone hasn't came [sic] to expected z of -3.000000 within time 15.000000 sec within error margin or rpclib: function 'takeoff' (called with 1 arg(s)) threw an exception\")",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 12, 2017",
            "body": "How come the drone looks ghosted and has no shadow? Is the drone under the ice? Mine looks like this:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 12, 2017",
            "body": "It is not under the ice, I am using reduced graphics settings so it does not display the shadow of the quad. What should the altitude typically be for this map? I read that this simulator uses the NED frame so should z initially be positive or close to 0?\nWhen I type position on drone shell, it displays Local position: x=0.0410362, y=-0.117092, z=11.0654\nGlobal position: lat=47.6415, lon=-122.14, alt=127.71",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 12, 2017",
            "body": "The z coordinate is a little high (NED coordinate system has negative z when drone is above ground).  This is what I get :position: x=0.00353828, y=0.0042316, z=1.02402\nGlobal position: lat=47.6415, lon=-122.14, alt=122.276Does your drone do a little freefall before hitting the ground when you hit Play ?  Are you getting no movement at all ?  Can you also try \"movetoposition -z -20\" ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 12, 2017",
            "body": "Yes it does do a free fall after we hit the play button but afterwards the drone does not take off at all (even when I type \"movetoposition -z -20\" . Additionally when I turned the graphics to maximum I see a shadow so I know the quad is not inside the ice. I moved the player start to a different location and when I type \"pos\" on DroneShell it says z = 0.271578 which is more reasonable than the previous position I was getting.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 12, 2017",
            "body": "Ok, try and lower your PlayerStart actor in the UE editor a little bit so the freefall is minimized.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 13, 2017",
            "body": "I just checked in a fix that might help with the case where local position 'z' coordinate is unusually high, please pull latest bits and try again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 13, 2017",
            "body": "Actually, just found a proper fix, please try again...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 14, 2017",
            "body": "\nI updated to most current commit, and I am running the simulator on windows. As you can see from the pictures, it is displayed that drone is armed and taking off, but the drone itself is not going anywhere. Using \"movetoposition -z -20\" seems to be making no difference also. Do you know any more solutions to this issue?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 14, 2017",
            "body": "There is a \"CPU\" setting  under Edit/Editor Preferences, search for \"CPU\" and uncheck the box labelled \"Use Less CPU when in Background\".Also, can you attach the PX4 log file mentioned when you ARM the drone, the above shows the path \"/fs/microsd/log/sess021\", you can download the log from PX4 using QGroundControl, or you can use the \"get\" command in MavLinkTest.exe, like this:Then zip the appropriate log and attach it to this issue...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "vatsagandhi",
            "datetime": "Apr 14, 2017",
            "body": "I used QGroundControl to save the log. When I opened QGroundControl though, the display was showing the drone was fluctuating between manual and arm mode before I put in any commands. After attempting to arm the drone, the display on QGroundcontrol was fluctuating between land, takeoff, disarm. The log file I posted is of those occurrences.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 14, 2017",
            "body": "This is the QGroundControl mavlink log, it is good, but I was looking also for the PX4 log (PX4 has an sdcard with it's own logs) which you can download using QGroundControl using this toolbar button:\n, then click\n\non the right side, to see the logs, pick the most recent one and download it.This QGroundControl mavlink log is interesting it does show the following telling status messages:If this is the problem (you have no RC) then you can tell PX4 to ignore that problem by setting the following PX4 parameters:NAV_RCL_ACT 0You can use QGroundControl to set this parameter, the value zero means \"disabled\":",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 14, 2017",
            "body": "I changed the NAV_RCL_ACT to 0 or disabled, and tried running the simulation again. This time, when I went to QGroundControl, the drone was not fluctuating between modes, but takeoff was being denied when I tried. I'm attaching the PX4 log you said to download on the comment above of these occurrences.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 14, 2017",
            "body": "Ok, that is progress, PX4 only denies takeoff if it is not happy with GPS signal being sent by simulator.  So here's some more things to try:Then send me the resulting logs from your d:\\temp\\log folder.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 14, 2017",
            "body": "While or after running the simulator, we go into the MavLinkCom\\MavLinkTest\\build\\x64\\Debug folder and attempt to open MavLinkTest, but the window that opens immediately closes.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 14, 2017",
            "body": "Run it from a DOS command prompt and capture any error message it prints.  I assume here that you are running with ~/Documents/AirSim/settings.json set to \"serial\":true, so the Simulator is connected to PX4 serial port and then the simulator is publishing to 14550 as per:So then MavLinkTest -server:127.0.0.1:14550 will pick up this QGC stream.  Which means if you are running QGC it will not work, which is why I listed \"kill QGroundControl\" above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 15, 2017",
            "body": "Attached is the picture of the error we get when running MavLinkTest. we killed QGroundControl but MavlinkTest still does not work.\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 15, 2017",
            "body": "Note: It is very unusual that anyone would install a git repro into this location:\nThis is the Visual Studio install location.  I normally create a folder at the root of my hard drive named \"git\" then I clone AirSim in there so I end up with d:\\git\\airsim.  Can you move AirSim to a location like this and try again?I'm also guessing you do not have a \"D:\" drive at all, which explains the crash.  Change this argument:-logdir:c:\\tempThis will cause MavLinkTest to write log files to your C:\\temp\\log folder.Then if it still crashes can you run MavLinkTeste.exe with the same command line using Visual Studio and attach the stack trace where it is crashing, thanks.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 15, 2017",
            "body": "\nThank you very much for the help, MavLinkTest worked and I am attaching the log files from MavLinkTest in this comment.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 17, 2017",
            "body": "Thanks for the log, using  I can load the *input.mavlink log and it shows something very weird on the Altitude:Why is the altitude plummeting to -81 meters below ground every 30 seconds or so, very weird.The heading is also drifting a lot, was the drone spinning around by any chance?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 17, 2017",
            "body": "The propellers spin in a very peculiar way; they will start spinning back and forth even before I arm the drone. I also notice the 3 bottom camera views sometimes wobble.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 19, 2017",
            "body": "I re-downloaded and updated all the files. I tried with a different pixhawk and ran into the same problems.This time when I try to run the simulator, the drone shell is unable to get a GPS location which may be a possible explanation to the weird graphs. I am attaching a screenshot of what our unreal environment looks like along with the drone shell. The FPS is usually at 60 before running the simulation, but drops to 15 when running the simulation, and it drops to 3 when running the drone shell as well.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "clovett",
            "datetime": "Apr 19, 2017",
            "body": "Yep, 3 is not enough to fly properly.  Try mouse click on airsim canvas and type \"0\" to remove the camera views, that should help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "clovett",
            "datetime": "Apr 19, 2017",
            "body": "I would also move off that bridge, it has weird collision mesh that is up higher than the bridge appears.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 21, 2017",
            "body": "I lowered the graphics. moved the drone to the location specified on the github tutorial, and removed the camera views. My FPS went up to about 60 before and during running the simulation. Running the simulation now, the propellers are not fluctuating around anymore; rather, the propellers go slower and faster depending on how much throttle I give from my RC controller. However, the drone is still not going anywhere. Also, the propellers do not stop spinning when I issue a disarm command from my RC controller. I then attempted to use the drone shell, but the FPS dropped to about 3 while I had the program running. The interesting thing I found was that the drone shell kept saying waiting for the GPS location, an issue I was not having before. Is it possible that is why my drone is not going anywhere, because the drone cannot pick up the GPS location?\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 21, 2017",
            "body": "See \"\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 21, 2017",
            "body": "Unchecking the \"Use Less CPU when in Background\" checkbox did have a dramatic FPS rise of 120 while using the RC controller, and 70 FPS while using the DroneShell. Although, the drone is still not moving with RC controller, and the DroneShell is still '\"Waiting for drone to report a valid GPS location...\"",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 21, 2017",
            "body": "Can you run MavLinkTest.exe -serial:*,115200, then when it is started type \"params c:\\temp\\px4params.log\" and attach the c:\\temp\\px4params.log file ?  I want to try the exact same parameters on my pixhawk and see what happens. Which version of px4 firmware are you using?  1.4.4 or the latest 1.6.0 ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 21, 2017",
            "body": "I am using the latest version of the px4 firmware (1.6.0). Here is the px4params.log file.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 21, 2017",
            "body": "If you are getting 70fps then flying should be excellent, no problems there.Your params are quite a bit different from mine, but I don't see anything obvious that should affect takeoff.\nYour radio trims are different of course, I assume you calibrated your radio using QGroundControl.I also included the firmware I'm using which I built a couple days ago just to rule out that variable:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 22, 2017",
            "body": "Thank you very much, I used the firmware you provided in the comment earlier and the quadcopter lifts off the ground now. Flight is very stable. I am closing this thread. I really appreciate the help.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lovettchris",
            "datetime": "Apr 22, 2017",
            "body": "great, thanks for closing the issue.  Love to know why my firmware works and yours didn't, but we can leave that for another day.  Cheers.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "udaydlsv",
            "datetime": "Apr 21, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#172"
        },
        {
            "user_name": "exploke",
            "datetime": "Apr 22, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/4469",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "hacheybj",
            "datetime": "Feb 3, 2021",
            "body": "\nThe list of content and transitions associated with a node in the left side panel are quite unfriendly to use. One must first hover over the content for action buttons to appear. This causes losing track of the row we are on, miss clicks, and a lot of back and forth with the mouse.\nFor the actions buttons to be available at all times on each row.\nN/A\nN/A",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "EFF",
            "datetime": "Dec 7, 2021",
            "body": "Not relevant as we have specs for BP 13 that fixes this and more",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Michael-N-M",
            "datetime": "Feb 4, 2021",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "sebburon",
            "datetime": "Mar 8, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#4631"
        },
        {
            "user_name": "hacheybj",
            "datetime": "Mar 9, 2021",
            "body": [],
            "type": "pull",
            "related_issue": "#4620"
        },
        {
            "user_name": "EFF",
            "datetime": "Dec 7, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/3334",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "LuizCesarLeite",
            "datetime": "May 17, 2020",
            "body": "I notice that when we have a flow that is bigger than the screen area, duplicated it force the flow view to fit every node in the same view, turning the nodes pretty small.Steps to reproduce the behavior:I'm on Linux 18.04 LTS, using BP 12.9.1",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 28, 2020",
            "body": "This is intended, the first time a flow is loaded, it will be displayed in its entirety, then you can zoom in to focus on a specific part",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LuizCesarLeite",
            "datetime": "Oct 28, 2020",
            "body": "Thanks for your answer .But when trying to zoom in/out (at least to me) it's necessary roll the mouse wheel A LOT of times.More than that, it's a bad UX approuch: the user, not the application, must decide the view size.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 30, 2020",
            "body": " Good point. We will soon bring back a small change on the flow so you can quickly change to a set of preconfigured zoom levels:Keeping the issue open until then",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2021",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Sep 20, 2021",
            "body": "Hi - fixed, we will close this, please reopen if need be.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "LuizCesarLeite",
            "datetime": "May 17, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "LuizCesarLeite",
            "datetime": "May 17, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "slvnperron",
            "datetime": "Oct 22, 2020",
            "body": [],
            "type": "unassigned",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 28, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 28, 2020",
            "body": [],
            "type": "added",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Oct 30, 2020",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Oct 30, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2021",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "J-FMartin",
            "datetime": "Sep 20, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/2579",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Namec",
            "datetime": "Nov 8, 2019",
            "body": "Hello all,I am new to botpress and trying To achieve a task that can make me send data through\nwindow.botpressWebChat.init method. And then storing this additional data as temp variable.Is there any solution to fetch the url custom paramsBest",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Nov 14, 2019",
            "body": "Hi  , what are you trying to achieve exactly? You can trigger an event sent by the user by sending  , then capture the event on Botpress.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Namec",
            "datetime": "Nov 14, 2019",
            "body": "Hello  yes this is exactly what i a m trying to achieve, but i need that process trigered at the begining of the chat session to route the flow the right was.Last but not the least, how To get Back this data with my flow.Best",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Nov 19, 2019",
            "body": "  Please have a look at the proactive example in the documentation: It triggers once the webchat is loaded, then you can send the event with your custom payload.You can create a hook to fetch the data. Ex:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Namec",
            "datetime": "Nov 20, 2019",
            "body": " thank you for you help, this is exactly what i am looking for. you'r the bestand this can make me really add any process i want. i just need to master the syntax and now all the variables.in my hook i am trying  to store the value of event.payload.text, on a temp variable,  and the get it back in the flow to show it to the user",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Namec",
            "datetime": "Nov 20, 2019",
            "body": "i have succeded getting and parsing a custom payload each time webchatOpened ! and that is what i am looking for.but now i 'am trying to store that parsed values, in temp variable, but i dosen't work !! or i don't to now how to write it down. (assigning a value to a temp variable and then getting that value back.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "allardy",
            "datetime": "Nov 21, 2019",
            "body": " When using the code editor, you have access to the typings, which auto-completes variables for you. There is a small difference between actions and hooks. In actions, we added a shortcut to access  variables.You can see the difference in signatures below:Basically, you just need to use  instead of using directly  when using the hookDon't hesitate to move your mouse over squiggly lines, it will tell you why there's an errorBTW, if you skip the dialog engine, the event will not trigger any flow, and when it ends, the temp variable is cleared. You can set the flag FORCE_PERSIST_STATE, and I recommend you use either  or  instead of temp (user is kept indefinitely for the user, while session is kept until the session timeouts, by default 30 mins)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "epaminond",
            "datetime": "Apr 25, 2020",
            "body": " , I assume you were able to succeed with your task so closing this for now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Namec",
            "datetime": "Nov 8, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Nov 8, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "epaminond",
            "datetime": "Apr 25, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/botpress/botpress/issues/2280",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": "This is so strange I am not sure if this may be somethign on my end specifically...?! Anyway, symptoms:If I launch the Studio and go into editing a bot, in the lower right of the screen are buttons to open the Emulator, to see notifications, etc. These buttons are non-responsive. I cannot click them using my mouse. I tried on a production server and also on a dev enviroment. I tried on 2 different computers. I also tried a brand new install of V12.1.1, and the same.HOWEVER, if I open the developer console in my browser (press F12), THEN these buttons become active. If I close the debug console, they become inactive again.Same happens on a different computer as well. One computer is a Windows 10, the other is a Windows Server 2012.In both cases I tried the latest Google Chrome browser and also Microsoft Edge. The same happens.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": "Here is a screen video of the above:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": "Honestly, have no idea why and how - as you saw we tried different installations of Botpress on different computers. And now on same two computers and same bot instances: it magically works again.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "lampyon-canada",
            "datetime": "Aug 22, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/4666",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "JoyMazumder",
            "datetime": "Sep 20, 2021",
            "body": "Hello everyone, I am experimenting with xodr file and tried to load it into carla. I run the following command  and got the output something like this. The load are not aligned properlyI followed this tutorial to install Carla  (Debian installation). OS linux 18.04, UE ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JoyMazumder",
            "datetime": "Sep 20, 2021",
            "body": "Running  works fine\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hitabm",
            "datetime": "Sep 25, 2021",
            "body": "After loading xodr map into CARLA, you can move camera via holding mouse click and WASD keys on keyboard.\nYou see roads as invisible because the camera is viewing them from bottom then just move camera upward and roads will show up correctly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CaffreyXu",
            "datetime": "Apr 13, 2022",
            "body": "hello, I have the same question as you. The lane line can't be seen. Do you solve the question? Thank you so much",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CaffreyXu",
            "datetime": "Apr 13, 2022",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "hitabm",
            "datetime": "Apr 16, 2022",
            "body": "Hi, you can draw string on each lane. It will show red signs for each lane in server side window. Use this code:\n\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "CaffreyXu",
            "datetime": "Apr 18, 2022",
            "body": "  Thank you so much! I saw many useful function in \"debug\" library, I will try to draw the line i want.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "JoyMazumder",
            "datetime": "Sep 20, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/3616",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "GustavoSilvera",
            "datetime": "Nov 21, 2020",
            "body": "Hello Carla team!I am working with the python clients for  and was wondering if there is a good way to move the perspective of the user (or camera) around with the mouse. Specifically a mechanism like the simulator's  view but on the client pygame instance would be ideal!I tried something with destroying and reinitializing the camera sensor but this is very slow. Similarly flooding the server with  requests for  is also very slow. I'm not sure of a better way to do this.Any help is appreciated!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Axel1092",
            "datetime": "Mar 4, 2021",
            "body": "Hi , you could use pygame to detect position and drag of the mouse to change the camera orientation accordingly  with the  function you mention but there is no better way to do this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GustavoSilvera",
            "datetime": "Mar 4, 2021",
            "body": "Hi , thanks for your suggestion. Unfortunately I already tried something similar to this and found that it does not suit my purposes well as it is very laggy with a moving vehicle and I was looking into VR support.In this regard I've transitioned to using the UE4 game itself (simulator 'server') as the main viewpoint since UE4 has native VR support and so far this approach has been successful.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Mar 4, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "corkyw10",
            "datetime": "Mar 4, 2021",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "GustavoSilvera",
            "datetime": "Mar 4, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/3578",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "flacomalone",
            "datetime": "Nov 12, 2020",
            "body": "Hi,When running CARLA server, if I try to use the mouse to drag and change the orientation of the camera (not the position), it will, but so much I have no control on it. Is there any way to change mouse sensibility? I don't remember having this issue in previous versions of CARLA",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "flacomalone",
            "datetime": "Nov 12, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/3080",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "RansML",
            "datetime": "Jul 18, 2020",
            "body": "I have been running carla_release_0.9.5 on Ubuntu 18.04 (with cuda 10.0) without having any trouble. I just downloaded the latest Carla binary (carla_release_0.9.9).Now  gives the following error.Running  shows nothing after printing the first two lines:It looks like the GPU is trying to run something.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jul 20, 2020",
            "body": " could it be that your driver doesn't support vulkan or that you haven't installed the vulkan dependencies? It seems that it is working perfectly using opengl (yes, that is what those 2 lines mean...everything is working fine). Could you check your driver, dependencies, etc?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RansML",
            "datetime": "Jul 20, 2020",
            "body": "Just to summarize,I also tried on a different laptop on Ubuntu 20.04. I get exactly the same error.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Axel1092",
            "datetime": "Jul 20, 2020",
            "body": "Hi ,\nUsing the  is intended to disable rendering (no window will appear) and it only works with the  option. After running  have you tried to connect to the server? Try running the  script in the PythonAPI/util folder.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RansML",
            "datetime": "Jul 20, 2020",
            "body": " When I remove  it loads the CarlaUE4 window and I can interact with it using my mouse. However, nothing shows on the CLI after, fails with the messageWhen I run v.0.9., lots of print messages are typically displayed on the CLI before I can actually connect successfully.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "wendersonj",
            "datetime": "Aug 24, 2020",
            "body": "Using  worked for me.\nGPU: GeForce MX130 (2GB)Cheers",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Oct 24, 2020",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jul 20, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jul 20, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Oct 24, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Nov 8, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/2879",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "NicoSchoenfisch",
            "datetime": "May 22, 2020",
            "body": "Hey everyone,so I have a problem right now that might be pretty basic, but I didn't find a solution in the docs for it.I followed the instructions to build Carla on Windows step by step from here: \nand am now stuck at the very last step.'make launch' and 'make PythonAPI' both finished successfully, and the project is also opened in UnrealEngine and looks like this:citing from the instructions, my steps after 'make launch' should be this:\"make launch compiles the server simulator and launches Unreal Engine. Press Play to start the spectator view and close the editor window to exit. Camera can be moved with WASD keys and rotated by clicking the scene while moving the mouse around.\"Now under the \"Play\" Tab in Unreal Engine, I can choose a mode. If I choose anything other than \"Standalone Game\", I get this message:If I choose to play anyway, this is what it looks like:On the other hand, if I choose \"Standalone Game\" as the mode, I don't get the warning or an error message, but I still end up with the black screen and this grey stick-thingy in the middle, like this:From what I understand from reading the instructions and watching some videos, by pressing \"Play\" at this point after the build, I should be able to start a simulation of the current map (Town03 as standard) in which I can move the camera with WASD, instead of having this black screen. It's definitely possible I missed something I have to do before it works, but I have no idea what it is.Could someone maybe help me out with this?Thanks in advance!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jun 26, 2020",
            "body": " you are probably the right person for this task.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bernatx",
            "datetime": "Jul 6, 2020",
            "body": "Hi  did you download all the content assets? It seems that you don't have it.\nRefer to this link with instructions: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 5, 2020",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jun 26, 2020",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "germanros1987",
            "datetime": "Jun 26, 2020",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 5, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 12, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/2763",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "ZhengZhongyuan11",
            "datetime": "Apr 18, 2020",
            "body": "I just want to know is there any way that we can enable mouse in carla and use the mouse to interact with the carla world? Just like click on a point in the map and get the location information of that point?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yaknostoyok",
            "datetime": "Apr 23, 2020",
            "body": "Hi,  !Unfortunately, this kind of feature is not implemented in CARLA.\nHowever, should I guess that you are working with a CARLA package? If so, maybe you could consider moving to a build from source. That would allow you to open CARLA with the Unreal Editor, and maybe provide you with the information you need, depending on the exact intentions.\nHere is a link to the docs, where the build process for Linux and Windows are explained: ",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ZhengZhongyuan11",
            "datetime": "May 1, 2020",
            "body": "OK I will try it, thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "yaknostoyok",
            "datetime": "Apr 23, 2020",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": null,
            "datetime": "Apr 29, 2020",
            "body": [],
            "type": "",
            "related_issue": "#2800"
        },
        {
            "user_name": "ZhengZhongyuan11",
            "datetime": "May 1, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/2525",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "anna-srl",
            "datetime": "Feb 27, 2020",
            "body": "Hi everyone and thanks for your amazing work!I'm a CARLA newbie and I recently installed CARLA 9.5 in windows.I was able to spawn npcs, run manual control and follow several initial steps by the available python modules.However, in the town window I have not been able to zoom in or out, either by the mouse wheel or the keyboard buttons. I am aware that there is an issue with my server (not the required GPU) and I will resolve this issue soon. However, since the ASDW buttons work,  I was wondering if the issue is something different..Any ideas?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "amstrudy",
            "datetime": "Mar 4, 2020",
            "body": "The \"zooming in and out\" is done through the WASDQE keys, where W is forwards, A is left, S is backwards, D is right, Q is down, and E is up in relation to your player frame of reference. You can only use the mouse to move around when using the editor. (Are you running via  or ? If it's the latter then you don't use the mouse to navigate.)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anna-srl",
            "datetime": "Mar 4, 2020",
            "body": "Yes, that was it! I am using  , that's why the mouse was not working. Thank you very much for your help!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "amstrudy",
            "datetime": "Mar 4, 2020",
            "body": "Yes, no problem! If you don't have any more questions, please close the issue :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anna-srl",
            "datetime": "Mar 4, 2020",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1844",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "benkmoore",
            "datetime": "Jul 4, 2019",
            "body": "CARLA 0.9.5\nUbuntu 16.04When I launch the CARLA sim the mouse becomes the change heading view control and I have to force quit the sim with tty6.Is there a way I change the controls so that the mouse is not used by default?Thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "benkmoore",
            "datetime": "Jul 8, 2019",
            "body": "Crtl+Alt+T to open a terminal and the mouse control comes back",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "benkmoore",
            "datetime": "Jul 8, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/2186",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "RonGros",
            "datetime": "Oct 25, 2019",
            "body": "Hi,\nI have a laptop (Dell 7490) with Ubuntu 19.X. I tried running Carla and it froze the entire computer.\nMeaning - if I run it, I can sometimes actually use the AWSD keys to navigate the demo, but if I try anything else (say CTRL+TAB) the entire OS freezes and I have to reboot the machine.\nI tried upgrading from 19.04 to 19.10 because I know some changes were made, but the same occurs there.\nNow - I don't know if it is an OS issue (was it ever tested on Ubuntu 19.X?) or a hardware issue (it has intel graphics and not nVidia).\nIs there something I can do? are there any logs I can raise and send for someone to have a look?Thanks!\nRon",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cassfalg",
            "datetime": "Oct 31, 2019",
            "body": "Out of memory? Is the disk active when it freezes? What specs does it have?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RonGros",
            "datetime": "Oct 31, 2019",
            "body": "Hi,\nI don't know if the disk is active because the computer totally freezes.... if you can help me in how to find the logs that can help.\nThe computer has 16gb of RAM so I doubt memory is the issue here....I really wanted to run the carla on windows (I have dual boot) so that I can try and understand if it is a hardware issue or an OS thing, but I could not find any pre-built binaries and we don't have visual studio license here... (we build on linux...)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cassfalg",
            "datetime": "Oct 31, 2019",
            "body": "I'm sorry, but this is not really a good place to ask for general Linux support. This is the carla issue tracker. It tracks issues within Carla. And while Carla might trigger those issues, it's not likely to be the issue itself. A lot of information can be found online with a search for \"ubuntu froze what do i do\" or similar.Like you don't think that memory is the issue. Have you checked? Googled for \"linux memory usage monitor\" or similar, tried a tool, started carla, checked if there's still free memory while it runs? 16 gig is not much. I'm running out of memory on my 16 gig machine using carla with my use case for example.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RonGros",
            "datetime": "Oct 31, 2019",
            "body": "Let me make it clear,\nThis is NOT a linux issue. My ubuntu runs flawlessly no matter what I do, and as a programmer I do run some memory hogging programs and never encountered any issue,\nNevertheless once I run Carla, even when it is the only thing running on my computer, the computer freezes completely.\nI suspect it is either an OS API that went bad or something similar\nI didn’t even load anything specific to Carla...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cassfalg",
            "datetime": "Oct 31, 2019",
            "body": "Well, if you could monitor memory consumption while you start carla we could exclude the scenario that the machine is simply running out of memory and swapping heavily? That can make a system so unresponsive that it feels frozen. Because from your description I am not sure if your system is frozen or just really really slow. A lot of PCs/Notebooks have indicator lights somewhere to indicate hard disk activity, that's what my question about hard disk activity was aiming at.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RonGros",
            "datetime": "Nov 6, 2019",
            "body": "Hi, so sorry for the late response but I had to create a presentation and was on my windows for the last few days.\nNow I am back to Ubuntu and tried it again. so first time I tried to run Carla it actually ran pretty smoothly (except for the fact that I didn't know how to release the mouse from it)\nThen I tried to change the window size and then it froze again. from the moment, even after I rebooted it started freezing the entire computer again\nI did run it with the system monitor, the memory was at around 40% all along, regardless of whether it worked a bit or not.\nI then thought maybe it was because I was using an external display, so I disconnected my laptop from the docking station and tried - but it was actually worse, it didn't even render a single frame before freezing the entire computer\nI also tried taking a video but because it freezes everything the video file wasn't written to disk...Any suggestion on how to debug it next?!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "cassfalg",
            "datetime": "Nov 6, 2019",
            "body": "You can Alt + Tab to release the mouse.How did you change the screen resolution? And of what? The  server windows, or some client?Did you disconnect the docking station while the simulator was running?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "RonGros",
            "datetime": "Nov 7, 2019",
            "body": "Okay, so some answers and updates....I did not change the screen resolution, I changed the size of the window (it was not on full screen) I changed the size of the CarleUE4 windowOf course not. I made sure all windows are closed, then disconnected and then tried itNow for the update:So it only happens without the -opengl (although I was sure I tested it before), and as a standalone window",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "kbarbora",
            "datetime": "Nov 8, 2019",
            "body": "is all about the GPU that you computer has. If you are running it with the CPU graphics, it will freeze.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jan 7, 2020",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jan 7, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jan 14, 2020",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1319",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "micmarty",
            "datetime": "Feb 28, 2019",
            "body": "Remote client command: \nInfo that probably doesn't matter (but who knows ):pygame opens a window for a very short time with text , then it crashes:Problem first noticed here:\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "micmarty",
            "datetime": "Feb 28, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "carla-admin",
            "datetime": "Mar 30, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1630",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "morningsky",
            "datetime": "May 10, 2019",
            "body": "According by , we can get collisions  info by , and it will print info about collisions (I use jupyter notebook), but how can I get the collision image or video, such as\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bernatx",
            "datetime": "May 13, 2019",
            "body": "Following the tutorial the steps are:That is how it was done in the tutorial.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "morningsky",
            "datetime": "May 13, 2019",
            "body": "  Thanks！ But... How can I start the replayer at that exact moment \"? Which API can do this or command? I mean how to let it run in simulation env",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bernatx",
            "datetime": "May 13, 2019",
            "body": "The command you wrote from the tutorial (\"client.replay_file(\"col2.log\", 13, 0, 122)\") means to start replaying the file 'col2.log' at time 13 (seconds), until the end of the replay, and following the vehicle with id.122. So, the second parameter is the time to start replaying.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "morningsky",
            "datetime": "May 15, 2019",
            "body": " Thanks very much! I know the mean of these params, but how can I get the images/vews when cilent.repaly_file()? when I run the code client.replay_file, it just prints info on terminal, and the pygame windows keep no change...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bernatx",
            "datetime": "May 15, 2019",
            "body": "Which info do you get from terminal?\nIt should say it is replaying the file, and then the server starts replaying the simulation.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "morningsky",
            "datetime": "May 15, 2019",
            "body": "  Thanks, I solved it. When I run CarlaUE4 it start a window 1,  then I run manual_control.py it start  window 2. when I run the code  client.replay_file , the replaying video running in the window 1.... I just focus on windows 2.\nAnd I have another problem, when I run CarlaUE4, my mouse blocked in the window 1, I have to get the mouse point by use Win+D, it's not convenient， Do you have any advice？\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "bernatx",
            "datetime": "May 13, 2019",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "morningsky",
            "datetime": "May 17, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1533",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "klemen666",
            "datetime": "Apr 12, 2019",
            "body": "I open Carla server on one computer and i open python client on another (py manual_control.py) which opens PyGame. In the HUD it says both server and client are running with 60fps, but the rendering in client is bad - seems like 15 fps. What could be the reason? Network between computers is 1Gbit. I'm using Carla 0.9.4 on Windows.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tanghui0102",
            "datetime": "Apr 19, 2019",
            "body": "\nMaybe I have the same problem with U.And I haven't dealt it yet.\nDo U run the map of carla own  or other maps?There is one tip is that when the carla server is runnning,never let the server in the background.That's to say,when the server running,the mouse and keyboard can only move the map's perspective and location of camera,cannot operate other programs.Otherwise,the fps is low.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Xiaoyu006",
            "datetime": "May 17, 2019",
            "body": "when I ran manual_control.py using carla 0.9.5,the fps of server is about 15 fps.\nthat of client is about 60fps, which is good enough.But when i ran manual_control_steeringwheel.py on carla 0.9.5.\nthe fps of server is about 15 fps.\nthat of client is about 10fps, which is bad enough.Not fixed now.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "tanghui0102",
            "datetime": "Jun 1, 2019",
            "body": "Now I see.In carla,we can set it to run in the background without lowering the cpu.Open 'Edit'-->'edit performance'-->'performance'-->'use less cpu when in background'.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jul 31, 2019",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jul 31, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Aug 7, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/312",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "GokulNC",
            "datetime": "Mar 27, 2018",
            "body": "When I run  from my VNC, I get the following error::\nOS: Ubuntu 16.04\nCARLA: Release version 0.7.0\nNvidia drivers version: 390.42\nOpenGL version: 4.6Also, if you notice, why does it have  's path? ;)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "felipecode",
            "datetime": "Mar 27, 2018",
            "body": "Hey \nThanks for the issue.Can you tell me exactly which commands you executed ?\nAre you using VGL as well ?\nAre using our tutorial ? \nIt seems your vnc didn't work, the display is not available.Cheers.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "GokulNC",
            "datetime": "Mar 28, 2018",
            "body": "Thanks  for you quick reply.I solved the problem.\nHere are the steps I took:So, I'm closing this issue :)I think apart from the docs, this is also another way to run CARLA on a headless server.\nDo you think it can be added to the docs (since it's actually very simple steps, and guaranteed to work)?Also, just out of curiosity, why did my error logs have nsubiron's path? ;)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "chankim",
            "datetime": "Nov 3, 2018",
            "body": "Hello, I exactly followed the doc : \nbut when I connect using RealVNC from windows 10 to my machine xx.yy.cc.dd:8, I get just the purple screen. seems like there is no windowmanager running.  I understand the last command \"DISPLAY=:8 vglrun -d :7.<gpu_number> $CARLA_PATH/CarlaUE4/Binaries/Linux/CarlaUE4\" should be run on the vnc window. Am I correct? (I mean, after this window manager problem is solved)\nand I didn't run \"sudo service lightdm stop\" before I ran \"sudo nohup Xorg :7 &\".\nIs the document above correct? shouldn't I modify the xstartup file for TurboVNC?\nUpdate : I tried \"/opt/TurboVNC/bin/vncserver -3dwm\", and now I can see folders on the background and can use right mouse click to make pull down menu appear, but it disappears and blocks constantly (with period 1,2 secons) so I can't use it. So my guess the window manager is not started was correct. Any adivce appreicated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nuomizai",
            "datetime": "Jun 18, 2020",
            "body": "Hey  , the doc link  does't work anymore. Does carla still support running from headless server?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marcgpuig",
            "datetime": "Mar 27, 2018",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Mar 27, 2018",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "felipecode",
            "datetime": "Mar 27, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "felipecode",
            "datetime": "Mar 27, 2018",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "GokulNC",
            "datetime": "Mar 28, 2018",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1390",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "HeYDwane3",
            "datetime": "Mar 15, 2019",
            "body": "Ubuntu 18.04\nCarla 0.9.4I created a map in roadrunner, but when I exported it and imported into the Carla, I can only see the part within -300<x<300, -300<y<300. Is there any way I can see the full map? I am not sure if it is the problem from Roadrunner or the Carla.Anyone can help me?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "Mar 18, 2019",
            "body": "Hi, The problem is from RoadRunner, if you would create a big map, you need to change the world settings. Wich version of RoadRunner do you use?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "eloy96vg",
            "datetime": "Apr 25, 2019",
            "body": "Hi I have the same problem to add the full map. I'm using version 2019.0.4 of RoadRunner. What do i need to change in the world settings to get the full map? Would i solve this problem by following the steps indicated with this new way (Docs/generate_map_from_fbx.md)?Thanks in advance.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "Apr 26, 2019",
            "body": "Hi If you create a huge map you need to change the world setting.  Move the blue frame until the map stays inside. ( put the mouse above of frame and move it) After that confirm the changes in \"Apply World Changes\".Every version RoadRunner change a bit, but the philosophy is the same.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DongChen06",
            "datetime": "Jul 3, 2019",
            "body": " Where can I found the Roadrunner files of Town 02 and Town01, so I can make some modification to create a new map.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "Jul 4, 2019",
            "body": "Hi, @Derekabc well, if you download our .xodr files you can load in RoadRunner. RoadRunner should generate the map with OpenDrive information.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 2, 2019",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Mar 18, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Mar 18, 2019",
            "body": [],
            "type": "assigned",
            "related_issue": null
        },
        {
            "user_name": "XGodina",
            "datetime": "Mar 18, 2019",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "carla-admin",
            "datetime": "Mar 30, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 2, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Sep 9, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1006",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "anshulpaigwar",
            "datetime": "Nov 30, 2018",
            "body": "Carla 0.91 has loads of useful features that can be used for research, thanks to the carla team.\nBut unlike in Carla 0.8x versions, my viewpoint in the map does not move with the player vehicle. We have to manually move it with arrow keys and mouse.I want to be able to see the environment around my player vehicle and what it is doing. I understand that this is not the case in Carla 0.9x due to multi-client support and we would have to do this using something like pygame. But it would be nice if we could directly shift viewpoints over different player vehicles in Carla window as in 0.8x version.\nIt would  be helpful if I could get the sample pygame code for viewing the camera outputs as in your video of Carla 9.0",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "M-Hammod",
            "datetime": "Nov 30, 2018",
            "body": "you easily can do that  by piloting the camera attached to the vehicle, after spawning the vehicle from python, in the unreal editor-->world outliner-->search for 'cam'-->you will see the a camera actor attached to the vehicle that you spawned-->rightclick on it and choose pilot.I had a proplem that I had to eject the player for the pilot option to get activatedhope this helps",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 30, 2018",
            "body": "The  provides a view similar to that in Python with pygame.Alternatively, you can also move the \"spectator\" actor from Python to move the simulator view.e.g.,  uses this trick.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anshulpaigwar",
            "datetime": "Nov 30, 2018",
            "body": "Thanks for the reply it helped to solve my problem.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "anshulpaigwar",
            "datetime": "Dec 4, 2018",
            "body": "How to attach spectator to the vehicle like a camera ?? I tried this simplyThis kind of works but there are two problems:",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Dec 4, 2018",
            "body": "Unfortunately, the transformation matrices are not yet implemented in the new API, but you can get them in Python from the old API at .The best way for synchronizing the client with simulator updates is using the wait for tick function",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Sakrust",
            "datetime": "Dec 5, 2018",
            "body": "Just use a little trick by placing a dummy actor in the scene:camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\ncamera_transform = carla.Transform(carla.Location(x=10, z=10))\ncamera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)\nspectator.set_transform(camera.get_transform())",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Dec 5, 2018",
            "body": "That's a nice workaround :D but I would use instead a cheaper sensor, collision sensor for instance, cameras make your FPS drop significantly.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Feb 3, 2019",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ChrisHRZ",
            "datetime": "Apr 17, 2019",
            "body": "How can i chose to view this camera?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Sakrust",
            "datetime": "Apr 17, 2019",
            "body": "You don't. Your simulator window view will be updated.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "gunki-geek",
            "datetime": "Jan 25, 2022",
            "body": "u can write two scripts master.py and slave.py and synchronize between the two like in the attached scripts\n",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 30, 2018",
            "body": [],
            "type": "self-assigned this",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 30, 2018",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Feb 3, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Feb 11, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1487",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "Narendarselva",
            "datetime": "Apr 3, 2019",
            "body": "Hi Team,\nI m working on a project which requires creation of vehicle on timely interval on a desired location on a desired lane . Is it possible using crala simulator ??\nI gone through the PythonAPI documents there are lot of options which can use random spawn_points which choices randomly . And i dont know exactly how the lane based APIs are used .\nKindly help me on this .\nIs there any specific examples for this is also welcome .\nThanks\nNaren",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Apr 4, 2019",
            "body": "Hi ,You can spawn vehicles at any location/rotation in the map, the only problem is that the autopilot won't work correctly everywhere, that's why we have recommended spawn points.Alternatively, you can use waypoints to project locations to the road",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Narendarselva",
            "datetime": "Apr 5, 2019",
            "body": "Thanks a lot.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Apr 5, 2019",
            "body": "",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Narendarselva",
            "datetime": "Apr 8, 2019",
            "body": "Thanks a lot.\nIs the transform_to_geolocation is provided as PythonAPI ??\nHow i can make my route in a MAP ??\nI could see predefined routes and spaw_points inside all maps , But what if i want to create my own route in MAP in UE4 editor?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DongChen06",
            "datetime": "Apr 19, 2019",
            "body": "Hi  , I have a similar problem as you,  I want to use the multi-lane senario of the Town3. So I want to put some car models along with a self-driving car on the particular position, Have you figure out how to implement this by a simple way?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2019",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "talsperre",
            "datetime": "Jun 26, 2019",
            "body": "Hi @Derekabc, could you find any way of positioning car models in other lanes near the ego vehicle?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "DongChen06",
            "datetime": "Jun 26, 2019",
            "body": "you can use the code like this :to inform the waypoint of the lane next to the ego car and then spawn a car on that point.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "talsperre",
            "datetime": "Jun 26, 2019",
            "body": "@Derekabc, thanks a lot. This works perfectly for my use case. Although, I had to generate the waypoints approximately 5-10m ahead in order to avoid collision.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "lichunly",
            "datetime": "May 22, 2020",
            "body": "@Derekabc hi, is the function that getting the location on map by mouse-pick possible now(carla 0.9.9)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "fty0724",
            "datetime": "Apr 22, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#1561"
        },
        {
            "user_name": "stale",
            "datetime": "Jun 18, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "Jun 25, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/1296",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "liqi198786",
            "datetime": "Feb 25, 2019",
            "body": "hello,i'm making a self-driving test platform with CARLA.\ni'm now working with CARLA 0.9.2. i just use the manualcontrol.py included in the realse version. i made some changes in the script that i added a lidar and attached it to the vehicle. the lidar has been added successfully and i could get the pointcloud from it. but in the game scene, i could not see the lidar being rendered.\nis there any method i could see the sensors added to the vehicle?thanks",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "YashBansod",
            "datetime": "Feb 26, 2019",
            "body": " in the  the following docstirng is provided:Which basically tells that you can change the sensor visualizations on the hud using the keys [1-9]. However, this only creates a new sensor using the camera manager (it creates lidar too) and visualizes that. To visualize the sensor you have added yourself, you can modify the code in the HUD to plot the point cloud generated by your lidar. (Similar to how its already implemented for lidar created using camera manager).",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Feb 26, 2019",
            "body": "hi ,really thank you for your reply. in fact,i didn't mean the visualization for pointcloud data from the sensor, i mean the sensor itsself like a lidar or a camera. i'd like to know where i have placed it on the vehicle.\n\njust as what shown in the picture above.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "YashBansod",
            "datetime": "Feb 26, 2019",
            "body": " I do not think that the sensors have any physical body in the simulator. I think this question would then be better handled by CARLA's members.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Feb 26, 2019",
            "body": ",thanks, so i would not be caught up in the issue. just wait. maybe they will add it in future versions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jagdishbhanushali",
            "datetime": "Mar 6, 2019",
            "body": "Hi  , Sensors do not have physical body as you mentioned in your image. But you can see where sensor is placed in carla simulator. For that please follow below steps.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Mar 7, 2019",
            "body": "hi,, thanks for your reply. the situation for me now is that i once compiled the 0.9.0 version(so i could launch the UE4editor of v0.9.0), but now i'm working on v0.9.2 with the released binary, and also have made some changes in the mannualcontrol.py script of v0.9.2.\ni just tried to start the mannualcontrol.py of v0.9.2 to connect the carla server launched from v0.9.0 UE4editor. unfortunately, it failed with the message 'RuntimeError: rpc::rpc_error during call in function get_episode_info'.\ni'd like to know if there is any way to edit the uncompiled(myself) released binary map level(like v0.9.2) in the once compiled UE4editor(v0.9.0 for my case) just through changing the folder content or something?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "jagdishbhanushali",
            "datetime": "Mar 7, 2019",
            "body": "You can't run manual_control.py of v0.9.2 with 0.9.0 server. However you can run carla v0.9.0 in Unreal editor, with manual_control.py of v0.9.0.  You should be able to do your task.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 6, 2019",
            "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "ShanshanYOU",
            "datetime": "May 16, 2019",
            "body": "Hi ,\nThanks for sharing all those steps. I've tried to follow it but just for the second step it was stoped. Python scripts were failed to launch because fo an runtime error : \"RuntimeError: rpc::rpc_error during call in function version \". Any further information will be really welcomed !Cheers !",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Feb 25, 2019",
            "body": [],
            "type": "changed the title",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Feb 26, 2019",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "liqi198786",
            "datetime": "Mar 7, 2019",
            "body": [],
            "type": "reopened this",
            "related_issue": null
        },
        {
            "user_name": "carla-admin",
            "datetime": "Mar 30, 2019",
            "body": [],
            "type": "added this to",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 6, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "stale",
            "datetime": "May 13, 2019",
            "body": [],
            "type": "",
            "related_issue": null
        },
        {
            "user_name": "ShanshanYOU",
            "datetime": "May 16, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#1015"
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/46",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "farzaa",
            "datetime": "Nov 21, 2017",
            "body": "Hi all!If I wanted to create my own driving dataset by driving in Carla manually myself (ex. for an hour), would I be able to grab the exact steering angle? Right now,  script obviously would let me do this. But, it seems the steering angle is set to either -1 or 1 depending on if I'm holding down A or D. But in reality, steering angles change smoothly between -1 and 1 and don't jump because a person needs to actually turn the wheel.The code from the script:\nSo, this would mean I would lose all those angles in between, correct? So my ground truth for steering angles would be just a bunch of -1's and 1's. Is this intended?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "felipecode",
            "datetime": "Nov 22, 2017",
            "body": "Hey farzaa.\nThat is correct !\nWell, this happens because of limitations of using a keyboard ( That keys are either pressed or not pressed)\nIf you have a different joystick, with sensitive buttons, you could access a continuous value and also use pygame.\nAlso consider using the in game AI for data collection , please check  .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Tak-Au",
            "datetime": "Nov 23, 2017",
            "body": "I modify the manual control file to take mouse input so you can steer left or right with the mouse.  This will give you continuous steering angle value.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adiveloper",
            "datetime": "Oct 22, 2018",
            "body": "How do I record data any idea ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "adiveloper",
            "datetime": "Oct 22, 2018",
            "body": "I am new to carla. can someone please explain me how to start recording data(camera and steer,brake) train the data and then run my model based on training?\nA very very brief overview will be highly appreciated....",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "Fangwq",
            "datetime": "Jan 29, 2019",
            "body": "I'm new to Carla. I want to gather the data during each driving simulation and use it to train my own model. How to do it ?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "haoliHuan9",
            "datetime": "Jul 21, 2022",
            "body": "hi i am just wondering if you solve this problem to record your own data like steer angle, brake.. and i will be very appreciate if you can share your solution. Thanks a lot!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 22, 2017",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 30, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "tanglrHello",
            "datetime": "Jan 8, 2019",
            "body": [],
            "type": "issue",
            "related_issue": "#1101"
        },
        {
            "user_name": "atusi-nakajima",
            "datetime": "Sep 5, 2020",
            "body": [],
            "type": "issue",
            "related_issue": "#3240"
        }
    ]
},
{
    "issue_url": "https://github.com/huggingface/datasets/issues/2708",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "danyaljj",
            "datetime": "Jul 22, 2021",
            "body": "The training instances are not loaded properly.For test and validation, we can see the examples in the output (which is good!):However, only a few instances are loaded for the training split, which is not correct.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "albertvillanova",
            "datetime": "Jul 23, 2021",
            "body": "Hi , thanks for reporting.Unfortunately, I have not been able to reproduce your problem. My train split has 8134 examples:and the content of the last 5 examples is:Could you please load again your dataset and print its shape, like this:and confirm which is your output?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "danyaljj",
            "datetime": "Jul 23, 2021",
            "body": "Hmm .... it must have been a mistake on my side. Sorry for the hassle!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "danyaljj",
            "datetime": "Jul 22, 2021",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "danyaljj",
            "datetime": "Jul 23, 2021",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        }
    ]
},
{
    "issue_url": "https://github.com/carla-simulator/carla/issues/26",
    "issue_status": " Closed\n",
    "issue_list": [
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 16, 2017",
            "body": "Hi,\nI am writing a ros wrapper for carla client, almost finished... but I get confused about the format of depth image received from the client.In your code, you do the post-processing like this for visualization purpose:andwe can see that you actually use the three channels of the received depth image. What is the physical meaning of the three channels? Why you normalize the three channels use different coefficients?How can reconstruct the ground truth depth from your format (I mean for example 1.2m 3.3m etc for every pixel)?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 16, 2017",
            "body": "what about the intrinsic parameter of the capturing camera? so I can reproject the depth to 3d point cloud",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marcgpuig",
            "datetime": "Nov 16, 2017",
            "body": "Hi ,\nThis is due our depth codification, we use the 3 different channels to accomplish the maximum precision that we can get from a image format of that size.24 bit floating precision point codified in the 3 channels of the RGB.\nThe order from less to more significant bytes is R -> G -> B.Our max render distance (far) is 1km.In this case Unreal Engine gives us the images in  (Blue, Green, Red, Alpha), so first we select only the  with  and then we reverse it to  with .Here you have a matlab implementation in case any of you need to use it. Thanks to your issue we noticed that we left this  on the client code, so the correct depth decodification is:We are gonna fix this soon.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 16, 2017",
            "body": "Hi  ,Thanks for your answer. Based on your answer, I get the true depth now. :)BTW: we are actually reversing the channels (with [:,:,::-1]) so that blue is the MSB, instead of choosing the blue channel, aren't we?",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marcgpuig",
            "datetime": "Nov 16, 2017",
            "body": "Hey !Sorry, I misunderstood the code before. I edited the comment with the right answer.\nThe thing is that we get the image from Unreal Engine in  format.The camera parameters (FOV, position, and so on) are set by you in settings, please take a look at the example in the Carla documentation . With these information you can build the intrinsic matrix.\nWe are working on the documentation of these topics. ;)Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dosovits",
            "datetime": "Nov 16, 2017",
            "body": "Hi  , btw, would be great if you can share your ROS integration code when it's done, I'm sure many would be happy to use it ( see e.g. issue  )",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 17, 2017",
            "body": "Hi I wrote a ros wrapper for my quick development. So I didn't plan to write a very official, well-shaped one and submit a pull request for this. Anyway, you can find my simple  here if someone is interested.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "dosovits",
            "datetime": "Nov 17, 2017",
            "body": " , thanks! Well, hopefully one day it will grow and become better-shaped :)",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "s7ev3n",
            "datetime": "Nov 21, 2017",
            "body": " Hi, thanks for your ros implementation. I have the same question about the camera intrinsic parameter, do you have any idea how to calculate it ? Thanks!",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 29, 2017",
            "body": "HiBased on FOV and image size you can compute it. Assume that the image is (800 * 600, FOV 100), you can simply get cx = 400, cy = 300, (assume fx = fy =f),   tan(FOV/2) = (1000/2) /f ==> f = 500/tan50 = 419.6...",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "WenchaoDing",
            "datetime": "Nov 29, 2017",
            "body": "Hi ,I found the z axis box extent for all the vehicles is strange. The z extent is always 0.32m, for all vehicles, no matter what type of the vehicle is. And the actually z size should be 0.32m * 2 = 0.64m? which is still too small.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 29, 2017",
            "body": "Hi ,Yes, the Z axis of vehicle's bounding boxes is always the same. The reason why is because we didn't care about the Z axis when we set up these boxes as they were meant to be used as 2D boxes for measuring the percentage of the car on other lane and off-road. Now that they are exposed to the client perhaps is worth to fit the height too, we will consider adding this.",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "marcgpuig",
            "datetime": "Nov 29, 2017",
            "body": "Hi ,Regarding the intrinsic matrix, look at my comment .",
            "type": "commented",
            "related_issue": null
        },
        {
            "user_name": "nsubiron",
            "datetime": "Nov 16, 2017",
            "body": [],
            "type": "added  the",
            "related_issue": null
        },
        {
            "user_name": "dosovits",
            "datetime": "Nov 17, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#12"
        },
        {
            "user_name": "nsubiron",
            "datetime": "Dec 1, 2017",
            "body": [],
            "type": "issue",
            "related_issue": "#64"
        },
        {
            "user_name": "nsubiron",
            "datetime": "Dec 13, 2017",
            "body": [],
            "type": "closed this as",
            "related_issue": null
        },
        {
            "user_name": "RehabHsn",
            "datetime": "Feb 3, 2022",
            "body": [],
            "type": "issue",
            "related_issue": "#4995"
        }
    ]
}
]