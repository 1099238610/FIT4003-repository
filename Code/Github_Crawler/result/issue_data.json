{ 
  "issue_list": 
[
{"issue_url": "https://github.com/botpress/botpress/issues/5423", "issue_status": " Closed\n", "issue_list": [{"user_name": "pooja-botpress", "datetime": "Sep 8, 2021", "body": "When a new Botpress instance is spun up, the first Super Admin Login should always be local", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Sep 9, 2021", "body": "Hey,  Can you please provide a little more details on what you mean by \"the first Super Admin Login should always be \"?", "type": "commented", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": "What I understand: When an SSO strategy is enabled and the default auth strategy is disabled on first boot, the registration screen to create the super admin does not appear.", "type": "commented", "related_issue": null}, {"user_name": "JustusNBB", "datetime": "Oct 8, 2021", "body": "I would like the ability to disable the user/pass local strategy from the login page while keeping accessibility through APIs (CI User = Superadmin)", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Sep 9, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Oct 13, 2021", "body": [], "type": "pull", "related_issue": "#5564"}, {"user_name": "laurentlp", "datetime": "Oct 14, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "EFF", "datetime": "Oct 22, 2021", "body": [], "type": "pull", "related_issue": "#5595"}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4326", "issue_status": " Open\n", "issue_list": [{"user_name": "laurentlp", "datetime": "Dec 18, 2020", "body": "\nRelated to \nWhen editing any content that contains pictures (image, card, carousel), replacing the current image does not delete the old one.\nSteps to reproduce the behavior:\nThe image should be deleted when it is replaced by another.The \"Cancel\" and \"Submit\" behavior make more sense compare to the one detailed in this issue \nNote that the old image is never deleted\nSee: \n is only  when the form is in  mode. This explains why the image is never deleted when editing!", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Dec 18, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Jan 5, 2021", "body": [], "type": "pull", "related_issue": "#4322"}, {"user_name": "Michael-N-M", "datetime": "Jan 23, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 3, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5763", "issue_status": " Closed\n", "issue_list": [{"user_name": "Sameer4real", "datetime": "Dec 16, 2021", "body": "I took a screen shot and thought I report it", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Dec 18, 2021", "body": "Hi  ,How did you get there? Any steps to.reproduction ?\nWhat's your os and Bowser versions.Thank you", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Dec 22, 2021", "body": ", could you please also tell us what OS you are using?", "type": "commented", "related_issue": null}, {"user_name": "Sameer4real", "datetime": "Dec 23, 2021", "body": "Yes,  \nI was just writing some random text in the text area trying to test the responses of the bot and the screen started glitching,\nI restarted the server, everything was fine but again the same thing appeared when I clicked on a node\nI experienced that same error a couple of times actually.I'm using Linux, hope that was helpful.", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Jan 10, 2022", "body": "Thanks  for the details, I have seen this issue happen on my machine too. On my end, it seemed to have been caused by some Xorg package I am using (or it might also be a bug with Chrome itself) and is most likely a \"Linux\" problem than something with the webapp. Usually, switching apps would remove the UI glitch.I will close this issue but feel free to re-open if the problem is persisting or if you can also replicate it on another operating system.Thanks again!", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Jan 11, 2022", "body": "For reference, here is an example of what it looks like when I see this glitch happen:\n(The image we see is my desktop background image)", "type": "commented", "related_issue": null}, {"user_name": "Sameer4real", "datetime": "Dec 16, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Sameer4real", "datetime": "Dec 16, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Jan 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2170", "issue_status": " Closed\n", "issue_list": [{"user_name": "moeidsaleem", "datetime": "Jul 29, 2019", "body": "\nNo, it actually an accessibility feature, a nice to have. This can be for someone who doesn't understand English that well or prefer to listen or only limited to listening - Accessibility feature. A audio button with every message to say that message loud in the user selected language.\nWe can use say.js for mac and festival for Linux, a simplest implementation of this and for converting text, a language server would be good start to provide content.\nWe can use azure cognitive or IBM Watson or any other cloud service available for achieving the following task.\nAs i am already, working on implementing this with say.js / festival and also Microsoft azure cloud support.", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Aug 16, 2019", "body": "That's indeed pretty interesting. At the moment, we're focusing on improving NLU and flow editor. Indeed HITL modules needs some love and speech would be very nice but would require a decent amount of work to support multiple languages and to keep this available on-prem. Integrating with cloud providers would be a good start though.Please share your work and your progress.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 12, 2020", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "moeidsaleem", "datetime": "Jul 29, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 12, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Mar 13, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4504", "issue_status": " Open\n", "issue_list": [{"user_name": "Michael-N-M", "datetime": "Feb 10, 2021", "body": "PROBLEMI'm frustrated by having to always edit the style sheet to make the avatar picture react to the size of the window. This is an essential facet of any interface with still (and sometimes motion) graphics. As a solution:\nSOLUTIONALTERNATIVE\nIt's not really the best, add default responsive styling in   in the standard which you prescribe for clarity and lack of obscurity. If it cannot be prioritized for inclusion in studio/admin UI then at least an example using the Botpress code composition standard stuff.CONTEXT\nThe first prototype chatbot developers expose to their customers is the web page . Later, after the bot has been linked to other channels the customer will come back and say \"May I please have access to the link you used to show us during meetings\"", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 10, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 11, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 23, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 23, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/11446", "issue_status": " Open\n", "issue_list": [{"user_name": "bassamtantawi-botpress", "datetime": "Feb 9, 2022", "body": "\nIf the utterances exceeded 100 the UI is broken\nSteps to reproduce the behavior:\nAdd more than 100 utterances\nIf applicable, add screenshots to help explain your problem.\n", "type": "commented", "related_issue": null}, {"user_name": "bassamtantawi-botpress", "datetime": "Feb 9, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/3320", "issue_status": " Closed\n", "issue_list": [{"user_name": "BPMJoannette", "datetime": "May 12, 2020", "body": "In the context of whitelabeling, some customers want to replace the UI Botpress brand references with their own. This is mostly achievable, except for the page title and favicon, which cannot be altered permanently.Expected behaviour:\nAdd the option to replace the page title and favicon in the whitelabeling context.", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "May 12, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "MaxCloutier", "datetime": "Oct 22, 2020", "body": " can this be closed?", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": "Fixed a long time ago", "type": "commented", "related_issue": null}, {"user_name": "BPMJoannette", "datetime": "May 12, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "BPMJoannette", "datetime": "May 12, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MaxCloutier", "datetime": "May 12, 2020", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "slvnperron", "datetime": "Oct 21, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/3265", "issue_status": " Closed\n", "issue_list": [{"user_name": "epaminond", "datetime": "Apr 23, 2020", "body": "\nImages are not displaying in node at flow editor\nSteps to reproduce the behavior:\nImage renders in node as it was before\n", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 22, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 20, 2021", "body": "Hi - fixed, we will close this, please reopen if need be.", "type": "commented", "related_issue": null}, {"user_name": "epaminond", "datetime": "Apr 23, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epaminond", "datetime": "Apr 23, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "epaminond", "datetime": "Apr 25, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "slvnperron", "datetime": "Oct 22, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 22, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 20, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/11429", "issue_status": " Closed\n", "issue_list": [{"user_name": "Jeremynano", "datetime": "Feb 8, 2022", "body": "Can't scroll down in the Q&A ???\nSteps to reproduce the behavior:\nIf applicable, add screenshots to help explain your problem.", "type": "commented", "related_issue": null}, {"user_name": "acclogin80", "datetime": "Feb 9, 2022", "body": "Having the same issue...OS: Windows (11)\nBrowser & Version: Edge (98.0.1108.43) & Chrome (98.0.4758.82)\nBotpress Version: 12.26.9", "type": "commented", "related_issue": null}, {"user_name": "acclogin80", "datetime": "Feb 11, 2022", "body": "I see this is closed, but I don't see a resolution, am I missing something?", "type": "commented", "related_issue": null}, {"user_name": "Jeremynano", "datetime": "Feb 11, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "acclogin80", "datetime": "Feb 11, 2022", "body": "Okay, thank you!", "type": "commented", "related_issue": null}, {"user_name": "Jeremynano", "datetime": "Feb 8, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Jeremynano", "datetime": "Feb 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2980", "issue_status": " Closed\n", "issue_list": [{"user_name": "cengizmurat", "datetime": "Feb 20, 2020", "body": "Hello,\nI have a  in our team and he has some difficulties to use Botpress chatbot.\nHe works with two screen readers  and , and there are different issues for interacting with Botpress chatbot :Thank you very much,\nBest regards", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Feb 20, 2020", "body": " Thank you for sharing your feedback. While we were aware that Botpress' interface is not accessible, we had no idea where to start as you might imagine there's quite a lot to tackle. Thanks for sharing your thoughts, we'll take this into account and do our best to make the interface more accessible for blind people (starting with the webchat).However, if you are willing to contribute this would help a lot.", "type": "commented", "related_issue": null}, {"user_name": "patlachance", "datetime": "Feb 21, 2020", "body": " /  :  is in my team, you can count on us to help you improving botpress accessibility by submitting PR. We may need your help to pinpoint appropriate sections to improve.", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Feb 21, 2020", "body": "Hello,I am the blind developer who use your chatbot.How would you like that we work together ?Do you wish that we send any accessible code examples to you, or do you first prefer to search the informations by yourself ?Best regards.", "type": "commented", "related_issue": null}, {"user_name": "asashour", "datetime": "Feb 21, 2020", "body": ", , please don't hesitate to ask, also in  if you need further assistance I would suggest that we try to resolve the most important things first", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Apr 29, 2020", "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Apr 29, 2020", "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Aug 17, 2021", "body": "We will close this issue.", "type": "commented", "related_issue": null}, {"user_name": "cengizmurat", "datetime": "Feb 20, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "EFF", "datetime": "Feb 20, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "slvnperron", "datetime": "Oct 22, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Aug 17, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5533", "issue_status": " Closed\n", "issue_list": [{"user_name": "mmmatej", "datetime": "Oct 4, 2021", "body": "\nI am experiencing issues with cards and carousels, showing up empty on bigger resolutions.Preconditions:Steps to reproduce the behavior:\nCard with previously defined content should show up.\n\n", "type": "commented", "related_issue": null}, {"user_name": "mmmatej", "datetime": "Oct 4, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Oct 13, 2021", "body": [], "type": "pull", "related_issue": "#5563"}, {"user_name": "laurentlp", "datetime": "Oct 15, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "EFF", "datetime": "Oct 22, 2021", "body": [], "type": "pull", "related_issue": "#5595"}, {"user_name": "Michael-N-M", "datetime": "Oct 26, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5253", "issue_status": " Closed\n", "issue_list": [{"user_name": "hacheybj", "datetime": "Aug 4, 2021", "body": "\nThe transcript goes blank sometimes when an agent click \"Assign to me\". The user and agent can no longer exchange messages.\nStill trying to figure out how to systematically reproduce it.\nIt currently only happens when the agent clicks the \"Assign to me\" button.\nLoad the transcript.\n\n12.23", "type": "commented", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": "fixed ", "type": "commented", "related_issue": null}, {"user_name": "hacheybj", "datetime": "Aug 4, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "EFF", "datetime": "Aug 25, 2021", "body": [], "type": "pull", "related_issue": "#5349"}, {"user_name": "Michael-N-M", "datetime": "Sep 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5060", "issue_status": " Closed\n", "issue_list": [{"user_name": "hacheybj", "datetime": "Jun 2, 2021", "body": "\nSome module icons are now broken, same for some custom skills.\n\n\nAdd any other context about the problem here.", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Oct 5, 2021", "body": "fixed", "type": "commented", "related_issue": null}, {"user_name": "hacheybj", "datetime": "Jun 2, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "daehli", "datetime": "Jun 17, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "daehli", "datetime": "Jun 17, 2021", "body": [], "type": "pull", "related_issue": "#5123"}, {"user_name": "EFF", "datetime": "Oct 5, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5051", "issue_status": " Closed\n", "issue_list": [{"user_name": "Michael-N-M", "datetime": "Jun 1, 2021", "body": "\nAll content elements except text are not rendered in HITL interface meaning agents miss some parts of the conversation. This is not the case with HITL Next but HITLNext has the disadvantage that you can only view chats of users who have requested for the intervention of an agent.\nConverse with a bot which has these content elements and check the conversation from the HITL Interface.\nMessages from both the chatbot and the user should be rendered as they appeared in the chat.\n\nThe HITL module is the only module which allows monitoring of conversations in progress. It also allows an agent to initiate the takeover process by providing a pause button.These two critical functionalities (monitoring and take-over) are vital for maintaining a low chatbot churn, understanding user behaviour and identifying workflows which can be improved. As such I feel that the HITL module remains relevant until the HITLNext module incorporates these functionalities.", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Jun 1, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 7, 2021", "body": [], "type": "issue", "related_issue": "#2244"}, {"user_name": "laurentlp", "datetime": "Dec 2, 2021", "body": [], "type": "pull", "related_issue": "#5736"}, {"user_name": "charlescatta", "datetime": "Dec 2, 2021", "body": [], "type": "issue", "related_issue": "#5716"}, {"user_name": "laurentlp", "datetime": "Dec 10, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4567", "issue_status": " Closed\n", "issue_list": [{"user_name": "nbesh761", "datetime": "Mar 2, 2021", "body": "I have my bot up and running on our IIS server however, when I click on Open Chat in the Botpress bot section. It will not load the responses until I hit the refresh button.\nSo the chat will open up in a new window which is running on my server, then I will type in Hello into the chat and hit send. Nothing will show up until I do a hard reset of the page. Any idea why this happens or how to fix it?", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 3, 2021", "body": "Hie there. Sometimes IIS has transport issues so in  try configuring to websocket alone and if this fails configure to polling alone. Either of the two should work depending on your Windows server.", "type": "commented", "related_issue": null}, {"user_name": "nbesh761", "datetime": "Mar 3, 2021", "body": "Hi  i have tried the bot chat based on your recommendation and its still not displaying the conversation unless refreshed and am getting a console error of \"failed: Error during WebSocket handshake: Unexpected response code: 500\".\nBelow is my config file. can you please check if am putting any of the contents correctly.Thanks", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 27, 2021", "body": "please move this discussion to forum.botpress.com", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 3, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 27, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/387", "issue_status": " Open\n", "issue_list": [{"user_name": "bryanlyon", "datetime": "Mar 25, 2022", "body": "\nWhen an image is rotated in metadata (Orientation flag) the tagging does not account for the rotation, leaving the tagged items in the wrong place.\nSteps to reproduce the behavior:\nBounding boxes should account for metadata rotation so they're in the right place instead of being shown in the wrong place.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 26, 2022", "body": "Thanks for reporting this  ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/367", "issue_status": " Open\n", "issue_list": [{"user_name": "hajo62", "datetime": "Dec 27, 2021", "body": "\nImages marked as private appear on the map\nSteps to reproduce the behavior:\nPrivate pictures should not been shown on the map, like they are not shown i.e. in the folder view.\nIf applicable, add screenshots to help explain your problem.Idea: For a  user there could be switch \"show/hide hidden/private images\" with hide as defautl behaviour.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/360", "issue_status": " Open\n", "issue_list": [{"user_name": "legraml", "datetime": "Nov 23, 2021", "body": "I am running the latest production version. Photonix keeps showing wrong thumbnails to original photos.", "type": "commented", "related_issue": null}, {"user_name": "jef1013", "datetime": "Nov 29, 2021", "body": "I am also having this issue and it is happening when I delete a photo from the source. For example I have a Nextcloud setup and I mounted the drive so that Photonix can read from it. If I delete a photo on Nextcloud this somehow messes up the photo thumbnails in Photonix. I am not sure if this is the reason but it has happened to me twice.  The first time it happened I deleted Photonix and started over and then it happened to me again when I deleted a photo once more.", "type": "commented", "related_issue": null}, {"user_name": "legraml", "datetime": "Nov 30, 2021", "body": "I am having this issue even though I did't delete any photos at all. It happens even when I add a new bunch of photos.", "type": "commented", "related_issue": null}, {"user_name": "damianbenente", "datetime": "Mar 8, 2022", "body": "And the processing is about the photo of thumbnails, because the face tag is showing in the place of the thumbnail and not in the original photo", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/346", "issue_status": " Open\n", "issue_list": [{"user_name": "nlester77", "datetime": "Sep 18, 2021", "body": "You cannot edit or remove any misidentifed objects, events etc.It would be good if you could clear these, or edit them. For instance I have photos that have an Airplane object, but its actually a Gul. I have some photos identified as a Halloween event, it is not Halloween.", "type": "commented", "related_issue": null}, {"user_name": "ovizii", "datetime": "Sep 23, 2021", "body": "Also very interested in this feature.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Oct 6, 2021", "body": "Thanks  and . I definitely want to add this feature soon.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Oct 6, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/372", "issue_status": " Open\n", "issue_list": [{"user_name": "KptnKMan", "datetime": "Jan 20, 2022", "body": "\nPlease, I'm trying to seek some help.\nNot sure if this is a \"bug\" but I've been able to reproduce it reliably.I have got Photonix \"running\" in a docker container on Unraid 6.9.2, but after setup when I got back to the login page it flashes up for a second then disappears.I've reinstalled multiple times, cleared postgres and redis, reinstalled all dependencies.I can login initially and setup Photonix, then next login the login screen flashes up for a second then disappears again. This happens from then on each time the login is accessed.\nThe logs don't seem to show any errors, shows postgres connected.\nI've tried accessing from different browsers, InPrivate mode, different client workstations, same login prompt flashes up for a second then is gone.Its quite irritating and I cannot find what is the matter.Can anyone help?\nUsing Unraid 6.9.2 stable\nContainers used (All available in Unraid \"Community Applications\"):Steps to reproduce the behavior:\nExpected Behaviour is for after initial setup, that the Web GUI page is accessible, and able to login for normal use.\nCan add screenshots, but not sure if \"blank page\" is useful.\nCan add a video screencap of this if required.\nHere are the docker logs for the \"rw-photonix\" container:", "type": "commented", "related_issue": null}, {"user_name": "neostim", "datetime": "Jan 22, 2022", "body": "I know this isn't a huge help but I had the same issue but can't remember what fixed it, I'm running 6.9.2 as well. I do remember following the instructions in the 2nd post on here:  and have had it working ever since.If there's any info I can provide I'd be happy to.EDIT: I see you posted on that thread lol, I also saw my comment there which is what fixed it for me:I also couldn't login (it hung), logs said it couldn't connect to postgres ... I had to set the postgres user to photonix accidentally, had to delete the postgres docker, clear it's app data, re-install and then it was fine.I do believe I also had misunderstood what password to use in the postgres docker (like the previous post to me on that thread).", "type": "commented", "related_issue": null}, {"user_name": "KptnKMan", "datetime": "Feb 3, 2022", "body": " hey sorry for the delayed response, I've still been trying to get this working.I do have a some questions, as I really want to get this working and understand what's happening.I have both my user and database set to \"photonix\", is that not a good idea?\nI created the \"photonix\" database separately after the default postgres user.\nI installed adminer, logged into the postgres server and executed an SQL command like this:I used these credentials to create the photonix container and have it access the database, which seems to be working correctly.\nThe logs above show connected to postgres, and I was able to log into photonix and set it up, but after setup I am unable to log in again. No idea why, the login prompt flashes up and is gone in a second.Did you change the username to something else?\nDid you only recreate the postgres container (and appdata) and nothing else?\nWhen you say reinstall, you mean only postgres or photonix as well?\nAre you using the same containers from Community Applications that I am?Was there anything else you did?", "type": "commented", "related_issue": null}, {"user_name": "neostim", "datetime": "Feb 3, 2022", "body": "So in my situation when I had the issue I just deleted and restarted with Postgres, right from the start in my docker compose I made the postgres default database \"photonix\" and username/password \"postgres\" (I know, terrible password practice).Then in Photonix I set the postgres user and password to \"postgres\" and the database to \"photonix\" and from there everything worked for me.Apologies if this isn't more helpful, I'm by no means an expert but was throwing it together fast to try it out, I ended up going with another comparable solution so haven't been using Photorec lately (it's great, I think the author is just buy being a dad right now :) ).", "type": "commented", "related_issue": null}, {"user_name": "KptnKMan", "datetime": "Feb 3, 2022", "body": "Hey, thanks for responding.\nRegarding the issue, I managed to get it working after some changes.\nI did a few things, and some seem to line up with your experience as well.First, I deleted everything.\nIts clear now to me that photonix stores basically everything in Postgres which is good, as it reduces the time to reinstall. On my unraid setup (using the containers listed in first post) only postgres11 stores any userdata, so I was able to wipe everything, delete all the configurations and start over. After that I saw I can just stop everything (photonix, redis & postgres) and delete postgres userdata, then start back up (postgres, redis, then photonix) and have a clean installation.The way I got everything working was I went through the photonix official  file and made sure I am using EXACTLY the same images, those being: ,  and .\nAfter that, I edited the unraid templates and tried to get everything inline with the  file.\nI also noticed that the instance refuses to work if a django secret is not set, it doesn't create one by itself as indicated. Considering so much fuss is made about known django secret keys, it seems weird that it will complain infinitely that no secret is set.Lastly, I used the default created postgres database, and I think this might also somehow be important.\nCreating a secondary database with all permissions (Using instructions in my previous comment) might well be an issue, but I haven't gone back to test this as I've been able to wipe and restart without issues. Still, would be a weird bug if that's the case that you have to dedicate an entire postgres instance or it will refuse to work as part of a shared instance.Can I ask what solution you went with in the end?\nI'm currently working through different solutions at the moment, with the intention/use-case of sharing self-hosted family photos amongst my family.", "type": "commented", "related_issue": null}, {"user_name": "neostim", "datetime": "Feb 3, 2022", "body": "I'm glad you got it working! I love Photonix but have been having issues with face detection so started exploring other options (at least for now). I found damselfly and started using it, it's much the same except it uses Azure for facial recognition (everything else is local), so far I've been pretty happy but still testing.", "type": "commented", "related_issue": null}, {"user_name": "KptnKMan", "datetime": "Feb 3, 2022", "body": "Yeah, I've been playing with Photonix for a few hours and I'm seeing issues as well.\nThe UI is having some issues, and I cannot seem to be able to edit anything from the initial admin user.\nI can see the images places in the  directory, but cannot add new face or object recognitions. I cannot seem to add or edit anything tbh, not sure what's going on there or if that's by design.\nAlso, the \"drop folder\" image import function seems to not work at all, I can just place new pictures in the  directory itself and they are picked up but I setup  as a dropbox folder and nothing seems to be seen (even after restarting container, I saw advised somewhere else).Furthermore, the  script appears to have some serious issues, and is creating issues for me already:The idea is that I just want a single central library initially, but achieving that seems problematic.I was looking at damselfly as well and heard good things, but encountered a strange \"Segmentation fault\" container crashing issue with it. I want to get back to trying samselfly as well.\nI should probably open an issue there to see if there's any solution, it seems in active development.", "type": "commented", "related_issue": null}, {"user_name": "neostim", "datetime": "Feb 3, 2022", "body": "Try posting on damselfly subreddit, I did this morning and had a reply from the author within 30~ minutes, I'm running it in docker on UnRAID without issue so far.I will say I prefer Photonix/Damselfly's object detection but Photoprism for face recognition and layout, I might run Damselfly for object detection and have it write to the EXIF data, then bring it into Photoprism for management/face matching.", "type": "commented", "related_issue": null}, {"user_name": "KptnKMan", "datetime": "Feb 3, 2022", "body": "Thanks  I don't want to get too much into the damselfly issue here, as its not this project to be fair, but I just now opened an issue on the damselfly github. I'll take a look at the subreddit and see if there's anything there as well, thanks!If you have time, I would really appreciate you taking a look at the github issue 334 and see if you can reproduce it as simply as I can? It would really help especially as you're using Unraid also.Apart from that, I'm still testing Photonix alongside Photoprism and Damselfly to see if I can get somewhere.", "type": "commented", "related_issue": null}, {"user_name": "KptnKMan", "datetime": "Jan 20, 2022", "body": [], "type": "issue", "related_issue": "#232"}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/336", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Sep 1, 2021", "body": "I recently realised that face and object analysis is performed on the full, original image. Whilst this is good in some ways (able to recognise tiny faces) it means that a lot more memory is used and some images types (such as raw) may not be analysed. There may be other classifiers that are using the full image so check all.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Sep 1, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Sep 1, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Sep 1, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Sep 1, 2021", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/319", "issue_status": " Open\n", "issue_list": [{"user_name": "gerroon", "datetime": "Aug 10, 2021", "body": "Hi,It would be very nice if random gallery display is possible. Basically it could be fully random as in grabbing random images from the whole collection or creates random gallery based on predefined stuff based on the existing UI features.thanks", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Aug 17, 2021", "body": "Hi . Thanks for the suggestion. I like the idea and will try to incorporate it.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Aug 17, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/296", "issue_status": " Open\n", "issue_list": [{"user_name": "Linecutterx", "datetime": "Jul 15, 2021", "body": "Latest 0.16 image\nFace recognition label is sometimes only a single character (or less) wide, & is limited by the size of the face bounding box regardless of the text.\nChecking the label (adding or reviewing) is therefore nearly impossible by eye.\nExpectation - label text box should increase in size when active (face selected). For multiple faces in a photo this likely means being able to select one face at a time.\nIdeally the label should be offset from the face when being edited, so that the subject can be positively identified (for multiple faces in one photo)", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jul 27, 2021", "body": "Thanks for reporting this . I agree there are a few quirks that need ironing out regarding the face labelling UI.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/230", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": "This problem happens on the \"connect to server\" view, when the user clicks the server address field. If the user's phone has a small screen then then when the keyboard pops up it can cover up the submit button.Add some padding below the server text field and button so when the keyboard is visible the field can scroll high enough.To determine the correct amount of padding, see if the height of the onscreen keyboard can be determined.", "type": "commented", "related_issue": null}, {"user_name": "GyanP", "datetime": "Jun 28, 2021", "body": "I have applied two different ways to solve this task issue. the task is done and solutions are deployed on branches.  and  .", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "GyanP", "datetime": "Jun 28, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/209", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Mar 22, 2021", "body": "Following on from  these are some desirable improvements to the thumbnailing service.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 22, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/37", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Jan 2, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 2, 2019", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/10", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Dec 29, 2018", "body": "Possible open source library solution:  . Will require a change to the thumbnail generation worker.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Dec 29, 2018", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/49", "issue_status": " Open\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Jan 6, 2019", "body": "As noticed by  , some cameras/phones give unrecognisable make/model names in their metadata. The user should be able to override the names of their cameras with a custom name.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 6, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 7, 2019", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10474", "issue_status": " Open\n", "issue_list": [{"user_name": "Horizon733", "datetime": "Dec 6, 2021", "body": "NaNaNaOtherOther In the above screenshot as you can see, the navbar is overlapping the menu box. Also, it doesn't stick on top of screen and it changes its position when clicked on hamburger. Please fix this, since looking at docs on mobile web version quite difficult.", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Dec 7, 2021", "body": "Exalate commented:sara-tagger commented:Thanks for raising this issue,  will get back to you about it soon\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "Horizon733", "datetime": "Dec 6, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10820", "issue_status": " Open\n", "issue_list": [{"user_name": "TimWaWIT", "datetime": "Feb 7, 2022", "body": "2.8.122.8.13.8Linux, Self hosted Cloud (openshift)When the action: action_restart is executed inside a conversation, the whole conversation-history is deleted (or at least empty in the UI). For further description with screenshots see: [ url].", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Feb 7, 2022", "body": "Exalate commented:sara-tagger commented:Thanks for raising this issue,  will get back to you about it soon\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "TimWaWIT", "datetime": "Mar 4, 2022", "body": "Exalate commented:TimWaWIT commented:Is there any update?  ", "type": "commented", "related_issue": null}, {"user_name": "TimWaWIT", "datetime": "Feb 7, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/6765", "issue_status": " Closed\n", "issue_list": [{"user_name": "erohmensing", "datetime": "Sep 23, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 23, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Sep 23, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 28, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 28, 2020", "body": [], "type": "pull", "related_issue": "#6822"}, {"user_name": "rasabot", "datetime": "Oct 3, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/11414", "issue_status": " Open\n", "issue_list": [{"user_name": "adamwojt", "datetime": "Aug 4, 2022", "body": "3.1.03.1.13.8LinuxWe have hand-off mechanism that (re)-tries to call API to trigger hand-off in chat system we are using. Action  is async and it either completes hand-off successfully or if all retries failed it utters message saying \"It seems I was unable ...\"There is also  defined in rasa   that should trigger first saying \"One moment, I'll get a live agent ...\". The stories are written like this:\nWhen testing out what happens is that when bot attempts to hand-off when hand-off API is not available both messages are sent at same time after pause (for retries) which means that . Is this expected behaviour ? How to fix it?Below are sent at same time after long pause (caused by async function retrying handoff API calls)\n", "type": "commented", "related_issue": null}, {"user_name": "ancalita", "datetime": "Aug 24, 2022", "body": " Could you share more details about your input channel implementation? Some of the channels which are async should send the  responses straightaway, however there are channels that work synchronously and hence the issue you're encountering.", "type": "commented", "related_issue": null}, {"user_name": "adamwojt", "datetime": "Sep 9, 2022", "body": " Currently using Rasa HTTP webhook.", "type": "commented", "related_issue": null}, {"user_name": "ancalita", "datetime": "Sep 22, 2022", "body": " We strongly recommend to use  as an async channel instead. Let us know if you encounter the same issue with this channel.", "type": "commented", "related_issue": null}, {"user_name": "adamwojt", "datetime": "Aug 4, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "ancalita", "datetime": "Aug 24, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10823", "issue_status": " Closed\n", "issue_list": [{"user_name": "koaning", "datetime": "Feb 7, 2022", "body": "I tried follow the docs to build a personal slack assistant but hit an issue similar to what is described . If you don't change anything, you get this message.It seems that slack has updated it's permissions and that our documentation is out of date. Per advice on  I've figured that we need to add the , ,  and . At the time of writing this issue, these don't seem to be listed on our docs.We're not out of the woods yet with just these changes. With these changes I can confirm that the slack UI allows me to speak to the assistant but the backend now throws errors.Here's a confirmation from the logs that I'm indeed receiving messages.The channel token  is interesting. I'm omitting a few characters just in case it's meant to be a secret, but this token is not equal to the  that I observe in the  file/slack app.But I'm also seeing this error in the logs.Something is telling me that the Slack API may have changed and that this might deserve more investigation.", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Feb 7, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Feb 7, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Feb 14, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Feb 15, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "sanchariGr", "datetime": "Feb 23, 2022", "body": [], "type": "pull", "related_issue": "#10940"}, {"user_name": "ancalita", "datetime": "Feb 23, 2022", "body": [], "type": "pull", "related_issue": "#10940"}, {"user_name": "m-vdb", "datetime": "Mar 3, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10803", "issue_status": " Open\n", "issue_list": [{"user_name": "travelmail26", "datetime": "Feb 2, 2022", "body": "3.04=.43.8OSXI'm getting an error in Rasa interactive that I can only sometimes recreate. It started when I upgraded to 3.0. It happens when I try to force the interactive to correct wrong intents or actions. It does not trigger actions.py. In the output below I retracted some sensitive info.", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Feb 3, 2022", "body": "Exalate commented:sara-tagger commented:Thanks for the issue,  will get back to you about it soon!", "type": "commented", "related_issue": null}, {"user_name": "travelmail26", "datetime": "May 30, 2022", "body": "I'm still encountering this issue. Any progress on it or workarounds?", "type": "commented", "related_issue": null}, {"user_name": "travelmail26", "datetime": "Feb 2, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10399", "issue_status": " Closed\n", "issue_list": [{"user_name": "Horizon733", "datetime": "Nov 26, 2021", "body": "NANANAOtherWindowsThe below image shows us, that inside changelog docs for 3.0 there is issue with indentation and needs a line break to see the changes written there.\n", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Nov 26, 2021", "body": "Thanks for raising this issue,  will get back to you about it soon", "type": "commented", "related_issue": null}, {"user_name": "Horizon733", "datetime": "Nov 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Nov 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Dec 1, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Dec 1, 2021", "body": [], "type": "pull", "related_issue": "#10431"}, {"user_name": "m-vdb", "datetime": "Dec 1, 2021", "body": [], "type": "pull", "related_issue": "#10431"}, {"user_name": "m-vdb", "datetime": "Dec 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10102", "issue_status": " Closed\n", "issue_list": [{"user_name": "georak", "datetime": "Nov 8, 2021", "body": "2.8.123.8Linux'greet' intent always return as the selected intent with confidence 1 when user types a message that doesn't exists in the training data.nlu.ymldomain.yml is the default\nconfig.yml is the default + added FallbackClassifier with threshold 0.3User input: \"I am writting something\"predicted intent is always \"greet\"", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 8, 2021", "body": "Just to confirm, does your  file only contains one intent? If so, this seems like expected behavior. If there is only one class to pick from, the classifier will do exactly that.", "type": "commented", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": "I Have multiple,\nI tried it also with a fresh rasa project which has multiple intents too (greet, affirm etc)FYI\nI have also set up a nlu_fallback rule", "type": "commented", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": "This is on a fresh project.the first identified as \"greet\" and the second as \"goodbye\"", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 8, 2021", "body": "Could you try running  so that we can see the confidence values of a few examples? The thing that's hard here is that an ML algorithm is able to give a confidence of 1.0 while giving the wrong answer. That means that right now it's possible that the fallback is not being triggered as opposed to the fallback being broken.", "type": "commented", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 8, 2021", "body": " please refrain from posting screenshots into the issue if it can also be copied/pasted as text. It's better for the screen real-estate and it also makes the contents searchable/copyable.That said, looking at your output I have no reason to assume that the fallback trigger is the issue. Instead, it seems like you expected low confidence for these utterances while the model seems to emit a high value. It may sound strange, but this isn't entirely unexpected.  might help explain why you're seeing this.Out of curiosity, how many examples do you have for each intent?", "type": "commented", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": "Ok Sorry, I will do!3-4 examplesThank you for our help.\nIs there any work arround?", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 8, 2021", "body": "My gut says that adding more examples will go a long way. I'd try having at least 12 examples per intent. At the moment, the model may be overfitting on the few examples that exist and is thus learning only to output high confidence values.", "type": "commented", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 10, 2021", "body": "I'll close this issue since it doesn't seem like Rasa is directly breaking here.If you'd like to discuss this issue further, please start a thread on our forum at . That's a preferable place to talk about specific bot issues.  You can ping me via my  user name there as well.", "type": "commented", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "georak", "datetime": "Nov 8, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "koaning", "datetime": "Nov 10, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/6771", "issue_status": " Closed\n", "issue_list": [{"user_name": "andrew-mcmaster", "datetime": "Sep 23, 2020", "body": ": Would like to be able to choose tag color from Rasa X during creation and change after creation.: If there was an edit button/icon beside each tag in the tags list on Rasa X it could trigger the tag name to be an input field or popup with another field to include a simple field with hex code with optional small box to show color. (if relevant): (if relevant)::", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Sep 24, 2020", "body": "Exalate commented:sara-tagger commented:Thanks for submitting this feature request\n\n\n  will get back to you about it soon!\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Sep 25, 2020", "body": "Exalate commented:akelad commented:  thoughts?", "type": "commented", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Sep 25, 2020", "body": "Exalate commented:TyDunn commented: is thinking through similar things at the moment", "type": "commented", "related_issue": null}, {"user_name": "edmacovaz", "datetime": "Sep 25, 2020", "body": "Exalate commented:edmacovaz commented:I'm just learning about how colors for tags work at the moment.I could see this being useful but I'm not sure how soon we would get to it. And I'm also unsure whether we would go straight to custom colours from the Rasa X UI or maybe just give people a list to choose from—hex colors seem straight forward to some people, but most people don't get them. So I'd like to avoid building a color picker...", "type": "commented", "related_issue": null}, {"user_name": "edmacovaz", "datetime": "Sep 25, 2020", "body": "Exalate commented:edmacovaz commented:Also, from what I understand the color codes are set from the front end and saved in the backend at the moment, so apparently you can define custom colors throgh the API already.", "type": "commented", "related_issue": null}, {"user_name": "andrew-mcmaster", "datetime": "Sep 25, 2020", "body": "Exalate commented:andrew-mcmaster commented:The  did not seem to straightforwardly say you could update just the color of an existing tag (at least my interpretation).currently it also requires a conversation id in the params of the request. I was just thinking a PUT request to /tags instead of /conversations/{conversation_id}/tags would make sense. Even for creation of new tags (not sure how it's done behind the scenese).When we add tags through X at the moment, if you add a bunch it seems to have a set list of a few through the spectrum but after some period of time it goes back to the start of that list, which is green I think. So if you add one every now and then it is looking like they would all be the same color anyway which feels a little like why bother. I could be wrong and that is just a coincidence though.At the moment we have a client who we are training to use X so they can add more content themselves, they are non-techinical and getting them to use an api is pretty much out.As a user myself I would see wanting an easier way to deal with tags, I think tags are a good feature and like using them, and having a personal/team color pattern for different types of tags was the first thought in my mind when adding them. I understand there is a lot going on though with the new 2.0 and get it might not be a priority or something that makes sense from the Rasa side.", "type": "commented", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Sep 26, 2020", "body": "Exalate commented:TyDunn commented: I have noticed this unequal distribution of the color of tags too. I think it is something worth investigating", "type": "commented", "related_issue": null}, {"user_name": "edmacovaz", "datetime": "Sep 28, 2020", "body": "Exalate commented:edmacovaz commented: That behaviour sounds like I bug, I would expect we would want to continue cycling through to have an even distribution among the colours. I'll investigate.", "type": "commented", "related_issue": null}, {"user_name": "Kadrian", "datetime": "Jul 2, 2021", "body": "Exalate commented:Kadrian commented:I can confirm that the cycling through the colors as discussed above is working now as expected – there shouldn't be just one color at some point – but the core of this issue:  is still not done. Is this still something we'd want to do if we have the cycling?", "type": "commented", "related_issue": null}, {"user_name": "edmacovaz", "datetime": "Jul 2, 2021", "body": "Exalate commented:edmacovaz commented: This is something we may want to do in the future, but isn't currently planned, so we should leave this open to keep this request documented.", "type": "commented", "related_issue": null}, {"user_name": "alexweidauer", "datetime": "Jun 8, 2022", "body": "Thank you for raising this issue about Rasa X. We decided to stop supporting the Community Edition (free version) of ‘Rasa X’ (see more info here: ). That’s why we’ve closed your issue. We suggest that Rasa Enterprise customers raise the issue with our customer support team if they haven’t done so already.", "type": "commented", "related_issue": null}, {"user_name": "andrew-mcmaster", "datetime": "Sep 23, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "alwx", "datetime": "Jan 29, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "alexweidauer", "datetime": "Jun 8, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5674", "issue_status": " Open\n", "issue_list": [{"user_name": "eslaaam3", "datetime": "Aug 16, 2022", "body": ": 0.9.13\n: Ubuntu 20.04\n: Custom vehicle is being continuously spawned\n: Custom vehicle spawned properly and only once\n: run \n (documentation you consulted, workarounds you tried):I have created a custom vehicle in Blender and used the above mentioned resources to create the blueprint in Carla.\nBut when I try to use  to spawn the vehicle, it keeps showing the errors shown in pictures below and keeps spawning the vehicle continuously and infinitely. I think every time the spawning fails, a new vehicle is spawned!\nI tried running  and the Audi A2 works perfectly.\nI also tried modifying Audi A2 in VehicleFactory to add my vehicle's blueprint and ran  again, but the same issue persists.", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#2925"}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5643", "issue_status": " Open\n", "issue_list": [{"user_name": "yangbiaoLeslie", "datetime": "Aug 3, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "yangbiaoLeslie", "datetime": "Aug 8, 2022", "body": "解决了，bin文件需要单独生成", "type": "commented", "related_issue": null}, {"user_name": "BigBbaymax", "datetime": "Aug 9, 2022", "body": "请问一下您的步骤是按着官方文档来操作的吗？我按着官方文档操作后，.bin文件也生成了，但是无法生成行人。有什么需要注意的细节吗？", "type": "commented", "related_issue": null}, {"user_name": "BigBbaymax", "datetime": "Aug 9, 2022", "body": "我的版本是0.9.12", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5627", "issue_status": " Open\n", "issue_list": [{"user_name": "MoritzNekolla", "datetime": "Jul 28, 2022", "body": "Hey,is it possible to spawn a bike or similar without collision sensors? In other words, the bike cannot be hit by a vehicle or any other object in the environment. Other objects would simply move through the bike instead of colliding with it.When looking up collision sensors:  Thus, we can not do anything about it?Greetings", "type": "commented", "related_issue": null}, {"user_name": "Pyrestone", "datetime": "Sep 1, 2022", "body": "bump, this would be very useful for 3d models of sensors which I would like to mount on top of (i.e. colliding with) vehicles.one would think that Actor.set_simulate_physics(False) would do this, but it doesn't", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5678", "issue_status": " Open\n", "issue_list": [{"user_name": "FrankZTH", "datetime": "Aug 17, 2022", "body": "Hello, everyone,\nI'm trying to add segmentation to fisheye, but I stuck in this problem for a long time.\nIf someone can solve this, that will be grateful.CARLA version: 0.9.13\nPlatform/OS: Linux 18.04\nProblem you have experienced: Sematic Segmentation looks weird in the fisheye sensor.\nWhat you expected to happen: The picture is like a standard Semantic Segmentation in the fisheye.\nSteps to reproduce:`\nvoid AFisheyeSegmentationSensor::SetUpSceneCaptureComponentCube(USceneCaptureComponentCube &Fisheye)\n{\nSuper::SetUpSceneCaptureComponentCube(Fisheye);ApplyViewMode(VMI_Unlit, true, Fisheye.ShowFlags);Fisheye.ShowFlags.SetNotDrawTaggedComponents(false); // TaggedComponent detects this and sets view relevance for proxy materialFisheye.ShowFlags.SetAtmosphere(false);Fisheye.PrimitiveRenderMode = ESceneCapturePrimitiveRenderMode::PRM_UseShowOnlyList;TArray<UObject *> TaggedComponents;\nGetObjectsOfClass(UTaggedComponent::StaticClass(), TaggedComponents, false, EObjectFlags::RF_ClassDefaultObject, EInternalObjectFlags::AllFlags);TArray<UPrimitiveComponent *> ShowOnlyComponents;\nfor (UObject *Object : TaggedComponents) {\nUPrimitiveComponent *Component = Cast(Object);\nFisheye.ShowOnlyComponents.Emplace(Component);\n}\n}void AFisheyeSegmentationSensor::BeginPlay()\n{\n// Called when the game starts or when spawned\n// Determine the gamma of the player\nCaptureRenderTarget1-> Init(GetImageWidth(), PF_B8G8R8A8);Fisheye->Deactivate();\nFisheye->TextureTarget = CaptureRenderTarget1;\nSetUpSceneCaptureComponentCube(*Fisheye);\nFisheye->CaptureScene();Fisheye->CaptureSource = ESceneCaptureSource::SCS_FinalColorLDR;Fisheye->UpdateContent();\nFisheye->Activate();UKismetSystemLibrary::ExecuteConsoleCommand(\nGetWorld(),\nFString(\"g.TimeoutForBlockOnRenderFence 300000\"));GetEpisode().GetWeather()->NotifyWeather();Super::BeginPlay();\n}\nvoid AFisheyeSegmentationSensor::PrePhysTick(float DeltaSeconds)\n{\nSuper::PrePhysTick(DeltaSeconds);// Add the view information every tick. It's only used for one tick and then\n// removed by the streamer.\nIStreamingManager::Get().AddViewInformation(\nFisheye->GetComponentLocation(),\nXSize,\nYSize);\n}\nvoid AFisheyeSegmentationSensor::PostPhysTick(UWorld *World, ELevelTick TickType, float DeltaSeconds)\n//Add the view information every tick. It's only used for one tick and then removed by the streamer.\n{\nSuper::PostPhysTick(World, TickType, DeltaSeconds);\n// ReadyToCapture = true;TArray<UObject *> TaggedComponents;\nGetObjectsOfClass(UTaggedComponent::StaticClass(), TaggedComponents, false, EObjectFlags::RF_ClassDefaultObject, EInternalObjectFlags::AllFlags);Fisheye->ClearShowOnlyComponents();\nfor (UObject *Object : TaggedComponents) {\nUPrimitiveComponent *Component = Cast(Object);\nFisheye->ShowOnlyComponents.Emplace(Component);\n}`My fisheye is referred annaornatskaya's fisheye.\n", "type": "commented", "related_issue": null}, {"user_name": "arukan404", "datetime": "Aug 24, 2022", "body": " I am trying annaornatskaya's fisheye commits to create a fisheye sensor, but it turns out it uses a modified CubemapHelpers class called CubemapHelpersFisheye from UnrealEngine. Any idea how to resolve these errors?carla/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/FisheyeSensor.cpp:219:3: error: use of undeclared identifier 'CubemapHelpersFisheye'; did you mean 'CubemapHelpers'?\ncarla/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/FisheyeSensor.cpp:219:26: error: no type named 'FFisheyeParams' in namespace 'CubemapHelpers'\ncarla/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/FisheyeSensor.cpp:229:25: error: use of undeclared identifier 'CubemapHelpersFisheye'; did you mean 'CubemapHelpers'?\ncarla/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/FisheyeSensor.cpp:229:48: error: no member named 'GenerateLongLatUnwrapFisheye' in namespace 'CubemapHelpers'; did you mean 'GenerateLongLatUnwrap'?", "type": "commented", "related_issue": null}, {"user_name": "Mnzs1701", "datetime": "Sep 2, 2022", "body": " have you applied the patch for Unreal Engine for the fisheye lens as mentioned , I have been trying to install the same  but it seems to be compatible with Unreal Engine 4.24 and not Unreal Engine 4.26\n could clarify?", "type": "commented", "related_issue": null}, {"user_name": "arukan404", "datetime": "Sep 2, 2022", "body": "Hi , Thanks for sharing the patch, I guess this is what I have been looking for. Let me try to integrate this and see if it works. Currently, I am doing equirectangular to fisheye projection on the python client side. It works but has a lot of latency... Hopefully, this patch should make it work", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5584", "issue_status": " Open\n", "issue_list": [{"user_name": "xiaoyang666", "datetime": "Jul 14, 2022", "body": "\n\nHello，\nWhen I draw a map with roadrunner and import the map with make import, the traffic lights at the intersection overlap. I would appreciate if any teacher could tell me why this happens.", "type": "commented", "related_issue": null}, {"user_name": "xiaoyang666", "datetime": "Jul 16, 2022", "body": "Hello，everyone", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/6568", "issue_status": " Closed\n", "issue_list": [{"user_name": "adityac07", "datetime": "Sep 3, 2020", "body": ": 1.10.12 (if used & relevant): 1.10.2 (if used & relevant):: 3.7 (windows, osx, ...): macOS: I have a custom action which runs fine as long as it finishes below 3 seconds. As soon as I put in time.sleep(4), no response is returned to slack.This can be reproduced as follows::: (if relevant): (if relevant):", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Sep 4, 2020", "body": "Exalate commented:sara-tagger commented:Thanks for raising this issue,  will get back to you about it soon\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "melindaloubser1", "datetime": "Sep 4, 2020", "body": "Exalate commented:melindaloubser1 commented:Is this action getting triggered by a slash command? If so, the three second timeout is configured on Slack's side - see . You can get around it by following the advice in ", "type": "commented", "related_issue": null}, {"user_name": "adityac07", "datetime": "Sep 4, 2020", "body": "Exalate commented:adityac07 commented:It is not triggered by a slash command. I trigger it through a normal conversation with the bot", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 21, 2021", "body": "Exalate commented:stale commented:This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "adityac07", "datetime": "Sep 3, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "tmbo", "datetime": "Nov 17, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "alwx", "datetime": "Jan 27, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 21, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "stale", "datetime": "Mar 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "stale", "datetime": "Mar 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Apr 2, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/6156", "issue_status": " Closed\n", "issue_list": [{"user_name": "howl-anderson", "datetime": "Jul 8, 2020", "body": "The web page for documentation has some weird CSS style, make it not easy to read. Here is a sample:", "type": "commented", "related_issue": null}, {"user_name": "sara-tagger", "datetime": "Jul 8, 2020", "body": "Thanks for the issue,  will get back to you about it soon!", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Jul 9, 2020", "body": "Thanks for bringing this up! We're aware of this -- I'm going to close it as its a duplicate of ", "type": "commented", "related_issue": null}, {"user_name": "howl-anderson", "datetime": "Jul 8, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "howl-anderson", "datetime": "Jul 8, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Jul 9, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Jul 9, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5643", "issue_status": " Open\n", "issue_list": [{"user_name": "yangbiaoLeslie", "datetime": "Aug 3, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "yangbiaoLeslie", "datetime": "Aug 8, 2022", "body": "解决了，bin文件需要单独生成", "type": "commented", "related_issue": null}, {"user_name": "BigBbaymax", "datetime": "Aug 9, 2022", "body": "请问一下您的步骤是按着官方文档来操作的吗？我按着官方文档操作后，.bin文件也生成了，但是无法生成行人。有什么需要注意的细节吗？", "type": "commented", "related_issue": null}, {"user_name": "BigBbaymax", "datetime": "Aug 9, 2022", "body": "我的版本是0.9.12", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5579", "issue_status": " Open\n", "issue_list": [{"user_name": "zengxianming", "datetime": "Jul 12, 2022", "body": "I got the same bug again. I set my sample resolution to 0.1m, it still doesn't work. I use CARLA 0.9.12. Here is my test code", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5574", "issue_status": " Open\n", "issue_list": [{"user_name": "zjh6252007", "datetime": "Jul 8, 2022", "body": "Hello everyone,\nI'm using the Carla 0.9.11 source version on ubuntu.\nAfter putting the .fbx and .xdor in the import file and make import.\nIt will create a .umap file. I use open level to open it but I can't see any texture.The map is exported from road runner, and some meshes have been combined.  I also tried manually import but it always crashes at about 70% progress.", "type": "commented", "related_issue": null}, {"user_name": "xiaoyang666", "datetime": "Jul 14, 2022", "body": "How big is the map.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5561", "issue_status": " Open\n", "issue_list": [{"user_name": "wujingda", "datetime": "Jul 4, 2022", "body": "Hi all,\nRecently I tried to use the \"lane_invasion\" sensor to detect the crossing lane behaviors of the ego vehicle. When crossing different types of lanes, the sensor will give the name of the lane crossed. Specifically, when crossing the curb line, the sensor will give the feedback signal of \"NONE\", which indicates the vehicle crossing the lane type called \"NONE\". Based on this principle, I could detect it when the ego vehicle crosses the curb line. However, in the official CARLA maps, the \"NONE\" signal occurs not only in crossing curb lines, but also in crossing some dashed white lines or dashed yellow lines. In this context, I cannot perfectly detect curb-line-crossing behaviors. Could somebody give insights on how to merely detect the curb line crossing behavior?Carla Version: 0.9.13\nPlatform: Win10", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5303", "issue_status": " Open\n", "issue_list": [{"user_name": "mariodoebler", "datetime": "Mar 30, 2022", "body": "Hi all,first of all thanks for your great work! Our issue is probably related to   and . Increasing the cloudiness leads to an \"unrealistic\" rendering, especially of the sky. Everything looks fine with a cloudiness of 30. Around 35 the sky gets unrealistically dark (till black), even though everything else is still quite bright. We used the provided environment.py for changing the weather parameters. This happens for Carla versions 0.9.11, 0.9.12, and 0.9.13 on Ubuntu 18.04 or 20.04 and Windows, packaged version and built from scratch.Using Carla version 0.9.9.4, everything looks as expected. I don't see how it could be related to the opengl issue mentioned in the previous issues. Also the rain and fog is rendered correctly, unlike mentioned in some of the previous issues. A contributor even mentioned that opengl isn't supported anymore for the newer Carla versions.We're more than happy to provide further details, if necessary.Hoping to find a solution! Thanks!", "type": "commented", "related_issue": null}, {"user_name": "mariodoebler", "datetime": "Apr 8, 2022", "body": "We generated a few screenshots to illustrate the problem. The following are sample images by using the provided environment.py, first setting the sun to day, followed by changing the weather (clear, overcast, rain).\n\n\n (the images are identical for versions 0.9.11 and 0.9.12 and probably, but we haven't tried it, for 0.9.10):\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "shh1v", "datetime": "Jul 18, 2022", "body": "Any update on this problem? The Carla community seems very inactive regarding this weather rendering issue.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5455", "issue_status": " Open\n", "issue_list": [{"user_name": "knelk", "datetime": "May 19, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "jhidalgocarrio", "datetime": "May 19, 2022", "body": "Yes, I also encountered the same effects. Significantly, the aliasing effect at a low image resolution.\nHere is a video in Town10: \nI thought this issue was solved in the newest releases. Any idea ", "type": "commented", "related_issue": null}, {"user_name": "knelk", "datetime": "May 19, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/49", "issue_status": " Open\n", "issue_list": [{"user_name": "ghost", "datetime": "Oct 5, 2021", "body": "How can I display Allen by activating the button?\nI mean, for example : click on SHOW BUTTON to activate Allen btn and click HIDE BUTTON to hide Allen btnPlease help me ", "type": "commented", "related_issue": null}, {"user_name": "sambhavsaxena", "datetime": "Oct 30, 2021", "body": " You can use Javascript to create an on-click method, or can say a toggle button, to hide/show the Alan interface button.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/55", "issue_status": " Closed\n", "issue_list": [{"user_name": "heymartinadams", "datetime": "Jun 9, 2022", "body": "Despite having explicitly allowed  access to the microphone in  (see screenshot below):\n...I’m still getting the following error:\nBrowser - Chrome 99.0.4844.51\nOS - macOS 12.2.1\nFramework - React, NextJS\nalan-sdk-web - 1.8.33", "type": "commented", "related_issue": null}, {"user_name": "heymartinadams", "datetime": "Jun 9, 2022", "body": "Closing since this issue seems to have something to do with , even though I specifically enabled . By disabling this package I was able to get Alan to work.", "type": "commented", "related_issue": null}, {"user_name": "heymartinadams", "datetime": "Jun 9, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "heymartinadams", "datetime": "Jun 9, 2022", "body": [], "type": "issue", "related_issue": "trezy/next-safe#44"}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/48", "issue_status": " Closed\n", "issue_list": [{"user_name": "zpreston123", "datetime": "Jun 24, 2021", "body": "Noticed  returns 404 error page.Image would display in the web page.", "type": "commented", "related_issue": null}, {"user_name": "zpreston123", "datetime": "Oct 1, 2021", "body": "Issue has been resolved.", "type": "commented", "related_issue": null}, {"user_name": "zpreston123", "datetime": "Oct 1, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/33", "issue_status": " Closed\n", "issue_list": [{"user_name": "GeneKung", "datetime": "Aug 13, 2020", "body": "Please include a simple reproducing example.If the current behavior is a bug, please provide the steps to reproduce and a minimal demo of the problem (if possible).\nYour bug will get fixed much faster if we can run your code.Did you use it with some framework (Angular, React, etc)?\nDid it work in the previous versions?Browser - ???\nOS - ???\nFramework - ???\nalan-sdk-web - ???Please describe your question here or ask it on Stack Overflow:\nProvide as much information as possible about your requested feature. Here are a few questions you may consider answering:", "type": "commented", "related_issue": null}, {"user_name": "okolyachko", "datetime": "Aug 17, 2020", "body": "Hi GeneKung,You can hide the speech-to-text panel for your app at any time. To do this, in the Alan Studio, go to the Integrations section and disable the Show \"Speech to text\" panel option:", "type": "commented", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 17, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "annmirosh", "datetime": "Dec 23, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/30", "issue_status": " Closed\n", "issue_list": [{"user_name": "avajshah", "datetime": "Aug 10, 2020", "body": "Please include a simple reproducing example.I added the Alan Button in my code but I cannot see it on the page.The button isn't showing up on the page after following the video.Browser - ChromeHope this can be solved soon!", "type": "commented", "related_issue": null}, {"user_name": "okolyachko", "datetime": "Aug 17, 2020", "body": "Hi avajshah,Please make sure you add all the components required for the Alan button to your webpage. The easiest way to check it is to see instructions provided in the Alan Studio itself: go to the Integrations section, in the Embed Code Example section, click the tab for the necessary platform and have a look at the integration steps there:For example, for a regular webpage, you need to:If this is not the case, please let us know.", "type": "commented", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 11, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 17, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/18", "issue_status": " Closed\n", "issue_list": [{"user_name": "harsha-iiiv", "datetime": "Jul 30, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "annmirosh", "datetime": "Jul 30, 2020", "body": "Hi,  !The default position of the Alan Button is in the bottom right corner. But it's possible to set a custom position for the Alan Button. You can set the bottom/left/right position of the button when you create it like this:or like this if you need to have the Alan Button on the left side of the web page:", "type": "commented", "related_issue": null}, {"user_name": "harsha-iiiv", "datetime": "Jul 30, 2020", "body": "Thanks   ", "type": "commented", "related_issue": null}, {"user_name": "annmirosh", "datetime": "Jul 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "harsha-iiiv", "datetime": "Jul 30, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/29", "issue_status": " Closed\n", "issue_list": [{"user_name": "aaronchan32", "datetime": "Aug 10, 2020", "body": "I added the AlanButton to my app:I get this error when I build it:", "type": "commented", "related_issue": null}, {"user_name": "annmirosh", "datetime": "Aug 11, 2020", "body": "Hi,  ,it looks like you're trying to use the Alan Button in the server-side rendered app. If it's a case, here is how you can use it:", "type": "commented", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 11, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "annmirosh", "datetime": "Dec 23, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13915", "issue_status": " Closed\n", "issue_list": [{"user_name": "williamFalcon", "datetime": "Jul 28, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "williamFalcon", "datetime": "Jul 28, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "JatheenA", "datetime": "Aug 20, 2022", "body": "  can you check on this? It worked on my iPad, but perhaps I missing something. CC: ", "type": "commented", "related_issue": null}, {"user_name": "yurijmikhalevich", "datetime": "Sep 2, 2022", "body": "This is fixed and live.", "type": "commented", "related_issue": null}, {"user_name": "williamFalcon", "datetime": "Jul 28, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "akihironitta", "datetime": "Jul 31, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "JatheenA", "datetime": "Aug 20, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "yurijmikhalevich", "datetime": "Sep 2, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13520", "issue_status": " Open\n", "issue_list": [{"user_name": "krishnakalyan3", "datetime": "Jul 3, 2022", "body": "If the underlying app's server sets headers like , lightning apps fail to show UI. This was observed for MLFLow and label studio.cc:  ", "type": "commented", "related_issue": null}, {"user_name": "krishnakalyan3", "datetime": "Jul 3, 2022", "body": "An Example:", "type": "commented", "related_issue": null}, {"user_name": "krishnakalyan3", "datetime": "Jul 3, 2022", "body": "Slack Discussion on ", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Jul 4, 2022", "body": "Hey @.Unfortunately, I believe there isn't much we can do unless convincing those frameworks to remove the header.Best,\nT.C@", "type": "commented", "related_issue": null}, {"user_name": "robert-s-lee", "datetime": "Jul 6, 2022", "body": "made code change on label studio to allow iframe", "type": "commented", "related_issue": null}, {"user_name": "robert-s-lee", "datetime": "Jul 15, 2022", "body": " so the hack used to work,  but it no longer works.When the server starts, it has 0.0.0.0 but the browser has the real address that no longer matches.", "type": "commented", "related_issue": null}, {"user_name": "robert-s-lee", "datetime": "Aug 22, 2022", "body": " is related issue with W&B", "type": "commented", "related_issue": null}, {"user_name": "krishnakalyan3", "datetime": "Jul 3, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "krishnakalyan3", "datetime": "Jul 3, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Jul 4, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "krishnakalyan3", "datetime": "Jul 6, 2022", "body": [], "type": "issue", "related_issue": "Lightning-AI/lightning-pose-app#5"}, {"user_name": "krishnakalyan3", "datetime": "Aug 27, 2022", "body": [], "type": "issue", "related_issue": "tshu-w/lit-doccano-component#1"}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13511", "issue_status": " Open\n", "issue_list": [{"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": "When my lightning app only contains one Web UI there is no reason to add a tab at the top of the app that takes up space.", "type": "commented", "related_issue": null}, {"user_name": "lantiga", "datetime": "Jul 3, 2022", "body": "Yep, definitely on the radar. Thanks for bringing this up ", "type": "commented", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Jul 4, 2022", "body": " This was recently enabled in the frontend and will come with the next release, if it hasn't already.\nHere are the  and you just make sure to return a single element in the configure_layout at the root.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "akihironitta", "datetime": "Jul 3, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Jul 4, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13496", "issue_status": " Open\n", "issue_list": [{"user_name": "tchaton", "datetime": "Jul 1, 2022", "body": "When creating a collection of works with their own UI, the only option is to create a tab for each one of them. Unfortunately, I had X works, this would probably don't work well.I believe it would be interesting to investigate the introduction of arrangement on the UI while embedding the iframe.The simplest one that I can think of is the Grid.Here is the possible API.cc ", "type": "commented", "related_issue": null}, {"user_name": "alecmerdler", "datetime": "Jul 2, 2022", "body": "There can be performance implications with rendering too many (10+)  on a single page at the same time. Basically, the browser is restricted on how many HTTP connections it can have open at once, and when 10+  are all loading in their HTML/CSS/JS/fonts/images/etc, it can cause the page to load very slowly (more than a minute, depending on internet speed).I agree we should explore different ways to declaratively generate layouts for app UIs, but I'm not sure that adding more  is the answer. If anyone in the community has experience with this type of thing, we would love to hear your thoughts.", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Jul 1, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "akihironitta", "datetime": "Jul 3, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "lantiga", "datetime": "Jul 3, 2022", "body": [], "type": "issue", "related_issue": "#13512"}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13507", "issue_status": " Open\n", "issue_list": [{"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": "Support dark mode!A lot of users prefer or require dark mode. The current lightning.ai logo (lower left corner) does not read well on a dark background (see lower, left corner of image below).Users of the Panel framework that I am using can toggle the theme dynamically. I would like the lightning.ai app theme to be able to update accordingly.cc ", "type": "commented", "related_issue": null}, {"user_name": "alecmerdler", "datetime": "Jul 2, 2022", "body": "Thanks for the issue ! I appreciate you noticing this, it would bother me too. Since the UI being rendered in your app could be anything, it's difficult for us to guess what the theme is and which logo to render. I'll ask the design team what we can do in this case and check back once I have an answer.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "akihironitta", "datetime": "Jul 3, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13942", "issue_status": " Closed\n", "issue_list": [{"user_name": "tshu-w", "datetime": "Jul 31, 2022", "body": "Following the  and run the app in the cloud command got FileNotFoundError on macOS:Following the  on ARM macOS with conda env.lightning run the app in the cloud correctly.", "type": "commented", "related_issue": null}, {"user_name": "lantiga", "datetime": "Aug 2, 2022", "body": "Hi  apologies.\nWe are going to issue a fix with the upcoming release, the issue the repo name starting with  (as checked from ).\nAs an immediate workaround, can you try removing the .git directory from within  and trying again?Related to /cc ", "type": "commented", "related_issue": null}, {"user_name": "rlizzo", "datetime": "Aug 2, 2022", "body": "Thanks for the report ! I'll be making a fix for this in the next day or two :)", "type": "commented", "related_issue": null}, {"user_name": "tshu-w", "datetime": "Aug 3, 2022", "body": "Yes, this workaround works.", "type": "commented", "related_issue": null}, {"user_name": "rlizzo", "datetime": "Aug 5, 2022", "body": "Hi  I've got a PR up which addresses this (see ). It will be scheduled as part of the next release.", "type": "commented", "related_issue": null}, {"user_name": "tshu-w", "datetime": "Jul 31, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "tshu-w", "datetime": "Jul 31, 2022", "body": [], "type": "issue", "related_issue": "#13943"}, {"user_name": "akihironitta", "datetime": "Jul 31, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rlizzo", "datetime": "Aug 2, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "rlizzo", "datetime": "Aug 4, 2022", "body": [], "type": "pull", "related_issue": "#14025"}, {"user_name": "robert-s-lee", "datetime": "Aug 4, 2022", "body": [], "type": "issue", "related_issue": "#13864"}, {"user_name": "awaelchli", "datetime": "Aug 8, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13772", "issue_status": " Closed\n", "issue_list": [{"user_name": "robert-s-lee", "datetime": "Jul 20, 2022", "body": "I have an .  The source code is at   constructed as below.The content of the Drive is not showing up in the Admin UI.\nTrainer logs confirm the new model has been createdGradio restart logs confirm Drive is working and new models are in use.Tensorboard also shows the new modelsStorage show the files in Drive", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Jul 21, 2022", "body": "Hey @,You need to refresh the page. This is a bug from the application side.", "type": "commented", "related_issue": null}, {"user_name": "robert-s-lee", "datetime": "Jul 21, 2022", "body": " Looked at it this morning after I restarted my browser.  The artifacts did show up.  I will try refresh the next time.  TY for the tip.", "type": "commented", "related_issue": null}, {"user_name": "robert-s-lee", "datetime": "Jul 20, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "tchaton", "datetime": "Jul 21, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "akihironitta", "datetime": "Jul 21, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "alecmerdler", "datetime": "Aug 18, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/pull/13732", "issue_status": " Merged\n", "issue_list": []},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1908", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 10, 2020", "body": "To reproduce:It shouldn't match at all, but it is also broken when it matches. The problematic code is here: I don't understand what  is supposed to be about, and it actually breaks things. I think I was trying to be clever, but the cleverness was never used.Maybe the entire logic needs to be removed.", "type": "commented", "related_issue": null}, {"user_name": "Simpcyclassy", "datetime": "Aug 11, 2020", "body": "I was about to raise an issue on the page name routine being broken as well as I just explained to you and saw this. Looks like it's a related issue affecting ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1913", "issue_status": " Open\n", "issue_list": [{"user_name": "julienreszka", "datetime": "Sep 16, 2020", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "Extarys", "datetime": "Sep 22, 2020", "body": "Yeah I discovered this like 5 minutes ago on the . Damn.I'll start donating to Mozilla. Can't believe I never donated.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Jan 30, 2021", "body": "I was not aware until I began seeking information in the absence of release notes for : does not exist.Re:  for a different (but related) extension, I assume that the release note for   will be similar to the release note for  :Please: where, exactly? (undated) is quite vague:– but there's no explanation for the decommissioning. I can guess a reason, but I'd prefer the explanation to come from Mozilla.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Feb 5, 2021", "body": "Cross reference:", "type": "commented", "related_issue": null}, {"user_name": "andrenatal", "datetime": "Feb 9, 2021", "body": "Hi . I'm a Mozilla employee and have created Voice Fill, Firefox Voice, our speech-proxy server, and numerous other voice projects here and was the one responsible for decommissioning the addons.The reason is simple and transparent: we are decommissioning the addons purely because the use of Google's STT backend won't scale for us, and we never managed to train production ready models through projects like Deep Speech and Common Voice with the quality required for VF and FxVoice.", "type": "commented", "related_issue": null}, {"user_name": "Extarys", "datetime": "Feb 9, 2021", "body": " I really appreciate you take the time to provide this explanation and it's totally understandable.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Feb 22, 2021", "body": "Likewise,  much appreciated.(After shifting the discussion to Discourse, I found some key posts on the subject; linked/quoted there. It's most useful to have your comment here as well.)", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1898", "issue_status": " Open\n", "issue_list": [{"user_name": "leo-lb", "datetime": "Aug 9, 2020", "body": "Hello!The initiative that Mozilla has started is interesting, but I am a bit troubled that this requires \"cloud\" services. I think Mozilla's initiative is especially interesting if it's offline and can operate locally, with the same performance as if it was running in a \"cloud\". I am personally not at ease sending my voice over the Internet even with promises of it not being saved.I am curious what are blockers to this and propose tracking them here.Probably those could be around:How does this exactly work on the \"cloud\" side? What would it take moving the \"cloud\" side entirely on the client side?I would imagine it's possible to train a model using \"cloud\" resources and later execute that model locally.Thank you!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/13335", "issue_status": " Closed\n", "issue_list": [{"user_name": "MarcSkovMadsen", "datetime": "Jun 19, 2022", "body": "Add PanelFrontend similarly to StreamlitFrontend.Panel and the HoloViz ecosystem provides unique and powerful features such as big data viz via DataShader, cross filtering via HoloViews, Streaming and much more.It ties into the PyData and Jupyter Ecosystem as users can develop in notebooks and use ipywidgets. They can also develop in .py files.This is not possible with Flask, Django, FastApi, Gradio, Dash or Streamlit.Panel is one of the 4 most popular data app frameworks in Python with more than 400.000 downloads a month. It's especially popular in the scientific community.Panel is used by for example Rapids to power CuxFilter, a CuDF based big data viz framework.An added benefit of adding a Panel Frontend is that afterwards supporting a BokehFrontend is almost copy paste.I can do the implementation and documentation, but would need help reviewing and similar.I can also promote the new opportunity to the Panel community.None.Coming sooncc ", "type": "commented", "related_issue": null}, {"user_name": "williamFalcon", "datetime": "Jun 20, 2022", "body": "\nlove this idea! go for it. We’ll help you land the PR once you are ready. In terms of promoting, we can put together an app or two to share with both communities.nice idea!", "type": "commented", "related_issue": null}, {"user_name": "lantiga", "datetime": "Jun 20, 2022", "body": " that’s great! Let us know if you need directions with designing this.For awareness, at a minimum you can already run a Panel application in a LightningWork and expose it through the tab frontend. This is a good example to take inspiration from: The more advanced integration is subclassing the  ABC \nThis gives users more flexibility to interact with the state of a flow + work tree. In comparison, the previous option will only be able to access the state of the Panel Work itself.Just a bit of orientation here, just feel free to explore, looking forward!", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 21, 2022", "body": "Thanks I will then start by making a PR for Panel similar to .Create a file called  containingCreate and activate a virtual environment. RunCreate a file called RunIn theory it should be as simple asUnfortunately I am on a forced VPN where lightning.ai is blocked so I cannot login and run the app.I will try to get the url whitelisted.If any one can run the app on the cloud and share a link I would be able to view it on my mobile phone. That would be nice.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 21, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "lantiga", "datetime": "Jun 21, 2022", "body": "Hi , this is great!Great question. You can access the origin host using a constant the  package exposes:I'm going to run the app now and will share the link with you.", "type": "commented", "related_issue": null}, {"user_name": "lantiga", "datetime": "Jun 21, 2022", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "philippjfr", "datetime": "Jun 25, 2022", "body": "What would be the best way for me to help out here? I really would love to see official support here and am happy to contribute to make that happen.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 25, 2022", "body": "Hi Seen from the Panel perspective you could 1) Check out the status list I've added to the initial post 2) Explore the repository  to get a basic understanding and try to deploy it 3) Try to deploy your own app.Think about whether we can/ should solve the \"slow\" first response on the Panel side. Right now I use  to speed up the first initial request. If its not fast the  server sends a new request and continues like that for ever.It would be very helpful for me to get suggestion and reviews/ feedback on the items in the status list above.", "type": "commented", "related_issue": null}, {"user_name": "williamFalcon", "datetime": "Jun 25, 2022", "body": "  can we help land this? very excited to enable this integration  ", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 25, 2022", "body": "Tomorrow I would like to try wrapping the \"view\" function such that the first response is light and fast and the following are given by the \"view\" function.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 25, 2022", "body": "I've tried out wrapping the  function like below.And this works around the problem in this issue by providing a fast enough first response  . I would have to figure out how to do the same if the page is a  or  pointing to a  or  file. The workaround is probably to patch the  method similarly to  method above..", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 26, 2022", "body": "I got the  patched. I've refactored my code into something more reuseable.It will send a fast first response no matter whether the page view is from a function or a file.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": "I'm working on the Panel Web UI Documentation here ", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": "The Panel Web UI Basic Documentation is ready for review. ", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 3, 2022", "body": "I have some questions I need help to answer before I can contribute a PanelFrontend. See . you can help here by trying to run the apps and suggest improvements and maybe also provide answers. I would like to understand what value  should be when running . Its seems it I set it to  then the Panel app cannot update the state globally, so for now its . But I have a feeling it should be  such that the  method will return and I can run it again with arguments that will update the Work state and potentially trigger updates of the Panel app.", "type": "commented", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jun 19, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kaushikb11", "datetime": "Jun 20, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "holoviz/panel#3641"}, {"user_name": "awaelchli", "datetime": "Jun 27, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "MarcSkovMadsen", "datetime": "Jul 2, 2022", "body": [], "type": "pull", "related_issue": "#13508"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#13531"}, {"user_name": "tchaton", "datetime": "Jul 15, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Sep 18, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1828", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 22, 2020", "body": "This is an incomplete concept and will require some discussion.The cards we get for many search results are quite valuable, but limited to Google search results. What if you could teach Firefox Voice to display other cards? While this is connected to the popup UI, it's also a start for generating voice, and for operating in the background and extracting the most relevant information.For instance, let's say I want to make a card from the . It looks like:But the card might look like this:This one is a little bit of a pain, as there's no good element that contains the card, I'm hoping usually there is.This card might be a static response to a particular phrase, like \"check irobot\". Maybe to create this you'd open the page, say \"create card check irobot\" and then select the card by dragging or hovering.Or, if we know  how to load these cards, then we can respond to \"search yahoo finance for irobot\" by loading that page in the background, and showing the card if it's available (focusing the page if not). We'd have to know that these pages should be loaded in the background.We don't have any way to create a simple alias for something like \"search yahoo finance for irobot\" as \"check stocks for irobot\", especially since it includes a slot. That might be nice, especially since Yahoo Finance could have a bang search but accessing other sites may be harder.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1907", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 10, 2020", "body": "Frequently (usually?) the browser won't identify any preferred/default voice, and so we'll have to pick one. Right now we pick the alphabetically first voice.We should choose a voice more intelligently. For instance my alphabetically first one is Agnes, which is not a good voice. Samantha however is a good voice.To see the available voices:My \"default\" is Alex, which I've never chosen. My system default is Samantha. Note that Samantha has the string \"premium\" in the .There may be heuristics on Windows and Linux, but we'll have to look more closely to determine them.Note that internally we should use null as the preference, and then determine the voice dynamically – and if the user sets an explicit preference we respect that.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1893", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 6, 2020", "body": "If you are in the options page and select a new voice, then we should immediately play a sample using that voice. It could be as simple as saying \"this is a test\" in the new voice.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1900", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "Aug 10, 2020", "body": "Immediately after installing the extension the onboarding screens should be displayed for users to make choices about telemetry and be educated on how to use Firefox Voice.Currently there appears to be a bug where new installations do not see the onboarding screens until after they press the microphone icon.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Aug 10, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1897", "issue_status": " Open\n", "issue_list": [{"user_name": "novellac", "datetime": "Aug 7, 2020", "body": "\nOn the  and  pages, the button element in the header marked \"Install Now\" doesn't do anything.\nNot quite sure!  - If an actual action isn't required here, perhaps use the same link which is used on the  on the .", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1896", "issue_status": " Open\n", "issue_list": [{"user_name": "novellac", "datetime": "Aug 7, 2020", "body": "\nThe Privacy Policy and Lexicon links at the top of the Lexicon page currently lead to 404s.\nEach link should lead to a valid page.\nIn partials/pageHeader, the links should be amended:", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1878", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 4, 2020", "body": "Instead of the \"Feedback?\" we should show a link to a survey periodically: ", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 4, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 4, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 4, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#1882"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1876", "issue_status": " Open\n", "issue_list": [{"user_name": "lisa-wolfgang", "datetime": "Aug 4, 2020", "body": "Firefox Voice's default keyboard shortcut is also the keyboard shortcut for opening the Firefox Multi-Account Containers extension.Furthermore, this cannot be changed during the onboarding process, which may turn away Containers users.", "type": "commented", "related_issue": null}, {"user_name": "GeraldNDA", "datetime": "Aug 5, 2020", "body": "Probs a dupe of  ?", "type": "commented", "related_issue": null}, {"user_name": "strorozhsergeich", "datetime": "Aug 5, 2020", "body": "The same issue and more. I am not able to change the default keyboard shortcut. Input box does not register my input. If, however, I am starting the new shortcut with a Key instead of MOD character ( e.g. A ) it detects my input and displays a message suggesting to start with one of three valid MOD buttons Ctrl, Alt or Shift. Should I report it as a new issue, perhaps?", "type": "commented", "related_issue": null}, {"user_name": "lisa-wolfgang", "datetime": "Aug 5, 2020", "body": "You have to type out the modifier keys with your keyboard: I believe actual key detection is a feature being worked on.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1867", "issue_status": " Closed\n", "issue_list": [{"user_name": "jcambre", "datetime": "Aug 3, 2020", "body": "We should clarify that Firefox Voice currently only supports English in the text beneath the install button on the homepage.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 3, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1869", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 3, 2020", "body": "When the extension is updated, users should get a \"NEW\"  on the toolbar icon.When the person next activates Firefox Voice, a new tab should open that shows the updates. This will be a page within the extension (design to be done in a different issue).The latest version of the feature list should be kept in browser storage. Not every release will necessarily have updates (e.g., a bugfix-only release won't need a proactive release page), so we should explicitly say what version last had an update and test against that. (The current version can be fetched via getManifest().)We may want to use a different icon instead of a badge.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 3, 2020", "body": [], "type": "issue", "related_issue": "#1870"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1843", "issue_status": " Open\n", "issue_list": [{"user_name": "12people", "datetime": "Jul 23, 2020", "body": "It'd be great if Firefox Voice could run  runs.\nMyCroft's skills all appear to be open-source.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 24, 2020", "body": "I believe MyCroft is running everything on a device with Python. So that's two barriers: a browser is... well, it's a device of its own, different than Linux. And it doesn't run Python (without a lot of work).I think our emphasis is on routing things to web-based services. So if we could send an utterance to a MyCroft device, then that would be quite sensible. You'd need to configure your personal MyCroft device. But if MyCroft has a local web page where you can type in some text (or put it in the query string) then the configuration could be as simple as saying \"this is my MyCroft page\" with the appropriate page open.That would open a second question: which phrases do we forward to MyCroft? The easy thing is to give the device a name and require the user to say \"tell mycroft to [something]\". We could also consider a configurable fallback service (Google is essentially our fallback service), but I doubt MyCroft would be useful that way – it doesn't handle random queries particularly well. The fanciest would be for MyCroft to either publish some phrases, or dynamically tell us if it thinks it can parse a phrase well. There's some danger there in MyCroft being greedy about handling phrases, but it would be an interesting question to try to figure out.Lastly, we just take some of those skills and replicate them here. Whenever possible we'd like to forward things to a web-based service, though in some cases implementing something natively is helpful, for instance there's reasons for us to have native timers.", "type": "commented", "related_issue": null}, {"user_name": "lb803", "datetime": "Aug 24, 2020", "body": "Someone on Mycroft forum asked how to submit utterances via api requests. Would this work for the present case?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1835", "issue_status": " Closed\n", "issue_list": [{"user_name": "krisgesling", "datetime": "Jul 23, 2020", "body": "Hi there, quick heads up that the site hosted on Github pages seems to be down:\n", "type": "commented", "related_issue": null}, {"user_name": "jofish", "datetime": "Jul 23, 2020", "body": "", "type": "", "related_issue": null}, {"user_name": "krisgesling", "datetime": "Jul 23, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1839", "issue_status": " Closed\n", "issue_list": [{"user_name": "danielamormocea", "datetime": "Jul 23, 2020", "body": "The  moved to ", "type": "commented", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Jul 23, 2020", "body": [], "type": "issue", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Jul 23, 2020", "body": [], "type": "pull", "related_issue": "#1840"}, {"user_name": "ianb", "datetime": "Jul 23, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 23, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1808", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 13, 2020", "body": "\"This is a test\" gives this:The background of the checkmark isn't great.Also the popup should probably stay open slightly longer than it does currently.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 13, 2020", "body": "Can we use the existing checkmark image that's typically seen when an intent is completed?Seen here: ", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 24, 2020", "body": "Yes, I think we could put that in, that would require using the Zap component instead of an image.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1826", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 21, 2020", "body": "One reason you might open the popup and say nothing is because Firefox Voice is listening to the wrong microphone. If we don't detect any speech we should display the microphone name so the user can understand if that's the problem.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 27, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1817", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 16, 2020", "body": "My rough understanding of what we need to do:You can also run  on that repository, but I don't know if any of those packages are necessary for our use, so may only be part of the initial setup.If you run  you'll get a working page on . This is a good example for what we have to include into You can see an example of a different Firefox Voice wakeword backend in :We don't want to copy in any UI bits of the Honkling example. My vague sense is that  is roughly the later we want to work with, but not so much  (which seems more UI-oriented). There's a high probability we want to slightly rewrite honklingController.Once we got it working we could pull in honkling as a submodule, but I find that kind of awkward. We  want to easily be able to update versions. But maybe we could checkout honkling into a directory and include a simple script () that copies the needed files into  and add those files to the repository. A little like ", "type": "commented", "related_issue": null}, {"user_name": "davidak", "datetime": "Jul 17, 2020", "body": " might be relatedare you in contact with ?", "type": "commented", "related_issue": null}, {"user_name": "ljj7975", "datetime": "Jul 17, 2020", "body": "Yeap, I am in.Thanks  for initiating this process.First of all,  is related as it is used to generate weights (as a js file) that honkling loads.\nHowever, the hey_firefox branch of Honkling already has the trained weights as . I.e for this deployment, you won't need to worry about it.To elaborate little more about how the code is structured. The main code you want to integrate is in For input processing, we were exploring two different libraries, one based on meyda and one based on tfjs.\nWe are going with meyda version. therefore all you need will be these  with modified Other files that you will need to integrate are basically .\nNote that melSpectogram file is unnecessary as this is only for tfjs versionI actually found minor bugs in some places so I will be updating this branch little bit.\nHowever, the changes will likely be small won't be changing the code structure.\nI will let you know directly on this thread when there is a change. It should be all cleared out by this weekend.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 21, 2020", "body": [], "type": "pull", "related_issue": "#1827"}, {"user_name": "ianb", "datetime": "Jul 28, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1819", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "Jul 17, 2020", "body": "Audio output creates a more hands free experience which we've seen as a compelling use case for voice interactions on desktop. To ensure the features isn't overlooked because it's buried in the preferences it should be enabled by default with an option to turn off from the popup window.Changes to make for audio output", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 17, 2020", "body": "Attaching speaker icons for enabled/disabled states\n", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 17, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1822", "issue_status": " Open\n", "issue_list": [{"user_name": "shelbyKiraM", "datetime": "Jul 21, 2020", "body": "Pretty straight forward? I don't want to give them ANY data I don't have to, that's a big part of why I use Firefox! When I do a thing like say \"Open DuckDuckGo\", it redirects through  for some reason!", "type": "commented", "related_issue": null}, {"user_name": "shelbyKiraM", "datetime": "Jul 21, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 27, 2020", "body": "@marv3lls Firefox Voice should respect the default search engine you've selected in the browsers preference. Do you have DuckDuckGo set as your default search engine?", "type": "commented", "related_issue": null}, {"user_name": "Vinnl", "datetime": "Jul 28, 2020", "body": " I have the same issue and yes, it's my default search engine:", "type": "commented", "related_issue": null}, {"user_name": "shelbyKiraM", "datetime": "Jul 30, 2020", "body": "Yes  ,  's comment shows basically the same as my setup.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 30, 2020", "body": "@marv3lls  to clarify when you do a voice search such as  the results you see are from your default search engine (DuckDuckGo) but when you navigate using a phrase such as  Google is acting as the referral and you'd prefer your default search engine be used in that instance?", "type": "commented", "related_issue": null}, {"user_name": "Vinnl", "datetime": "Jul 30, 2020", "body": "That phrase goes to Google as well, but doesn't execute a search:(Sorry, my computer was struggling while I recorded that.)", "type": "commented", "related_issue": null}, {"user_name": "Duckbilled", "datetime": "Aug 14, 2020", "body": "Yes I have the same issue as Vinnl.Also, when I say \"use duckduckgo to search for Chickens\" It opens one empty google page and one google page that searches for the whole sentence.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 30, 2020", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1792", "issue_status": " Closed\n", "issue_list": [{"user_name": "US-S-R", "datetime": "Jul 6, 2020", "body": "Text in box and settings page produced by clicking on the settings gear is unreadable regardless of the background style. The settings page in addons however is clear.\nsettings page\n\nTextbox below\n", "type": "commented", "related_issue": null}, {"user_name": "US-S-R", "datetime": "Jul 12, 2020", "body": "Gnome settings issue.", "type": "commented", "related_issue": null}, {"user_name": "US-S-R", "datetime": "Jul 12, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1789", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Jun 30, 2020", "body": "The function  basically does a string search of the URL. That's not a great way to match, especially with voice input.Instead we should get all tabs that otherwise match the query, and use  as a text search of the URL and title of the tabs (it doesn't need to include tab content).", "type": "commented", "related_issue": null}, {"user_name": "PascalUlor", "datetime": "Jul 1, 2020", "body": "Can I work on this?", "type": "commented", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 10, 2020", "body": "Can I work on this?\n ", "type": "commented", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 11, 2020", "body": " I have made changes for the getMatchingTabs() Can you please look into it once?", "type": "commented", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 11, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 11, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 13, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 14, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 15, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/976", "issue_status": " Closed\n", "issue_list": [{"user_name": "Sav22999", "datetime": "Feb 7, 2020", "body": "I think it's a good idea add a voice command to open an extension just with the voice.\nFor example. I installed uBlock. If I want to open it, i should click the icon on address bar.\nSo, why don't add \"Open uBlock extension\" voice command to do the same thing?", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": "This is going to be hard on a couple levels – invoking another extension isn't clear, but it probably means click the accompanying toolbar button. That's not something that we easily have access to. But then once you've done that you have a popup, and an extension popup is impossible for us to control at all, so you would be forced to continue interacting with your mouse and not your voice.Because of this, we don't think this will be a useful experience for someone.Note that we are very open to allowing other extensions to be controlled by voice, if those extensions want to add code to their own extension to run those commands.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/704", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 10, 2019", "body": "Mic permission are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA command is made beforehand.Button changes color when hovered.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": "Also the frowny face is not centered", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 10, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": "#719"}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 22, 2020", "body": [], "type": "unassigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1327", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA Google Docs tab is opened.\nThe tab that the user wants to copy the image from is made active.Image is copied in the Google Doc successfully.Other information is copied or nothing happens.Reproduced on Mac 10.14 and Win10x64 with Firefox Nightly 76.0a1 (2020-03-18).\nIf the user right clicks with the mouse on the image and selects \"Copy Image\", then it is possible to copy the picture in the desired tab, e.g. Google Docs, Google Slides, Google Sheets, . (\"Paste\" command works for calmlywriter or Google Sheets. For Google Docs and Google Slides, the paste keyboard command should be used)", "type": "commented", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 18, 2020", "body": "I was able to reproduce the bug. May I try to fix it?\nUpdate: fixed", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 31, 2020", "body": "Verified on Mac 10.14 with Firefox Nightly 76.0a1 (2020-03-31).\n\"Paste\" commands are not working in Google Docs and Google Slides, like in , but image is copied with the paste keyboard shortcut.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 19, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 22, 2020", "body": [], "type": "pull", "related_issue": "#1335"}, {"user_name": "ianb", "datetime": "Mar 30, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 30, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 31, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/514", "issue_status": " Closed\n", "issue_list": [{"user_name": "awallin", "datetime": "Oct 30, 2019", "body": "When Google returns a sidebar card for a search we should display in the doorhanger (and preference over the main column snippit).Examples utterances:", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 31, 2019", "body": "Should be fixed by ", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Nov 4, 2019", "body": "Cards are not/no longer being displayed for sidebar cards.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Oct 30, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 31, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "awallin", "datetime": "Nov 1, 2019", "body": [], "type": "issue", "related_issue": "#520"}, {"user_name": "awallin", "datetime": "Nov 4, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 4, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1505", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Apr 8, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\n\"A privacy reminder from Google\" is reviewed and agreed and the yellow language pop-up is closed. (if applicable)Doorhanger doesn't have display issues.There is a scrollbar present that shows a partial white border of the black background.Reproduced on Mac 10.14 with Firefox Nightly 77.0a1 (2020-04-08).", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": "  I'd like to work on this.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 8, 2020", "body": "Go for it ", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": "  I've made some CSS changes and tested on Local Machine.\nPlease have a look and confirm.\n\nOS: Windows 10x64\nFirefox Nightly: Version 75.0a1\nNode: v12.16.1", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 25, 2020", "body": "  If everything is working fine then please close this issue. Let me know in case I've to make any other changes?", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 27, 2020", "body": "Looks good. Closing.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 5, 2020", "body": "Verified on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-04).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Apr 8, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 8, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": [], "type": "pull", "related_issue": "#1513"}, {"user_name": "awallin", "datetime": "Apr 27, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 5, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/689", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA command is made beforehand.Emoticons change to blue and change size.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": "The colors do change, but it's so subtle and the lines are so thin that it's hard to tell. to give guidance on colors.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 11, 2019", "body": " from the mock-up, the emoticons are filled with blue and increased in size after being clicked and I assume this happens also on hover. But  can clarify exactly, if changes also apply when icons are hovered, or only when they are clicked, or both.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Dec 11, 2019", "body": "We'll want a slightly different visual for hover than clicked to ensure there change between the two states is obvious.Attached are SVGs with a hover state for the two icons using a background with a slight opacity.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit). Not sure if there are changes when emoticons are clicked though.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": "#730"}, {"user_name": "ianb", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 22, 2020", "body": [], "type": "unassigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1329", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nThe \"Preferences\" page is opened, by using the shortcut or clicking on the mic icon from the browser toolbar and selecting the \"Settings\" icon from the bottom left side of the doorhanger.\"Routines\" page doesn't have discrepancies compared to the mock-up from .There are some discrepancies between the mock-up and the actual display.Reproduced on Mac 10.14 and Win10x64 with Firefox Nightly 76.0a1 (2020-03-16).\nThe discrepancies between the mock-up and the actual display are as followed:Some of these can change, depending on the desired user experience. One example is the position of the \"+ New Routine\" button.Mock-up:\n", "type": "commented", "related_issue": null}, {"user_name": "Shulammite-Aso", "datetime": "Mar 18, 2020", "body": "Hi  I think Issue    Is a part of this,  or are they different?\nIf they are the same and you intend to close the other, then I would like to work on this.", "type": "commented", "related_issue": null}, {"user_name": "Shulammite-Aso", "datetime": "Mar 18, 2020", "body": "    I can start working on this after you give your thought on it.", "type": "commented", "related_issue": null}, {"user_name": "shreyaa-s-zz", "datetime": "Mar 18, 2020", "body": "Hey, can I work on this one?  \nThanks!", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 19, 2020", "body": "  has a more general approach for the \"Routines\" page display (the page itself seems a bit zoomed in compared to other pages), while this report looks at more specific changes that need to be done. Hope that clarifies things.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 23, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Jun 30, 2020", "body": [], "type": "pull", "related_issue": "#1787"}, {"user_name": "ianb", "datetime": "Jun 30, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 30, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "Simpcyclassy", "datetime": "Jul 15, 2020", "body": [], "type": "issue", "related_issue": "#1328"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/678", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 6, 2019", "body": "Mic permissions are enabled.\"Settings\" page is displayed without issues.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).\nReproduced also for the \"Feedback?\" link when the \"Doorhanger\" is first opened.\nNot reproduced in other \"Doorhanger\" instances, like \"Type Input\", \"In progress\", \"Sorry, there was an issue\", \"Thanks for the feedback\".\n\"Settings\" button and \"Feedback?\" link work, when the \"How was your last experience?\" is displayed in the \"Doorhanger\".\nOn Mac the \"Feedback?\" link can be accessed if the mouse is place on the text underline. (2nd gif)", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": "Myself, , and  haven't been able to reproduce this. Do you have any other thoughts  ?", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Dec 6, 2019", "body": "Unable to reproduce.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": "Sorry, it seems something was not right from my part, although I don't know what that was. After several attempts where I made new profiles on both Win and Mac, it was reproducing every time. When I tried it again today, the link and button work.", "type": "commented", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 13, 2019", "body": "The issue with some buttons not being clickable was because of a hidden text input field in the view. Even though it wasn't visible it was overlaying some things and preventing clicks. This has been resolved.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 16, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 6, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 9, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 13, 2019", "body": [], "type": "issue", "related_issue": "#745"}, {"user_name": "alexandra-martin", "datetime": "Dec 16, 2019", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/486", "issue_status": " Closed\n", "issue_list": [{"user_name": "dholbert", "datetime": "Oct 24, 2019", "body": "STR:\n(1) Click the mic icon on your toolbar.\n(2) Watch the gear icon (and try to click it, pretending you're a bit slow with your mouse)ACTUAL RESULTS:\nThe gear moves between three different positions, due to the dialog resizing as its content changes.  (There are three phases: \"Listening\", \"One Second\", and then \"Sorry, there was an issue\".  Each phase has a different dialog height and a different position of the gear.)EXPECTED RESULTS:\nThe gear should be at a consistent position so that it is easy to click.", "type": "commented", "related_issue": null}, {"user_name": "xlisachan", "datetime": "Mar 8, 2020", "body": " I am an Outreachy applicant. Can I be assigned to this issue?", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 9, 2020", "body": "Unless  feels differently, I think there's nothing left to actually fix on this ticket. It does move around a little bit when the examples open up, but otherwise it's OK. Though if  wanted to move it to a more consistent location (e.g., the top of the popup) then we could reopen.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Mar 9, 2020", "body": "Yea, it's ok to close.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 25, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 3, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 9, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/213", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Sep 7, 2019", "body": "These UX guidelines might be of some interest:  (particularly page 3)", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Sep 9, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/503", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Oct 25, 2019", "body": "I'm not 100% sure we want to do this, but we could  the recorder tab instead of pinning it.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": "Looking at the  it says: \"Tabs that are sharing the screen, microphone or camera cannot be hidden.\"So we can't do this.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": "I'll experiment with this a bit more to see if something is possible.", "type": "commented", "related_issue": null}, {"user_name": "poperigby", "datetime": "Jan 10, 2020", "body": "Maybe just automatically close it when the user closes the FV popup.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": "With  this doesn't seem so important.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 25, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "issue", "related_issue": "#823"}, {"user_name": "ianb", "datetime": "Jan 14, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 23, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 23, 2020", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/632", "issue_status": " Open\n", "issue_list": [{"user_name": "sandip-narwade", "datetime": "Mar 27, 2018", "body": "previusly I used Http IP camera, for that I used following codebut now they are providing stream of RTSP. How I can used RTPS for above code", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 27, 2018", "body": "Hi @sandipJava,I have bad news. Unfortunately there is no possibility in Webcam Capture API to read from RTSP :( The only way to w/a this is to transcode RTSP to MJPEG and use Webcam Capture API on top of transcoded MJPEG.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Mar 27, 2018", "body": "Can I have sample code, It will be more helpful to me", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 27, 2018", "body": "For example: ", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Mar 28, 2018", "body": "is xuggler will work for taking an image from rtsp?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 28, 2018", "body": "Hi @sandipJava,I'm not sure. Xuggler is discontinued and most of the code migrated to , but please note that I'm not very familiar with this framework and can't give you exact details on how this can be done.For Xuggler, I found this:  but never test is.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Mar 29, 2018", "body": "ok sir I will try", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Mar 29, 2018", "body": "Hello Sir I got following error", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 29, 2018", "body": "Hi @sandipJava, I can't help you with that. It's a Xuggler issue, which I'm not very familiar with. You may try to use some Xuggle group on Google or Stack Overflow, to seek help.", "type": "commented", "related_issue": null}, {"user_name": "jaysentang", "datetime": "Apr 6, 2018", "body": "Hi @sandipJava\nTry this code below, it's working fine for my ipcamera.``", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 15, 2018", "body": "  It is not working is showing below warningand", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2018", "body": "@sandipJavaThese warnings are just warnings, not errors, and therefore are harmless. Are you sure your RTSP URI is correct? If you can open it in VLC you should also be able to open it via Webcam Capture with VLCJ Capture Driver.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 15, 2018", "body": "  it is opeming in VLC. I used same code that above given by  . Its not saving image and not showing video in jframe", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2018", "body": "@sandipJava Can you show me the code sample?", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 15, 2018", "body": "}", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 15, 2018", "body": "Using this link I am able to see video in VLC player", "type": "commented", "related_issue": null}, {"user_name": "jaysentang", "datetime": "Jun 16, 2018", "body": "@sandipJava have you modify your vlcjdevice class, please refer this issue  , i got it work after i read this issues.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 18, 2018", "body": "  & \nnow following warning are coming\n", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": "@sandipJava These warnings are just , not errors, and can be ignored. There is no functional impact due to these informations being printed.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 18, 2018", "body": " then why not video streaming in not comingshowing like this", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 18, 2018", "body": "that already commented", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": "Hi @sandipJava,Sorry for hanging with this for so long. I had some other important to do first. I did a quick look into the code to check VLCJ args in accordance to the code provided in  (thanks  for pointing this out).After these changes provided RTPS endpoints (at least the ones I tested) work fine.I also prepared  based on your code:After I run it:Please note that this fix has  yet and is available in SNAPSHOT version only!!! You need to download 0.3.13-SNAPSHOT version of webcam-capture-driver-vlcj from here:Please make sure to update all webcam-capture JARs to 0.3.12 in case you are using older version.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": "@sandipJava please let us know if this is working with your RTSP endpoint.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 18, 2018", "body": "your code is working Big Buck Bunny video is playingbut for my RTSP not working", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": "Dammit :(@sandipJava is there any way you can provide me with the access to the RTSP endpoint you are using? E.g. via public IP or a demo webcam somewhere in the internet?", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 18, 2018", "body": "rtsp://192.168.15.233/StreamingSetting?version=1.0&action=login&username=admin&password=Nipun&getRTSPStream&ChannelID=1&ChannelName=Channel1this my IP camera link username and password too", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 19, 2018", "body": "Hi @sandipJavaThe address you provided (192.168.15.233) is a  which I'm unable to connect to. It's available only from your own LAN, not from Internet. You have to have public IP with port forwarding configured on your router to temporarily allow users from internet to connect to your camera. The way to configure public IP accessibility strongly depends on your ISP (Internet Service Provider) and the router you have at your place.", "type": "commented", "related_issue": null}, {"user_name": "sandip-narwade", "datetime": "Jun 19, 2018", "body": " , I dont have that much permission , Can we find any test IP camera from anywhere(Google)", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 19, 2018", "body": "@sandipJava,In regards to what you wrote:This would be the best, but I have no time for this (I have my regular job and not much free time to spare), but if you can find such test IP anywhere in the Internet and provide it here with login and password, that would be great and I will use it to debug the problem.", "type": "commented", "related_issue": null}, {"user_name": "polar1shu", "datetime": "Apr 16, 2019", "body": "sorry to bother you \nwhen I run WebcamRtspExample demo, it shows no image available\nit woks macos 10.14, Is this error in my computer system problem?", "type": "commented", "related_issue": null}, {"user_name": "megahertzon", "datetime": "Jun 24, 2020", "body": "Hi friends,\nI have tried to test this code but it does not work:rtsp stream:\nrtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.movimport com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;import javax.swing.JFrame;\nimport uk.co.caprica.vlcj.medialist.MediaListItem;\nimport com.github.sarxos.webcam.ds.vlcj.VlcjDriver;public class Main {}The pom file:please help me!", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 18, 2018", "body": [], "type": "issue", "related_issue": "#355"}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/704", "issue_status": " Open\n", "issue_list": [{"user_name": "edckt", "datetime": "Mar 26, 2019", "body": "I can open the webcam when first the Java file. But after doing the first mouseClicked, the second trigger of captureImage shows 'device error' on the WebcamPanel.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/172", "issue_status": " Closed\n", "issue_list": [{"user_name": "cmadsen", "datetime": "Jan 27, 2014", "body": "I would like to add pan, tilt and zoom, snapshot etc. support/controls (mouse and/or buttons) to the WebcamPanel.The camera is an axis with support for the VAPIX http APIAny hints on how to achieve this?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2014", "body": "Hi Assuming that you would like to control everything by the  instance, I would start by creating some kind of controller class which will register all necessary listeners in the panel (like for mouse wheel to handle zoom or mouse click and/or mouse drag to control tilt and/or pan).Then extend  with your own functionalities - if you take a look into the  class you will notice that it is using  to access remote URLs, and this is just a derive class of  (version 4.2.x) from well known , so everything you need to query camera endpoints with POST/GET payloads is already there.The PDF you're referring to seems to describe the API very clearly so it should not be a problem to implement what you are looking for.", "type": "commented", "related_issue": null}, {"user_name": "cmadsen", "datetime": "Jan 30, 2014", "body": "Ok, thanks for the hints.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 4, 2014", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/320", "issue_status": " Closed\n", "issue_list": [{"user_name": "henriquem", "datetime": "Mar 16, 2015", "body": "Hello Everybody!SomeOne have more experience than me for solved my problem ?I dont have idea about it, if you have some experience for clarify ! is welcome...this camera is Tec Voice model: td2316sworks fine in IE\n  [ will ask for install webclient for run in web ]\nuser: admin\npass: 1\nThe format is h.264.The problem is, How I could wrote in Java for transmit those images.I copied the code bellow, but the line not response in methods s_in.read():\nwhile((bytesRead = s_in.read()) > 0)When press F12 from IE not show me in Tab Network, as Sarxos recommends in another issues when IP CAMERA is Mjpeg format.Any Suggestions will be welcome..\nI dont understand how the applications Mobile ( Android ) can works well done!Or if I replace the line:\ns_in = new BufferedReader(new InputStreamReader(s.getInputStream()));\nfor:My Source Code Full\nclass DVR {", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 29, 2015", "body": "Hi ,The problem with h.264 is that there is really very few ready to use production code written in Java that is able to decode this format. Every one I take a look into has some limitations you would have to w/a when you decide to use it.The Webcam Capture API does not (currently) support reading h.264. If you want to use it you need to configure your camera to serve MJPEG instead of h.264.In regards to:I'm not using Internet Explorer (because it sucks) and I would be completely astonished if they have such useful feature implemented. For the development I strongly suggest using  with  or , but please note that the last one has problems with rendering MJPEG, so I would stick to the Firefox.Android  its own codecs to read h.264 frames. These codecs are not available in standard Java distribution. This is the \"magic\" behind it working on Android and not in regular Java application.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 29, 2015", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 29, 2015", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/839", "issue_status": " Closed\n", "issue_list": [{"user_name": "stolarek", "datetime": "Sep 24, 2021", "body": "Hello everybody,\nI use the nice library in my system with multiple cameras. It works just fine. The system takes pictures once a day via the connected cameras. Now to my problem: I want to display the live stream of individual cameras on my settings page and the user should be able to switch between the images of the cameras and align the cameras. I use currently for every webcam one \"WebcamStreamer\" class with port 8080 + n and switch the livestream with them. After a few minutes of inactivity, the cameras should be switched off. I use WebcamStreamer stop(). Is it possibele to restart an existing Streamer or schould a new object be created?\nI tried different combinations but at some point I can't see the livestream.\nWhat is the best way? Is it possible to switch different cameras with just one \"WebcamStreamer\"?\nI would be happy if there are suggestions here.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "Hi  ,so sorry to tell you that restart webcam streamer, webcamera API must reallocate stream reading postion again. but this is not implemented, please see comments:/**", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/847", "issue_status": " Open\n", "issue_list": [{"user_name": "yourfriendoss", "datetime": "Jan 12, 2022", "body": "I use\n to force camera size and it's still around\n200px bigger in width than if I view it from Windows Camera application ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/425", "issue_status": " Closed\n", "issue_list": [{"user_name": "ghost", "datetime": "Dec 23, 2015", "body": "Hi, I am trying to implement a QR Code Webcam scanner. However, I could not download the JAR file for the webcam capture from the link provided as I am redirected to a 404 page. Is it possible that the link is updated? Thank you.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 23, 2015", "body": "Hi @TorukoFenix,To what link? All JARs are available from , just search \"webcam-capture\".", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 23, 2015", "body": "You can also download ZIP. Check main page, section Download.", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 23, 2015", "body": "Hi, it's the link that you have provided in the description of your GitHub project. I have already found the JAR file on Maven Central though, thanks a lot. I have referenced your webcam-capture JAR to my project already, including the ZXing JAR as well. Referring to your QR Code example, is there a reason why the private Webcam and private WebcamPanel declaration contains an error even after the referencing? I'm sorry if my questions seemed a little silly, I'm quite new to programming Java with the usage of an external library, and I'm kinda pressed for time. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 23, 2015", "body": "@TorukoFenix,Thank you, I found the broken link. It points to snapshot (latest build created from HEAD). It's stored in Sonatype repo, but they keep such builds only for some time (several months I think). From the quick inspection of the repo I see they removed old files. Just before a minute I deployed the newest (0.3.12) snapshot and fixed the link s it points to the correct version (previously it was pointing to 0.3.11).In regards to your question:I do not understand what do you mean by . Can you please explain?", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 24, 2015", "body": "There was the typical red line under the Webcam and WebcamPanel. When I mouse over to the error a \"cannot find symbol\" was shown. I'm currently using NetBeans by the way. Is it related to the way I name the class? I have named it to WebcamTester but I have already included the extends and implements. One thing I noticed as well was import com.google.zxing.client.j2se.BufferedImageLuminanceSource; had an error where they mentioned \"package does not exist\". However I could find that file inside the ZXing folder, although I'm using Core.JAR. Is it because of the way I import the library?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 24, 2015", "body": "This is because you haven't include required JARs into the classpath.\nPlease google \"how to add JAR to classpath in Netbeans\".2015-12-24 3:17 GMT+01:00 TorukoFenix :", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 24, 2015", "body": "Yea, I have done that already, I've included core-2.2.jar for ZXing and webcam-capture-0.3.11.jar for your project. Is it somewhat related to the version of the JAR files?", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 25, 2015", "body": "Hi, I have fixed the problem after slowly importing the usage of your class files. Thank you so much!", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 25, 2015", "body": "Cool! Marry Christmas and a Happy New Year! Take care :)", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 25, 2015", "body": "Merry Christmas to you too, and a great new year!", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 23, 2015", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 25, 2015", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/182", "issue_status": " Closed\n", "issue_list": [{"user_name": "humber1000", "datetime": "Feb 21, 2014", "body": "First, let me congratulate you for your work. Your Webcam Capture library is awesome! Very effective and much more easy to use than the JMF from sun-oracle. Secondly, forgive me for my bad english, if it is. It's not my native language.\nWell, now... the issue... I'm making and application that shows the images of several webcams in a window (JFrame), and if you select one of them, it is shown in the main windows (a JFrame too). Everything was going well, the webcams was shown in the selection window, and the selected webcam was shown in the main window. But when I get to the moment of taking a picture (using the getImage() method) in the main window, the problem comes out. Every picture that I take is stored in a Vector, which is passed as the argument for a viewer window (JFame), and when I open it, it turns out that every image in the sequence of the viewer window is the same, the last one. It seems as if the last picture taken replaces the earlier ones in the vector.\nBut there's more. Whenever I clik a button in the viewer window, the webcam takes a snapshot and replaces the central image of the viewer panel (the bigger one) with the new one.So we have two issues in here:I would be very thankful if help me out with this. You'll see, this is part of my thesis, and if you take some time for helping out to solve this issues I would really appreciate it.I'm going to show you the relevant parts of my code so you can understand the way I'm doing the things.For detecting the several webcams connected I use the following code:In the selection window, the several webcams are showing through a square that extends from JToggleButton. This is the constructor for the squares:Now, once I do click over any square, the image appears immediately in the main window, with a bigger size. The main window implements the ActionListener interface, and this is the relevant part of the actionPerformed method:Well, as I told you before, I would be very grateful if you help me solve this problems.\nThanks in advanced.\nAH!... And my OS is Windows 7 (32-bit), and I'm usin webcam-capture-0.3.10-RC6-dist. My webcmas are Genius ISlim 300X.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 21, 2014", "body": "Hi ,This hack with unlocking webcam to open it in parallel may lead you to many problems, including Java crash, especially when the webcam is closed, and this method will work only on Windows (because of the bug in the videoinput library actually). On Linux or Mac OS you will simply get an error.Just wanted to explain this - opening single webcam in parallel is fundamentally wrong and you should avoid this. If you want more details I can explain you later because now I have to catch the last bus to my apartment or I will have to go 13 km by foot at night.In regards to the image replacement - I guess this may be caused by the fact you are using non-blocking mode where image is buffered (open method with arg true), but I have to verify this. Try opening webcam with open(false) and check again. Seems like the images are not really replaced, but by the fact that you are always getting the same image. Please correct me if I'm wrong.", "type": "commented", "related_issue": null}, {"user_name": "humber1000", "datetime": "Feb 26, 2014", "body": "Hi, sarxos.\nThanks for you soon reply.\nI tried what you said and the code for the CamSquare constructor looked like this:As you can see, I opened the webCam with the 'false' argument, and I removed the line where I unlocked it... but the result was the same.\nBy the way, although I removed the unlock line and I used the 'false' argument to open the webCam, I still can have two webCamPanel of the same webCam playing in diferent panels of the application. But as I told you... the problem regards the getImage() method persists.\nWhat about if we do this... for you must be very easy and quick to make... Create a WebcamPanel in the usual way, creating its Webcam as you always do, and try this: get several images using the getImage() method and store them in a Vector at the same time you are getting them, one by one. Then, use the vector to display the images in a diferente window, and you'll see that they all will be the same.\nAnd don't forget the most weird issue... when I press any button in the viewer panel, which has nothing to do with the main window, where the getImage() method stays, all images change into a new captured image. As if the webcam took an snapshot alone, just by pressing any button. I could think in a logic error if it happened in the main window, where the getImage() method stays, but, in the viewer panel?... Why? I don't understand.I hope you don't bother for helping me out. Your library is very good, and I'd like to use it properly.\nThanks in advanced, sarxos.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 26, 2014", "body": ",You are absolutely correct. The image instances are actually completely different objects, but the data beneath the raster references the same  array () containing image data. Damn, I was so happy year ago when I was able to drop down the memory consumption... Sadly, seems like we need to increase it again because my  actually introduced a bug you've discovered.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 26, 2014", "body": "I uploaded newest JAR into Sonatype repo, you can download it from here:I already confirmed it's fixing problem you've found, but please confirm it as well.In regards to the problem with button... It's pretty weird since  does not expose any camera controls for mouse or keyboard - it's just a plain area where image is being drawn. I suppose this may be side effect of the bug, so please try with new JAR.As for the parallel playing, I haven't said it's not possible - you can of course play image from one webcam in unlimited number of panels (as long as you have resources for that), but you  open it twice (the webcam device). This is because when you open camera, it allocates native instance which is using DirectShow on Windows, libv4l on Linux and QuickTime on Mac. The problem arose because Windows allows two separate instances to use the same camera device (Linux and Mac does not allow such operation) via the same capture graph, and when you close one instance, it will destroy capture graph, so when second instance will try to read it, it will most likely crash because of segmentation fault, or with some strange error in console (if you are lucky enough).Here is the small program I used to debug the problem (as you suggested). I moved it into examples:Thank you very much for finding this bug :)Take care!", "type": "commented", "related_issue": null}, {"user_name": "humber1000", "datetime": "Feb 26, 2014", "body": "Thanks to you, for your great collaboration.\nI'm going to try with the new .jar.\nTake care.", "type": "commented", "related_issue": null}, {"user_name": "humber1000", "datetime": "Feb 26, 2014", "body": "sarxos... I'm sorry for bringing you bad news... but the new .jar works the same than the old one. The problem persists.\nIn other way, I wanted to ask you something. The driver OpenIMAJGrabber.dll found in Webcam Capture\\webcam-capture-0.3.10...\\com\\github\\sarxos\\webcam\\ds\\buildin\\lib\\win32 should be placed in some folder? If I remember well you didn't leave instructions about that driver. Where should I put it?\nAnd what about the OpenIMAJGrabber.class? Which cases should it be used in?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 27, 2014", "body": "The OpenIMAJGrabber.dll should  put it in any folder. It's automatically extracted by BridJ. Just replace RC6 JAR in the classpath with the new one and you should be fine. This is default driver, embedded in Webcam Capture JAR, which should work fine as it is, so it doe not require any specific instructions.When you say \"it works the same\", what do you actually mean? I used the simple example I prepared to test issue you've described (images replaced in vector) and confirmed it was there. After I fixed the bug, the same code confirmed it does not persist any more.Have you removed previous Webcam Capture JAR from classpath and replaced by new one, or you just added new one without removing previous? Also, are you using Maven? In such case, JAR replacement will not work since you have to change dependency in your POM if this is the case.", "type": "commented", "related_issue": null}, {"user_name": "humber1000", "datetime": "Feb 28, 2014", "body": "Maven? I'm not using nothing with such a name, as far as I know. I'm only using your .jar files.\nAnd when I said \"I works the same\", I ment that the problem persisted. The images stored in the vector are replaced for the last one taken, which by the way, is taken (I realized that later) when I press the button to open the viewer window. As if the getImage() method executes by itself when I press any buton. Because, remember what I told you, anytime I pressed a button in the viewer window, which is a class apart (that has NOTHING to do with the getImage() method) the camera takes a snapshot and replaces the central image.\nAnd as for the procedure you are telling me regard the .jar files, I did just like what you said. I removed the old .jar and replaced for the new one, and configured my IDE (JCreator) to use the new .jar file. But it didn't help me.\nBut, you know something?... I just test your TakePicturesAndPlayExample.java code and with the new .jar it works fine. But it doesn't in my code. So I realized that something in my code or in my configuration must be wrong.\nCould you help me figure out what could it be?\nThanks in advanced.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 1, 2014", "body": "I can help you with the code (just drop it somewhere or put on Github), but not with the configuration, since I'm completely unfamiliar with JCreator (I'm using Eclipse IDE).Maven is a dependency manager. In small projects dependencies managing problem does not usually exist, but when the project grows, you found that number of dependencies (JARs) you have to use use in code, constantly grows, and you realize in one moment that some classes are missing, some methods does not exits, etc - this is because of wrong or missing JARs included in classpath. Maven takes this burden from you and create dependency tree which contains all required JARs and download them from the  which contains more than half million of officially released files. It knowns what JARs are required by other JARs because their creators declared this when they put them in Maven Central.Learning Maven may be a little hard for the beginning, but after you are familiar with this tool, it's like a blessing in your everyday programming life. All my projects I created in several years back from now, are using Maven and I cannot imagine managing all required JARs manually.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 26, 2014", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 26, 2014", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 22, 2014", "body": [], "type": "issue", "related_issue": "#155"}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/601", "issue_status": " Open\n", "issue_list": [{"user_name": "reginavaleria96", "datetime": "Dec 18, 2017", "body": "Is it possible to access and take picture from a phone connected through  using this project?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 18, 2017", "body": "Hi ,It seems you can, but I don't know the exact URL to be used with . This is the claim from their web page:When you already know what is the URL to access MJPEG stream from the camera you can take a look at the IP camera examples of Webcam Capture to learn more on how to integrate it into your solution:Please let me know if you have any further questions.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 19, 2017", "body": ", FYI, I found information on MJPEG here:Check this section: .", "type": "commented", "related_issue": null}, {"user_name": "neilyoung", "datetime": "Dec 19, 2017", "body": " Just out of curiosity: Would it be possible to  from such a cam?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 20, 2017", "body": ", yes, but the performance will be much below what you can get with UVC (USB). This is due to:In addition to 2, you can skip this and get RAW bytes, but these won't be RGB, but a JPEG frame, which you will have to convert to RGB to have compatibility with  (thus doing the same what  is doing underneath).", "type": "commented", "related_issue": null}, {"user_name": "neilyoung", "datetime": "Dec 20, 2017", "body": " Clear and understandable. Thanks for the explanations. Merry Christmas to you and thanks too for all your efforts.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 10, 2018", "body": "I have included webcam-capture-0.3.12-20171103.095135-4.jar in my library but I'm getting  and \nWhere can I find IpCamDeviceRegistry and IpCamMode?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": ", use these JARs: ", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": "And remove webcam-capture-0.3.12-20171103.095135-4.jar beforehand. The zip I posted above, already contains newest webcam-capture dependency.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": ", and one more missing JAR file: ", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": "Thanks for the JARs!\nDo you mind explaining why is it necessary to have\n\n( line 57 onward)?and do webcam.open() and webcam.close() work the same way for IPcam?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "Hi ,In regards to your questions:This is the code which is executed when action is performed on a \"Snapshot\" button. When you click this button the  will iterate over all  instances in  list, get image and save it to file with . The file names will be , , , etc (as many as you have webcams). Without this code you would not have ability to save images into files.And in regards to:No. The  and  works in a different for IP cameras when you compare to  webcam.In case of classic webcam (the ) when you invoke  then hardware device is turned on (LED is powered) and USB capacity is allocated to stream image frames. After you invoke  the USB capacity is released and UVC device is turned off. There is a direct connection between UVC being turned on/off and the webcam state (unless something is broken).The  from the other hand is always turned on, unless you turn it off with a manual switch, a power cord or administration panel. In this case when you invoke  a persistent HTTP connection is made which is used to stream MJPEG frames from camera to your computer, but this is done with the IP/TCP instead of USB (as in case of UVC). Then, when you invoke , the webcam is not turned off, but the HTTP connection is closed instead and IP camera device remains active.This is a difference. I hope I was able to describe it in a understandable way.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "Just for your information, if you would like to avoid managing JARs manually, I suggest you to take a look at .Maven is a tool to build Java projects and manage JAR dependencies. It may be a little hard to learn at the beginning, but there a dozens of tutorials on the internet. The ZIP bundles I sent you are also a list of files prepared by Maven.The more JARs you have in the project the higher chance you will have to use some dependency manager (Maven, Gradle or Ivy). When Java project grows in time it comes into the point where managing JARs manually is impossible (due to massive amount of incompatible libraries).My Webcam Capture API project is also managed and build with Maven.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": " Thank you for your explanation, it was clear and understandable. I was confused about the webcam iteration, that was why I had to ask. I hope you don't mind.If I have the default onboard webcam on my laptop and an IP camera connected via droidCam, how do I tell the program to specifically take one picture from my DroidCam camera?And thank you for sharing about Maven, I have heard of it and had considered using it. My mistake was thinking that my small project won't have so many libraries and deciding to manage them manually, but now I know that I was wrong :')", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "In regards to:Yes, sorry for this. The code I initially wrote was to support multiple cameras, but finally I left only one IP camera in the code.You have to use composite driver to \"compose\" two drivers into one, example:You set it in the same way as you would do it with a different drivers:You can access cameras by index so the ones from first driver will be first one the list (e.g. index 0), and the ones from second driver will be added later (e.g. index 1, 2, 3, etc).The other option is to use camera names, but this may be tricky on Windows since it may work well on your computer, but different computers may have different camera names. In Linux this is simple because on different computers every all cameras are simply , , etc, and on Windows this can be , , etc. depending on the webcam hardware.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": "Thanks, that is one helpful explanation! I am going to try it now. I hope you don't mind if I still have more questions in the future.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "If you link your carrier with Java then Maven is a tool you have to, at least, be familiar with. It has thousands of plugins and configuration options which may be overwhelming at the beginning because different people on the internet may suggest different solution, but after you understand how it works it will become one of the fundamental tools in your toolbox.When you want to manage you project with Maven it's enough to put  file in the project main directory.The simplest example for your project can be:Instead of providing many JARs I only had to add one dependency in the XML, that is:And when I  into the project and run  I can see what JARs are used:Which will display all JARs required by your project:And e.g. if I want to export all required JARs together with your own application JAR I can do the following:And all JARs will be magically be placed in a  directory:I spent many years working in a terminal environments and I'm used to do things from command line, but the same can be done from a decent IDEs, e.g. Eclipse or IntelliJ.For example when you are using Eclipse IDE for Java EE Developers (basic Eclipse installation does not have Maven plugin installed) you can create Maven project from the mouse menu and manage dependencies with a very nice POM editor.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "And one last thing - almost all (probably 99.99%) of all decent Java frameworks and libraries are deployed into Maven Central repository which is used as a source from where all dependencies are downloaded.Jersey, JBoss, Hibernate, JSP, all Apache Commons, SL4J, Logback, Jetty, Netty, Vavr, Akka, and of course Webcam Capture API, all are available from Maven.", "type": "commented", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 29, 2018", "body": "Hello Sir,\nI am trying to use  as a Webcam. The program is identifying the device but it not showing the panel. I referred  example. Please help me.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2018", "body": "", "type": "", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 29, 2018", "body": "Yes, I am able to view the Image in browser. I didn't make any changes to the code. I just copied the code from example program. Here is the code.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 30, 2018", "body": "Hi ,If you can view it in browser and you can't in a Java code, means that your code is broken. The URL you provided to IP cam driver:Points to a web page, not MJPEG stream! A static HTML, nothing to stream video feed from. Please take a look at the DridCam web page and ! You will find this information:Therefore, your code should be fixed to use proper URL:I installed DroidCam and easily found it myself by checking what is the image source (just press F12 in Firefox):This is my code:And it works perfectly well:After reading this far please also take a look at the problem I described below.I noticed that you can connect  to the DroidCam. Therefore if you have your DroidCam web page open in a browser you  to view it from code and vice versa - when you have your DroidCam streaming open from the Java code, you won't be able to view it in a browser. This is DroidCam limitation, not the Webcam Capture issue.", "type": "commented", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 31, 2018", "body": "Thank You very much Sir. Now it is working", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 19, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 30, 2018", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/852", "issue_status": " Open\n", "issue_list": [{"user_name": "shadowwalkersteam", "datetime": "Feb 9, 2022", "body": "I'm getting this \"library not loaded successfully from file\" error I'm using snapshot version of sarxos webcam and using the default camera drive.\nAlready added the latest bridJ library but the issue is still there. I also tried to copy the OpenIMAJGrabber.dll and BridJ.dll into the system32 folder and in the folder where my application jar was present but still getting the same error. Please check the picture attached.\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/858", "issue_status": " Open\n", "issue_list": [{"user_name": "GeorgiG9", "datetime": "Apr 3, 2022", "body": "Hi all\nI am trying to use the  plugin for ImageJ, which is based on the Webcam Capture library.I am trying to use it on Raspberry Pi 4B+ 4Gb with the Raspberry Pi camera and not with a USB camera connected to the Pi. I have tried on Ubuntu and Rasbian 32, but I get an error on both OS.Here is the exception that I get running on Ubuntu for Raspberri:(Fiji Is Just) ImageJ 2.3.0/1.53q; Java 1.8.0_312 [64-bit]; Linux 5.13.0-1008-raspi; 147MB of 400MB (36%)com.github.sarxos.webcam.WebcamException: java.util.concurrent.ExecutionException: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat com.github.sarxos.webcam.WebcamDiscoveryService.getWebcams(WebcamDiscoveryService.java:118)\nat com.github.sarxos.webcam.Webcam.getWebcams(Webcam.java:692)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:755)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:732)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:710)\nat IJ_webcam_plugin.run(IJ_webcam_plugin.java:63)\nat ij.IJ.runUserPlugIn(IJ.java:243)\nat ij.IJ.runUserPlugIn(IJ.java:258)\nat ij.IJ.runPlugIn(IJ.java:204)\nat ij.Executer.runCommand(Executer.java:152)\nat ij.Executer.run(Executer.java:70)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat java.util.concurrent.FutureTask.report(FutureTask.java:122)\nat java.util.concurrent.FutureTask.get(FutureTask.java:192)\nat com.github.sarxos.webcam.WebcamDiscoveryService.getWebcams(WebcamDiscoveryService.java:110)\n... 11 more\nCaused by: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat com.github.sarxos.webcam.WebcamProcessor$AtomicProcessor.process(WebcamProcessor.java:57)\nat com.github.sarxos.webcam.WebcamProcessor.process(WebcamProcessor.java:120)\nat com.github.sarxos.webcam.WebcamTask.process(WebcamTask.java:35)\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver$WebcamNewGrabberTask.newGrabber(WebcamDefaultDriver.java:46)\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver.getDevices(WebcamDefaultDriver.java:118)\nat com.github.sarxos.webcam.WebcamDiscoveryService$WebcamsDiscovery.call(WebcamDiscoveryService.java:35)\nat com.github.sarxos.webcam.WebcamDiscoveryService$WebcamsDiscovery.call(WebcamDiscoveryService.java:25)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n... 1 more\nCaused by: java.lang.UnsatisfiedLinkError: org.bridj.Platform.sizeOf_ptrdiff_t()I\nat org.bridj.Platform.sizeOf_ptrdiff_t(Native Method)\nat org.bridj.Platform.(Platform.java:235)\nat com.github.sarxos.webcam.ds.buildin.natives.OpenIMAJGrabber.(OpenIMAJGrabber.java:59)\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver$WebcamNewGrabberTask.handle(WebcamDefaultDriver.java:56)\nat com.github.sarxos.webcam.WebcamProcessor$AtomicProcessor.run(WebcamProcessor.java:66)\n... 3 moreMy question is, is this still the problem with the driver (as explained ) or not? Under  is written that if I want to run it on the Raspberry, I need to replace version 0.6.2 of BridJ JAR by the  or newer . So I have done this, but I still have the error. is another driver that can be used for the Raspberry.So did someone lately succeed in running it on the Raspberry?As I do not have knowledge in Java, I am not sure how I can change the drivers in the plugin  for ImageJ, and I am not sure if this is the solution to my problem.Here is the error that I get in the ImageJ software:\nI really appreciate any help you can provide.\nBest regards\nGeorgi", "type": "commented", "related_issue": null}, {"user_name": "shinobisoft", "datetime": "Apr 5, 2022", "body": "From what I'm seeing at a glance... You're trying to load 64 bit webcam-capture on a 32 bit OS. I'm not familiar with Raspberry Pi CPU architecture unfortunately.To my knowledge, this has always been troublesome on Raspberry Pi and MacOS.Note that this library is designed to get images from USB and IP cameras.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/853", "issue_status": " Open\n", "issue_list": [{"user_name": "MuffinCat1", "datetime": "Feb 14, 2022", "body": "someone can help me im trying to make a camera that you can take photo of yourself and the photo will be stored on your desktop as a jpg file but when im trying to run this code at first its working but then it says that there is an error with the camera but when im disconnects and reconnects the camera i can do these again and then again it says i have an error someone can help me please these is the code raw: ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/859", "issue_status": " Open\n", "issue_list": [{"user_name": "svcsrao", "datetime": "Apr 27, 2022", "body": "for Mac I am using \"OpenImajDriver(); ==> it is streaming perfectly every time.for Windows I am using \"CaptureManagerDriver()\" ==> it is streaming first time and from second time onwards, it is not streaming and displaying last captured image.First time camera opening (Displaying the led light), second time onwards it is not displaying the led light", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/854", "issue_status": " Open\n", "issue_list": [{"user_name": "johnl5124", "datetime": "Feb 16, 2022", "body": "Hello,Currently, I'm working towards running  from BoofCV on my Raspberry Pi. As I'm having problems with that I thought it'd be best to start with trying to get  working.However, I get...It then goes on to loop \"we shouldn't be here...\" for infinity. I'm new to working with Raspberry Pis' and camera drivers. I was just wondering if someone could provide some insight to this problem? As far as I'm aware, I have all imports and dependencies sorted but then again I may be wrong. Here is the link to my file.Thanks", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/851", "issue_status": " Open\n", "issue_list": [{"user_name": "svcsrao", "datetime": "Feb 8, 2022", "body": "Tried many ways using saros 0.3.12 jar and not able to access webcam (both internal and external) in latest MAC OS versions.", "type": "commented", "related_issue": null}, {"user_name": "Erhannis", "datetime": "Feb 8, 2022", "body": "Same here.  Here's some other similar tickets; seems like Mac has been difficult to maintain.\n\n\n\nThough actually, it looks like the final post in  (  ) works - I don't know which components are strictly necessary, but after swapping out my dependencies for the ones that look relevant, and adding the repositories, my test code compiled and ran, apparently correctly fetching a default webcam (which was where I was stuck before).", "type": "commented", "related_issue": null}, {"user_name": "Erhannis", "datetime": "Feb 8, 2022", "body": "I removed the dependency on , and it still fetched a webcam correctly, but both the repositories seem necessary, and presumably both  and  are necessary.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/833", "issue_status": " Closed\n", "issue_list": [{"user_name": "brianmichalk1", "datetime": "Aug 9, 2021", "body": "Sometimes my USB glitches while in a getImage() call, which hangs my program.  I've put this into a separate thread and use a semaphore timeout to indicate when getImage() takes too long.  Killing the thread sometimes works but doesn't seem to be the correct approach.  Should I use Webcam.shutdown?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Aug 13, 2021", "body": "Hi  ,Java is working at high level, for webcam-capture, it is a lighweight of wrapping of camera accessing via JNA. so device glitch is not handled well well though we introduced WebcamDiscoveryListener.\nWhen you open camera without reset new Driver explicitly, default is:\n\nit is thread unsafe and blocking getImage().My suggestion is:\nyou can attach a WebcamDiscoveryListener to watch camera devices though it is not fully guaranteed.\nmanage the getImage() thread gracefully with timeout and WebcamDiscoveryListener", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/832", "issue_status": " Closed\n", "issue_list": [{"user_name": "chottu33", "datetime": "Jul 12, 2021", "body": "Hellowhile the recording is on, webcam is not getting connected again, if the camera is disconnected and connected back.\nObservation: Discovery Service of the camera is not running\nOS: Ubuntu 20, Centos 8\njars version - webcam-capture-0.3.13-20200330.202351-7.jar, bridj-0.7-20140918-2.jar\njava version: 1.8.0", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Jul 24, 2021", "body": "could u please provider hardwaew info?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/837", "issue_status": " Closed\n", "issue_list": [{"user_name": "GivouDev", "datetime": "Sep 6, 2021", "body": "Hello Guys,the API doesnt detect any Webcams on my Linux Machine.The Code i use is not that important, because...its not doing really something but here:\n`package org.givou.ai;import com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;import javax.swing.*;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;public class Main {\nstatic JFrame ui;}\n`I have installed all Libraries and slf4j-simple extra, because i got this StaticLoggerBinder error.\nNow when i try to fire up my project it shows me this:\n[main] INFO com.github.sarxos.webcam.Webcam - WebcamDefaultDriver capture driver will be used\n[main] WARN com.github.sarxos.webcam.Webcam - No webcam has been detected!And Webcam.getDiscoveryService().getWebcams() is also showing 0 webcams.\nhwinfo --usb finds my webcam aswell, and i can access it with mplayer.\nThanks to anybody who tries to help me :)", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "hi ,\nthere indeed some cases camera can not be detect. The root cause is that default built-in camera driver depends on bridj which is a JNA solution from java to native code. Unfornaturely, bridj community stopped update for a long time so that can not work for new OS.\nWhat you can do is change other driver.Alex", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/834", "issue_status": " Closed\n", "issue_list": [{"user_name": "cameronboddie", "datetime": "Aug 13, 2021", "body": "Hello I have a demo where I need to use the webcam to capture a QR. Simultaneously, I will be on a zoom call with my PM. Is there a way to make this work? It works while not on Zoom but doesn't work even if my camera is off. I haven't been able to properly troubleshoot the issue.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Aug 13, 2021", "body": " underlay camera hardware is an exclusive resource, once you are in Zoom meeting, the camera is taken by Zoom. You can not open it again from another process. at JVM high level, it is hard to debug.", "type": "commented", "related_issue": null}, {"user_name": "cameronboddie", "datetime": "Aug 15, 2021", "body": "Thank you for the quick response. I appreciate it, I'll look into some work arounds for the demo.", "type": "commented", "related_issue": null}, {"user_name": "cameronboddie", "datetime": "Aug 15, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/838", "issue_status": " Open\n", "issue_list": [{"user_name": "cinlung", "datetime": "Sep 15, 2021", "body": "Dear SarxosFirst, I would love to say that your API are amazing for webcam and IP cam. Very easy to learn and easy implementation. With this message, I want to ask your guidance to connecting to the IP camera that I have. It has OnFiv standard and it has web interface protected with username and password. When entering the camera requested me to install VLC, I guess it uses VLC protocol for the image.Using your example and tutorial for IP Cam connection I was able to connect to a sample camera you provided (), but upon testing with my camera it is stuck with no error whatsoever.Here are parts of my codes:\n`System.out.println(\"Adding driver for ip cam...\");\nWebcam.setDriver(new IpCamDriver());The code stuck at Would you please tell me what is wrong with my camera? Is there additional steps needed? Thank you for your helps in advance.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "it happens here：\n`\tpublic WebcamPanel(Webcam webcam, Dimension size, boolean start, ImageSupplier supplier) {", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "it happens:start will goto IPCamera driver:so please check your MJPEG stream server is qualified server.Alex", "type": "commented", "related_issue": null}, {"user_name": "cinlung", "datetime": "Sep 30, 2021", "body": "", "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/828", "issue_status": " Closed\n", "issue_list": [{"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "I tried to run this codeAnd  got this error:Please help me to fix this error.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": "The SLF4J logging framework JAR is missing in your classpath (with other required dependencies as well, e.g. BridJ). See  wiki page for more details on how you can fix it. Please also check the  section on the main page for the ZIP. In short - you need to have Webcam Capture JAR in the classpath, but also SLF4J and BridJ (or JARs for other capture framework if you are using something non-default, e.g. MJPEG).", "type": "commented", "related_issue": null}, {"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "Thanks for replying", "type": "commented", "related_issue": null}, {"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "I added that but now the picture that is captured is totally black.\nSo, How should I fix that?\n", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/829", "issue_status": " Closed\n", "issue_list": [{"user_name": "kashis7870", "datetime": "Jun 12, 2021", "body": "Please help me to fix this problem, i am getting a black screen at the given location and also getting few errors.\nPlease find attached.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "please check your camera, probably VGA is not proper for your hardware", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/830", "issue_status": " Closed\n", "issue_list": [{"user_name": "shichiye", "datetime": "Jun 15, 2021", "body": "At present, I know that the camera can be obtained by a given name, but I cannot confirm which camera it is. I want to know whether the corresponding USB camera can be obtained by the USB camera similar to the hardware id. I don’t seem to find the corresponding api.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2021", "body": "There is no such API in the Webcam Capture project. What we have here is a very basic abstraction over the UVC device. There is no API to get the USB information. Even if we go into the lower level and dive deep into the native C++ code, there is no such thing as well. The operating system abstracts the device and completely decouples it from the USB transport.One could in theory use some 3rd party library like  to scan the ports and correlate UVC data from Webcam Capture with the hardware device class present in the USB port, but I never tried this and cannot even tell if such thing is possible. This is just a brief idea that came into my mind.", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Jun 16, 2021", "body": "I'm working on publishing as open source a native driver capable of providing actual device ids and very good performance. For the moment it will work on Windows only. Maybe tomorrow it's ready, I will reply back.We use the driver in a production app and works really well.", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Jun 17, 2021", "body": "Hi,I just published the driver here: Please let me know if you try it successfully or you have any problem.To get the device id you will need to cast  to ", "type": "commented", "related_issue": null}, {"user_name": "shichiye", "datetime": "Jun 18, 2021", "body": "  Thank you both for your answers.\nI will try it, and then I will reply to you ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Could be possible to know the id device of the USB with the relative istance ?USB\\VID_0C45&PID_6366&MI_00\\6&183AF011&0&0000", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": "Yes, you can with  and casting  to ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Could you help me with an example in pure Java... I tried to follow the tutorial but after loading the dll no output of which webcam is connected to the computer. Without loading dll everything is working well. i tried to contact you on twitter also.\n", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Also dll are loaded in the project Netbeans as in photo.\n\nI also tried to put in same place where opencv read it's own dll. OpenCv could work well without any issue but this driver won't work....", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": " This library tries to load its DLLs from the natives folder relative to where the program is executed. Adding to netbeans or as jar won't do anything.Please make sure to run your program in a folder where natives is located. Also I recommend setting up a slf4j backend such as logback for viewing logging messages.", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "It works. Was a matter of libraries... I'm not using maven project... Standalone application developing here.\nThanks for the support", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": " Great! ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "I think the best ways to solve this issue in the future... release a zip file with all the dependency ahah", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/807", "issue_status": " Closed\n", "issue_list": [{"user_name": "zy5651768", "datetime": "Feb 23, 2021", "body": "I use default driver ,fps is low on ubuntu (  3-5 fps ) ,maybe change driver will good?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 23, 2021", "body": "Hi,\nBecause pure Java preview panel and decoding is not high performance as native c, if your camera is 1080p, base one your PC performance, 3-5 fps make sense.Regarding other driver, maybe you can try JNI based driver like opencv or vlcj which are litter faster but not significant. and also you can set System properties in your launch arguments:OpenGL or XRender is enabled to get low latency, something like\nexport _JAVA_OPTIONS=-Dsun.java2d.opengl=True\nexport _JAVA_OPTIONS=-Dsun.java2d.xrender=Truebase on your hardware, maybe higher fps can be there.\nAlex", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 27, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/818", "issue_status": " Closed\n", "issue_list": [{"user_name": "hqwl159", "datetime": "May 6, 2021", "body": "When the device number is greater than 15, the device information cannot be obtained.In Centos7\nI can read the camera info on the path of \"/dev\"\nI use the webcam-capture codeThe output  is \"1\".But I can read  all the  camera device info when the device name ranges from \"video0\" to \"video15\".So I guess the code cannot get device information after “video15”.Is my code error ?", "type": "commented", "related_issue": null}, {"user_name": "flx5", "datetime": "May 7, 2021", "body": "That is an issue with the default driver OpenIMAJ. It only scans the device files 0 to 15. .\nYou might want to try the JavaCV driver. In that implementation every device in /dev/video* should be recognized.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": "Thank you most sincerely!!!\nThe webcam-capture can work without camera driver.  Does the webcam-capture or Linux OS  use the  OpenIMAJ as default driver?", "type": "commented", "related_issue": null}, {"user_name": "flx5", "datetime": "May 7, 2021", "body": "If you don't specify which driver should be used with setDriver, webcam-capture uses the default builtin driver which is OpenIMAJ regardless of the operation system.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": "Thanks a lot!!!\nI find  it in README.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/813", "issue_status": " Open\n", "issue_list": [{"user_name": "fgms0115i", "datetime": "Apr 20, 2021", "body": "When called webcam.open(), SIGSEGV occurs.\nCamera is UVC CAM from Sonix Technology Co., Ltd.[src code]\nvoid main() {\nwebcam= Webcam.getWebcams().get(0);\nwebcam.open();_\n}", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/801", "issue_status": " Closed\n", "issue_list": [{"user_name": "schdai", "datetime": "Jan 5, 2021", "body": "Hi, I'm using video capture card to get image from some devices. There are more than one input, including  S-Video, Composite and etc.  Can webCam select input channel？ How？TKS。", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 2, 2021", "body": "use WebcamCompositeDriver", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": "The drivers from WebcamCapture project supports only UVC devices and so will not be able to detect cameras connected via S-Video or Composite Cinch, but in general, if you implement (create) custom driver for every input you desire, then you can bundle these drivers together using the  as  suggested.Just for your information. I never found a Java project to stream video from S-Video or Composite Cinch. I guess you will need some kind of TV tuner for this. Not sure, I never worked with analog video data.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/821", "issue_status": " Closed\n", "issue_list": [{"user_name": "LuisHenriqueFA14", "datetime": "May 26, 2021", "body": "I don't use java a lot, so i don't know almost nothing about java (what that i need to compile or what is a .class file), but i'm trying to make a camera app to me, but i can't use it...I was having some troubles with package, i solved it, and when i was done, it was an error like:./com/github/sarxos/webcam/Webcam.java:20: error: package org.slf4j does not exist\nimport org.slf4j.Logger;So i started my search to solve it. I found a lot of issues about that, and i found something about put the  dir into the classpath (idk where are the classpath lol)So, i need some help with the libs, like, where to put it ?\nI need to put the  dir to my root path ?\nI need to put the  dir to my root path or the   ?\n(Please, be so specific bc i'm dumb)\nThank you, and sorry about my english ", "type": "commented", "related_issue": null}, {"user_name": "jrbgarcia29", "datetime": "May 28, 2021", "body": "you need to build your project by running \"mvn install\" in your project base dir so that dependencies will be downloaded and missing jars will be available to your local repository", "type": "commented", "related_issue": null}, {"user_name": "LuisHenriqueFA14", "datetime": "May 28, 2021", "body": "When i run  it returns an error talking .\nHow can i install it ?\nIt has another way to build my project ?\nThank you again!", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "May 31, 2021", "body": "This is not issue, please find java maven guileline by yourself", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "May 31, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/799", "issue_status": " Open\n", "issue_list": [{"user_name": "jhess32259", "datetime": "Dec 29, 2020", "body": "Greetings, .I'm receiving an exception when instantiating a JPanel form. Environment is NetBeans 12.1 with OpenJDK15. Below is the code to the JPanel, the exception text, and the POM. Any help would be greatly appreciated.Regards,\nJoe Hesspackage sst01;import com.github.sarxos.webcam.Webcam;public class WebCamera extends javax.swing.JPanel {}\ncom.github.sarxos.webcam.WebcamException: java.util.concurrent.ExecutionException: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat com.github.sarxos.webcam.WebcamDiscoveryService.getWebcams(WebcamDiscoveryService.java:124)\nat com.github.sarxos.webcam.Webcam.getWebcams(Webcam.java:893)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:956)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:933)\nat com.github.sarxos.webcam.Webcam.getDefault(Webcam.java:911)\nat sst01.WebCamera.(WebCamera.java:25)\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\nat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\nat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\nat java.base/java.lang.reflect.ReflectAccess.newInstance(ReflectAccess.java:128)\nat java.base/jdk.internal.reflect.ReflectionFactory.newInstance(ReflectionFactory.java:350)\nat java.base/java.lang.Class.newInstance(Class.java:645)\nat org.netbeans.modules.form.CreationFactory.createDefaultInstance(CreationFactory.java:155)\nat org.netbeans.modules.form.RADComponent.createBeanInstance(RADComponent.java:227)\nat org.netbeans.modules.form.RADComponent.initInstance(RADComponent.java:166)\nat org.netbeans.modules.form.MetaComponentCreator.initComponentInstance(MetaComponentCreator.java:1495)\nat org.netbeans.modules.form.MetaComponentCreator.createVisualComponent(MetaComponentCreator.java:962)\nat org.netbeans.modules.form.MetaComponentCreator.access$300(MetaComponentCreator.java:54)\nat org.netbeans.modules.form.MetaComponentCreator$2.run(MetaComponentCreator.java:257)\nat org.netbeans.modules.form.FormLAF$2.run(FormLAF.java:268)\nat org.netbeans.modules.openide.util.NbMutexEventProvider$Event.doEventAccess(NbMutexEventProvider.java:115)\nat org.netbeans.modules.openide.util.NbMutexEventProvider$Event.readAccess(NbMutexEventProvider.java:75)\nat org.netbeans.modules.openide.util.LazyMutexImplementation.readAccess(LazyMutexImplementation.java:71)\nat org.openide.util.Mutex.readAccess(Mutex.java:225)\nat org.netbeans.modules.form.FormLAF.executeWithLookAndFeel(FormLAF.java:251)\nat org.netbeans.modules.form.MetaComponentCreator.precreateVisualComponent(MetaComponentCreator.java:254)\nat org.netbeans.modules.form.MetaComponentCreator.precreateVisualComponent(MetaComponentCreator.java:273)\nat org.netbeans.modules.form.HandleLayer$NewComponentDrag.init(HandleLayer.java:3344)\nat org.netbeans.modules.form.HandleLayer$NewComponentDrag.(HandleLayer.java:3336)\nat org.netbeans.modules.form.HandleLayer$NewComponentDropListener.dragEnter(HandleLayer.java:3633)\nat java.desktop/java.awt.dnd.DropTarget.dragEnter(DropTarget.java:355)\nat java.desktop/sun.awt.dnd.SunDropTargetContextPeer.processEnterMessage(SunDropTargetContextPeer.java:334)\nat java.desktop/sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchEnterEvent(SunDropTargetContextPeer.java:810)\nat java.desktop/sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchEvent(SunDropTargetContextPeer.java:778)\nat java.desktop/sun.awt.dnd.SunDropTargetEvent.dispatch(SunDropTargetEvent.java:48)\nat java.desktop/java.awt.Component.dispatchEventImpl(Component.java:4855)\nat java.desktop/java.awt.Container.dispatchEventImpl(Container.java:2321)\nat java.desktop/java.awt.Component.dispatchEvent(Component.java:4822)\nat java.desktop/java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4919)\nat java.desktop/java.awt.LightweightDispatcher.retargetMouseEnterExit(Container.java:4699)\nat java.desktop/java.awt.LightweightDispatcher.trackDropTargetEnterExit(Container.java:4648)\nat java.desktop/java.awt.LightweightDispatcher.trackMouseEnterExit(Container.java:4661)\nat java.desktop/java.awt.LightweightDispatcher.processDropTargetEvent(Container.java:4614)\nat java.desktop/java.awt.LightweightDispatcher.dispatchEvent(Container.java:4484)\nat java.desktop/java.awt.Container.dispatchEventImpl(Container.java:2307)\nat java.desktop/java.awt.Window.dispatchEventImpl(Window.java:2769)\nat java.desktop/java.awt.Component.dispatchEvent(Component.java:4822)\nat java.desktop/java.awt.EventQueue.dispatchEventImpl(EventQueue.java:772)\nat java.desktop/java.awt.EventQueue$4.run(EventQueue.java:721)\nat java.desktop/java.awt.EventQueue$4.run(EventQueue.java:715)\nat java.base/java.security.AccessController.doPrivileged(AccessController.java:391)\nat java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:85)\nat java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:95)\nat java.desktop/java.awt.EventQueue$5.run(EventQueue.java:745)\nat java.desktop/java.awt.EventQueue$5.run(EventQueue.java:743)\nat java.base/java.security.AccessController.doPrivileged(AccessController.java:391)\nat java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:85)\nat java.desktop/java.awt.EventQueue.dispatchEvent(EventQueue.java:742)\nat org.netbeans.core.TimableEventQueue.dispatchEvent(TimableEventQueue.java:136)\nat java.desktop/java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:203)\nat java.desktop/java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:124)\nat java.desktop/java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:113)\nat java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:109)\nat java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)\nat java.desktop/java.awt.EventDispatchThread.run(EventDispatchThread.java:90)\nCaused by: java.util.concurrent.ExecutionException: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\nat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)\nat com.github.sarxos.webcam.WebcamDiscoveryService.getWebcams(WebcamDiscoveryService.java:116)\n... 65 more\nCaused by: com.github.sarxos.webcam.WebcamException: Cannot execute task\nat com.github.sarxos.webcam.WebcamProcessor$AtomicProcessor.process(WebcamProcessor.java:72)\nat com.github.sarxos.webcam.WebcamProcessor.process(WebcamProcessor.java:140)\nat com.github.sarxos.webcam.WebcamTask.process(WebcamTask.java:46)\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver$WebcamNewGrabberTask.newGrabber(WebcamDefaultDriver.java:45)\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver.getDevices(WebcamDefaultDriver.java:117)\nat com.github.sarxos.webcam.WebcamDiscoveryService$WebcamsDiscovery.call(WebcamDiscoveryService.java:36)\nat com.github.sarxos.webcam.WebcamDiscoveryService$WebcamsDiscovery.call(WebcamDiscoveryService.java:26)\nat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\nat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.sarxos.webcam.ds.buildin.natives.OpenIMAJGrabber\nat com.github.sarxos.webcam.ds.buildin.WebcamDefaultDriver$WebcamNewGrabberTask.handle(WebcamDefaultDriver.java:55)\nat com.github.sarxos.webcam.WebcamProcessor$AtomicProcessor.run(WebcamProcessor.java:81)\n... 3 more", "type": "commented", "related_issue": null}, {"user_name": "farshidmh", "datetime": "Dec 30, 2020", "body": "I'm getting the exact ( almost ) same error on my logger serverI can't replicate this on my machine  but some of my clients are getting this\nPOM: webcam-capture 0.3.12", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Jan 30, 2021", "body": "Hi ,\nI am not sure what is your OS, linux x86 arm?\nit looks OpenIMAJ precompiled native. i.e. JNI is not supporred by your OS. natives are not resolved.com.github.sarxos.webcam.ds.buildin.natives.OpenIMAJGrabber.Please set up log level to DEBUG and share your detailed log.", "type": "commented", "related_issue": null}, {"user_name": "farshidmh", "datetime": "Jan 30, 2021", "body": "users OS is mostly Windows 7,8 or 10 (both x86 and x64) and the application is delivered with bundled JRE (1.8).It gets worse, camera may work today but stops working tomorrow!", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Jan 30, 2021", "body": "Hi ,\nThe biggest challenge is reproduce the issue. For windows, openIMAG definitely supported, besides windows, linux, arm, armhf...are supported.\"Caused by: java.lang.NoClassDefFoundError: Could not initialize class\" no means class not in classpath, really reason is the static block of lading JNI failed.so questions are:regarding stops for long time running, a bells ring in head is that we will review if any thread safe issue that open/close session not in pair causing potential leak issue. we will check.", "type": "commented", "related_issue": null}, {"user_name": "farshidmh", "datetime": "Jan 30, 2021", "body": "Hi I'm using maven 0.3.12, it will install bridj 0.7.0As I mentioned we are using 1.8 thus javafx is builtin", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2021", "body": [], "type": "issue", "related_issue": "#805"}, {"user_name": "alexmao86", "datetime": "May 31, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/827", "issue_status": " Closed\n", "issue_list": [{"user_name": "mayurb90", "datetime": "Jun 7, 2021", "body": "Hi,\nI am looking for a way to get HardwareId for the webcam using Saroxs library. I can get the camera name but looking for VIDPID of it.\nAny help would be much appreciated .Thanks", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": "Hi ,Sorry to disappoint you. Unfortunately, this is not possible. The Webcam Capture API get the webcam name and the webcam ID (nothing fancy, 0 for the 1st device, 1 for the 2nd device, 2 for the 3rd device, etc) from the operating system. These are the only parameters the underlying driver can access from Java. On Linux there is no ID at all, only the camera name, and not always. Sometimes it's only the device file from the  directory.", "type": "commented", "related_issue": null}, {"user_name": "mayurb90", "datetime": "Jun 12, 2021", "body": "Hi ,Thanks for the information but do you have any suggestion on what could be done in the scenario where system is connected with multiple webcams but we just want to use one for the feed. Also, when you said webcam name from the OS, do we know how OS gets this information, does it have something to do with driver?Thanks", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/798", "issue_status": " Open\n", "issue_list": [{"user_name": "DanielMartensson", "datetime": "Dec 22, 2020", "body": "I'm using OpenJDK 11 on Raspberry Pi 4 B.Here is my code.", "type": "commented", "related_issue": null}, {"user_name": "DanielMartensson", "datetime": "Dec 23, 2020", "body": "Does anyone here have the same issue with Raspberry Pi 4B?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1763", "issue_status": " Closed\n", "issue_list": [{"user_name": "madisonestabrook", "datetime": "Nov 26, 2018", "body": "\nThe project is not as accessible as it could be, especially to users who have disabilities:\nThe project should be accessible to all users.\n\nComplete the following steps to reproduce the accessibly issues:\n\nNotice how accessibility can be improved.\n\nI would very much like to work on this issue!", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " How do you propose to improve the accessibility of the app? As the issue, you raised falls under the category of enhancement so I guess a bit more elaboration is called for.", "type": "commented", "related_issue": null}, {"user_name": "madisonestabrook", "datetime": "Nov 27, 2018", "body": "@AakashMallik  ARIA; however, I am willing to learn whatever tools are best for the job. I am currently studying for Google's Mobile Web Specialist Certification and recently graduated from the Grow with Google Front-End Web Development Nanodegree Scholarship program.", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " Covering so many components in a single issue is not advisable. How about we start with one component at a time.", "type": "commented", "related_issue": null}, {"user_name": "madisonestabrook", "datetime": "Nov 27, 2018", "body": "@AakashMallik Okay; good point. How about doing fix search area first?", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " Cool, go for it. Good luck.  Edit your issue description accordingly as now you would open one issue for each major componet.", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Dec 11, 2018", "body": " Any update on this issue?", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "May 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1632", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": "Currently, the lighthouse score is low for Performance, AccessibilityImprove itGo to Audit in inspect element, then run test for Performance, Accessibility.Current score:\nYes", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "pull", "related_issue": "#1634"}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "anshumanv", "datetime": "Oct 26, 2018", "body": [], "type": "pull", "related_issue": null}, {"user_name": "anshumanv", "datetime": "Oct 26, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "pulkit1joshi", "datetime": "Jan 20, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/3278", "issue_status": " Closed\n", "issue_list": [{"user_name": "anishagg17", "datetime": "Jan 14, 2020", "body": "Some features to be included:Video loop\nChange voice volume\nChange video quality\nChange playbackRateYes", "type": "commented", "related_issue": null}, {"user_name": "anishagg17", "datetime": "Jan 14, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2488", "issue_status": " Closed\n", "issue_list": [{"user_name": "mariobehling", "datetime": "Jul 7, 2019", "body": "Please add the following options in the device page to configure local devices:", "type": "commented", "related_issue": null}, {"user_name": "harsh-jindal", "datetime": "Jul 8, 2019", "body": "On it", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 7, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 7, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#2499"}, {"user_name": "mariobehling", "datetime": "Jul 14, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 16, 2019", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2510", "issue_status": " Closed\n", "issue_list": [{"user_name": "mariobehling", "datetime": "Jul 11, 2019", "body": "Change:Add slides:", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 11, 2019", "body": "On it", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 11, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 11, 2019", "body": "Looks good. Some small changes:\nPlease increase text size a bit and center vertically.Compare:\n\n", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 11, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 11, 2019", "body": " I have uploaded these images on admin panel.", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 11, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 11, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 11, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 13, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 13, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1880", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Dec 21, 2018", "body": "Microphone toggle is not workingFix Enable mic to give voice input toggleYes", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Dec 21, 2018", "body": "Could you show a screenshot of the issue, coz I think I have a PR open that has fixed this. ", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Dec 21, 2018", "body": "", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Dec 21, 2018", "body": "Oh.. cool... ", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Dec 21, 2018", "body": [], "type": "pull", "related_issue": "#1884"}, {"user_name": "akshatnitd", "datetime": "Dec 25, 2018", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2849", "issue_status": " Closed\n", "issue_list": [{"user_name": "chauhanparth210", "datetime": "Aug 14, 2019", "body": "\nFor both individual case, it should play individual voice respectively and should display the current selected voice language on dropdown.\n\nYes", "type": "commented", "related_issue": null}, {"user_name": "kpulkit29", "datetime": "Oct 9, 2019", "body": " Are you working on it?", "type": "commented", "related_issue": null}, {"user_name": "chauhanparth210", "datetime": "Oct 9, 2019", "body": "No", "type": "commented", "related_issue": null}, {"user_name": "kpulkit29", "datetime": "Oct 10, 2019", "body": "I guess for any change it plays the same sound", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Aug 27, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kpulkit29", "datetime": "Oct 15, 2019", "body": [], "type": "pull", "related_issue": "#2957"}, {"user_name": "akshatnitd", "datetime": "Oct 28, 2019", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1238", "issue_status": " Closed\n", "issue_list": [{"user_name": "batbrain7", "datetime": "May 25, 2018", "body": "The voice used on clicking the mic is not the same one as that used by google.The voice used by SUSI should be the same as that of googleClick on the mic icon on the chat and say something and wait for the response.No, anyone can take up this issue", "type": "commented", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "May 25, 2018", "body": "I would like to take this up", "type": "commented", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "May 27, 2018", "body": " We can achieve this by changing the values of speech speed and pitch from the Settings menu. Should I change the default values to match Google Assistant's voice ?", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 15, 2018", "body": " Yes, please go ahead and update us on the progress.", "type": "commented", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jun 15, 2018", "body": " Okay, I ll change the speed and pitch and send a PR.", "type": "commented", "related_issue": null}, {"user_name": "fliptrail", "datetime": "Feb 9, 2019", "body": "Since the issue is still open, I would like to work on it.", "type": "commented", "related_issue": null}, {"user_name": "fliptrail", "datetime": "Feb 10, 2019", "body": "I think that getting exact same voice as Google Assistant would be very difficult due to the fact that it used Google Text To Speech Wavenet model.\n\nHowever, it's rate and pitch could be reduced to make it sound more \"humanlike\"", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 15, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "May 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1163", "issue_status": " Closed\n", "issue_list": [{"user_name": "SudhanvaMG", "datetime": "Mar 23, 2018", "body": "The error message that says \"Sorry didn't hear anything , please speak again\" pops up even before the speak now box has been shown to the user to give an voice input .The error message should pop up after the speak now dialog box is shown and the user hasn't given any input.There are 2 ways you can reproduce this error", "type": "commented", "related_issue": null}, {"user_name": "Akshat-Jain", "datetime": "Jun 7, 2018", "body": " As this has seen no activity for a long time, I'm taking this up.", "type": "commented", "related_issue": null}, {"user_name": "Akshat-Jain", "datetime": "Jun 7, 2018", "body": [], "type": "pull", "related_issue": "#1307"}, {"user_name": "Akshat-Jain", "datetime": "Jun 8, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "Akshat-Jain", "datetime": "Jun 8, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 10, 2018", "body": [], "type": "pull", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 10, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "pulkit1joshi", "datetime": "Jan 20, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2614", "issue_status": " Closed\n", "issue_list": [{"user_name": "mariobehling", "datetime": "Jul 17, 2019", "body": "When the web client uses the sound output, there is no way to stop the output. How could we achieve this and stop the sound output in regards to:How to use the stop or pause command?", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 17, 2019", "body": " we can have a button to stop the currently playing audio, where should we place it? ", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 17, 2019", "body": "Sure, there are buttons for YouTube already. Would be good to have it for other skills too. Final goal would be to stop this with a voice command \"stop\".", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 17, 2019", "body": " Okay, I'll try implementing it", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 17, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 17, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 17, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 18, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 18, 2019", "body": [], "type": "pull", "related_issue": "#2633"}, {"user_name": "fragm3", "datetime": "Jul 18, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 18, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 19, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jul 19, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jul 19, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "pulkit1joshi", "datetime": "Jan 20, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/795", "issue_status": " Closed\n", "issue_list": [{"user_name": "mariobehling", "datetime": "Sep 16, 2017", "body": "With the switch to https we got Text to Speech and Speech to Text working on Chrome. However, Text to Speech is not working on Linux Mint 18 and other test systems. Google.com voice is working on Chromium itself though.", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Sep 16, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Sep 27, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Oct 20, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/856", "issue_status": " Closed\n", "issue_list": [{"user_name": "nifey", "datetime": "Oct 11, 2017", "body": "When we are scrolling the messages container using the mouse scroll wheel, if there is a map element in between the map starts zooming instead of scrolling the messages container.It could be better if the map zooms after we click on it. After the mouse cursor moves out of the map, the zooming of the map should stop and instead the message container should be scrolled.No", "type": "commented", "related_issue": null}, {"user_name": "nileshgulia1", "datetime": "Oct 12, 2017", "body": "hey  , When After the mouse cursor moves out of the map, the zooming is normal.but when  it hovers on the map ,the map itself starts zooming in and out.I think it is initially made good.still it needs improvement?", "type": "commented", "related_issue": null}, {"user_name": "nifey", "datetime": "Oct 12, 2017", "body": " for times when we want to scroll to the very top , if there is a map in between , it slows down the scrolling. Every time there is a map message we have to move the cursor out of the map and continue scrolling to reach the top.", "type": "commented", "related_issue": null}, {"user_name": "nileshgulia1", "datetime": "Oct 12, 2017", "body": " .So u are telling when we scroll through the messages, onhovering mouse on map it zooms automatically.fine, I think it needs some more discussion by members because i find it the initial UX proposed on chat.", "type": "commented", "related_issue": null}, {"user_name": "tstreamDOTh", "datetime": "Oct 18, 2017", "body": "I would like to work on this", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Oct 20, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "tstreamDOTh", "datetime": "Nov 5, 2017", "body": [], "type": "pull", "related_issue": "#890"}, {"user_name": "daminisatya", "datetime": "Nov 6, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Nov 6, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1317", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Jun 9, 2018", "body": "On settings page, in account page select default language and in Mobile page does not have any search option for user to search.Bug: The select timezone when selected, changes the UI of the page and on entering text through keyboard, the mouse selection doesn't work.Search option should be there for all three and UI should not be changed when selected(on dropdown)YES", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jun 17, 2018", "body": "Thinking of using  for this.  ", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 15, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fragm3", "datetime": "May 27, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2189", "issue_status": " Closed\n", "issue_list": [{"user_name": "akshatnitd", "datetime": "Jul 19, 2018", "body": "\nYes", "type": "commented", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jun 28, 2019", "body": "Not required.", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jun 4, 2019", "body": [], "type": "transferred this issue from fossasia/susi_skill_cms", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 5, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jun 28, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1973", "issue_status": " Closed\n", "issue_list": [{"user_name": "AvinashAgarwal14", "datetime": "Jan 21, 2019", "body": "Words suggestion is not implemented for the input in the text box.While typing in the text box, words should be suggested as in google keyboard.Yes", "type": "commented", "related_issue": null}, {"user_name": "AvinashAgarwal14", "datetime": "Jan 21, 2019", "body": [], "type": "pull", "related_issue": "#1974"}, {"user_name": "mariobehling", "datetime": "May 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/17", "issue_status": " Closed\n", "issue_list": [{"user_name": "daminisatya", "datetime": "Aug 13, 2016", "body": "As of now we are texting to susi and click enter (from keyboard) to send the text. Right now the send button is static right now, and must get it's functionality.", "type": "commented", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Sep 17, 2016", "body": "The bug got fixed, hence closing this", "type": "commented", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Aug 13, 2016", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Aug 13, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Orbiter", "datetime": "Aug 28, 2016", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Sep 17, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/360", "issue_status": " Closed\n", "issue_list": [{"user_name": "rishiraj824", "datetime": "Jun 30, 2017", "body": "When using the keyboard from phone the header is not fixed.One should keep the header fixed while typing.Anyone can take it up.", "type": "commented", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Aug 15, 2017", "body": "closing as low priority", "type": "commented", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 4, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 4, 2017", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 12, 2017", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Aug 15, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1317", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Jun 9, 2018", "body": "On settings page, in account page select default language and in Mobile page does not have any search option for user to search.Bug: The select timezone when selected, changes the UI of the page and on entering text through keyboard, the mouse selection doesn't work.Search option should be there for all three and UI should not be changed when selected(on dropdown)YES", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jun 17, 2018", "body": "Thinking of using  for this.  ", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 15, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fragm3", "datetime": "May 27, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/426", "issue_status": " Closed\n", "issue_list": [{"user_name": "uday96", "datetime": "Jul 8, 2017", "body": "This is a sub issue of \nThere is no setting for choosing mic input option\nHave a setting to choose between keyboard input and mic inputYes", "type": "commented", "related_issue": null}, {"user_name": "uday96", "datetime": "Jul 8, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "uday96", "datetime": "Jul 8, 2017", "body": [], "type": "pull", "related_issue": "#427"}, {"user_name": "uday96", "datetime": "Jul 8, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 9, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 26, 2017", "body": [], "type": "issue", "related_issue": "#400"}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/434", "issue_status": " Closed\n", "issue_list": [{"user_name": "uday96", "datetime": "Jul 10, 2017", "body": "This is a sub issue of There is no setting for choosing speech output option always ON i.e for speech input as well as keyboard inputHave a setting to choose between speech output always ON or OFFYes", "type": "commented", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 10, 2017", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 10, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "uday96", "datetime": "Jul 11, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "uday96", "datetime": "Jul 11, 2017", "body": [], "type": "pull", "related_issue": "#442"}, {"user_name": "mariobehling", "datetime": "Jul 12, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "rishiraj824", "datetime": "Jul 26, 2017", "body": [], "type": "issue", "related_issue": "#400"}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/2532", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Jul 12, 2019", "body": "With PR , we can enhance it furtherWe can enhance it further for other pages, by redirecting to skills page results on the keyboard enter keyYes", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 12, 2019", "body": "   need your input", "type": "commented", "related_issue": null}, {"user_name": "akshatnitd", "datetime": "Jul 12, 2019", "body": "Yes, go ahead with it.", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 12, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 12, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 13, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 13, 2019", "body": [], "type": "pull", "related_issue": "#2537"}, {"user_name": "fragm3", "datetime": "Jul 13, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "fragm3", "datetime": "Jul 13, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 13, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 13, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "pulkit1joshi", "datetime": "Jan 20, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/9", "issue_status": " Closed\n", "issue_list": [{"user_name": "daminisatya", "datetime": "Jul 21, 2016", "body": "The current template is being developed using Handlebars and JQuery. It as a pretty good User Interface with the following features.", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 21, 2016", "body": "How about all those account features? The goal defined by  is also to add accounts to Susi. How could we integrate this with loklak_depot?", "type": "commented", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Jul 21, 2016", "body": "Currently Telegram supports login through phone numbers, In place of that we can have the loklak_depot and them redirect to the asksusi page.\nAnother advantage is the code base is in Angular which syncs up with the loklak_depot code.\nThis is what i had interpreted. For the thorough implementation of accounts part, I need to go through the code once.", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jul 21, 2016", "body": " A clear idea how to connect with the Angular accounts part is important as we would meet this question later anyways. If this would have complications, it might be a reason to consider implementing AskSusi with Angular as well and work on the same stack.", "type": "commented", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Aug 4, 2016", "body": "     I initially denied on converting the whole structure to Telegram. If porting to angularJS is importing, I need someone to help me to get this done. Telegram has a very vast codebase, which eventually leading to a slower progress. It highly convoluted, and cannot be done all alone, forming a team and breaking down the code into pieces can make it easier to explore and make the changes.", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Aug 4, 2016", "body": "It seems like using Telegram as a basis for the chat client is not useful therefore. It is too much work in the short time left. We would have expected this kind of insight 2 weeks ago.", "type": "commented", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Jul 21, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "daminisatya", "datetime": "Jul 21, 2016", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "Orbiter", "datetime": "Aug 28, 2016", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "mariobehling", "datetime": "May 10, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3875", "issue_status": " Open\n", "issue_list": [{"user_name": "H-Shafiei", "datetime": "Nov 7, 2021", "body": "HiI have a job containing 2 images which I start to annotate with the polygon tool. After annotating more than 800 polygons per image, the UI gets extremely slow. How can this be fixed?Thanks", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2021", "body": " , which version of CVAT do you use? Could you please fill issue template?", "type": "commented", "related_issue": null}, {"user_name": "WiktorAdamczyk1", "datetime": "Nov 11, 2021", "body": "Hey, I'm having the same issue while annotating on cvat.org. I'm reaching 5000 polygon annotations over 3 images with resolutions around 10000x5000.UI should be able to handle a large number of annotations on a single image without making every action delayed.Loading an image with around 3000 polygon annotations causes multiple \"Page unresponsive\" prompts, before said image loads. After an image is loaded every action (hovering over UI elements, padding or zooming the image, editing or creating a new annotation) is delayed by 2-5 seconds.After detecting a drop in responsiveness (long image load time, a large number of annotations, or the time it takes to perform certain actions) present a user with a way of increasing responsiveness by allowing the user to choose a smaller part of an image that he currently wants to work on which could work as follows:I'm trying to annotate stars on photos made by astronomic telescopes that are usually large files with thousands of visible stars.I'm using the cvat.org with Brave browser.\nGPU: AMD Radeon 5700 XT\nCPU: AMD Ryzen 3600\nRAM: 16GBI assumed that hosting CVAT locally would not provide any improvement however, let me know if I'm wrong.", "type": "commented", "related_issue": null}, {"user_name": "pharrellyhy", "datetime": "Feb 16, 2022", "body": "Hi, any update on this issue? CVAT becomes super slow when selecting the label if I have few thousands of label candidates.", "type": "commented", "related_issue": null}, {"user_name": "Errin890", "datetime": "Sep 9, 2022", "body": "Hello, I am also wondering if there is any update on this issue? I regularly deal with images that require thousands of labels and after about 700-800 there is a noticeable lag between every single action (selection, creation, deletion, resizing, moving, etc.), sometimes as a large as whole seconds.As of this moment, I use the latest version in the develop branch.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Nov 12, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 12, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 12, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 19, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Errin890", "datetime": "Sep 13, 2022", "body": [], "type": "issue", "related_issue": "#4945"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3128", "issue_status": " Open\n", "issue_list": [{"user_name": "summeroff", "datetime": "Apr 25, 2021", "body": "Projects page opens fast and not make unnecessary web and db requests.It was faster in the beginning but with growing number of projects and tasks it get slower and slower.\nRight now to open Projects it takes 30+ seconds.In browsers dev tools it shows what /projects request takes 30 seconds to complete and then followed by huge amount of /data?type=preview requests.Also with enabled query logging on db. It logs like 30000+ sql select queries just when Projects page requested.\nTo get this logs I add this to compose configrefreshed page in browser and saved logs by\n", "type": "commented", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 21, 2021", "body": ", it was happening due to attempting to serialize and return all nested tasks in the projects' request. So it resulted in numerous queries to the database. The related patch should fix it by retrieving only necessary previews and no nested tasks in the projects' request. Could you give an approximate amount of tasks and labels in your projects for reference, please?", "type": "commented", "related_issue": null}, {"user_name": "summeroff", "datetime": "May 21, 2021", "body": "Checked on dk/fix-issue-3128 branch.\nProjects page is fast now but only for a first page \"projects?page=1\". Any other page is noticeable slower.Pages of individual projects still slow for projects what has big number of tasks.", "type": "commented", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 28, 2021", "body": ", I would reopen the issue and move it to a backlog, if slow responses remain. You are free to open PR if you have a solution.", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Apr 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 20, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 21, 2021", "body": [], "type": "pull", "related_issue": "#3223"}, {"user_name": "ActiveChooN", "datetime": "May 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 25, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 28, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 28, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Nov 18, 2021", "body": [], "type": "pull", "related_issue": "#3910"}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "added this to", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3843", "issue_status": " Closed\n", "issue_list": [{"user_name": "troycarlson", "datetime": "Oct 29, 2021", "body": "I have a project with ~600 tasks. Currently when loading the project details page () the initial project request includes all tasks in the serialized response. This initial request can take 30+ seconds to resolve. Then, a network request is made to fetch the thumbnail for each task. This is on reasonably fast host with plenty of CPU/memory.I would expect the project details page to only fetch a subset of tasks and provide a pagination mechanism.The project details page fetches all tasks in the initial project request. It seems all tasks are fetched from the DB, serialized, and returned in the response body.Paginating the task list the same way the  page is paginated would likely solve this problem. It may make sense to fetch the task list in a separate XHR request which can be paginated independently from the project request, using the  API endpoint, such as .We use a small number of projects so that we can share label specs across a large number of related tasks. Without the shared project labels, it is extremely difficult to keep label specs in sync across tasks. Because of this, the project details page is usually the starting point for accessing task lists.If there were an easier mechanism for sharing label specs across tasks then the project details page would be less important.", "type": "commented", "related_issue": null}, {"user_name": "troycarlson", "datetime": "Nov 27, 2021", "body": "Thank you!", "type": "commented", "related_issue": null}, {"user_name": "troycarlson", "datetime": "Oct 29, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 31, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 31, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 31, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 3, 2021", "body": [], "type": "removed this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 3, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 3, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 3, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Nov 18, 2021", "body": [], "type": "pull", "related_issue": "#3910"}, {"user_name": "nmanovic", "datetime": "Nov 19, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 19, 2021", "body": [], "type": "removed this from", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Nov 26, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Nov 26, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1633", "issue_status": " Open\n", "issue_list": [{"user_name": "gradientsky", "datetime": "Mar 17, 2021", "body": "AutoGluon separated scheduler and searcher as two different inputs into HPO. However, in the code [1] only search_strategy is available. Currently we have to do this workaround: [2][3].[1] \n[2] \n[3] ", "type": "commented", "related_issue": null}, {"user_name": "gradientsky", "datetime": "Mar 17, 2021", "body": [], "type": "pull", "related_issue": "awslabs/autogluon#1002"}, {"user_name": "zhreshold", "datetime": "Mar 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Mar 25, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1122", "issue_status": " Open\n", "issue_list": [{"user_name": "chinakook", "datetime": "Dec 24, 2019", "body": "FeatureExtractor, FPNFeatureExpander cannot support float16", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Dec 30, 2019", "body": "It can support float16, but would require some knowledge of the legacy module way to cast to float16, which is a bit tricky, once I found an elegant way to do it I will update the Extractor module.", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Jan 3, 2020", "body": "See updates here: ", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Dec 30, 2019", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/2005", "issue_status": " Open\n", "issue_list": [{"user_name": "saraswat40", "datetime": "Aug 18, 2022", "body": "I am just trying to follow the tutorial  in a local jupyter notebook:\nsomething like this often hangs:\nsession.view = dataset.take(10)sometimes it works sometimes it doesn'tAlso the de duplicate view doesn't print all the duplicates. Just a few. Are all these known issues?Ubuntu 22.04.1/Python 3.8/jupyter-lab", "type": "commented", "related_issue": null}, {"user_name": "saraswat40", "datetime": "Aug 19, 2022", "body": "It may be possible to get this working on Ubuntu 22.04 by downgrading the version of ssl. I have not tried this.Instead I created a VirtualBox Ubuntu 20.04 VM. I did a minimal install of Ubuntu with no updates so some of the steps I had to go through may not be needed in your case.Fiftyone 0.16.x didn't work for me.Instead I recommend using fiftyone 0.15.1create a virtual environment:", "type": "commented", "related_issue": null}, {"user_name": "saraswat40", "datetime": "Aug 18, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "brimoor", "datetime": "Sep 8, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1995", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 11, 2022", "body": "I think we should add some indicator to the  tab whenever a selection has been made, to indicate to the user that they'll need to clear their selection in that component if they want to fully reset their view.Possible ideas:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 11, 2022", "body": [], "type": "issue", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 17, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1988", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 8, 2022", "body": "MongoDB has issues on Ubuntu 22.04: We need to document a recommend approach(es) for installing FiftyOne on Ubuntu 22.04, which may include:Here are a variety of relevant places in the existing documentation:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 21, 2022", "body": "Users have reported success getting MongoDB (and hence FiftyOne) running on Ubuntu 22.04 as follows:For reference, the actual error observed when trying to import FiftyOne on Ubuntu 22.04 is the following:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 8, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "Anddraca", "datetime": "Aug 8, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "gorogm", "datetime": "Aug 23, 2022", "body": [], "type": "issue", "related_issue": "#1499"}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1948", "issue_status": " Open\n", "issue_list": [{"user_name": "ritch", "datetime": "Jul 15, 2022", "body": "Users have reported several issues with Collab notebooks. This issue is a placeholder for that information. If you are reading this and have additional issues when using fiftyone in a collab notebook, please add a comment here.The two reported issues are:", "type": "commented", "related_issue": null}, {"user_name": "ritch", "datetime": "Jul 15, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1561", "issue_status": " Open\n", "issue_list": [{"user_name": "r0f1", "datetime": "Jan 23, 2022", "body": "Hi, I am using fiftyone via Python in its Jupyter environment to display images that are 64x64 pixels in size. Setting the grid zoom slider to its leftmost value, renders images that are still too large for me.Is there a possibility to make the grid of images even smaller?I saw that I can pass  to the app, but this does not really help me as this only programmatically sets the slider to its leftmost position.Thank you.", "type": "commented", "related_issue": null}, {"user_name": "benjaminpkane", "datetime": "Jan 27, 2022", "body": "Hi . I hear you. I will try to increase the limit a little. There are practical limits, of course. We don't want to offer a lower bound that results in a poor experience.Generally speaking, App optimization is an ongoing effort and the limit will be increased accordingly.", "type": "commented", "related_issue": null}, {"user_name": "r0f1", "datetime": "Jan 23, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1539", "issue_status": " Open\n", "issue_list": [{"user_name": "wongsinglam", "datetime": "Jan 14, 2022", "body": "Hi,I tried v0.14.3-0.14.1, it seems Windows version does not work well, and the dataset cannot be visualized and shows like ''oops, it seems there are some problems''. When I tried v0.14.0, it can finally visualize the dataset but it runs quite slow.I also tried v0.14.3 in Ubuntu. It works perfectly and the speed is very fast.My datasets are in COCO format.", "type": "commented", "related_issue": null}, {"user_name": "wongsinglam", "datetime": "Jan 14, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/350", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 6, 2020", "body": "The recipe/tutorial cards () do not scale to match the required size of their contents; therefore, on some screen sizes, they have unsightly scroll bars.It would be desirable for the cards to expand to contain the contents with no scroll bars.", "type": "commented", "related_issue": null}, {"user_name": "lethosor", "datetime": "Aug 6, 2020", "body": "What browser? Looks fine in Chrome, but I got different behavior from your screenshot in Firefox. This appears to be an upstream issue - PyTorch tutorial pages have the same issue.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 6, 2020", "body": "That screenshot came from Jason, so idk.I can get the same behavior on Mac + Chrome by decreasing my page width a bit", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 6, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 6, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "lethosor", "datetime": "Nov 12, 2020", "body": [], "type": "removed their assignment", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/2184", "issue_status": " Open\n", "issue_list": [{"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": "Add a flag to use tied positional embeddings in transformer/generator and transformer/retrieval, and implement the tied weights. Should be False by default for backwards compatibility, and upgrade_opt should be used to migrate old models.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": "This issue has not had activity in 30 days. Marking as stale.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 25, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Dec 20, 2019", "body": [], "type": "issue", "related_issue": "#2228"}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "issue", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/472", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 28, 2020", "body": "From : I totally get why double-clicking was chosen as the way to get detail about the image, but it’s not discoverable. I can’t think of any contemporary UI that requires a double-click to invoke an action. There should be some other way to view the horse llama in all its glory", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 28, 2020", "body": "I definitely agree with this one. Early on in the design, we had the typical  vs  toggle where it says  in the screenshot below. However, that went away when we dropped support for list view (for now).Since this is a familiar location to put a toggle that controls visual layout, I think we should reintroduce a two icon toggle there: would be selected when viewing the image grid view, and clicking  would open up the modal.", "type": "commented", "related_issue": null}, {"user_name": "lethosor", "datetime": "Aug 28, 2020", "body": "This is essentially already covered by  and .I think having an icon like this open a modal is unintuitive, as well as hard to implement - how do you decide which image to display in the modal if no images or more than one image are selected?", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 28, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "lethosor", "datetime": "Aug 28, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 28, 2020", "body": [], "type": "issue", "related_issue": "#139"}, {"user_name": "brimoor", "datetime": "Sep 21, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "brimoor", "datetime": "Sep 21, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/4579", "issue_status": " Open\n", "issue_list": [{"user_name": "ajyl", "datetime": "Jun 9, 2022", "body": "Hi,I am interested in generating regression tests for a multi-task learning scenario.While I am able to train or interact with the agents (ex: ), when I try to create a regression test in a similar manner, I run into an exception while running .This becomes especially problematic when I have task-specific arguments that I want to include. For instance, in a perfect world, my test would look like the following:I believe the problem is that commas are overloaded with multiple meanings -- 1) to indicate multiple tasks and 2) to indicate multiple arguments.For what it's worth, the traceback I have when running  is as following:Thanks!", "type": "commented", "related_issue": null}, {"user_name": "ajyl", "datetime": "Jun 9, 2022", "body": "I suppose I could try removing the task-specific arguments in my regression test to see if multitask regression tests can be made, and that could narrow down the scope.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 9, 2022", "body": "This issue has not had activity in 30 days. Please feel free to reopen if you have more issues. You may apply the \"never-stale\" tag to prevent this from happening.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "klshuster", "datetime": "Jul 13, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/2221", "issue_status": " Open\n", "issue_list": [{"user_name": "jaseweston", "datetime": "Nov 24, 2019", "body": "Each of these could be spun into a sub-issue, and requires some scoping to be done. The following improvements could benefit self-chat in particular.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": "This issue has not had activity in 30 days. Marking as stale.", "type": "commented", "related_issue": null}, {"user_name": "dianaglzrico", "datetime": "Jun 9, 2020", "body": "would like to work on the first checkbox so I'm assigning myself to this one ", "type": "commented", "related_issue": null}, {"user_name": "jaseweston", "datetime": "Nov 24, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 25, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Dec 20, 2019", "body": [], "type": "issue", "related_issue": "#2228"}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "issue", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "dianaglzrico", "datetime": "Jun 9, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/2215", "issue_status": " Open\n", "issue_list": [{"user_name": "stephenroller", "datetime": "Nov 23, 2019", "body": "HuggingFace's  has a lot of great models built in. We can make a generic interop to it.I believe we want both a  and . We might need some special cases depending on which transformer, but as a first milestone I'd like to support a GPT-2 generator and Transformer-XL.As a second milestone, we should a more generic interop utility.Tests and coverage mandatory. Examples in the task docstring also.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": "This issue has not had activity in 30 days. Marking as stale.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 23, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 23, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Dec 20, 2019", "body": [], "type": "issue", "related_issue": "#2228"}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 2, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "issue", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 2, 2020", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/2222", "issue_status": " Closed\n", "issue_list": [{"user_name": "stephenroller", "datetime": "Nov 25, 2019", "body": "PyTorch 1.3 added a beta of quantization to the platform. We should have the ability to leverage this.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "May 9, 2020", "body": "This was experimented with briefly in internal branches. For a transformer/polyencoder, the improvement on CPU was marginal. For a large generative model, the performance improvement was better. After investigation, the primary bottleneck is the  operator. If this had a quantized form, the change would be worthwhile.Marking as abandoned, with hope it will be revisited later.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 25, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Dec 20, 2019", "body": [], "type": "issue", "related_issue": "#2228"}, {"user_name": "stephenroller", "datetime": "May 9, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "stephenroller", "datetime": "May 9, 2020", "body": [], "type": "issue", "related_issue": "#2103"}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/2163", "issue_status": " Closed\n", "issue_list": [{"user_name": "stephenroller", "datetime": "Nov 18, 2019", "body": "Let's add type checking to ParlAI!Once it's introduced, fix as many type check errors as you reasonably can and document the rest.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 18, 2019", "body": "Then begin typing as many places as you can. I highly suggest we codemod everything to type everything named  to type ", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 18, 2019", "body": "Also start annotating the output of act/batch_act and input of observe to be Message objects.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 6, 2020", "body": "This issue has not had activity in 30 days. Please feel free to reopen if you have more issues. You may apply the \"never-stale\" tag to prevent this from happening.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 6, 2020", "body": "I think this is probably at a level reasonable enough to close this issue.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 18, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "issue", "related_issue": "#2174"}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 19, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Nov 25, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Dec 20, 2019", "body": [], "type": "issue", "related_issue": "#2228"}, {"user_name": "domrigoglioso", "datetime": "Apr 30, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 6, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 6, 2020", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 6, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/904", "issue_status": " Closed\n", "issue_list": [{"user_name": "stephenroller", "datetime": "Jun 22, 2018", "body": "the  and  is an unfortunate aspect that's bleeds into the user's namespace. It would be nice if this could be abstracted away magically. Easiest way would be to implement a default  that calls some special user implemented method.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jul 23, 2018", "body": "Fixed in  with new default  implementation.", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jun 22, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "alexholdenmiller", "datetime": "Jul 9, 2018", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "alexholdenmiller", "datetime": "Jul 18, 2018", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "alexholdenmiller", "datetime": "Jul 18, 2018", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Jul 26, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Jul 26, 2018", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/340", "issue_status": " Open\n", "issue_list": [{"user_name": "aaronchantrill", "datetime": "Aug 6, 2021", "body": "Naomi's microphone is not active while Naomi is thinking, which can lead to Naomi missing commands or parts of commands. This is especially annoying if the Naomi is processing audio that turns out not to be a command while you are trying to get it to do something. I find that figuring out when I can start speaking to Naomi requires visual feedback.Currently, Naomi runs on a loop. Listen for speech, use the passive stt engine to check for the presence of a wake word, use the active stt engine to verify the presence of the wake word and extract command audio, use the tti engine to identify the intent, use the intent to activate the correct speech handler, then add the audio output from the handler to the speech queue, and finally return to listening. Any sounds directed to Naomi during the processing are never recorded. This would allow Naomi to \"cache\" audio that is being spoken while Naomi is processing audio so it can be processed later.We should be able to create a separate listener thread that uses the vad.get_audio() method to push blocks of audio into a queue for sequential processing in the main loop, or just a listening thread that puts all audio blocks into a queue that vad.get_audio() would read from. The passive listening engines seem to be fast enough that it shouldn't slow down response time too much if Naomi is still checking an earlier audio block for a wake word when you start speaking. We shall see.We also need some way of clearing the buffer, so that when the 'expect()' or 'confirm()' functions are used the stream can be reset as Naomi starts asking the question. Otherwise, it will quite likely accept some sound made prior to the question will be accepted as the response. Simply clearing the buffer every time Naomi starts speaking would probably work well enough for now. We also need to be able to \"pause\" the buffering for users who don't have the ability to cancel audio output from the audio input stream.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Dec 10, 2021", "body": "I have implemented buffer clearing when using expect() or confirm() by ending the listening and processing anything in the buffer just before Naomi begins asking a question. This is done by setting an arg in profile which is called \"resetmic\". This is picked up by the VAD plugin which will immediately stop listening and return any captured audio before Naomi asks the question.A better approach would probably be to block Naomi from asking the question until the VAD indicates that the user is no longer speaking.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 19, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/265", "issue_status": " Open\n", "issue_list": [{"user_name": "fracpete", "datetime": "Apr 27, 2020", "body": "So far, I've only come across readily available language models etc for the various STT/TTS plugins.\nMy question is, what steps are necessary in order to add a completely new language, e.g., an indigenous one, to Naomi? Code and/or configuration changes? What would be necessary to use DeepSpeech in such a scenario (I presume some form of training on a new audio corpus)?", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 28, 2020", "body": "Hey Peter. That's a good question. There is a sort of answer here:  with regard to adding Esperanto specifically, although nobody has yet gone through the whole process of adding a whole new language.As far as changes to the core naomi code, you would have to add your language as an option to the get_language method in naomi/commandline.py (this really isn't the right place for this function, so I imagine it will move eventually, and it should also be modified to key off of the list of .po files in naomi/data/locale instead of a hard-coded list) if you want people to be able to select it during the initial configuration (once a language is selected, Naomi should switch to communicating in that language, but the translation files for French and German currently need to be updated - see pull request ).The process of adding a new language could be broken down into three projects: STT (speech to text), TTI (text to intent), and TTS (text to speech).With Speech to Text, you first have to decide what engine you want to work with. Pocketsphinx and Deepspeech are both good choices for a more or less complete solution, and Kaldi is good for a solution once you are more comfortable with the concepts used in STT. They all have tutorials where building a new speech recognition model is discussed:Speech to text is generally broken down into three main concepts of acoustic model, phoneme to grapheme, and language model. The language model flows directly into the next project, Text to Intent, since it is using expectations to determine what it most likely heard.For Text to intent, you currently have to modify the intents() methods in each of the speechhandler plugins, and also generate new .po gettext translation files for the core and plugins.The intents() methods return a list of things the user might say to activate the plugin, which then get fed into whichever Text to Intent plugin you are using. Since there are different numbers of ways to say things in different languages, it just didn't work to use literal translations here. This gives an intent author better control of how an intent is constructed in a specific language, but does require someone who is adding a new language to do a lot more work, and modify every speechhandler plugin. There are instructions for writing intents here: I have considered defining a JSON format file for holding intents, so that someone adding a new translation would be adding new files, not modifying the intents() method of the plugin itself. One benefit of using that kind of file structure is that it could provide a fairly easy method of identifying plugins in the Naomi Plugin Exchange by the locales they are configured to work with, and possibly even allow people to attach additional translations to remotely hosted plugins without having to modify the plugin itself. Currently there is no indication on the Naomi Plugin Exchange of which languages a plugin supports.Generating the .po files is simply a matter of running the \"update_languages.sh -l <locale_identifier>\" with the locale identifier. If there is no locale identifier for the language you want to add, you can just make it up. It only has to be consistent within Naomi. This generates a bunch of files called <local_identifier>.po. Unfortunately, you have to go in and manually translate all the phrases in those .po files. This allows Naomi to translate its responses into another language to either display on the screen or say to the user.Last, you need Naomi to be able to say the response, so you need a Text to Speech system which is trained to speak your language so is able to pronounce the words being fed to it. If you have a mismatch between the locale and the voice, it can be difficult to understand (as an analog, I have been told by a Swiss friend that pronouncing Maori words correctly is much easier if you try to pronounce them in German than in English). Voice building can be a pretty complex task. Here are instructions for building a new voice for the Mary TTS system:  and Festival: So it is certainly not easy, but it can be done. If someone would be interested in doing this and documenting their progress, I think that would be incredibly helpful to others. The whole process is a lot easier if you can find a ready made STT model and TTS voice. If you are generating a new language from scratch then you will need a lot of labeled recordings. Naomi is able to help with that, especially if you can find a cloud provider that already provides STT and TTS in the target language. Then, once you have customized the intents and built translation files, you could use Naomi normally with audiolog enabled to build up a collection of labeled samples which could be used to build both the STT models and TTS voice.Please let me know if you have more questions or if you see any mistakes above. I hope that's helpful and not overly dense. I could go into a lot more detail, and would be willing to work directly with someone attempting to do this, especially if they would be willing to help document the process. Do you have a specific use case in mind?", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "Apr 30, 2020", "body": "Thanks for the detailed reply, I will have to mull over that a bit, as there is quite a bit of work involved. A possible use case would have been to add Maori as a language.\nBTW What do you think of meta-STT and meta-TTS wrappers?\nFor example, you could take the output of your base STT and push that through Google translate (from Maori to English) to avoid having to update all your plugins for handling this language. Any English text that would have to be spoken could be translated again, e.g., through Google translate again, before pushing it out through a Maori TTS. I could imagine that this kind of translation approach might work relatively well, as long as the incoming and outgoing text is relatively simple and short.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 1, 2020", "body": "I agree, it's a lot of work. I don't know that google translate could help all that much. My experience with google translate is that it works well enough to get the intent across, but rarely beyond pidgin level language.My biggest concern is with regard to 3rd party speechhandler plugins. I'd like to have an easy way for someone to add a translation \"pack\" to some else's plugin consisting of a .po translation file and an intent file, and for another user to then download the plugin with the added translation files.I have thought about expanding the \"update_translations.py\" program to use Google translate to generate the translation files, which should work fine in our current state without leaking usage data to Google. That way a user could generate all the translation files needed for their whole system to get Naomi up and running in a new language quickly and later go and fix the translations.For the time being, I see Naomi as more of a development kit than a finished product. My hope is that through being able to experiment with different technologies, we will eventually be able to come up with an effective platform that runs locally rather than in the cloud. Once something really seems to be working, Naomi could be used as a template to write a system optimized for the specific plugins.Have you ever used a piece of software called Simon ()? It used to be part of the KDE desktop, but the lead developer went to work for Apple's Siri division a few years ago and the project sort of fell apart after that. The idea was to have a simple means of controlling the desktop using voice, sort of like the scene in Bladerunner where Harrison Ford is zooming in on an image on his computer. One thing it included was the ability to train STT systems, especially the Julius STT engine. One of my visions for Naomi is to provide that same ability to train speech recognition engines which can then be re-applied to your own projects.You would still need both Speech to Text and Text to Speech systems with acoustic models optimized for the specific language, which I consider to be the most difficult part. Doing the actual translations is pretty straightforward.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": " can you recommend what form you would like this project to take? Do you want some specific documentation around it? This is kind of a big question, and given that Naomi uses plugins, it's impossible to give a specific set of instructions that cover every use case. At the same time, there are some definite steps that would always have to be done that can be documented along with some vague information about generating new STT and TTS models for those who need to. I'm just not sure how to resolve this.", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "May 4, 2020", "body": " had a discussion around this today. For the time being, we will concentrate on getting a handle on STT (DeepSpeech) and TTS (MaryTTS), with building up a speech corpus as the first step. Once model performance is satisfactory, we will look into a tighter integration into Naomi.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": "Wow, that's awesome. Definitely keep me informed.What I have learned from working with Naomi, though, is that you don't need perfect recognition to get good comprehension at the intent level, especially if you are willing to accept a pretty simple model where only one intent is triggered at a time.Adding some humorous responses can be a good strategy for generating some good will with users and keeping them engaged when the computer is having some trouble understanding them, as long as they don't happen too often.The point is that speech recognition will never be perfect, since even you are processing language at multiple levels to make sense of it. I have some hearing loss, so often what I actually hear is garbled, but I can usually work out the speaker's intent from context.A good illustration of the process of of the process our brains engage in for listening would be the \"Mares eat oats\" song which can sound like gibberish until you get to the \"wouldn't you?\" part of the song and realize that the whole thing has been in english the whole time.Since I started verifying/correcting transcriptions with the NaomiSTTTrainer.py software, I have gained a lot of understanding of how exactly the computer hears things and the process of matching the sounds up with a meaningful sentence. Often a little nudging in the language model is enough to get much better comprehension, and that is why edit distances and soundex type matching can be a huge help when dealing with spoken language.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Aug 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 20, 2021", "body": [], "type": "removed  the", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi_server/issues/632", "issue_status": " Closed\n", "issue_list": [{"user_name": "abishekvashok", "datetime": "Dec 20, 2017", "body": "Travis CI build is not parallelizedTravis CI bulild should be parallelized\nWe should do this to reduce build time and make user interaction with the logs easy and to improve accessibility of the logs. It also helps us the pin point which component was broken.\nJob 1 - build, test, deploy.\n(we can't change this as we should deploy only when we pass all tests and compilation is successful)\nJob 2 - CodecovSee a Travis Ci build log or .travis.yml fileSure, taking it up as part of GCI.", "type": "commented", "related_issue": null}, {"user_name": "RahulMetre03", "datetime": "Dec 21, 2017", "body": "I would like to work on that issue.Plz guide about I should do", "type": "commented", "related_issue": null}, {"user_name": "abishekvashok", "datetime": "Dec 21, 2017", "body": " sorry for that, I have worked on it and opened a PR.", "type": "commented", "related_issue": null}, {"user_name": "abishekvashok", "datetime": "Dec 20, 2017", "body": [], "type": "pull", "related_issue": "#635"}, {"user_name": "the-dagger", "datetime": "Dec 30, 2017", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi_server/issues/707", "issue_status": " Closed\n", "issue_list": [{"user_name": "Nikmachine", "datetime": "May 7, 2018", "body": "In our fork, the mic button is not working.\nTo use the voice control to communicate with the bot.there are none in the browserI worked on solving this and can't fix it.", "type": "commented", "related_issue": null}, {"user_name": "Orbiter", "datetime": "May 7, 2018", "body": "I believe this bug belongs to the web client \nDo you use a fork? Which repository?\nI would like to ask if you merged recent updates to your repository.", "type": "commented", "related_issue": null}, {"user_name": "Nikmachine", "datetime": "May 7, 2018", "body": "we checked out origin/master earlier today and pushed it to our server, so we aren't using a diverging fork", "type": "commented", "related_issue": null}, {"user_name": "Nikmachine", "datetime": "May 19, 2018", "body": "this is our repos: ", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "Jun 14, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/littlecodersh/ItChat/issues/547", "issue_status": " Closed\n", "issue_list": [{"user_name": "yuanfengyun", "datetime": "Nov 7, 2017", "body": "如题，如果能，是否可以增加相关接口。", "type": "commented", "related_issue": null}, {"user_name": "Fmajor", "datetime": "Nov 17, 2017", "body": "同求问\n刚看了itchatmp的似乎是有card接口的?", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Dec 15, 2017", "body": " 微信后台限制了这个功能，所以目前没有办法。 itchatmp是有的，就是卡券接口，itchat没有。", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Dec 15, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Dec 15, 2017", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/littlecodersh/ItChat/issues/408", "issue_status": " Closed\n", "issue_list": [{"user_name": "hsmj1412", "datetime": "Jun 12, 2017", "body": "您的itchat版本为：1.3.7。\n其他的内容或者问题更详细的描述都可以添加在下面：\n尝试添加发送语音功能失败，不清楚哪出了问题，有一些方向么？\n万分感谢", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Jun 27, 2017", "body": " 只是为了和mp做一个统一所以增加了一个关键字，目前网页版无法发送语音。", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Jun 27, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Jun 27, 2017", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/littlecodersh/ItChat/issues/430", "issue_status": " Closed\n", "issue_list": [{"user_name": "JanzenLiu", "datetime": "Jun 27, 2017", "body": "在提交前，请确保您已经检查了以下内容!请使用运行，并将输出粘贴在下面:您的itchat版本为：。（可通过获取）其他的内容或者问题更详细的描述都可以添加在下面：", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Sep 20, 2017", "body": " 拿的：", "type": "commented", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Sep 20, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "littlecodersh", "datetime": "Sep 20, 2017", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/907", "issue_status": " Open\n", "issue_list": [{"user_name": "brr53", "datetime": "Jul 2, 2022", "body": "I have attempted to disable the black \"shading\" that occurs when the user is dragging the box out. I haven't been successful in doing so.How can I disable the black shading? (Image attached)\nThank you.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/904", "issue_status": " Open\n", "issue_list": [{"user_name": "cptDUNN", "datetime": "Jun 29, 2022", "body": "", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/883", "issue_status": " Open\n", "issue_list": [{"user_name": "impleotv", "datetime": "May 17, 2022", "body": "Hi,The application is running and I can draw annotations... But they are not saved... I get this error:I tried the latest Python (3.10) and then downgraded to 3.9, as someone suggested. Same results...\nI'm using binary version.\nAny ideas?Thanks,Alex", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/877", "issue_status": " Open\n", "issue_list": [{"user_name": "Evannuo", "datetime": "May 4, 2022", "body": "", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/876", "issue_status": " Open\n", "issue_list": [{"user_name": "Evannuo", "datetime": "May 4, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "lhtuling", "datetime": "May 15, 2022", "body": "同样的问题!!!苦恼!!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/828", "issue_status": " Open\n", "issue_list": [{"user_name": "karantai", "datetime": "Dec 7, 2021", "body": "I have some satellite images that are converted from float32 Gtiff to PNG  Uint16.  When I open them in a gis software they appear correctly yet when I open them in labelImg they appear way too dark with almost no detail. I use windows 10. Beloew are the images of PNG images\n\n", "type": "commented", "related_issue": null}, {"user_name": "pletnes", "datetime": "Jan 24, 2022", "body": "I expect that this is caused by very small numeric values in the gtiff, i.e. not using the full 16 bit range. Did you examine the details of the conversion process? Did you consider changing the conversion process?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/646", "issue_status": " Open\n", "issue_list": [{"user_name": "yerimeeee98", "datetime": "Sep 17, 2020", "body": "", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/581", "issue_status": " Open\n", "issue_list": [{"user_name": "1hulyo", "datetime": "Apr 23, 2020", "body": "Can anyone help me? The shortcut C not working on MAC computer but all other shortcut keys are working fine. MAC OS is El Capitan. labelimg 1.3.1 version.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/194", "issue_status": " Open\n", "issue_list": [{"user_name": "mrgloom", "datetime": "Nov 7, 2017", "body": "Is it possible to move through image via dragging by mouse?", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Nov 17, 2017", "body": "Can you elaborate more? Now, it can use the mouse to drag each bounding box.", "type": "commented", "related_issue": null}, {"user_name": "mrgloom", "datetime": "Nov 17, 2017", "body": "I talking not about dragging bboxes, but about behaviour of 'dragging image view' like we have on google maps, etc.", "type": "commented", "related_issue": null}, {"user_name": "luoyetx", "datetime": "Dec 7, 2017", "body": "I think this feature is helpful to label a big image.", "type": "commented", "related_issue": null}, {"user_name": "cooli7wa", "datetime": "Jul 30, 2018", "body": "\nI add dragging in , you can try.", "type": "commented", "related_issue": null}, {"user_name": "mrgloom", "datetime": "Nov 17, 2017", "body": [], "type": "issue", "related_issue": "wkentaro/labelme#37"}, {"user_name": "vdalv", "datetime": "May 24, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rjdbcm", "datetime": "Apr 25, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/860", "issue_status": " Closed\n", "issue_list": [{"user_name": "YanivHollander", "datetime": "Mar 24, 2022", "body": "When opening an image to annotate, sometimes the image is too dark (or too bright). This makes it difficult to annotate small objects that may be lost because of poor image saturation. Can buttons be added to labelImg to control image saturation without actually overriding the image file itself?", "type": "commented", "related_issue": null}, {"user_name": "YanivHollander", "datetime": "Mar 24, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "breunigs", "datetime": "Jun 11, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "breunigs", "datetime": "Jun 11, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "breunigs", "datetime": "Jun 11, 2022", "body": [], "type": "pull", "related_issue": "#888"}, {"user_name": "tzutalin", "datetime": "Jun 12, 2022", "body": [], "type": "pull", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Jun 12, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/303", "issue_status": " Closed\n", "issue_list": [{"user_name": "leejinho0408", "datetime": "May 18, 2018", "body": "For example,quit = action('&Quit', self.close, 'Ctrl+Q', 'quit', u 'Quit application')\nFrom my understanding 'quit' part is the name of the image file.\nEven if i change the name of image file, it doesn't work..\nplz help me... thank you so much", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 18, 2018", "body": "It looks like that info is in the resources.qrc file:So you'd add a an icon by:", "type": "commented", "related_issue": null}, {"user_name": "leejinho0408", "datetime": "May 22, 2018", "body": "Thank you so much...\nIt works well!!!", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 23, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/187", "issue_status": " Closed\n", "issue_list": [{"user_name": "jensdenbraber", "datetime": "Oct 30, 2017", "body": "\"Verify Image\" button has no icon in the windows binaries v1.5.2", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 31, 2017", "body": "It seems \"Verify Image\" never has an icon before. It should have.", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 31, 2017", "body": "Hi ,\nI added a new one. What do you think?\n", "type": "commented", "related_issue": null}, {"user_name": "jensdenbraber", "datetime": "Nov 1, 2017", "body": "Looks nice", "type": "commented", "related_issue": null}, {"user_name": "jensdenbraber", "datetime": "Nov 1, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/811", "issue_status": " Open\n", "issue_list": [{"user_name": "zaitsman", "datetime": "Nov 18, 2021", "body": "I installed Python 3.10, PyQt5 and libxml using instructions in readme. mdI used pyrcc5 to create resources.qrc successfully.The UI loads, and I can get to my folder, and see my images. As soon as I try creating an annotation (pressed W on my keyboard) the script crashes with the following logSame thing works a treat on mac os.", "type": "commented", "related_issue": null}, {"user_name": "Bouffard94", "datetime": "Nov 19, 2021", "body": "Same issue here since I installed labelImg last week. Windows 10.", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Nov 24, 2021", "body": " I think you can either downgrade python version(< 3.10) or change some values' type manually.For me, I reinstalled python 3.9 and it worked well.You can change the type from float to int and it works but it omits other errors.( I think the problem is that  method takes  type, but the  function returns  type as well as  method ? )", "type": "commented", "related_issue": null}, {"user_name": "pradoz", "datetime": "Dec 19, 2021", "body": "switching back to python 3.9 worked for me. thanks ", "type": "commented", "related_issue": null}, {"user_name": "YanisSadou", "datetime": "Dec 26, 2021", "body": "Hi guys, I have the same problem! But I don't know where is the  to change into an  in the file about  talked about. (line 530, 531 etc...)And if someone would explain to me how to downgrade the python version  as  said above with cmd.\n", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 26, 2021", "body": ", You can replace\nfrom \nto \ndue to values which are float.However, It does not fix everything so there will be other problems while working with it.So I recommend you to switch the python version.First, install any python version lower than 3.10.Then, If you use windows os, you can change windows  system variables to make reordering python versions.or you can use and then install dependencies what you want in the python virtual environment.", "type": "commented", "related_issue": null}, {"user_name": "YanisSadou", "datetime": "Dec 26, 2021", "body": " I just replace some lines in the canvas.py thanks to your advice, change all 'float' to 'int':It seems to work well. You are a goat!", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 27, 2021", "body": " No worries !", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Dec 27, 2021", "body": "Hi\nSeems windows qt has a problem. I will make changes to fix it according to  changes", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 27, 2021", "body": " I think you need to change more than  that  replaced.I don't have a laptop right now. There are another lines need to be fixed in order not to get errors while using it.Because lots of  returns  instead .", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Dec 27, 2021", "body": "  ,could you help submit a PR for this bug since I cannot reproduce this on my mac.Basically, I guess it should have converted to int before calling QT methods like .  We probably don't need to convert all floats to int. We can keep them. Instead, changing the float to int before calling QT's methodsThanks in advance!", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 27, 2021", "body": ", Sure !I will submit a PR after my work done.I have one question thatyou meant, change like  to  ?", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Dec 27, 2021", "body": "Thanks. ya~ I was thinking about this which is the same as you post above", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 27, 2021", "body": " No worries ! Okay, good :)", "type": "commented", "related_issue": null}, {"user_name": "YakaTouji", "datetime": "Jun 18, 2022", "body": "can you give me the result of changing the int? i dont know how to change it\ni mean the source code after it", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Jun 19, 2022", "body": "Hello, I think you can follow this  ?", "type": "commented", "related_issue": null}, {"user_name": "SyrusBG", "datetime": "Aug 2, 2022", "body": "created a new venv using python 3.8 and then pip3 install labelImg then simply type labelImg in the terminal", "type": "commented", "related_issue": null}, {"user_name": "richeym-umich", "datetime": "Sep 12, 2022", "body": "Also have this issue- when can we expect this to be upgraded to work with Python 3.10?", "type": "commented", "related_issue": null}, {"user_name": "sammiee5311", "datetime": "Dec 27, 2021", "body": [], "type": "pull", "related_issue": "#833"}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/211", "issue_status": " Closed\n", "issue_list": [{"user_name": "IAvor", "datetime": "Dec 12, 2017", "body": "Hello guys.Just to let you know that the installation for macOS is not working as described.\n1.The qt4 is missing...I get this error in brew: Error: No available formula with the name \"qt4\"\n2.Since qt4 is missing and cannot be installe via homebrew - this is not working also \"make qt4py2\".To get it working on macOS I had to install qt5 and instead of using \"make qt4py2\" I have used \"make qt5py3\" and this solved the issue.Please change the this in the description for the installation,because I am pretty sure that there will be people banging their heads right now...\nThanks.\nKind Regards,\nIAvor Kolev", "type": "commented", "related_issue": null}, {"user_name": "campdav", "datetime": "Dec 13, 2017", "body": "Thanks !  installation is working instead of , on .\nIn the main time, modification on the script  seams to be mandatory in this case, as  does not have  (it returns directly a python string).\nkind regards", "type": "commented", "related_issue": null}, {"user_name": "titanbender", "datetime": "Dec 14, 2017", "body": "I still get the error on High Sierra:\nmake: *** No rule to make target `qt5py3'.  Stop.After cloning the repo from git and cd into it, it's possible to run the 'make qt5py3'. However, then I get the error: No module named 'lxml' when I run python labelImg.py or python3 labelImg.py...Did anyone experience the same? I do have lxml installed :-(...", "type": "commented", "related_issue": null}, {"user_name": "RodolfoFerro", "datetime": "Dec 21, 2017", "body": "Hey , , this worked for me:", "type": "commented", "related_issue": null}, {"user_name": "titanbender", "datetime": "Dec 28, 2017", "body": "Thanks for the note !\nI'm not sure why, but I still get the same error message:\n\"make: *** No rule to make target `qt5py3'.  Stop.\"\nThis happens for the command: \"make qt5py3\"...", "type": "commented", "related_issue": null}, {"user_name": "RodolfoFerro", "datetime": "Dec 29, 2017", "body": "Looks like it is a dependencies problem.\nAre you using brew to manage the dependencies ?To install qt5 with brew:\nYou can also try to reinstall libxml2. The following link might be useful for that:\n", "type": "commented", "related_issue": null}, {"user_name": "titanbender", "datetime": "Dec 29, 2017", "body": "Thanks  - it worked out perfectly after lxml was installed directly.", "type": "commented", "related_issue": null}, {"user_name": "RodolfoFerro", "datetime": "Dec 29, 2017", "body": "Nice!\nHow did you install lxml ?", "type": "commented", "related_issue": null}, {"user_name": "tleyden", "datetime": "Jan 13, 2018", "body": "This worked for me on OSX High Sierra:", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 23, 2018", "body": "This issue appears to have been resolved.", "type": "commented", "related_issue": null}, {"user_name": "zjmvwb", "datetime": "Aug 24, 2018", "body": "Thanks  , I have tried many ways, only your method works.", "type": "commented", "related_issue": null}, {"user_name": "SidMalladi", "datetime": "Jan 7, 2021", "body": " You're a life saver, thank you!", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 23, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/55", "issue_status": " Closed\n", "issue_list": [{"user_name": "ghost", "datetime": "Mar 30, 2017", "body": "Not even speaking, too. Just noises.This doesn't happen on the distribution version, but when I run it from source, the issue happens.", "type": "commented", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Mar 30, 2017", "body": "In the latest version on github, you have to manually set the threshold depending on your ambient mic/noise level. To do this, you can edit the \"athena/keyphrases.txt\" file and change the values:\nathena /1e-30/\nto something like athena /1e-20/ or athena /1e-10/ (less sensitive)\n(or up to around /1e-60/ if you need it to detect more easily)The wake-up-word/keyphrases use pocketsphinx behind the scenes. I'm hoping to find a way to auto-adjust the thresholds soon (e.g. have a background thread adjust for the ambient noise level... then if false triggers still occur, the user can respond saying \"false trigger\" and athena will adjust to a lower sensitivity threshold\"", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Apr 10, 2017", "body": "I see. I've just been editing the modules in my dist-packages directory lol. Thank you for answering!", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": "Apr 10, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "shepnathand", "datetime": "Sep 2, 2019", "body": [], "type": "issue", "related_issue": "#72"}, {"user_name": null, "datetime": [], "body": [], "type": null, "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/177", "issue_status": " Closed\n", "issue_list": [{"user_name": "vcasadei", "datetime": "Oct 21, 2017", "body": "Hi, I would like to know if it's something simple to add a new line color for each new label created.This would be a great feature for many people I believe (including me).I tried to read the code and add such feature, but I had no success.Does anyone know how could I do such thing?", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 23, 2017", "body": "Hi  ,\nI have implemented today. I am not sure if it satisfies your request.\n", "type": "commented", "related_issue": null}, {"user_name": "vcasadei", "datetime": "Oct 26, 2017", "body": "Thank you!", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 25, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/12", "issue_status": " Open\n", "issue_list": [{"user_name": "rcbyron", "datetime": "Jan 12, 2016", "body": "It's easier to say \"Athena, what's the weather?\", rather than \"Athena...  what's the weather?\"", "type": "commented", "related_issue": null}, {"user_name": "loctruong96", "datetime": "Feb 19, 2017", "body": "Nice one, siri does the same thing that you are asking, you can just say \"hey siri, what is the weather\" without wait for beep (used to be though). However, siri \"wake word\" recognition  is more effective and accurate. If you are able to make Athena's wake word detection to be more accurate and faster (better mic and more efficient code) then you can skip the double beep with ease. Otherwise, you won't know when Athena is responding and when she isn't.", "type": "commented", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 12, 2016", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/13", "issue_status": " Open\n", "issue_list": [{"user_name": "rcbyron", "datetime": "Jan 12, 2016", "body": "In case the active listening stalls out, set a maximum time to cut off the active listening.", "type": "commented", "related_issue": null}, {"user_name": "reflexity", "datetime": "Apr 3, 2016", "body": "I think this is a very good idea! Personally, I have quite some moments when the active listening keeps listening, almost forever. It is very frustrating ;) I know we have the ambient noise code in the STT script, but shouldn't we place specifically a timeout within the active listening function? Let's say for 10 seconds. Am I wrong about this? I am just throwing some suggestions.", "type": "commented", "related_issue": null}, {"user_name": "loctruong96", "datetime": "Feb 19, 2017", "body": "you can, go to the stt.py in athena folder, then change the following line on active_listening function\nfrom audio = r.listen(src) --------->To\naudio = r.record(src,duration = 4)\nit will only listen for 4 second. because athena uses speech_recognition api and they already did this.\nIf you use google speech api, they max it out at 15 seconds for the duration of each request", "type": "commented", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 12, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 23, 2016", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/10", "issue_status": " Open\n", "issue_list": [{"user_name": "rcbyron", "datetime": "Jan 11, 2016", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "GuiltyPixel", "datetime": "Mar 3, 2016", "body": "I'll second this.  A smooth audio duck while listening for commands would be awesome.", "type": "commented", "related_issue": null}, {"user_name": "loctruong96", "datetime": "Feb 19, 2017", "body": "if you are using python 3 then you can use this package \nto write a module for Athena to change volume right after the active_function() in brain.py of athena packagethere is some bugs in that code, but if you just want  quick implementation, it works!", "type": "commented", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 11, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 23, 2016", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/49", "issue_status": " Closed\n", "issue_list": [{"user_name": "suryashekhawat", "datetime": "Sep 6, 2016", "body": "hey i have install Athena on windows 10~ Logged in as: jarviss~ APIs: 'user_api', 'voice_browse_api', 'weather_api', 'spotify_api'~ Looking for modules in: 'C:\\Users\\Surya\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\athena\\modules\\active'~ Module Order: 'athena_control', 'emotion', 'bitcoin', 'conversation', 'geo_info', 'music', 'spotify', 'twitter', 'voice_browse', 'weather', 'wolfram'| |  | |                /\\  | | | |\n| |   ___ _ __   __ _\n|  __  |/ _ \\ | | |   / /\\ |  \\ || |\n||/    |__,**/~ Hey there, jarviss!~ Try asking:~ Waiting to be woken up...after this i am unable to provide my voice input(through laptop microphone) please help me to fix this", "type": "commented", "related_issue": null}, {"user_name": "suryashekhawat", "datetime": "Sep 18, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/54", "issue_status": " Closed\n", "issue_list": [{"user_name": "ghost", "datetime": "Mar 30, 2017", "body": "~ Hey there, Bro!~ Try asking:ALSA lib pcm_dmix.c:1041:(snd_pcm_dmix_open) unable to open slave\nALSA lib pcm.c:2450:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\nALSA lib pcm.c:2450:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\nALSA lib pcm.c:2450:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\nALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map\nALSA lib pcm_dmix.c:1041:(snd_pcm_dmix_open) unable to open slave~ Waiting to be woken up...~ (runtime error)~ Something went wrong. Would you like to see the error message?\nUnexpected error loading library libavbin.so.10: libavbin.so.10: wrong ELF class: ELFCLASS64~ Unknown Google TTS issue: libavbin.so.10: wrong ELF class: ELFCLASS64\nContinue? (Y/N) y~ Traceback (most recent call last):\nFile \"/usr/local/lib/python3.5/dist-packages/athena/brain.py\", line 222, in run\ntext = stt.active_listen()\nFile \"/usr/local/lib/python3.5/dist-packages/athena/stt.py\", line 62, in active_listen\nwith speech_recognition.Microphone() as src:\nFile \"/usr/local/lib/python3.5/dist-packages/speech_recognition/__init__.py\", line 79, in __init__\nself.pyaudio_module = self.get_pyaudio()\nFile \"/usr/local/lib/python3.5/dist-packages/speech_recognition/__init__.py\", line 113, in get_pyaudio\nraise AttributeError(\"PyAudio 0.2.9 or later is required (found version {})\".format(pyaudio.__version__))\nAttributeError: PyAudio 0.2.9 or later is required (found version 0.2.8)(END LOG)Fresh install of Ubuntu GNOME 16.10, thinkpad x201 tablet, intel core vpro i7.Followed instuctions from , under section \"Ubuntu/Raspberry Pi/Linux\".Help?(P.S: Github wont let me put the log into a code thing, sorry)", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 30, 2017", "body": "I fixed this by installing the latest PyAudio, by downloading it from pypi's website in tar.gz format and executing its setup.py file with , and installing the 32 bit version of avbin.", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": "Mar 30, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": null, "related_issue": null}]},
{"issue_url": "https://github.com/rcbyron/hey-athena-client/issues/51", "issue_status": " Closed\n", "issue_list": [{"user_name": "PhDinBlunts", "datetime": "Jan 10, 2017", "body": "I would like to change the wake up word to \"help\". I followed the instructions as best I could, but it does not work for me. I changed the WAKE_UP_WORD in settings.py to \"help\", I uncommented the POCKET_DICT =      join(DATA_DIR, 'athena.dict') line, and I editted the athena.dict so the only line in it is \"help HH EH L P\"After all that, it still doesnt work. Any advice? Below are pictures of the editted code.\n\n", "type": "commented", "related_issue": null}, {"user_name": "rcbyron", "datetime": "Jan 10, 2017", "body": "It looks like there is an extra space before the second \"POCKET_DICT\" line which should throw a syntax error in Python. Other than that, everything looks good. I just tried it on my machine and it works fine with \"help HH EH L P\" added. Did the word \"athena\" work? Does it hang on the \"Waiting to be woken up...\" part? If so, a better microphone might help as the key word recognition is far from perfect. I've noticed it helps to say the wake up word at different pitches/speeds/loudness.", "type": "commented", "related_issue": null}, {"user_name": "PhDinBlunts", "datetime": "Jan 10, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "PhDinBlunts", "datetime": "Jan 12, 2017", "body": "I fixed it. Turns out I'm just a dumbass. I was editing the wrong file the whole time qq.", "type": "commented", "related_issue": null}, {"user_name": "PhDinBlunts", "datetime": "Jan 12, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/smellslikeml/ActionAI/issues/33", "issue_status": " Closed\n", "issue_list": [{"user_name": "gohjiayi", "datetime": "Jul 9, 2020", "body": "I've been working on to detect my own set of exercises, specifically push ups and sit ups. However, when running inference on my CSI camera or on a video file with my Jetson Nano, the FPS flickers and ranges from 2-10. I want to be able to run inference with an FPS of around 20, how can I do this?Edit: I figured out that it can only be increased by changing my hardware, nothing to do with my software! Jetson Hacks has a video on how to improve FPS on the Nano itself on Youtube.", "type": "commented", "related_issue": null}, {"user_name": "gohjiayi", "datetime": "Aug 6, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/16076", "issue_status": " Closed\n", "issue_list": [{"user_name": "superp0tat0", "datetime": "May 25, 2021", "body": ":\n2.1.0 (use ):::Airflow will crash python when calling urllib.request.urlopen(built_url)[2021-05-25 19:00:35,929] {taskinstance.py:1087} INFO - Executing <Task(PythonOperator): web_scrape> on 2021-05-25T23:00:27.873736+00:00\n[2021-05-25 19:00:35,931] {standard_task_runner.py:52} INFO - Started process 5924 to run task\n[2021-05-25 19:00:35,936] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'print_time_python', 'web_scrape', '2021-05-25T23:00:27.873736+00:00', '--job-id', '163', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/python_dag.py', '--cfg-path', '/var/folders/m2/27pb09hn4xgbmdgqhst_0jpw0000gn/T/tmptp44eqf_', '--error-file', '/var/folders/m2/27pb09hn4xgbmdgqhst_0jpw0000gn/T/tmpnfwhk2s7']\n[2021-05-25 19:00:35,938] {standard_task_runner.py:77} INFO - Job 163: Subtask web_scrape\n[2021-05-25 19:00:35,968] {logging_mixin.py:104} INFO - Running <TaskInstance: print_time_python.web_scrape 2021-05-25T23:00:27.873736+00:00 [running]> on host siyis-MacBook-Pro.local\n[2021-05-25 19:00:35,990] {taskinstance.py:1280} INFO - Exporting the following env vars:\nAIRFLOW_CTX_DAG_EMAIL=\nAIRFLOW_CTX_DAG_OWNER=airflow\nAIRFLOW_CTX_DAG_ID=print_time_python\nAIRFLOW_CTX_TASK_ID=web_scrape\nAIRFLOW_CTX_EXECUTION_DATE=2021-05-25T23:00:27.873736+00:00\nAIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-25T23:00:27.873736+00:00\n[2021-05-25 19:00:35,992] {logging_mixin.py:104} INFO - Processing posts from UofT from 2021-05-24 to 2021-05-25\n[2021-05-25 19:00:35,992] {logging_mixin.py:104} INFO - Format url finished\n[2021-05-25 19:00:36,036] {local_task_job.py:151} INFO - Task exited with return code Negsignal.SIGABRT:Here is the python problem report.Process:               Python [5924]\nPath:                  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/Resources/Python.app/Contents/MacOS/Python\nIdentifier:            Python\nVersion:               3.9.5 (3.9.5)\nCode Type:             X86-64 (Native)\nParent Process:        Python [5923]\nResponsible:           Terminal [346]\nUser ID:               501Date/Time:             2021-05-25 19:00:35.994 -0400\nOS Version:            macOS 11.3.1 (20E241)\nReport Version:        12\nBridge OS Version:     5.3 (18P4556)\nAnonymous UUID:        9A4530E5-2125-1848-3D9C-2633B1CD9124Time Awake Since Boot: 3000 secondsSystem Integrity Protection: enabledCrashed Thread:        0  Dispatch queue: com.apple.main-threadException Type:        EXC_CRASH (SIGABRT)\nException Codes:       0x0000000000000000, 0x0000000000000000\nException Note:        EXC_CORPSE_NOTIFYTermination Reason:    Namespace OBJC, Code 0x1Application Specific Information:\nobjc[5924]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n+[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\ncrashed on child side of fork pre-execThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libsystem_kernel.dylib        \t0x00007fff2067958e __abort_with_payload + 10\n1   libsystem_kernel.dylib        \t0x00007fff2067afdd abort_with_payload_wrapper_internal + 80\n2   libsystem_kernel.dylib        \t0x00007fff2067af8d abort_with_reason + 19\n3   libobjc.A.dylib               \t0x00007fff2054a9e3 _objc_fatalv(unsigned long long, unsigned long long, char const*, __va_list_tag*) + 114\n4   libobjc.A.dylib               \t0x00007fff2054a971 _objc_fatal(char const*, ...) + 135\n5   libobjc.A.dylib               \t0x00007fff2053aa89 performForkChildInitialize(objc_class*, objc_class*) + 308\n6   libobjc.A.dylib               \t0x00007fff2053b0a3 initializeNonMetaClass + 617\n7   libobjc.A.dylib               \t0x00007fff2053c190 initializeAndMaybeRelock(objc_class*, objc_object*, mutex_tt&, bool) + 232\n8   libobjc.A.dylib               \t0x00007fff2052bdc0 lookUpImpOrForward + 1126\n9   libobjc.A.dylib               \t0x00007fff2052b39b _objc_msgSend_uncached + 75\n10  com.apple.CoreFoundation      \t0x00007fff20725589 -[__NSDictionaryM __setObject:forKey:] + 499\n11  com.apple.CoreFoundation      \t0x00007fff207227be parseXMLElement + 2863\n12  com.apple.CoreFoundation      \t0x00007fff207223ae parseXMLElement + 1823\n13  com.apple.CoreFoundation      \t0x00007fff207211aa _CFPropertyListCreateFromUTF8Data + 1332\n14  com.apple.CoreFoundation      \t0x00007fff2082252f _CFPropertyListCreateWithData + 590\n15  com.apple.CoreFoundation      \t0x00007fff20720708 CFPropertyListCreateWithData + 51\n16  com.apple.CoreFoundation      \t0x00007fff2071f618 _CFBundleCopyInfoDictionaryInDirectoryWithVersion + 906\n17  com.apple.CoreFoundation      \t0x00007fff2071ef0f _CFBundleRefreshInfoDictionaryAlreadyLocked + 111\n18  com.apple.CoreFoundation      \t0x00007fff2071ee81 CFBundleGetInfoDictionary + 44\n19  com.apple.CoreFoundation      \t0x00007fff20821981 _CFBundleCreate + 902\n20  com.apple.CoreFoundation      \t0x00007fff2070f319 CFBundleGetMainBundle + 139\n21  com.apple.CoreFoundation      \t0x00007fff20731431 _CFPrefsGetCacheStringForBundleID + 71\n22  com.apple.CoreFoundation      \t0x00007fff20737e80 -[CFPrefsPlistSource setDomainIdentifier:] + 92\n23  com.apple.CoreFoundation      \t0x00007fff20737dbd -[CFPrefsPlistSource initWithDomain:user:byHost:containerPath:containingPreferences:] + 99\n24  com.apple.CoreFoundation      \t0x00007fff20737ca5 __85-[_CFXPreferences(PlistSourceAdditions) withManagedSourceForIdentifier:user:perform:]_block_invoke + 156\n25  com.apple.CoreFoundation      \t0x00007fff2086a325 -[_CFXPreferences withSources:] + 60\n26  com.apple.CoreFoundation      \t0x00007fff208b112e -[_CFXPreferences withManagedSourceForIdentifier:user:perform:] + 240\n27  com.apple.CoreFoundation      \t0x00007fff20732b5d -[CFPrefsSearchListSource addManagedSourceForIdentifier:user:] + 98\n28  com.apple.CoreFoundation      \t0x00007fff2088b47e __108-[_CFXPreferences(SearchListAdditions) withSearchListForIdentifier:container:cloudConfigurationURL:perform:]_block_invoke.166 + 283\n29  com.apple.CoreFoundation      \t0x00007fff2088b152 -[_CFXPreferences withSearchLists:] + 60\n30  com.apple.CoreFoundation      \t0x00007fff207328f8 __108-[_CFXPreferences(SearchListAdditions) withSearchListForIdentifier:container:cloudConfigurationURL:perform:]_block_invoke + 279\n31  com.apple.CoreFoundation      \t0x00007fff2088b2e1 -[_CFXPreferences withSearchListForIdentifier:container:cloudConfigurationURL:perform:] + 372\n32  com.apple.CoreFoundation      \t0x00007fff207323c5 -[_CFXPreferences copyAppValueForKey:identifier:container:configurationURL:] + 137\n33  com.apple.CoreFoundation      \t0x00007fff207322fa _CFPreferencesCopyAppValueWithContainerAndConfiguration + 101\n34  com.apple.SystemConfiguration \t0x00007fff211c799b SCDynamicStoreCopyProxiesWithOptions + 155\n35  _scproxy.cpython-39-darwin.so \t0x00000001061bba8f get_proxies + 22\n36  org.python.python             \t0x00000001044ef84a cfunction_vectorcall_NOARGS + 104\n37  org.python.python             \t0x000000010459cc53 call_function + 164\n38  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n39  org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n40  org.python.python             \t0x000000010459cc53 call_function + 164\n41  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n42  org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n43  org.python.python             \t0x000000010459cc53 call_function + 164\n44  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n45  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n46  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n47  org.python.python             \t0x00000001044a4907 _PyObject_FastCallDictTstate + 87\n48  org.python.python             \t0x0000000104513aea slot_tp_init + 192\n49  org.python.python             \t0x000000010451d2cc type_call + 272\n50  org.python.python             \t0x00000001044a46da _PyObject_MakeTpCall + 129\n51  org.python.python             \t0x000000010459ccbd call_function + 270\n52  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n53  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n54  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n55  org.python.python             \t0x000000010459cc53 call_function + 164\n56  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n57  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n58  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n59  org.python.python             \t0x000000010459cc53 call_function + 164\n60  org.python.python             \t0x000000010459a526 _PyEval_EvalFrameDefault + 45324\n61  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n62  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n63  org.python.python             \t0x00000001044a4ee1 PyVectorcall_Call + 164\n64  org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n65  org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n66  org.python.python             \t0x000000010459cc53 call_function + 164\n67  org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n68  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n69  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n70  org.python.python             \t0x00000001044a7e7a method_vectorcall + 160\n71  org.python.python             \t0x000000010459cc53 call_function + 164\n72  org.python.python             \t0x000000010459a612 _PyEval_EvalFrameDefault + 45560\n73  org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n74  org.python.python             \t0x000000010459cc53 call_function + 164\n75  org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n76  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n77  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n78  org.python.python             \t0x000000010459cc53 call_function + 164\n79  org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n80  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n81  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n82  org.python.python             \t0x00000001044a4ee1 PyVectorcall_Call + 164\n83  org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n84  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n85  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n86  org.python.python             \t0x00000001044a7e7a method_vectorcall + 160\n87  org.python.python             \t0x000000010459cc53 call_function + 164\n88  org.python.python             \t0x000000010459a612 _PyEval_EvalFrameDefault + 45560\n89  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n90  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n91  org.python.python             \t0x000000010459cc53 call_function + 164\n92  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n93  org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n94  org.python.python             \t0x000000010459cc53 call_function + 164\n95  org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n96  org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n97  org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n98  org.python.python             \t0x00000001044a4ee1 PyVectorcall_Call + 164\n99  org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n100 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n101 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n102 org.python.python             \t0x00000001044a4ee1 PyVectorcall_Call + 164\n103 org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n104 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n105 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n106 org.python.python             \t0x000000010459cc53 call_function + 164\n107 org.python.python             \t0x000000010459a612 _PyEval_EvalFrameDefault + 45560\n108 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n109 org.python.python             \t0x000000010459cc53 call_function + 164\n110 org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n111 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n112 org.python.python             \t0x000000010459cc53 call_function + 164\n113 org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n114 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n115 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n116 org.python.python             \t0x000000010459cc53 call_function + 164\n117 org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n118 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n119 org.python.python             \t0x000000010459cc53 call_function + 164\n120 org.python.python             \t0x000000010459a489 _PyEval_EvalFrameDefault + 45167\n121 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n122 org.python.python             \t0x000000010459cc53 call_function + 164\n123 org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n124 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n125 org.python.python             \t0x000000010459cc53 call_function + 164\n126 org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n127 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n128 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n129 org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n130 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n131 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n132 org.python.python             \t0x000000010459a75e _PyEval_EvalFrameDefault + 45892\n133 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n134 org.python.python             \t0x00000001044a52f8 _PyFunction_Vectorcall + 376\n135 org.python.python             \t0x000000010459cc53 call_function + 164\n136 org.python.python             \t0x000000010459a526 _PyEval_EvalFrameDefault + 45324\n137 org.python.python             \t0x00000001044a5240 _PyFunction_Vectorcall + 192\n138 org.python.python             \t0x000000010459cc53 call_function + 164\n139 org.python.python             \t0x000000010459a562 _PyEval_EvalFrameDefault + 45384\n140 org.python.python             \t0x000000010458e2d6 _PyEval_EvalCode + 403\n141 org.python.python             \t0x00000001045f1567 run_eval_code_obj + 126\n142 org.python.python             \t0x00000001045f14b7 run_mod + 96\n143 org.python.python             \t0x00000001045eedac pyrun_file + 173\n144 org.python.python             \t0x00000001045ee7ad pyrun_simple_file + 272\n145 org.python.python             \t0x00000001045ee677 PyRun_SimpleFileExFlags + 67\n146 org.python.python             \t0x000000010460d365 pymain_run_file + 327\n147 org.python.python             \t0x000000010460cab5 pymain_run_python + 460\n148 org.python.python             \t0x000000010460c8ad Py_RunMain + 23\n149 org.python.python             \t0x000000010460dbca pymain_main + 35\n150 org.python.python             \t0x000000010460dea0 Py_BytesMain + 42\n151 libdyld.dylib                 \t0x00007fff206a7f3d start + 1Thread 0 crashed with X86 Thread State (64-bit):\nrax: 0x0000000002000209  rbx: 0x0000000000000080  rcx: 0x00007ffeeb7cda18  rdx: 0x0000000000000000\nrdi: 0x0000000000000008  rsi: 0x0000000000000001  rbp: 0x00007ffeeb7cda60  rsp: 0x00007ffeeb7cda18\nr8: 0x00007fd9d179e490   r9: 0x0000000000000080  r10: 0x0000000000000000  r11: 0x0000000000000246\nr12: 0x0000000000000000  r13: 0x0000000000000000  r14: 0x0000000000000001  r15: 0x0000000000000008\nrip: 0x00007fff2067958e  rfl: 0x0000000000000246  cr2: 0x000000010443c000Logical CPU:     0\nError Code:      0x02000209\nTrap Number:     133Thread 0 instruction stream not available.Thread 0 last branch register state not available.Binary Images:\n0x10442a000 -        0x10442dfff +org.python.python (3.9.5 - 3.9.5) <972EAB54-AEE8-3DA2-82FA-7397C255C348> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/Resources/Python.app/Contents/MacOS/Python\n0x10443d000 -        0x10470cfff +org.python.python (3.9.5, [c] 2001-2019 Python Software Foundation. - 3.9.5)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/Python\n0x1049a2000 -        0x1049a5fff +_heapq.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_heapq.cpython-39-darwin.so\n0x104ab2000 -        0x104ab9fff +_json.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_json.cpython-39-darwin.so\n0x104b86000 -        0x104b91fff +math.cpython-39-darwin.so (0) <05B5DC42-1269-378D-A8AB-D5FB23FA35B6> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/math.cpython-39-darwin.so\n0x104b9e000 -        0x104badfff +_datetime.cpython-39-darwin.so (0) <2FAFDB7F-D1F2-3252-B1BF-992293CA5537> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_datetime.cpython-39-darwin.so\n0x104bfe000 -        0x104c05fff +_struct.cpython-39-darwin.so (0) <333E481A-F950-30AF-9D5B-43A96207D17E> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_struct.cpython-39-darwin.so\n0x104c52000 -        0x104c6dfff +_decimal.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_decimal.cpython-39-darwin.so\n0x104c7e000 -        0x104c99fff +libmpdec.3.dylib (0) <1BEE3ECE-A6D5-3B00-A1E7-5E14EF5E3858> /usr/local/opt/mpdecimal/lib/libmpdec.3.dylib\n0x104caa000 -        0x104cadfff +_bisect.cpython-39-darwin.so (0) <937073EE-CC90-3420-959B-10B275B7AFE4> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_bisect.cpython-39-darwin.so\n0x104cfa000 -        0x104cfdfff +_iso8601.cpython-39-darwin.so (0) <55D8EC7D-E793-3BD3-99FB-53B8CBE2231F> /usr/local/lib/python3.9/site-packages/pendulum/parsing/_iso8601.cpython-39-darwin.so\n0x104d0a000 -        0x104d0dfff +_helpers.cpython-39-darwin.so (0) <6919B406-F22F-3079-AEFA-4365E6503594> /usr/local/lib/python3.9/site-packages/pendulum/_extensions/_helpers.cpython-39-darwin.so\n0x104e1a000 -        0x104e1dfff +_opcode.cpython-39-darwin.so (0) <77715530-8031-3519-9585-9552F63E8B09> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_opcode.cpython-39-darwin.so\n0x104e6a000 -        0x104e6dfff +grp.cpython-39-darwin.so (0) <9C770334-D194-33E7-AF04-7B27B63BB3F1> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/grp.cpython-39-darwin.so\n0x104e7a000 -        0x104e7dfff +_posixsubprocess.cpython-39-darwin.so (0) <51C96F40-CAB9-38B4-B248-4583A603F9C1> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_posixsubprocess.cpython-39-darwin.so\n0x104e8a000 -        0x104e91fff +select.cpython-39-darwin.so (0) <44B431D6-807B-3D7F-9692-EC234B110E57> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/select.cpython-39-darwin.so\n0x104ede000 -        0x104ee5fff +binascii.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/binascii.cpython-39-darwin.so\n0x104f32000 -        0x104f49fff +_pickle.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_pickle.cpython-39-darwin.so\n0x104f9a000 -        0x104fa1fff +_hashlib.cpython-39-darwin.so (0) <13154AD0-A32C-3FDC-B41A-54C5EFC49CD9> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_hashlib.cpython-39-darwin.so\n0x104fae000 -        0x104ffdfff +libssl.1.1.dylib (0) <3D9A4A37-800F-31E5-B385-558175C1732E> /usr/local/opt/openssl@1.1/lib/libssl.1.1.dylib\n0x10502a000 -        0x1051e9fff +libcrypto.1.1.dylib (0) <8379949D-F788-34D2-9C44-CF7386DF4E12> /usr/local/opt/openssl@1.1/lib/libcrypto.1.1.dylib\n0x105282000 -        0x105289fff +_blake2.cpython-39-darwin.so (0) <9CA11013-B59A-3A88-BC1E-559169C64569> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_blake2.cpython-39-darwin.so\n0x105456000 -        0x105459fff +cprocessors.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/sqlalchemy/cprocessors.cpython-39-darwin.so\n0x105562000 -        0x105565fff +cutils.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/sqlalchemy/cutils.cpython-39-darwin.so\n0x1055ae000 -        0x1055b1fff +cresultproxy.cpython-39-darwin.so (0) <173D248B-2587-377D-8841-75BD69AEE03F> /usr/local/lib/python3.9/site-packages/sqlalchemy/cresultproxy.cpython-39-darwin.so\n0x1055fa000 -        0x1055fdfff +_random.cpython-39-darwin.so (0) <147C2A11-1A2F-3759-982B-32DCAFB70C7B> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_random.cpython-39-darwin.so\n0x10560a000 -        0x105611fff +_sha512.cpython-39-darwin.so (0) <7B11B785-DAEF-31F7-9E7A-AA13213B4D9E> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_sha512.cpython-39-darwin.so\n0x10589e000 -        0x1058adfff +_socket.cpython-39-darwin.so (0) <5C329849-3AA3-37B7-8771-5F47108A5617> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_socket.cpython-39-darwin.so\n0x1058fa000 -        0x105901fff +array.cpython-39-darwin.so (0) <3CC328A3-BA8C-39A8-A320-831DCD49A4EA> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/array.cpython-39-darwin.so\n0x10594e000 -        0x105951fff +termios.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/termios.cpython-39-darwin.so\n0x1059de000 -        0x1059e9fff +_sqlite3.cpython-39-darwin.so (0) <0B063206-2D05-3943-87B9-14E64B5713F6> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_sqlite3.cpython-39-darwin.so\n0x1059fa000 -        0x105addfff +libsqlite3.0.dylib (0) <6B505F74-FFE3-30CD-8C39-4E97C40B0E17> /usr/local/opt/sqlite/lib/libsqlite3.0.dylib\n0x105b02000 -        0x105b09fff +_csv.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_csv.cpython-39-darwin.so\n0x105b56000 -        0x105b5dfff +zlib.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/zlib.cpython-39-darwin.so\n0x105b6a000 -        0x105b6dfff +_bz2.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_bz2.cpython-39-darwin.so\n0x105b7a000 -        0x105b81fff +_lzma.cpython-39-darwin.so (0) <82715680-A4DB-3518-A68B-61634A343984> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_lzma.cpython-39-darwin.so\n0x105b8e000 -        0x105ba9fff +liblzma.5.dylib (0)  /usr/local/opt/xz/lib/liblzma.5.dylib\n0x105cb0000 -        0x105cb3fff +_queue.cpython-39-darwin.so (0) <4F79620B-47FA-31AA-B336-E65C27B0E190> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_queue.cpython-39-darwin.so\n0x105d00000 -        0x105d13fff +_ctypes.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_ctypes.cpython-39-darwin.so\n0x105d24000 -        0x105d27fff +_speedups.cpython-39-darwin.so (0) <09215114-E1CD-3C04-AF11-ECAA6C8DD048> /usr/local/lib/python3.9/site-packages/markupsafe/_speedups.cpython-39-darwin.so\n0x105df0000 -        0x105efbfff +unicodedata.cpython-39-darwin.so (0) <72FAC84E-D8E7-3290-9072-683DF5F6C2A6> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/unicodedata.cpython-39-darwin.so\n0x106048000 -        0x10605ffff +_ssl.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_ssl.cpython-39-darwin.so\n0x1061b8000 -        0x1061bbfff +_scproxy.cpython-39-darwin.so (0) <49D680BF-EBB2-31F9-B7A3-1C7031ED222A> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_scproxy.cpython-39-darwin.so\n0x106248000 -        0x10624bfff +_uuid.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_uuid.cpython-39-darwin.so\n0x106458000 -        0x10645bfff +_contextvars.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_contextvars.cpython-39-darwin.so\n0x106568000 -        0x10656ffff +_asyncio.cpython-39-darwin.so (0) <5A6210F0-F3DC-31E0-929D-19223B30F085> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_asyncio.cpython-39-darwin.so\n0x106b40000 -        0x106b44fff +cext.cpython-39-darwin.so (0) <05C7C68F-2E7D-384C-B400-8EE7A578B24D> /usr/local/lib/python3.9/site-packages/lazy_object_proxy/cext.cpython-39-darwin.so\n0x106bca000 -        0x106bcdfff +_padding.abi3.so (0)  /usr/local/lib/python3.9/site-packages/cryptography/hazmat/bindings/_padding.abi3.so\n0x106bd6000 -        0x106bf4fff +_cffi_backend.cpython-39-darwin.so (0) <83ED7E06-3D61-3FF8-B6C9-48D715A458B7> /usr/local/lib/python3.9/site-packages/_cffi_backend.cpython-39-darwin.so\n0x106d89000 -        0x106d90fff +pvectorc.cpython-39-darwin.so (0) <35A4C31E-18A4-3BFA-B32B-1E57801A6AA5> /usr/local/lib/python3.9/site-packages/pvectorc.cpython-39-darwin.so\n0x1071dd000 -        0x1071ddfff +speedups.cpython-39-darwin.so (0) <2E9F7405-57B2-3261-B6DA-20A7DB8AB337> /usr/local/lib/python3.9/site-packages/tornado/speedups.cpython-39-darwin.so\n0x107220000 -        0x107233fff +_curses.cpython-39-darwin.so (0)  /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_curses.cpython-39-darwin.so\n0x107344000 -        0x107347fff +_multiprocessing.cpython-39-darwin.so (0) <229D0412-52BB-399F-9C78-1C5895934C94> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_multiprocessing.cpython-39-darwin.so\n0x107354000 -        0x107357fff +setproctitle.cpython-39-darwin.so (0) <33F5D080-97FD-3431-84FE-75C01494B373> /usr/local/lib/python3.9/site-packages/setproctitle.cpython-39-darwin.so\n0x107424000 -        0x10742bfff +_psutil_osx.cpython-39-darwin.so (0) <1D8BA00C-88D7-3969-B646-BAA9D4008EC2> /usr/local/lib/python3.9/site-packages/psutil/_psutil_osx.cpython-39-darwin.so\n0x107434000 -        0x107437fff +_psutil_posix.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/psutil/_psutil_posix.cpython-39-darwin.so\n0x107580000 -        0x10797ffff +_mysql_connector.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/_mysql_connector.cpython-39-darwin.so\n0x107d08000 -        0x107d63fff +libssl.1.1.dylib (0) <5A105495-0718-36A0-A8A1-CCA48949717C> /usr/local/lib/python3.9/site-packages/mysql/vendor/libssl.1.1.dylib\n0x107d90000 -        0x107f6ffff +libcrypto.1.1.dylib (0) <50ECFAD7-0DDC-341B-8C1E-2DBDCD600184> /usr/local/lib/python3.9/site-packages/mysql/vendor/libcrypto.1.1.dylib\n0x108248000 -        0x1085bdfff +_multiarray_umath.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\n0x1086d2000 -        0x10c129fff +libopenblas.0.dylib (0) <07E4ABF0-E967-375A-90D9-1C54B8AF2F58> /usr/local/lib/python3.9/site-packages/numpy/.dylibs/libopenblas.0.dylib\n0x10c355000 -        0x10c46cfff +libgfortran.3.dylib (0) <9ABE5EDE-AD43-391A-9E54-866711FAC32A> /usr/local/lib/python3.9/site-packages/numpy/.dylibs/libgfortran.3.dylib\n0x10c4d0000 -        0x10c506fff +libquadmath.0.dylib (0) <7FFA409F-FB04-3B64-BE9A-3E3A494C975E> /usr/local/lib/python3.9/site-packages/numpy/.dylibs/libquadmath.0.dylib\n0x10c515000 -        0x10c52afff +libgcc_s.1.dylib (0) <7C6D7CB7-82DB-3290-8181-07646FEA1F80> /usr/local/lib/python3.9/site-packages/numpy/.dylibs/libgcc_s.1.dylib\n0x112635000 -        0x112642fff +_multiarray_tests.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/core/_multiarray_tests.cpython-39-darwin.so\n0x1126d3000 -        0x1126d4fff +lapack_lite.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/linalg/lapack_lite.cpython-39-darwin.so\n0x1126d8000 -        0x1126f1fff +_umath_linalg.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/linalg/_umath_linalg.cpython-39-darwin.so\n0x112780000 -        0x112791fff +_pocketfft_internal.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/fft/_pocketfft_internal.cpython-39-darwin.so\n0x112815000 -        0x112886fff +mtrand.cpython-39-darwin.so (0) <26C3326C-CD5C-3EB6-ADC1-1501DEFF4824> /usr/local/lib/python3.9/site-packages/numpy/random/mtrand.cpython-39-darwin.so\n0x1128da000 -        0x1128f8fff +bit_generator.cpython-39-darwin.so (0) <0AA58188-F7BE-3259-B73D-F73A44D18892> /usr/local/lib/python3.9/site-packages/numpy/random/bit_generator.cpython-39-darwin.so\n0x112913000 -        0x112948fff +_common.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/random/_common.cpython-39-darwin.so\n0x11295f000 -        0x1129b5fff +_bounded_integers.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/random/_bounded_integers.cpython-39-darwin.so\n0x1129d6000 -        0x1129e4fff +_mt19937.cpython-39-darwin.so (0) <317BAC67-09BA-3FCD-951A-1F8665EB2AB2> /usr/local/lib/python3.9/site-packages/numpy/random/_mt19937.cpython-39-darwin.so\n0x1129f0000 -        0x1129fcfff +_philox.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/random/_philox.cpython-39-darwin.so\n0x112a07000 -        0x112a11fff +_pcg64.cpython-39-darwin.so (0) <97A10DFD-6916-3251-84B1-FCCF8D908E08> /usr/local/lib/python3.9/site-packages/numpy/random/_pcg64.cpython-39-darwin.so\n0x112a1b000 -        0x112a22fff +_sfc64.cpython-39-darwin.so (0) <4A390EEB-7105-3235-85B2-4C8D98B3587C> /usr/local/lib/python3.9/site-packages/numpy/random/_sfc64.cpython-39-darwin.so\n0x112a2a000 -        0x112ab0fff +_generator.cpython-39-darwin.so (0)  /usr/local/lib/python3.9/site-packages/numpy/random/_generator.cpython-39-darwin.so\n0x112bc9000 -        0x112bccfff +_lsprof.cpython-39-darwin.so (0) <8DC3E8E7-DDFC-3F9F-A142-2DE25FA5CDB3> /usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/_lsprof.cpython-39-darwin.so\n0x11400f000 -        0x1140aafff  dyld (851.27) <7EAA668B-F906-3BAA-A980-139BBE6E8766> /usr/lib/dyld\n0x7fff203c4000 -     0x7fff203c5fff  libsystem_blocks.dylib (79) <1C8538C3-F2BE-3F02-89AE-E7052DE66D51> /usr/lib/system/libsystem_blocks.dylib\n0x7fff203c6000 -     0x7fff203fbfff  libxpc.dylib (2038.100.48)  /usr/lib/system/libxpc.dylib\n0x7fff203fc000 -     0x7fff20413fff  libsystem_trace.dylib (1277.100.21) <1AAE1F8F-9F9D-3327-8A95-3A6887CED713> /usr/lib/system/libsystem_trace.dylib\n0x7fff20414000 -     0x7fff204b1fff  libcorecrypto.dylib (1000.100.38) <96A88875-7771-394E-A88E-389DCD02A935> /usr/lib/system/libcorecrypto.dylib\n0x7fff204b2000 -     0x7fff204defff  libsystem_malloc.dylib (317.100.9) <029B5632-62B4-39F6-981C-BCA99C1FBF1D> /usr/lib/system/libsystem_malloc.dylib\n0x7fff204df000 -     0x7fff20523fff  libdispatch.dylib (1271.100.5) <6B7B23E3-2FD4-3EA2-8A89-CE06244CCA98> /usr/lib/system/libdispatch.dylib\n0x7fff20524000 -     0x7fff2055dfff  libobjc.A.dylib (824) <929E3040-4605-3C14-885B-D742EF02F2CB> /usr/lib/libobjc.A.dylib\n0x7fff2055e000 -     0x7fff20560fff  libsystem_featureflags.dylib (28.60.1)  /usr/lib/system/libsystem_featureflags.dylib\n0x7fff20561000 -     0x7fff205e9fff  libsystem_c.dylib (1439.100.3)  /usr/lib/system/libsystem_c.dylib\n0x7fff205ea000 -     0x7fff2063ffff  libc++.1.dylib (905.6)  /usr/lib/libc++.1.dylib\n0x7fff20640000 -     0x7fff20655fff  libc++abi.dylib (905.6) <22AFC7FC-2DB6-3EF0-9CC0-EFFB9B65D5E2> /usr/lib/libc++abi.dylib\n0x7fff20656000 -     0x7fff20685fff  libsystem_kernel.dylib (7195.101.2) <62A19DE4-50C5-3866-B5B2-43220E379C3B> /usr/lib/system/libsystem_kernel.dylib\n0x7fff20686000 -     0x7fff20691fff  libsystem_pthread.dylib (454.100.8) <52F807B1-41A0-3D1E-AE89-115CA570863F> /usr/lib/system/libsystem_pthread.dylib\n0x7fff20692000 -     0x7fff206cdfff  libdyld.dylib (851.27) <9F95C644-D1BD-38D9-9612-6188FE9EA53C> /usr/lib/system/libdyld.dylib\n0x7fff206ce000 -     0x7fff206d7fff  libsystem_platform.dylib (254.80.2) <03429519-EBEA-3549-84A6-0FD426CB7373> /usr/lib/system/libsystem_platform.dylib\n0x7fff206d8000 -     0x7fff20703fff  libsystem_info.dylib (542.40.3)  /usr/lib/system/libsystem_info.dylib\n0x7fff20704000 -     0x7fff20ba1fff  com.apple.CoreFoundation (6.9 - 1775.118.101) <895AFD1C-0307-32B3-81CB-BA33DA368DE1> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n0x7fff20ba2000 -     0x7fff20dd4fff  com.apple.LaunchServices (1122.33 - 1122.33)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices\n0x7fff20dd5000 -     0x7fff20ea8fff  com.apple.gpusw.MetalTools (1.0 - 1)  /System/Library/PrivateFrameworks/MetalTools.framework/Versions/A/MetalTools\n0x7fff20ea9000 -     0x7fff21105fff  libBLAS.dylib (1336.101.1) <28ABAD61-A323-33C6-8674-8A14118D4C20> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\n0x7fff21106000 -     0x7fff21152fff  com.apple.Lexicon-framework (1.0 - 86.1) <27959773-C4F4-33BC-9A68-39EF2037F1E5> /System/Library/PrivateFrameworks/Lexicon.framework/Versions/A/Lexicon\n0x7fff21153000 -     0x7fff211c1fff  libSparse.dylib (106)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparse.dylib\n0x7fff211c2000 -     0x7fff2123ffff  com.apple.SystemConfiguration (1.20 - 1.20)  /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration\n0x7fff21240000 -     0x7fff21274fff  libCRFSuite.dylib (50)  /usr/lib/libCRFSuite.dylib\n0x7fff21275000 -     0x7fff214adfff  libmecabra.dylib (929.9) <87ACCBB5-FD09-3044-B6FF-1A94A7129DDF> /usr/lib/libmecabra.dylib\n0x7fff214ae000 -     0x7fff2180bfff  com.apple.Foundation (6.9 - 1775.118.101) <5B112EDB-35C1-31A7-BFDA-E185D1B49D93> /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation\n0x7fff2180c000 -     0x7fff218f4fff  com.apple.LanguageModeling (1.0 - 247.3) <81DEF845-C1A0-3BD0-9820-D1C308AFBE09> /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling\n0x7fff22513000 -     0x7fff22867fff  com.apple.security (7.0 - 59754.100.106)  /System/Library/Frameworks/Security.framework/Versions/A/Security\n0x7fff22868000 -     0x7fff22ac7fff  libicucore.A.dylib (66112) <478D57C3-FFF8-35E1-A64F-8490A616AB37> /usr/lib/libicucore.A.dylib\n0x7fff22ac8000 -     0x7fff22ad1fff  libsystem_darwin.dylib (1439.100.3) <28AB0CBC-61F6-3A01-BCE2-A53DA1AECB0F> /usr/lib/system/libsystem_darwin.dylib\n0x7fff22ad2000 -     0x7fff22dbdfff  com.apple.CoreServices.CarbonCore (1307.2 - 1307.2)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore\n0x7fff22dfd000 -     0x7fff22e37fff  com.apple.CSStore (1122.33 - 1122.33) <209D2E9A-FC95-3E51-A0DC-4F32C21B2266> /System/Library/PrivateFrameworks/CoreServicesStore.framework/Versions/A/CoreServicesStore\n0x7fff22e38000 -     0x7fff22ee6fff  com.apple.framework.IOKit (2.0.2 - 1845.100.19) <99034CC2-EC1B-38C0-A8DE-CE37672DF139> /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit\n0x7fff22ee7000 -     0x7fff22ef2fff  libsystem_notify.dylib (279.40.4) <1AA89769-E07F-37CD-BBCF-6DBD345862EB> /usr/lib/system/libsystem_notify.dylib\n0x7fff24355000 -     0x7fff249d4fff  libnetwork.dylib (2288.100.111) <8D567D13-AE70-34BF-834D-9A65C702A8EA> /usr/lib/libnetwork.dylib\n0x7fff249d5000 -     0x7fff24e73fff  com.apple.CFNetwork (1237 - 1237) <347078F9-34AC-3AD5-AA02-B7E5E1D11FB6> /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork\n0x7fff24e74000 -     0x7fff24e82fff  libsystem_networkextension.dylib (1295.101.1) <33F45B5A-D346-3E7F-AB34-DFC4387E5A3C> /usr/lib/system/libsystem_networkextension.dylib\n0x7fff24e83000 -     0x7fff24e83fff  libenergytrace.dylib (22.100.1)  /usr/lib/libenergytrace.dylib\n0x7fff24e84000 -     0x7fff24ee0fff  libMobileGestalt.dylib (978.100.37) <1B957D3E-C0F7-36AF-98E4-8897F8633BEA> /usr/lib/libMobileGestalt.dylib\n0x7fff24ee1000 -     0x7fff24ef7fff  libsystem_asl.dylib (385) <4D4E0D4F-8B40-3ACC-85E1-16375966D6CC> /usr/lib/system/libsystem_asl.dylib\n0x7fff24ef8000 -     0x7fff24f0ffff  com.apple.TCC (1.0 - 1) <10E022E6-5939-32DF-80E7-11BEA294F987> /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC\n0x7fff2622d000 -     0x7fff263e0fff  libsqlite3.dylib (321.3) <39129A81-0E78-3130-85AD-5FA0BFBCC6FA> /usr/lib/libsqlite3.dylib\n0x7fff2653f000 -     0x7fff265b3fff  com.apple.AE (918.4 - 918.4) <5377134F-CCA8-3610-8888-8598115A8E8F> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE\n0x7fff265b4000 -     0x7fff265bafff  libdns_services.dylib (1310.100.10)  /usr/lib/libdns_services.dylib\n0x7fff265bb000 -     0x7fff265c2fff  libsystem_symptoms.dylib (1431.100.22) <226C631F-5380-395A-AA20-D686AEA05B9B> /usr/lib/system/libsystem_symptoms.dylib\n0x7fff26748000 -     0x7fff26777fff  com.apple.analyticsd (1.0 - 1) <356D0732-7AC4-3579-A96E-640FD4C9AB56> /System/Library/PrivateFrameworks/CoreAnalytics.framework/Versions/A/CoreAnalytics\n0x7fff26778000 -     0x7fff2677afff  libDiagnosticMessagesClient.dylib (112) <04F0E364-262E-3826-B0AF-1076FBAF304A> /usr/lib/libDiagnosticMessagesClient.dylib\n0x7fff2677b000 -     0x7fff267c7fff  com.apple.spotlight.metadata.utilities (1.0 - 2150.16)  /System/Library/PrivateFrameworks/MetadataUtilities.framework/Versions/A/MetadataUtilities\n0x7fff267c8000 -     0x7fff26862fff  com.apple.Metadata (10.7.0 - 2150.16)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata\n0x7fff26863000 -     0x7fff26869fff  com.apple.DiskArbitration (2.7 - 2.7)  /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration\n0x7fff2686a000 -     0x7fff26ed1fff  com.apple.vImage (8.1 - 544.4) <427DE83B-D133-3DCB-A5EA-BC4A644B9283> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage\n0x7fff2742a000 -     0x7fff27439fff  com.apple.OpenDirectory (11.3 - 230.40.1) <6A59F2E7-1E42-3894-8F40-2D174D5A7938> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory\n0x7fff2743a000 -     0x7fff27459fff  com.apple.CFOpenDirectory (11.3 - 230.40.1) <1084A655-1A23-309D-8C74-B8B0135ABAC7> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory\n0x7fff2745a000 -     0x7fff27462fff  com.apple.CoreServices.FSEvents (1290.101.1 - 1290.101.1) <43164A51-72ED-30E1-8F28-40B3598BCF5C> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents\n0x7fff27463000 -     0x7fff27487fff  com.apple.coreservices.SharedFileList (144 - 144)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList\n0x7fff27488000 -     0x7fff2748afff  libapp_launch_measurement.dylib (14.1) <99F97E05-95A4-3E88-8723-6F92C63F875D> /usr/lib/libapp_launch_measurement.dylib\n0x7fff2748b000 -     0x7fff274d2fff  com.apple.CoreAutoLayout (1.0 - 21.10.1) <9CBC3DB1-FE97-311D-A19F-1A09B4F2CEFA> /System/Library/PrivateFrameworks/CoreAutoLayout.framework/Versions/A/CoreAutoLayout\n0x7fff274d3000 -     0x7fff275b5fff  libxml2.2.dylib (34.9)  /usr/lib/libxml2.2.dylib\n0x7fff285be000 -     0x7fff285cefff  libsystem_containermanager.dylib (318.100.4) <3BAF45DD-75D2-3D8A-845E-83A7058CBA08> /usr/lib/system/libsystem_containermanager.dylib\n0x7fff285cf000 -     0x7fff285e0fff  com.apple.IOSurface (290.7 - 290.7) <1A0AD030-00A3-330D-A821-16C0BBA88A82> /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface\n0x7fff285e1000 -     0x7fff285eafff  com.apple.IOAccelerator (442.9 - 442.9) <9D9FC91B-2544-3D53-A7AC-DD2E2961DE16> /System/Library/PrivateFrameworks/IOAccelerator.framework/Versions/A/IOAccelerator\n0x7fff285eb000 -     0x7fff2870efff  com.apple.Metal (244.119 - 244.119) <99A6E326-7BCC-33C5-B0C8-D80C29059592> /System/Library/Frameworks/Metal.framework/Versions/A/Metal\n0x7fff2926a000 -     0x7fff292d0fff  com.apple.MetalPerformanceShaders.MPSCore (1.0 - 1) <55937FF4-F2FA-3671-84DE-88950DD81A88> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSCore.framework/Versions/A/MPSCore\n0x7fff292d1000 -     0x7fff292d4fff  libsystem_configuration.dylib (1109.101.1) <49050F1E-3143-31B9-8B5C-FCCE4C6F23E5> /usr/lib/system/libsystem_configuration.dylib\n0x7fff292d5000 -     0x7fff292d9fff  libsystem_sandbox.dylib (1441.101.1) <24C8DF94-E258-3E24-AC96-2D1FB85F076A> /usr/lib/system/libsystem_sandbox.dylib\n0x7fff292da000 -     0x7fff292dbfff  com.apple.AggregateDictionary (1.0 - 1) <652D4FC1-1335-3E5C-86F4-A92D2413AF81> /System/Library/PrivateFrameworks/AggregateDictionary.framework/Versions/A/AggregateDictionary\n0x7fff292dc000 -     0x7fff292dffff  com.apple.AppleSystemInfo (3.1.5 - 3.1.5)  /System/Library/PrivateFrameworks/AppleSystemInfo.framework/Versions/A/AppleSystemInfo\n0x7fff292e0000 -     0x7fff292e1fff  liblangid.dylib (136)  /usr/lib/liblangid.dylib\n0x7fff292e2000 -     0x7fff29386fff  com.apple.CoreNLP (1.0 - 245.2)  /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/CoreNLP\n0x7fff29387000 -     0x7fff2938dfff  com.apple.LinguisticData (1.0 - 399) <53F0DCFF-114F-3954-9EA2-820C3CB598E1> /System/Library/PrivateFrameworks/LinguisticData.framework/Versions/A/LinguisticData\n0x7fff2938e000 -     0x7fff29a36fff  libBNNS.dylib (288.100.5)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib\n0x7fff29a37000 -     0x7fff29c09fff  libvDSP.dylib (760.100.3) <8AC1F990-C583-3354-B5D2-00EF0B696C51> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib\n0x7fff29c0a000 -     0x7fff29c1bfff  com.apple.CoreEmoji (1.0 - 128.4) <1E5E625B-1B19-37E6-BAD2-05301D7BB0FC> /System/Library/PrivateFrameworks/CoreEmoji.framework/Versions/A/CoreEmoji\n0x7fff29c1c000 -     0x7fff29c26fff  com.apple.IOMobileFramebuffer (343.0.0 - 343.0.0) <86325AA4-4D75-36AC-800B-3E5574A65F02> /System/Library/PrivateFrameworks/IOMobileFramebuffer.framework/Versions/A/IOMobileFramebuffer\n0x7fff29f30000 -     0x7fff29fbbfff  com.apple.securityfoundation (6.0 - 55240.40.4) <0B2C0366-0C1A-3A48-BC0E-02F95EDCC259> /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation\n0x7fff29fbc000 -     0x7fff29fc5fff  com.apple.coreservices.BackgroundTaskManagement (1.0 - 104) <59A88CF5-6FD6-3801-AAB9-D0B358ABECD2> /System/Library/PrivateFrameworks/BackgroundTaskManagement.framework/Versions/A/BackgroundTaskManagement\n0x7fff29fc6000 -     0x7fff29fcafff  com.apple.xpc.ServiceManagement (1.0 - 1) <309EC085-9520-3D62-BBCE-6D44117308B7> /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement\n0x7fff29fcb000 -     0x7fff29fcdfff  libquarantine.dylib (119.40.2)  /usr/lib/system/libquarantine.dylib\n0x7fff29fce000 -     0x7fff29fd9fff  libCheckFix.dylib (31) <439FF8FA-1F07-31BC-A578-06CBA45B4E43> /usr/lib/libCheckFix.dylib\n0x7fff29fda000 -     0x7fff29ff1fff  libcoretls.dylib (169.100.1)  /usr/lib/libcoretls.dylib\n0x7fff29ff2000 -     0x7fff2a002fff  libbsm.0.dylib (68.40.1) <8817CFED-17DD-3E83-9F48-1028E4D8B97D> /usr/lib/libbsm.0.dylib\n0x7fff2a003000 -     0x7fff2a04cfff  libmecab.dylib (929.9) <4558AEA4-814E-3EFE-8976-BA7BECCAB27E> /usr/lib/libmecab.dylib\n0x7fff2a04d000 -     0x7fff2a052fff  libgermantok.dylib (24) <82F85FB5-9D23-3BFD-9CA7-43F0F76752BD> /usr/lib/libgermantok.dylib\n0x7fff2a053000 -     0x7fff2a068fff  libLinearAlgebra.dylib (1336.101.1) <25376663-3EF4-32D9-A567-A57B719E5E5F> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib\n0x7fff2a069000 -     0x7fff2a287fff  com.apple.MetalPerformanceShaders.MPSNeuralNetwork (1.0 - 1) <0C639E2D-0C0E-3FF4-8194-782EE0824C3B> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSNeuralNetwork.framework/Versions/A/MPSNeuralNetwork\n0x7fff2a288000 -     0x7fff2a2d7fff  com.apple.MetalPerformanceShaders.MPSRayIntersector (1.0 - 1) <3527EC40-E19E-3400-9A6A-1A5457947C94> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSRayIntersector.framework/Versions/A/MPSRayIntersector\n0x7fff2a2d8000 -     0x7fff2a439fff  com.apple.MLCompute (1.0 - 1) <7A1CD731-D17D-3394-93D8-612C0C47C12D> /System/Library/Frameworks/MLCompute.framework/Versions/A/MLCompute\n0x7fff2a43a000 -     0x7fff2a470fff  com.apple.MetalPerformanceShaders.MPSMatrix (1.0 - 1)  /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSMatrix.framework/Versions/A/MPSMatrix\n0x7fff2a471000 -     0x7fff2a4c7fff  com.apple.MetalPerformanceShaders.MPSNDArray (1.0 - 1) <58BD130C-DC37-3CC0-8DD5-4B783EFBE467> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSNDArray.framework/Versions/A/MPSNDArray\n0x7fff2a4c8000 -     0x7fff2a558fff  com.apple.MetalPerformanceShaders.MPSImage (1.0 - 1) <860DA94F-2106-380A-A90A-CA4979A06777> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSImage.framework/Versions/A/MPSImage\n0x7fff2a559000 -     0x7fff2a568fff  com.apple.AppleFSCompression (125 - 1.0) <935B76EE-49A3-3FC6-99BB-994D83B3FA26> /System/Library/PrivateFrameworks/AppleFSCompression.framework/Versions/A/AppleFSCompression\n0x7fff2a569000 -     0x7fff2a575fff  libbz2.1.0.dylib (44) <111C16DD-70F2-3BFF-A3E1-928C7F64B0C1> /usr/lib/libbz2.1.0.dylib\n0x7fff2a576000 -     0x7fff2a57afff  libsystem_coreservices.dylib (127)  /usr/lib/system/libsystem_coreservices.dylib\n0x7fff2a57b000 -     0x7fff2a5a8fff  com.apple.CoreServices.OSServices (1122.33 - 1122.33) <4B4148C5-9CFB-3AA1-89B0-0EB988C0BCD2> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices\n0x7fff2a777000 -     0x7fff2a789fff  libz.1.dylib (76) <45B3DDDD-FCA5-3387-98B8-38D9BFF68588> /usr/lib/libz.1.dylib\n0x7fff2a78a000 -     0x7fff2a7d1fff  libsystem_m.dylib (3186.100.3) <2F75F75F-DB36-3841-988F-48A7237ED6E9> /usr/lib/system/libsystem_m.dylib\n0x7fff2a7d2000 -     0x7fff2a7d2fff  libcharset.1.dylib (59) <5B967C5A-B6AC-322F-B731-4D653B8462B8> /usr/lib/libcharset.1.dylib\n0x7fff2a7d3000 -     0x7fff2a7d8fff  libmacho.dylib (980)  /usr/lib/system/libmacho.dylib\n0x7fff2a7d9000 -     0x7fff2a7f4fff  libkxld.dylib (7195.101.2)  /usr/lib/system/libkxld.dylib\n0x7fff2a7f5000 -     0x7fff2a800fff  libcommonCrypto.dylib (60178.100.1)  /usr/lib/system/libcommonCrypto.dylib\n0x7fff2a801000 -     0x7fff2a80bfff  libunwind.dylib (201) <32F74246-BEB8-3249-8C51-CAD863EF158E> /usr/lib/system/libunwind.dylib\n0x7fff2a80c000 -     0x7fff2a813fff  liboah.dylib (203.42)  /usr/lib/liboah.dylib\n0x7fff2a814000 -     0x7fff2a81efff  libcopyfile.dylib (173.40.2)  /usr/lib/system/libcopyfile.dylib\n0x7fff2a81f000 -     0x7fff2a826fff  libcompiler_rt.dylib (102.2)  /usr/lib/system/libcompiler_rt.dylib\n0x7fff2a827000 -     0x7fff2a829fff  libsystem_collections.dylib (1439.100.3) <8B0EA86C-AFA8-3B51-916F-7ED9A42669A4> /usr/lib/system/libsystem_collections.dylib\n0x7fff2a82a000 -     0x7fff2a82cfff  libsystem_secinit.dylib (87.60.1) <18494A83-796C-33D5-A6D0-BD4E0C2BBFC4> /usr/lib/system/libsystem_secinit.dylib\n0x7fff2a82d000 -     0x7fff2a82ffff  libremovefile.dylib (49.101.1)  /usr/lib/system/libremovefile.dylib\n0x7fff2a830000 -     0x7fff2a830fff  libkeymgr.dylib (31) <1A66B854-350D-3064-A606-2D9BFE71C400> /usr/lib/system/libkeymgr.dylib\n0x7fff2a831000 -     0x7fff2a838fff  libsystem_dnssd.dylib (1310.100.10) <600B1851-C1D4-3ACF-BE2A-99E7E40E6FC0> /usr/lib/system/libsystem_dnssd.dylib\n0x7fff2a839000 -     0x7fff2a83efff  libcache.dylib (83)  /usr/lib/system/libcache.dylib\n0x7fff2a83f000 -     0x7fff2a840fff  libSystem.B.dylib (1292.100.5)  /usr/lib/libSystem.B.dylib\n0x7fff2a841000 -     0x7fff2a844fff  libfakelink.dylib (3) <3EDDC5DA-6FAD-3031-91C1-E3465C34E643> /usr/lib/libfakelink.dylib\n0x7fff2a845000 -     0x7fff2a845fff  com.apple.SoftLinking (1.0 - 1) <82988A53-B066-3D39-A314-CE3C5C3868DE> /System/Library/PrivateFrameworks/SoftLinking.framework/Versions/A/SoftLinking\n0x7fff2a846000 -     0x7fff2a87dfff  libpcap.A.dylib (98.100.3) <4D1E2F3F-484D-3FDC-B764-EB1DB10AFCFB> /usr/lib/libpcap.A.dylib\n0x7fff2a87e000 -     0x7fff2a96efff  libiconv.2.dylib (59)  /usr/lib/libiconv.2.dylib\n0x7fff2a96f000 -     0x7fff2a980fff  libcmph.dylib (8) <7603021E-45F7-31A8-9449-A356A81712DD> /usr/lib/libcmph.dylib\n0x7fff2a981000 -     0x7fff2a9f2fff  libarchive.2.dylib (83.100.2) <7896CEB5-AE27-3E35-8C0D-DF3771C3EDF7> /usr/lib/libarchive.2.dylib\n0x7fff2a9f3000 -     0x7fff2aa5afff  com.apple.SearchKit (1.4.1 - 1.4.1)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit\n0x7fff2aa5b000 -     0x7fff2aa5cfff  libThaiTokenizer.dylib (3) <3B0B24A3-94AA-3876-803D-7E849EF7F5FF> /usr/lib/libThaiTokenizer.dylib\n0x7fff2aa5d000 -     0x7fff2aa7ffff  com.apple.applesauce (1.0 - 16.28) <6451578D-F1BF-3476-838D-D45ED05CF78F> /System/Library/PrivateFrameworks/AppleSauce.framework/Versions/A/AppleSauce\n0x7fff2aa80000 -     0x7fff2aa97fff  libapple_nghttp2.dylib (1.41) <11629AC3-024A-3972-AAD8-A875AEAEBB30> /usr/lib/libapple_nghttp2.dylib\n0x7fff2aa98000 -     0x7fff2aaaefff  libSparseBLAS.dylib (1336.101.1) <1C78861A-2B74-3172-9EB0-A13275785517> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib\n0x7fff2aaaf000 -     0x7fff2aab0fff  com.apple.MetalPerformanceShaders.MetalPerformanceShaders (1.0 - 1) <25F9308E-312C-3C30-B7B4-27E98EED7E4A> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/MetalPerformanceShaders\n0x7fff2aab1000 -     0x7fff2aab5fff  libpam.2.dylib (28.40.1) <0386A3AB-7134-34FB-A456-3B84D52DFAB1> /usr/lib/libpam.2.dylib\n0x7fff2aab6000 -     0x7fff2aacefff  libcompression.dylib (96.100.7)  /usr/lib/libcompression.dylib\n0x7fff2aacf000 -     0x7fff2aad4fff  libQuadrature.dylib (7) <5D8EB379-1B51-3B80-AF16-B3B05F04A182> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib\n0x7fff2aad5000 -     0x7fff2ae72fff  libLAPACK.dylib (1336.101.1) <1AC818CD-DB01-3F84-91E4-71D14DB47E41> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib\n0x7fff2ae73000 -     0x7fff2aec2fff  com.apple.DictionaryServices (1.2 - 341)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices\n0x7fff2aec3000 -     0x7fff2aedbfff  liblzma.5.dylib (16) <3FC910D3-F9C8-323F-8373-4CF8D3702396> /usr/lib/liblzma.5.dylib\n0x7fff2aedc000 -     0x7fff2aeddfff  libcoretls_cfhelpers.dylib (169.100.1) <6784E023-906B-3FDF-8953-089BEE9E0973> /usr/lib/libcoretls_cfhelpers.dylib\n0x7fff2aede000 -     0x7fff2afd8fff  com.apple.APFS (1677.100.114 - 1677.100.114) <949210B0-BE79-3BB4-A0D2-22C8EDE6E8BA> /System/Library/PrivateFrameworks/APFS.framework/Versions/A/APFS\n0x7fff2afd9000 -     0x7fff2afe6fff  libxar.1.dylib (452) <19D3ABA0-D327-37AB-92F7-BB87FC58AB2B> /usr/lib/libxar.1.dylib\n0x7fff2afe7000 -     0x7fff2afeafff  libutil.dylib (58.40.2)  /usr/lib/libutil.dylib\n0x7fff2afeb000 -     0x7fff2b013fff  libxslt.1.dylib (17.4) <70C9C559-98AB-3FAB-BFDB-5022E00058D5> /usr/lib/libxslt.1.dylib\n0x7fff2b014000 -     0x7fff2b01efff  libChineseTokenizer.dylib (37)  /usr/lib/libChineseTokenizer.dylib\n0x7fff2b01f000 -     0x7fff2b0dcfff  libvMisc.dylib (760.100.3)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib\n0x7fff2d0c9000 -     0x7fff2d0e1fff  libresolv.9.dylib (68) <7EA918BC-50B3-3ABB-9021-88CB852FF4D6> /usr/lib/libresolv.9.dylib\n0x7fff2dc35000 -     0x7fff2dc35fff  liblaunch.dylib (2038.100.48) <3C5E58AF-F753-378B-8E0C-F3C9BB82E04D> /usr/lib/system/liblaunch.dylib\n0x7fff2dc36000 -     0x7fff2dc3bfff  libffi.dylib (27) <01EF1727-2BDE-3DFF-907E-FADC521AF71D> /usr/lib/libffi.dylib\n0x7fff300b9000 -     0x7fff300b9fff  libsystem_product_info_filter.dylib (8.40.1) <4CCFBF85-6483-364F-A8D6-A5D2956BEC52> /usr/lib/system/libsystem_product_info_filter.dylib\n0x7fff30191000 -     0x7fff30191fff  com.apple.Accelerate.vecLib (3.11 - vecLib 3.11) <533CA6C2-D07A-35F6-A711-4A55A4319C52> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib\n0x7fff301b7000 -     0x7fff301b7fff  com.apple.CoreServices (1122.33 - 1122.33)  /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices\n0x7fff3036e000 -     0x7fff3036efff  com.apple.Accelerate (1.11 - Accelerate 1.11)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate\n0x7fff40f16000 -     0x7fff40f45fff  libncurses.5.4.dylib (57) <64DA10BA-71BD-33B0-B19B-28984C171A7D> /usr/lib/libncurses.5.4.dylib\n0x7fff6bbe7000 -     0x7fff6bbedfff  libCoreFSCache.dylib (200.8) <5917BFDD-5553-3258-A468-C86A5BF62FD5> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreFSCache.dylibExternal Modification Summary:\nCalls made by other processes targeting this process:\ntask_for_pid: 0\nthread_create: 0\nthread_set_state: 0\nCalls made by this process:\ntask_for_pid: 0\nthread_create: 0\nthread_set_state: 0\nCalls made by all processes on this machine:\ntask_for_pid: 0\nthread_create: 0\nthread_set_state: 0VM Region Summary:\nReadOnly portion of Libraries: Total=667.9M resident=0K(0%) swapped_out_or_unallocated=667.9M(100%)\nWritable regions: Total=273.7M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=273.7M(100%)REGION TYPE                        SIZE    COUNT (non-coalesced)\n===========                     =======  =======\nActivity Tracing                   256K        1\nKernel Alloc Once                    8K        1\nMALLOC                            83.5M       48\nMALLOC guard page                   16K        4\nMALLOC_LARGE (reserved)            256K        2         reserved VM address space (unallocated)\nSTACK GUARD                          4K        1\nStack                             16.0M        1\nVM_ALLOCATE                      140.5M      183\nVM_ALLOCATE (reserved)            32.0M        1         reserved VM address space (unallocated)\n__DATA                            7745K      231\n__DATA_CONST                      7531K      147\n__DATA_DIRTY                       288K       61\n__LINKEDIT                       506.4M      131\n__OBJC_RO                         70.1M        1\n__OBJC_RW                         2480K        2\n__TEXT                           161.9M      224\n__UNICODE                          588K        1\nshared memory                        8K        2\n===========                     =======  =======\nTOTAL                              1.0G     1042\nTOTAL, minus reserved VM space   997.0M     1042Model: MacBookPro15,2, BootROM 1554.100.64.0.0 (iBridge: 18.16.14556.0.0,0), 4 processors, Quad-Core Intel Core i5, 2.4 GHz, 16 GB, SMC\nGraphics: kHW_IntelIrisGraphics655Item, Intel Iris Plus Graphics 655, spdisplays_builtin\nMemory Module: BANK 0/ChannelA-DIMM0, 8 GB, LPDDR3, 2133 MHz, Samsung, K4EBE304EC-EGCG\nMemory Module: BANK 2/ChannelB-DIMM0, 8 GB, LPDDR3, 2133 MHz, Samsung, K4EBE304EC-EGCG\nAirPort: spairport_wireless_card_type_airport_extreme (0x14E4, 0x7BF), wl0: Feb 16 2021 02:10:42 version 9.30.444.10.32.5.67 FWID 01-a00c1314\nBluetooth: Version 8.0.4d18, 3 services, 27 devices, 1 incoming serial ports\nNetwork Service: Wi-Fi, AirPort, en0\nUSB Device: USB 3.1 Bus\nUSB Device: Generic Billboard Device\nUSB Device: Apple T2 Bus\nUSB Device: Touch Bar Backlight\nUSB Device: Touch Bar Display\nUSB Device: Apple Internal Keyboard / Trackpad\nUSB Device: Headset\nUSB Device: Ambient Light Sensor\nUSB Device: FaceTime HD Camera (Built-in)\nUSB Device: Apple T2 Controller\nThunderbolt Bus: MacBook Pro, Apple Inc., 47.4\nThunderbolt Bus: MacBook Pro, Apple Inc., 47.4:\nCreate Airflow PythonOperator to call urllib.request.urlopen(built_url).\n(It works in python terminal, but will crash python if called from airflow):\nThis issue happens everytime when call urllib.request.urlopen(built_url)", "type": "commented", "related_issue": null}, {"user_name": "boring-cyborg", "datetime": "May 25, 2021", "body": "Thanks for opening your first issue here! Be sure to follow the issue template!", "type": "commented", "related_issue": null}, {"user_name": "superp0tat0", "datetime": "May 26, 2021", "body": "I think this issue is more relate to other libraries instead airflow. So I decide to close them, sorry for the inconvenience.", "type": "commented", "related_issue": null}, {"user_name": "superp0tat0", "datetime": "May 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "superp0tat0", "datetime": "May 26, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/3257", "issue_status": " Closed\n", "issue_list": [{"user_name": "allenyummy", "datetime": "Sep 18, 2019", "body": "I haven't found the dataset mentioned from Reasoning Over Paragraph Effects in Situations, Lin et al., CL-2019.Is there any download link for ROPES dataset ?Thanks for reply.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 18, 2019", "body": "Coming soon.  The paper was just accepted to MRQA, we'll probably update to a camera ready version and include more information, including a webpage / download link / leaderboard / etc.  , can you update this issue when we're ready for it?", "type": "commented", "related_issue": null}, {"user_name": "kl2806", "datetime": "Sep 18, 2019", "body": "Yep, I'll update this issue with more information soon.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Oct 11, 2019", "body": " any update?", "type": "commented", "related_issue": null}, {"user_name": "kl2806", "datetime": "Oct 14, 2019", "body": "We have been actively working on the dataset over the last month; the main page will be on the AllenNLP website () and I'll update here when it's live.", "type": "commented", "related_issue": null}, {"user_name": "kl2806", "datetime": "Oct 25, 2019", "body": "Please see the allennlp site for the ROPES webpage ().", "type": "commented", "related_issue": null}, {"user_name": "kl2806", "datetime": "Sep 18, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "kl2806", "datetime": "Oct 25, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/2667", "issue_status": " Closed\n", "issue_list": [{"user_name": "ghost", "datetime": "Mar 29, 2019", "body": "\nI am running wikitable predict in python multiprocessing module. After the predict is over and I get the result back, the java process made by allennlp is not getting killed. It is taking down my server consuming all cpu and memory. Following is a sample process that keeps on running even after the main process gets over:\nSteps to reproduce the behavior\nrun process in multiprocessing pool. Even close or terminate or join is not killing these processes.\nThere is be no such java process after predict is over\nAdd any other context about the problem here.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Mar 29, 2019", "body": "That code is not designed to be thread-safe, so it's unfortunately not surprising to me that it doesn't work.  We also will be replacing the existing SEMPRE-based WikiTables model with a much better one that does not depend on java within a week or two, so this issue will go away.  I'm going to close this as \"won't fix\".", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Apr 8, 2019", "body": "Hi  , We are waiting for Wikitable release. Did not get any notification (watching releases on github). May we know when we should expect this", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Apr 8, 2019", "body": "Watch this PR: .  When that gets merged, it should come with a new model that gets ~44% accuracy on WikiTableQuestions ( better than the existing model).   was busy cleaning up his paper for the NAACL camera ready deadline, and he's been on vacation.  I expect you'll see some movement on that PR relatively soon.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Mar 29, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4634", "issue_status": " Closed\n", "issue_list": [{"user_name": "ryoma310", "datetime": "Jul 21, 2022", "body": "I try to use stereo_image_proc to depth estimation from images obtained by airsim_ros_pkg.\nIt seems to work well as I see disparity image.\nBut when I check the point cloud, it shows only one point.This is my setting.jsonI search about it and found that camera_info may have same error.\nI checked the topic detail and find that the projection matrix of left and right camera are same.\n(because of this, stereo_image_proc node cannot calculate the depth.)", "type": "commented", "related_issue": null}, {"user_name": "ryoma310", "datetime": "Jul 28, 2022", "body": "I resolved it by myself!", "type": "commented", "related_issue": null}, {"user_name": "CWC107753035", "datetime": "Aug 12, 2022", "body": "For those who struggling on this question I find out the problem.Because in airsim ros-node it doesn't provide baseline in camera_info. So if you want to get the point cloud through disparity image, you need to provide the Tx parameter manually in one of  in camera_info.\nsee also  for more information. Cheers.", "type": "commented", "related_issue": null}, {"user_name": "ryoma310", "datetime": "Aug 27, 2022", "body": "\nThat's exactly what I did to solve this issue.\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jul 25, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "ryoma310", "datetime": "Jul 28, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/smellslikeml/ActionAI/issues/50", "issue_status": " Closed\n", "issue_list": [{"user_name": "Amyhds", "datetime": "Nov 15, 2021", "body": "First, thank you for sharing this amazing work.\nI'm using Jetson TX2 and usb webcam.\nI'm wondering that this library automatically classifies normal actions like I mentioned without additional training.\nIf it doesn't, does it only classify squatting and spinning?\nI look forward to your reply!", "type": "commented", "related_issue": null}, {"user_name": "smellslikeml", "datetime": "Nov 15, 2021", "body": "Hi and thank you for your interest! We included a sample model under  to differentiate between those two actions for demo purposes.\nThis repo should help you train your own model for your desired set of action categories following the guidance in the README section\nWe were able to differentiate between a handful of movements very easily with a handful of samples as shown in the IVA example.", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 16, 2021", "body": "I followed the steps but there's a issue.\nYou said that the pickled model will be saved in the models/ directory after running train.py.\nIs the pickled model classifier.sav?\nSo I changed the model and motion dict as my own model and classes but got this error.", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 23, 2021", "body": "It succeeded when I ran with video file, but I still don't know why my webcam doesn't work..\nIt is not that accurate so I wonder if I should get more image samples.\n(only 8 pictures in each class..I couldn't use MPII because there's no enough memory)", "type": "commented", "related_issue": null}, {"user_name": "smellslikeml", "datetime": "Nov 23, 2021", "body": "From the traceback, it looks like the device  is busy, meaning another process is using it. Perhaps you could look to see what process may be occupying the webcam or attach another webcam and try it (it might be read as  and so you'd pass a 1 to the  script)Some suggestions for improving accuracy:We are exploring major updates to the project. Feel free to join us on  with your feedback or thoughts about improvements!", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 24, 2021", "body": "I'm really thankful for your help!!! ", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 24, 2021", "body": "sorry but I got problems again:(I thought iva.py worked with mp4 using classifier.sav but actually lstm_spin_squat.h5 was used.These are the result after running train.pyand I got these errors running iva.py with classifier.sav.what type of file I get after running train.py? I think it should be like \"[modelname].h5\" but there are 'classifier.sav, human_pose.json, lstm_spin_squat.h5, pose.tflite, resnet18_baseline_att_224x224_A_epoch_249_trt.pth' in models directory.", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 29, 2021", "body": "Hi  !\nThis time I modified PoseExtractor to TRTPoseExtractor in train.py and then got this error.\nWhat is the problem?\nThank you!", "type": "commented", "related_issue": null}, {"user_name": "smellslikeml", "datetime": "Nov 29, 2021", "body": "These look like warnings. These questions are best to post on stack overflow where someone may have already answered it.", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 30, 2021", "body": "my current situation", "type": "commented", "related_issue": null}, {"user_name": "LawrenceXu13467", "datetime": "Aug 9, 2022", "body": "Hi Amyhds,Following your previous conversation, have you figured out how to solve these problems? I got into a similar situation here. Thank you.Sincerely,\nLawrence", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Aug 9, 2022", "body": "Hi \nI could not find solutions..sorry", "type": "commented", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 23, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Amyhds", "datetime": "Nov 24, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "smellslikeml", "datetime": "Nov 29, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/3166", "issue_status": " Closed\n", "issue_list": [{"user_name": "TheShadow29", "datetime": "Aug 17, 2019", "body": "I am not sure if it should be posted here or  as I have not been able to find where the error occurs.The Bug: I am using the demo here: For the sentence:I get the following output (only showing the erroneous output):For some reason, \"two of\" is showing inside square brackets which I find quite perplexing.Thank you for your patience.", "type": "commented", "related_issue": null}, {"user_name": "TheShadow29", "datetime": "Aug 17, 2019", "body": "Update: I saw the tags of the above sentence, which are as follows:Basically, after 'O', the output is 'I-ARG-TMP', since there is no 'B-ARG-TMP' the output is incorrect.I am not sure what the correct output should be in such a case. Do we ignore outputs which don't start off with 'B'?", "type": "commented", "related_issue": null}, {"user_name": "DeNeutoy", "datetime": "Aug 19, 2019", "body": "damn, I thought I had fixed this. Let me look into this a bit.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 18, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Sep 1, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Sep 15, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Sep 29, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Oct 13, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Oct 27, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Nov 11, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 19, 2020", "body": "Closing in favor of .", "type": "commented", "related_issue": null}, {"user_name": "DeNeutoy", "datetime": "Aug 19, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "DeNeutoy", "datetime": "Aug 19, 2019", "body": [], "type": "issue", "related_issue": "#3167"}, {"user_name": "plroit", "datetime": "Nov 18, 2020", "body": [], "type": "issue", "related_issue": "#4803"}, {"user_name": "dirkgr", "datetime": "Nov 18, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 19, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4617", "issue_status": " Closed\n", "issue_list": [{"user_name": "EvelynM7", "datetime": "Jul 12, 2022", "body": "I have now tried many processes of setting up Ubuntu, ROS, RVIZ, and AirSim as well as different edits to the airsim_ros_wrapper code. The following steps are for the setup that has worked best for me, but I am still running into at least one error getting RVIZ to connect properly with AirSim showing the camera.As can be seen in the images attached below, this gave me Status: Warn like  The error for Camera Info says \"No CameraInfo received on [/airsim_node/Drone1/Test0_0/camera_info]. Topic may not exist.\"Both  and  also showed a screenshot with Frames in . I only have world_enu and world_ned as options in RVIZ. I think this and the lack of topics could be causing the error. How do I fix both of these? I've included the RVIZ screenshot and the list of topics from the terminal (there are some missing, I think, when compared to 's list).Any assistance would be greatly appreciated. Thank you!\nIn RVIZ: \"Status: Warn\". The error for Camera Info also says \"No CameraInfo received on [/airsim_node/Drone1/Test0_0/camera_info]. Topic may not exist.\"", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 13, 2022", "body": "Hi ,\nIt's hard to say what is wrong, but what I do see is that you might modified the files a bit differently than , otherwise your topic would be  and not .\nIt seems like you modifed the files manullay, I suggest you using  as is, with  and report back.", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 13, 2022", "body": "Hello  ,I'm not sure what you mean by . I tried that command both in Git Bash and Ubuntu, but it didn't work. Could you elaborate, please?I did go back and copy directly the changes made in  as listed in {. I also closely checked the spacing, spelling, etc., and mine is now identical to the one listed there. I am still having to manually input the topic for the camera as shown in the images attached below. I think this is where you are saying my topic should be different. I believe I followed what was written out as  but  is what shows up in your topic list so I've included screenshots of that inputted topic. You can see that I still don't have the frames (even when all are enabled) and topics you had in  as well. Why could this be, and how do I fix it? Are you using a different rviz file? Am I missing a step?\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 14, 2022", "body": " is github cli so you can use it to checkout a branch from a fork easier - .\nI'm using a standard installation of ROS2 galactic included rviz2.\nIs  showing something? error/warning?You topic list doesn't make sense, make a test:If not - it seems like your node crashed or didn't connect to the simulation.", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 14, 2022", "body": "There's an info comment when running : .Following your steps gives me the below screenshot (the last topic command that also includes  is the one after starting ).", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 14, 2022", "body": "Do you run the simulation on the same machine?\nYour node isn't connecting the simulation", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 14, 2022", "body": "Yes, we do.\nDo you have any suggestions on how to fix this or test why this might be happening?", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 15, 2022", "body": "You should specify the host ip which run the simulation.", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 15, 2022", "body": "Is the simulation host ip different then the computer IP address?", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 15, 2022", "body": "No. It's the ip address of your computer running the simulation", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 18, 2022", "body": "Thank you so very much! That did seem to connect properly.I do have a couple more questions if you don't mind:\n", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jul 19, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Jul 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4517", "issue_status": " Closed\n", "issue_list": [{"user_name": "bene-sim", "datetime": "May 9, 2022", "body": "water foam and wave effects of the sea do not apply when in airsim mode.It is a sea environment purchased from the marketplace of Unreal Engine, and as shown in the image below, the effects of the sea are well applied in the basic gamemode, but the effects are not applied in the airsim mode. (The Sea Asset is made in blue print.)Various effects work well when it is not in the air sim mode, but the effects disappear when the air sim mode is executed.\ndoes anyone know how to solve same problem?-﻿ThirdPersonGameMode(basic gamemode)\n-AirSimGameMode\n{\n\"SettingsVersion\": 1.7,\n\"CameraDefaults\": {\n\"CaptureSettings\": [\n{\n\"ImageType\": 0,\n\"Width\": 3840,\n\"Height\": 2160,\n\"FOV_Degrees\": 90,\n\"AutoExposureSpeed\": 100,\n\"MotionBlurAmount\": 0\n}\n]\n},\n\"SimMode\": \"ComputerVision\"\n}Set the game mode to the airsim mode in the sea environment and then start the game.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 11, 2022", "body": ", can you try commenting , and following the steps to build AirSim from source (build, upgrade_from_git etc.)?", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "May 12, 2022", "body": "Hi , from looking at our AirSimGameMode, there's nothing that jumps out to me that could be causing the rendering of an effect like this to get disabled. The ThirdPersonGameMode you were using before isn't an engine class, so it may be doing some project-specific initialization of classes or variables that the wave effect depends upon. I would check the implementation of that class to see if it's doing anything that needs to be copied over to AirSimGameMode. Another possibility is that the wave effect isn't correctly configured to render in a cinematographic camera. We switched over to the CineCameraComponent in , you try with a version that doesn't have that change applied (v1.6.0 or lower) to see if this is due to which camera we're using.", "type": "commented", "related_issue": null}, {"user_name": "bene-sim", "datetime": "May 18, 2022", "body": "UE4.25 - airsim1.6\nUE4.26 - airsim1.6\nUE4.26 - airsim1.7\nThe list above has the same problem.\nThis is a new fact that not only the \"airsim\"mode but also the \"functionaltestgame\" mode has the same problem.", "type": "commented", "related_issue": null}, {"user_name": "bene-sim", "datetime": "May 18, 2022", "body": "The problem with the picture below is originally a problem for this ocean.\nI'm sorry for the confusion.\nTherefore, I will delete it from the original text.\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 19, 2022", "body": " Thanks for these new reports. Now it doesn't seem to be an AirSim issue. You would need to modify other game modes (including AirSim game mode) to get that plugin to work with them.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 21, 2022", "body": "Closed due to inactivity. Please, feel free to ask for it to be reopened if you wish to continue posting.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 9, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "HSC-1", "datetime": "May 12, 2022", "body": [], "type": "issue", "related_issue": "#3551"}, {"user_name": "jonyMarino", "datetime": "Jun 21, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4588", "issue_status": " Closed\n", "issue_list": [{"user_name": "firemount", "datetime": "Jun 22, 2022", "body": "Should Airsim open tcp port 4560 if \"PhysicsEngineName\" is set to ExternalPhysicsEngine in settings.jsonHi I am trying out the ExternalPhysicsEngine parameter in settings.json to be able to use inject a FDM of my drone using PX4 with gazebo_iris and QGroungControl. Everything worked eventually but there is something I quite don't get. Following the manual  that says:\"First run the AirSim simulator and your Gazebo model and then execute this from your AirSim root folder:cd GazeboDrone/build\n./GazeboDrone\n\"\nThe end result will be that the UnrealEngine hangs non-responsive, and gazebo aborts starting the GUI ending up with\n[Err] [ConnectionManager.cc:121] Failed to connect to master in 30 seconds.\n[Err] [gazebo_shared.cc:78] Unable to initialize transport.\n[Err] [gazebo_client.cc:56] Unable to setup GazeboHowever if I use the following start sequence:\n(PX4) make px4_sitl gazebo_iris\nUnreal (./Myproject.sh)\n(airsim) ./GazeboDronePX4 will launch gazebo that binds to port 4560 since this is the simulator in PX4 context and the port is used for Steplock I guess gazebo should be the one that is the tcp server for port 4560Unreal starts and complains among other things\n\"2022.06.22-21.26.31:525][  0]LogTemp: Opening mavlink connection\nOpening mavlink connection\n[2022.06.22-21.26.31:525][  0]LogTemp: Disconnecting mavlink vehicle\nDisconnecting mavlink vehicle\nWaiting for mavlink vehicle...\nDisconnecting mavlink vehicle\nWaiting for TCP connection on port 4560, local IP 127.0.0.1\nAccepting TCP socket failed, is another instance running?\nTcpClientPort socket bind failed with error: 98\"When starting GazeboDrone and QGroundControl everything starts to work. Maybe someone can shed some light on what I got wrong.Ubuntu 20.04 LTS, 5.13.0-52-generic ~20.04.1-Ubuntu SMP Thu Jun 16 21:21:28 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\nAirsim 1.7.0\nPython 3.8.10\nUnreal 4.26.2settings.json\n{\n\"SeeDocsAt\": \"\n\",\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"Multirotor\",\n\"ClockType\": \"SteppableClock\",\n\"PhysicsEngineName\":\"ExternalPhysicsEngine\",\n\"Vehicles\": {\n\"PX4\": {\n\"VehicleType\": \"PX4Multirotor\",\n\"UseSerial\": false,\n\"LockStep\": true,\n\"UseTcp\": true,\n\"TcpPort\": 4560,\n\"ControlPortLocal\": 14540,\n\"ControlPortRemote\": 14580,\n\"EnableCollisionPassthrogh\": false,\n\"EnableCollisions\": false,\n\"AllowAPIAlways\": true,\n\"RC\": {\n\"RemoteControlID\": 0,\n\"AllowAPIWhenDisconnected\": false\n},\n\"Sensors\": {\n\"Imu\" : {\n\"SensorType\": 2,\n\"Enabled\": true\n},\n\"Barometer\":{\n\"SensorType\": 1,\n\"Enabled\": true,\n\"PressureFactorSigma\": 0.0001825\n}\n},\n\"Cameras\": {\n\"front_center_custom\": {\n\"CaptureSettings\": [\n{\n\"PublishToRos\": 1,\n\"ImageType\": 0,\n\"Width\": 640,\n\"Height\": 480,\n\"FOV_Degrees\": 27,\n\"DepthOfFieldFstop\": 2.8,\n\"DepthOfFieldFocalDistance\": 200.0,\n\"DepthOfFieldFocalRegion\": 200.0,\n\"TargetGamma\": 1.5\n}\n],\n\"X\": 0.50, \"Y\": 0, \"Z\": 0.10,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0\n}\n},\n\"Parameters\": {\n\"NAV_RCL_ACT\": 0,\n\"NAV_DLL_ACT\": 0,\n\"COM_OBL_ACT\": 1,\n\"LPE_LAT\": 47.62062,\n\"LPE_LON\": -122.34934},\n\"OriginGeopoint\": {\n\"Latitude\": 47.62062,\n\"Longitude\": -122.34934,\n\"Altitude\": 149\n},\n\"SubWindows\": [\n{\"WindowID\": 1, \"ImageType\": 0, \"CameraName\": \"front_center_custom\", \"Visibl\ne\": false}\n]\n},  , , ,", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 24, 2022", "body": "Hi and welcome  ! The external physics engine feature doesn't use the flight controllers that AirSim uses. That is because there should be a way to inform the FDM about the power signal sent to the rotors by the FC to make it possible to use, for example, PX4. Given that, you should use the settings for a SimpleFlight drone and connect PX4 to your gazebo drone.", "type": "commented", "related_issue": null}, {"user_name": "firemount", "datetime": "Jun 25, 2022", "body": "Hi and thanks, that pointer made things a lot more clear.", "type": "commented", "related_issue": null}, {"user_name": "firemount", "datetime": "Jun 23, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 24, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "firemount", "datetime": "Jul 7, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4571", "issue_status": " Closed\n", "issue_list": [{"user_name": "dellhuyongcai", "datetime": "Jun 15, 2022", "body": "I use the version 1.6.0 of Airsim to detect objects in UE4,client.simAddDetectionFilterMeshName(camera_name, image_type, \"\") make the ue4 crash at each time,I check the mesh '\"road_2_cross\",it really exists,what is the problem?", "type": "commented", "related_issue": null}, {"user_name": "dellhuyongcai", "datetime": "Jun 15, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4569", "issue_status": " Closed\n", "issue_list": [{"user_name": "hoangvietdo", "datetime": "Jun 15, 2022", "body": "The timestamp of odom_local_ned topic and stereo camera topic is weird. In addition, there is a significant delay between the left and right camera (around 0.02 seconds which can be considered as significant for the stereo VINS algorithms). As shown in the figure, the publish rate of odom_local_ned and IMU are similar (~100 Hz), however, the actual recorded data illustrates that some data are lost in odom_local_ned topic.\n\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 22, 2022", "body": "Hi  ! It is posible that  can help reduce time differences. Let me know if that is the case.", "type": "commented", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jun 24, 2022", "body": "Hi   , thanks so much for your help, unfortunately, I am not willing to switch to ros2 at the moment.\nI am trying to make airsim_ros_wrapper of ros2 be identical to ros1. However, there is some concept of ros2  (e.g., callBackGroup) that I could not find any alternative in ros1. Any help?", "type": "commented", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jun 24, 2022", "body": "In addition, I tested the PR and checked the recorded data. It seems like ros2 has the worst performance compared to ros1. It not only doesn't solve the time delay between 2 images, but it also makes the timestamp of IMU mixed up.AirSim Version/#commit: \nUE/Unity version: 4.27\nautopilot version:\nOS Version: Ros2 - Ubuntu 20.04 (Foxy)\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 22, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jun 24, 2022", "body": [], "type": "issue", "related_issue": "#4428"}, {"user_name": "hoangvietdo", "datetime": "Aug 8, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4484", "issue_status": " Closed\n", "issue_list": [{"user_name": "Jeriko3", "datetime": "Apr 13, 2022", "body": "Using the C++ API in the following detection example codeThe error acknowledging simSetDetectionFilterRadius requires 5 argument, but only 4 were passed to the function.Analyzing the code in AirSim/AirLib/src/api/RpcLibClientBase.cpp at line 220 \nOn the other hand in AirSim/AirLib/src/api/RpcLibServerBase.cpp at line 271 \nThe same problem is for functions\n\nChanging the function like this in the AirSim/AirLib/src/api/RpcLibClientBase.cpp, adding imagetype argument in the example and recompiling made it all work with the example code.Here's the example updated:The error acknowledging simSetDetectionFilterRadius requires 5 argument, but only 4 were passed to the function.", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Apr 25, 2022", "body": "Thank you for finding this, ; I submitted a PR containing your suggested fix in , feel free to test with it applied.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 14, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Apr 25, 2022", "body": [], "type": "pull", "related_issue": "#4496"}, {"user_name": "jonyMarino", "datetime": "Apr 25, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4557", "issue_status": " Closed\n", "issue_list": [{"user_name": "lukas-utopiacompression", "datetime": "Jun 1, 2022", "body": "Changing the  attribute in  in the settings.json file (see below) has no effect on the FOV during the simulation. I've set it to  and to  and the images produced inside the subwindow, as well as images captured with  always produce the same image.When executing the following python snipped you can see that the horizontal FOV is always set to 89.9036, no matter the value in the settings.json file.However, setting the FOV at runtime through the Python API does indeed change the FOV, as shown in the subwindow and images captures with :I've changed attributes  and  in the settings.json file. These changes were instantly reflected after restarting the simulation. So that works.Also, I've played around with the  attribute to set the FOV, but without any success.We have another system running AirSim 1.4. I tested the FOV settings there and it worked.Thank you for your help!", "type": "commented", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "Jun 6, 2022", "body": [], "type": "pull", "related_issue": "#4558"}, {"user_name": "jonyMarino", "datetime": "Jun 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 20, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4542", "issue_status": " Closed\n", "issue_list": [{"user_name": "Jeriko3", "datetime": "May 24, 2022", "body": "Using the following commandAssets aren't destroyed in the new mapBefore load:\n\nAfter load\nFurthermore, the origin point after the loading isn't the origin of the new map but the origin of the previous map (tested loading another map)Just use the C++ API calling the loadLevel command in the client RPC class", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 25, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "alonfaraj", "datetime": "May 30, 2022", "body": [], "type": "pull", "related_issue": "#4551"}, {"user_name": "jonyMarino", "datetime": "Jun 29, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4475", "issue_status": " Closed\n", "issue_list": [{"user_name": "dzywater", "datetime": "Apr 11, 2022", "body": "The camera_tf(optical, body, optical/static, body/static) header.child_frame_id has no vehicle name part.This leads ambiguous tf2_ros::Buffer::lookupTransform betweent two camera tfs when multiple vehicles have same camera name.Besides, it is also ambiguous  when creating camera's subframes.Other sensors(imu,gps,lidar...) have no this issue. their tf header.child_frame_id  has vehicle name part.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 12, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "dzywater", "datetime": "Apr 13, 2022", "body": [], "type": "pull", "related_issue": "#4478"}, {"user_name": "zimmy87", "datetime": "May 2, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4469", "issue_status": " Closed\n", "issue_list": [{"user_name": "dzywater", "datetime": "Apr 8, 2022", "body": "In my settings.json has following two configs.\"CameraDefaults\":\n{\n\"CaptureSettings\":\n[\n{ \"ImageType\": 0,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 },\n{ \"ImageType\": 1,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 },\n{ \"ImageType\": 2,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 },\n{ \"ImageType\": 3,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 },\n{ \"ImageType\": 4,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 },\n{ \"ImageType\": 5,  \"Width\": 320, \"Height\": 240, \"FOV_Degrees\": 90 }\n]\n}\"Vehicles\":\n{\n\"drone_0\":\n{\n\"VehicleType\": \"SimpleFlight\",\n\"X\": 0, \"Y\": 0,  \"Z\": 0, \"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0,\n\"Cameras\":\n{\n\"top_center\": { \"X\": 0, \"Y\": 0, \"Z\": -0.12, \"Roll\": 0, \"Pitch\": 90, \"Yaw\": 0 }\n}\n}\n}\n}I capture the default camera images(front_center, back_center, ...), images are expected size (320x240).\nI capture my camera (top_center), image is still 255x144.\nI know top_center can be set size alone, but I don't want that for less manual configs.\nI am using v1.7.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 12, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Apr 13, 2022", "body": [], "type": "pull", "related_issue": "#4481"}, {"user_name": "zimmy87", "datetime": "Apr 25, 2022", "body": [], "type": "pull", "related_issue": null}, {"user_name": "dzywater", "datetime": "Aug 3, 2022", "body": [], "type": "issue", "related_issue": "#4654"}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4453", "issue_status": " Closed\n", "issue_list": [{"user_name": "dzywater", "datetime": "Apr 1, 2022", "body": "The static_camera_optical_tf is in AirsimROSWrapper::append_static_camera_tf(...)The camera_optical_tf is in AirsimROSWrapper::publish_camera_tf(...)If isENU_==false, they are not right results.Moreover, no need to do repeated conversions between quaternion and ratotion matrix, and just do quaternion multiplication once.The modification are following (raw codes are commented).void AirsimROSWrapper::append_static_camera_tf(VehicleROS* vehicle_ros, const std::string& camera_name, const CameraSetting& camera_setting)\n{\ngeometry_msgs::msg::TransformStamped static_cam_tf_body_msg;\nstatic_cam_tf_body_msg.header.frame_id = vehicle_ros->vehicle_name_ + \"/\" + odom_frame_id_;\nstatic_cam_tf_body_msg.child_frame_id = camera_name + \"_body/static\";\nstatic_cam_tf_body_msg.transform = get_transform_msg_from_airsim(camera_setting.position, camera_setting.rotation);}void AirsimROSWrapper::publish_camera_tf(const ImageResponse& img_response, const rclcpp::Time& ros_time, const std::string& frame_id, const std::string& child_frame_id)\n{\nunused(ros_time);\ngeometry_msgs::msg::TransformStamped cam_tf_body_msg;\ncam_tf_body_msg.header.stamp = airsim_timestamp_to_ros(img_response.time_stamp);\ncam_tf_body_msg.header.frame_id = frame_id;\ncam_tf_body_msg.child_frame_id = child_frame_id + \"_body\";\ncam_tf_body_msg.transform = get_transform_msg_from_airsim(img_response.camera_position, img_response.camera_orientation);}Above functions can be tested by rviz2 visually.", "type": "commented", "related_issue": null}, {"user_name": "dzywater", "datetime": "Apr 1, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dzywater", "datetime": "Apr 1, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dzywater", "datetime": "Apr 6, 2022", "body": [], "type": "pull", "related_issue": "#4466"}, {"user_name": "jonyMarino", "datetime": "Apr 7, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Apr 29, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4447", "issue_status": " Closed\n", "issue_list": [{"user_name": "yhabib29", "datetime": "Mar 30, 2022", "body": "I use AirSim with Unreal Engine to simulate a drone with a stereo camera. I am using DJI S900 model and a combination of few settings I have found on the AirSim doc. The simulation launches, I can control using the PX4 autopilot and the Python API.\nHowever, when I want to retrieve camera images using AirSim ROS wrapper, I observe a memory leak, the GPU memory increases until it completely saturates.\nThe memory Ieak even occurs with a single camera (\"front_left_custom\"), it just take more time to saturate.\nBefore launching AirSim ROS wrapper:\n\nAfter launching AirSim ROS wrapper:\n", "type": "commented", "related_issue": null}, {"user_name": "yhabib29", "datetime": "Apr 6, 2022", "body": "I found a solution.\nMy Environment is packaged for Linux (No Editor), and it was launching in fullscreen. So I had to reduce the screen (Alt+Tab) to switch back to my terminal and launch AirSim ROS wrapper. When starting the ROS node, the GPU memory usage was increasing until it saturates.The workaround is to launch the Game using  argument, for instance:Now it works well !", "type": "commented", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jun 15, 2022", "body": "  Could you get synced images across 2 images when using stereo camera?", "type": "commented", "related_issue": null}, {"user_name": "yhabib29", "datetime": "Apr 6, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4440", "issue_status": " Closed\n", "issue_list": [{"user_name": "kutilanak", "datetime": "Mar 28, 2022", "body": "Hello guys, newbie here.I'm trying to follow this tutorial here\n\nwith his code here \nand his car model here\nbut there's two kind of error that I got when running the model:I just want the model to work well with AirSim just like the tutorial.I'm using python 3.6.9\nAirSim 1.6.0\nVS2019\nTensorflow 1.15\nUE 2.4bad cast error:WSAECONNREFUSED error:N/AThank you!", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 28, 2022", "body": " regarding the bad cast, seeing your stack trace, I think you are sending a False as vehicle name.\nRegarding the connection refuse, maybe your firewall is preventing it to connect?", "type": "commented", "related_issue": null}, {"user_name": "kutilanak", "datetime": "Mar 29, 2022", "body": "How can I solve the False?\nRegarding the firewall, I've click the \"Allow access\" when the notice pop up and tried turning off the firewall but the problems persist. Is it because its the older version of airsim?thank you for your response.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Mar 30, 2022", "body": "The API call you're doing is - \nYou cannot combine both  and , use either of the two.\nSuch as , or \nThere's the , and various examples in the scripts, please refer to them.", "type": "commented", "related_issue": null}, {"user_name": "kutilanak", "datetime": "Mar 30, 2022", "body": "It works for me! thank you so much! I'm closing this", "type": "commented", "related_issue": null}, {"user_name": "kutilanak", "datetime": "Mar 28, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 28, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kutilanak", "datetime": "Mar 30, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4375", "issue_status": " Closed\n", "issue_list": [{"user_name": "Villotadavid", "datetime": "Feb 25, 2022", "body": "Hello!\nI am trying to get two different images with the same instance of \"simGetImages\". For that I have written the following line:In the settings.json file I defined the following vehicle with its cameras as follows:But whenever I run the code I always get the following error:\nI have tried several options having Airsim documentation as reference but I haven´t managed to run the code properly.My current setup is:\nAirsim: 1.5.0\nPython: 3.9.7", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 26, 2022", "body": "Cameras should be present inside the  section, see ", "type": "commented", "related_issue": null}, {"user_name": "Villotadavid", "datetime": "Feb 26, 2022", "body": "Hi,\nThanks ! So I have corrected as you said but I still get the following error, now in runtime.I have tested in several setups and with different GPUs to make sure it is not because of limited processing capabilities.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 26, 2022", "body": "Optical Flow was added in  in Nov 2021, but 1.5.0 release is much earlier, in March. You should probably try the 1.7.0 pre release binaries, or build from source", "type": "commented", "related_issue": null}, {"user_name": "Villotadavid", "datetime": "Feb 28, 2022", "body": "Okay, so now I have downloaded some version 1.7.0 binaries and using the same code of line as in the main description I am getting the following error:\n\nI do not understand why I am getting this error if I am using only two arguments, the first as a list and the second for the vehicle name.", "type": "commented", "related_issue": null}, {"user_name": "Villotadavid", "datetime": "Mar 1, 2022", "body": "Finally, after doing some research I found out that this is a version error.\nWhen using v1.7.0. binaries, Airsim v1.6.0. should be used.\nAfter updating to the last version, everything worked fine.", "type": "commented", "related_issue": null}, {"user_name": "githjz", "datetime": "May 15, 2022", "body": "Could you elaborate about how you solved the error please ?\nYou switched from which version of Airsim to which? And what do you mean by \"When using v1.7.0. binaries, Airsim v1.6.0. should be used.\"?I ran into the same error using Airsim v1.7.0, so I wonder if there is any other  solutions.  ", "type": "commented", "related_issue": null}, {"user_name": "belongtothenight", "datetime": "Jun 4, 2022", "body": "I have encountered the same problem and successfully debugged it. As a result, I made two changes according to the author's comment above.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 25, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "Villotadavid", "datetime": "Mar 1, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4348", "issue_status": " Closed\n", "issue_list": [{"user_name": "s-h-o-t", "datetime": "Feb 11, 2022", "body": "I'm wondering how to disable the manual controls of the cars altogether. I am attempting to control them via the Python api and would like to use the free-cam mode to position the camera. However when positioning the camera with WASD and the arrow keys, both the cars that I am simulating respond to the keyboard commands as well. Is there a way to disable manual control entirely?N/ALet me know if you feel any extra details would be relevant and I can provide them.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 11, 2022", "body": "Can you try setting  param for each vehicle to 0 or something else, assuming that you don't have a remote controller attached.", "type": "commented", "related_issue": null}, {"user_name": "s-h-o-t", "datetime": "Feb 14, 2022", "body": "Thanks for the reply . I have tried setting  for each car but still see the same behaviour. I have tried with numerous values other than 0 as well.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 28, 2022", "body": "Sorry for the late reply, have you tried calling  for the car before positioning the cameras? You could call this in the beginning, and then wait for keyboard input in the script before proceeding to the actual testing.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 9, 2022", "body": "You can also comment  if you need the keyboard disabled at the beginning.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 23, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "microsoft", "datetime": "Jun 9, 2022", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 9, 2022", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4318", "issue_status": " Closed\n", "issue_list": [{"user_name": "qcsui", "datetime": "Jan 25, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Jan 26, 2022", "body": "i face the same problem ,can not accurate result,in my testing,object is moving by spline.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 3, 2022", "body": "Continue in  with zimmy", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 3, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4367", "issue_status": " Closed\n", "issue_list": [{"user_name": "chongjeelee", "datetime": "Feb 23, 2022", "body": "when run the environment ,i found some objects can not be detected always! i change the object position,and the problem also  occured ,so i want to know why the detection from camera is not very stable?this problem can directly result in the auto-testing about  my own detect algorithom's accuracy.\ni add the video below, above the image of video ,you can see the drone can not be detected all the time.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Feb 25, 2022", "body": "The problem is -most probably- related to the logic behind how detection is computed. See this:\nHere, the code sends random rays to 10 different points and checks whether any ray hits the actor (or, let's say, object). This is a kind of occlusion check, I think. However, the drawback is that when the object occupies less space in its bounding box, all rays likely miss the object.I don't know what the most effective solution is. I set the upper limit from 10 to 20 (or any higher number) in this loop:\n\nAnd it worked for me. Again this might not be the optimal solution. I hope someone can come up with a better fix.", "type": "commented", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Feb 27, 2022", "body": "thank you,bozcani!\nit does work for me,when i set the upper limit to 100.\nmaybe this is not a best method,but it can work,thank you again.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Feb 28, 2022", "body": "Great! Do you see any performance decrease when you set the upper limit to 100?", "type": "commented", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Mar 1, 2022", "body": "Hi,i can not find any performace decrease currently .\nbut i find another problem,the 2Dbox is not very close to the object...hhha", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Mar 1, 2022", "body": "Could you upload a sample image for this problem?", "type": "commented", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Mar 2, 2022", "body": "you can see that the 2d bounding box on the moving plane is not very close to plane's boundary...", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Mar 2, 2022", "body": "Ok, I see. I also encounter the same issue. I use segmentation images to refine object bounding boxes.I couldn't figure out why Detection API can't give tight boxes for some cases. I think we can open another issue for this", "type": "commented", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Mar 3, 2022", "body": "ok,you said right!", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Mar 15, 2022", "body": "Hi  and , I have filed  to track the issue raised about the detection APIs not giving tight boxes around some objects. Part of the reason for this is that the detection APIs rely on  for calculating the 2D box for each detection. As you can see , AActor::GetActorBounds may return a rectangle larger or smaller than the visible meshes attached to an actor depending upon the size and shape of the colliders attached to the actor as well as the presence of any invisible components.As an immediate work-around for this issue, I recommend adjusting the size of the colliders of any actor who you feel doesn't receive a tight box currently to match the size of the actor's visible meshes as closely as possible. If this doesn't produce desirable results, we may need to spend some time researching alternatives to AActor::GetActorBounds inside of UDetectionComponent::calcBoundingFromViewInfo. I have one potential solution mentioned inside my issue, but it may not be performant and other better solutions may be preferable here. Feel free to brainstorm alternatives with a comment inside .", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 24, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "chongjeelee", "datetime": "Feb 27, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Mar 15, 2022", "body": [], "type": "issue", "related_issue": "#4412"}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4275", "issue_status": " Closed\n", "issue_list": [{"user_name": "cumtchenchang", "datetime": "Jan 7, 2022", "body": "Can Airsim support fisheye model, such as Kananla Brandt 8 model?\nThank you", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Jan 31, 2022", "body": "See the discussion here: . There's a plugin called OpenCV LensDistortion which can do this. Currently AirSim does not include this plugin, so you'll need to edit AirSim's source to support this.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 31, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Jan 31, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4271", "issue_status": " Closed\n", "issue_list": [{"user_name": "ab3nd", "datetime": "Jan 4, 2022", "body": "AirSim Version/#commit: v1.6.0-windows\nUE/Unity version: 4.26\nautopilot version: N/A\nOS Version: Ubuntu 18.04.5 LTS (In WSL) and Windows 10 ProI have a settings.json that includes an external camera named \"FixedCamera1\", it's the same as the one in  . When I attempt to get the camera information for that camera, the session of the Unreal Editor that I'm using to run AirSim crashes.Little interactive session to trigger the crash:Beginning of Unreal's output once the crash is detected:", "type": "commented", "related_issue": null}, {"user_name": "ab3nd", "datetime": "Jan 4, 2022", "body": "In the file that it mentions, PIPCamera.cpp:Line 303 is the line to get the position of the camera.", "type": "commented", "related_issue": null}, {"user_name": "ab3nd", "datetime": "Jan 4, 2022", "body": "Found it.c.simGetCameraInfo(\"FixedCamera1\") -> SUDDEN DEATH\nc.simGetCameraInfo(\"FixedCamera1\", external=True) -> Information, which is indeed about cameras\"External\" is VERY not optional.", "type": "commented", "related_issue": null}, {"user_name": "ab3nd", "datetime": "Jan 4, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4263", "issue_status": " Closed\n", "issue_list": [{"user_name": "bozcani", "datetime": "Jan 3, 2022", "body": "AirSim  gives incorrect bounding boxes for custom added objects. The detected bounding boxes flicker across the frames even if the drone is stationary. Here is the first screenshot. All objects are detected by Object Detection API but note that some detections still don't have tight bounding boxes as that cylinders have.In the second image (just next frame), some detections disappear:\nThe detected bounding boxes appear and disappear randomly through frames. I tried with different settings (e.g., different flight paths, object types and sizes, object detection API parameters) but detection continues flickering. I don't think it is because of a processing power issue but it could be related with how object detection API is implemented.Related with  but not the same.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Jan 3, 2022", "body": "Not sure if you have already tried it, but can you increase the radius value using  to  a high value, say 1000m or what you feel is the max possible distance, to confirm it isn't due to the object being just at the edge", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Jan 4, 2022", "body": "Yes, I tried a wide range of radius values. It doesn't fix. I can confirm that it is not related with being at the edge.", "type": "commented", "related_issue": null}, {"user_name": "Zartris", "datetime": "Jan 4, 2022", "body": "So I narrowed down the \"problems\":First is the missing/flickering bounding boxes.\nThis problem occurs in the , To check if it's actually visible or hidden, we check against 8 extend points by using LineTraceSingleChannel (meaning we are raytracing to see if we hit the object or if it is occluded). If the actor is in camera view but didn't hit any point out of 8 extended points, then check against 10 random points. This introduces some problems as the object might be considered \"occluded\" by mistake if it occupies a low number of pixels in the bounding box because a ray likely misses the target object (like t-shape object).\nThe solution to this... is still unknown to me, but if you really want the bounding boxes, you can disable the aforementioned lines and you will remove the occlusion check, but it will not be flickering or missing. (So this needs to be fixed in the future).Secondly is the weird sized bounding boxes.\nI tracked this down to only be a problem when working with the character class, and a solution to this is to change \n to \nHereby setting the OnlyCollidingComponents option to true. For reference:The best way to explain this is to watch the video about  as he visually shows the box that we are hitting using the trace line.I'm not solving all your problems but I hope it helps for future reference.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Jan 4, 2022", "body": "Hey , thanks for the help!Setting  to  seems to solve the whole problem. By setting this, the actor bounds computed by Unreal seem more correct. Here is when  (current implementation of Airsim)\n\nWhen we set  3D box seems tighter.\nWhen the 3D box is tighter, rays in the loops don't miss the object. So it seems there is no need to change the loops after setting . The detection looks much better now. I will test it with more scenes locally and possibly create a MR for the fix.\n", "type": "commented", "related_issue": null}, {"user_name": "Gastastrophe", "datetime": "Jan 4, 2022", "body": "If you want a temporary fix, I use the segmentation camera to create bounding boxes. You can then use the leftmost, bottommost, etc. coordinates in the image that contain the color corresponding to the object in question to create a bounding box. It's slow, but the accuracy is perfect.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Jan 4, 2022", "body": "Yes, it is the workaround solution that I use so far. However, I also need GPS data of the detected objects, and it is possible only if Object Detection API works properly.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Jan 4, 2022", "body": [], "type": "pull", "related_issue": "#4270"}, {"user_name": "jonyMarino", "datetime": "Jan 4, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4252", "issue_status": " Closed\n", "issue_list": [{"user_name": "lnexenl", "datetime": "Dec 29, 2021", "body": "How to change the baseline of stereo cameras when requesting DisparityNomalized images in ComputerVision mode?I am using ComputerVision mode to gather some stereo pictures and corresponding disparity images with a baseline of 1 meter.OS: Manjaro 21.2.0 Qonos\nAirsim v1.6.0\nUnrealEngine 4.27.1\nsettings:I tried to modify , change  to , and re-compiled AirSim, then copied the Plugin folder to my Unreal project. But it seems didn't work. The baseline is still 25cm.Also, it seems that the build of AirSim didn't use this file?", "type": "commented", "related_issue": null}, {"user_name": "lnexenl", "datetime": "Dec 30, 2021", "body": "I have solved this problem with another method. I record DepthPerspective image instead, mapping the image to depth, and convert depth to disparity with known focal length and baseline.", "type": "commented", "related_issue": null}, {"user_name": "lnexenl", "datetime": "Dec 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4174", "issue_status": " Closed\n", "issue_list": [{"user_name": "sguttikon", "datetime": "Nov 20, 2021", "body": "As you can see from the output, when simulation is paused - the offset between car and camera position increased after sleep.\nExpected: the relative position should almost remain constant since vehicle moving in straight path and simulation is paused.\nMay be this is an old/known issue, simPause() seems to no pausing physics engine or these is some other issue.Code Snippet:", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Nov 21, 2021", "body": "Thanks a lot for the reproducible example! Could you try out  and see if it fixes the problem for you?", "type": "commented", "related_issue": null}, {"user_name": "sguttikon", "datetime": "Nov 21, 2021", "body": "Thank you , it seems your changes fixed this issue. :)", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 22, 2021", "body": "@suresh-guttikonda   This somehow solves the issue you had in driving the car with the keyboard after calling simPause() as you described in ?", "type": "commented", "related_issue": null}, {"user_name": "sguttikon", "datetime": "Nov 23, 2021", "body": " Unfortunately it didn't fix the , i tried the code snippet as mentioned in that issue. But still I can see x-position difference between left, center, right cameras even though i am using stereo camera setupcode snippetoutput:", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Nov 21, 2021", "body": [], "type": "pull", "related_issue": "#4175"}, {"user_name": "jonyMarino", "datetime": "Nov 22, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Dec 8, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/42", "issue_status": " Closed\n", "issue_list": [{"user_name": "morfeusys", "datetime": "Nov 12, 2019", "body": "Aimybox  could be invoked from everywhere of the application. Not only by clicking on the mic button, but also through the voice trigger event or any other trigger (camera, sensors, etc). Thus  should appear once a recognition event is fired.", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 28, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 28, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Jan 20, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4154", "issue_status": " Closed\n", "issue_list": [{"user_name": "nooreldeensalah", "datetime": "Nov 9, 2021", "body": "I'm using a pre-compiled binary of the simulator, specifically the  binary, I wanted to remove all moving vehicles and pedestrians from the environment, because I'm interested in capturing images without the vehicles on the road, nor the pedestrians.So my attempt was to call  on both the  and  objects.Calling  works as intended, it removes the pedestrians from the environment, but calling  causes the simulator to crash as shown in the next image: with the simulator crashing on  method call.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 10, 2021", "body": " v1.5 is suitable for you?", "type": "commented", "related_issue": null}, {"user_name": "nooreldeensalah", "datetime": "Nov 11, 2021", "body": "\nI apologize for the late reply,\nI've downloaded v1.5, and I think it's suitable for my purposes, because it has no pedestrians nor vehicles.But with that being said, feel free to keep or close the issue depending on if this bug can be fixed or not.", "type": "commented", "related_issue": null}, {"user_name": "nooreldeensalah", "datetime": "Nov 11, 2021", "body": "\nOne last thing, I had a that I was using to collect images, the script requires an external camera (what the script briefly does is; get the pose of the ground vehicle as it's moving, adjust the pose of the external aerial camera accordingly, then it captures images from the ground camera attached to the vehicle, and the aerial external camera).The script worked well for airsim v1.6.0, but it's now causing the simulator to crash, I've downgraded my airsim pip package to 1.5.0 and after some investigation, turned out that external cameras aren't supported in v1.5.0 as in  (which was added after the v1.5.0 release).Any tips on how to handle this case? (i.e. taking aerial images for the vehicle)", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Nov 18, 2021", "body": "Hi , thank you for reporting this issue. This is happening because car and bicycle paths in the city environment store a reference to the CityEnviron_NPCVehicleManager instance and call methods on it during their Tick() method, so destroying the CityEnviron_NPCVehicleManager instance immediately causes a nullptr exception. To remove the CityEnviron_NPCVehicleManager, you'll need to first remove all intersection logic, bicycle paths, and car paths. The following script works for me locally:", "type": "commented", "related_issue": null}, {"user_name": "nooreldeensalah", "datetime": "Nov 18, 2021", "body": "Hi ,Thanks for your reply, your script has worked for me as well.Feel free to keep this issue open or close it, hopefully this gets fixed or documented in a future release.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 10, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 18, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "microsoft", "datetime": "Nov 18, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 18, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4142", "issue_status": " Closed\n", "issue_list": [{"user_name": "imanf94", "datetime": "Nov 3, 2021", "body": "I want to get uncompressed images from image API but it returns compressed images. I tried to change the args and the settings.json to get uncompressed images but it couldn't help. I expect a 1280x720 image but the output is a 255x160ish image.The name of the camera is CAM0 and here is my code:", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Nov 4, 2021", "body": "I think the internal implementation uses the  settings only if the entire setting for the camera is missing. Since there are some settings present for , it's using those alongwith with default values of height, width, etc.  Try specifying the H,W as well in the  setting.\nChanging this behaviour inside the code might be a bit messy, and IMO it's easier to figure out the exact settings from one place rather than piecing them together from multiple, but upto the maintainers. In any case, the documentation should definitely be updated to clarify the intention", "type": "commented", "related_issue": null}, {"user_name": "imanf94", "datetime": "Nov 4, 2021", "body": "Thanks,  for your reply. This couldn't help am I'm still getting compressed images.\nHere is my new setting:And here is my code:I need to mention that if I change the Camera name to 1 (default front camera) as below I get uncompressed images. However, the pinch and the position is not applied to these images:\nAny idea how I can get uncompressed images from a camera with customized position and orientation?", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Nov 5, 2021", "body": "The  setting should also be in the same format as , something like -", "type": "commented", "related_issue": null}, {"user_name": "imanf94", "datetime": "Nov 7, 2021", "body": "Thanks, ! That fixed the issue.", "type": "commented", "related_issue": null}, {"user_name": "imanf94", "datetime": "Nov 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/267", "issue_status": " Closed\n", "issue_list": [{"user_name": "Adrianilloo", "datetime": "Apr 1, 2017", "body": "Hi,I followed the first example listed on the web to handle a command. I did that on my webserver and it isn't working: means, the callback isn't getting called (I ensured it by getting empty output after a logging, anyways my custom code wasn't being executed, despite not having pasted it here). The web uses HTTPS already due to a requirement by another library, and the mic has permissions:`voiceCommands = {\n'ping': function() {\n// code for my project\n}\n}if(annyang) {\nannyang.addCommands(voiceCommands)\nannyang.start()\n}`Note: I also tried with \"hello\" command, it didn't work aswell.Also, I made a simple log to check if the present \"annyang\" condition is entered, thus being initialized, and it is. I can speak the examples from the annyang web and it works without problem.\nWhat could it be? Am I doing something wrong?Many thanks.", "type": "commented", "related_issue": null}, {"user_name": "peterdillon", "datetime": "Apr 1, 2017", "body": "Have you made a fiddle or pen to troubleshoot?", "type": "commented", "related_issue": null}, {"user_name": "Adrianilloo", "datetime": "Apr 1, 2017", "body": "", "type": "commented", "related_issue": null}, {"user_name": "popbijoux", "datetime": "Apr 1, 2017", "body": "the only thing that REALLY worked for me was hosting the page on a github page. Getting HTTPS permission on your computer is not enough, you need to host it on a localhost (via python/terminal, I'm on a MAC) or just host it elsewhere.  I ran into this problem before with d3 where I also had to host it on github or it would not appear on the browser, I don't like doing localhost if I am working on it all the time. I just have a random page on a repository that I use to test everything voice-related.", "type": "commented", "related_issue": null}, {"user_name": "Adrianilloo", "datetime": "Apr 1, 2017", "body": "It turns out that the speech recognition has many difficulty to hear for some short words. Regarding 'hello', I was just not saying it loud enough (I'm using a webcam mic which I need to keep using instead of a headset, since I'm doing an interactive camera+mic pong game, and player should be able to play it being a bit away from screen and moving to control the game player block, so having to use a headset would be annoying).The case is, I need at least the word 'ping' to be handled in my minigame web, and it isn't being recognized. Is there anything to do here, like maybe calling some annyang functions to \"maximize\" hearing of short words like this? Otherwise if it's a mere inability of the SpeechRecognition, I don't know what could be done.Here you can check what I mean. Delete/change the 'es-ES' language setting if you want, but you should have the same problem. Instead, it seems the speech recognition works well saying loud phrases with certain length.", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Apr 8, 2017", "body": "It seems that annyang is working correctly here... the only issue is that Google's speech recognition engine incorrectly identifies these two words, as they are not very common (not many people just say \"ping\" as a sentence, so the Speech Recognition engine assumes it must be something else). You correctly identified that when you say a longer sentence it does work, as you are providing the Speech Recognition engine with some context, so it finds it easier to correctly recognize what you are saying.The only possible solution I can offer is to try different keywords, or phrases... the rest is out of annyang's hands.", "type": "commented", "related_issue": null}, {"user_name": "kylebakerio", "datetime": "Jun 20, 2020", "body": "Just wanted to mention that, since it seems annyang returns an array of things you may have possibly said, you could sort through those results and hunt for any indication that 'ping' is present in any possible interpretations, to select for that interpretation more aggressively. (Perhaps this feature was not available in 2017, and I'm suggesting this only after a first reading of the docs myself.)", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Apr 8, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/185", "issue_status": " Closed\n", "issue_list": [{"user_name": "wumpeth", "datetime": "Mar 29, 2016", "body": "I had the repeating microphone permission problem that this forum helped me fix, then I got an SSL and now it no longer even appears! Why not!", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Mar 29, 2016", "body": "Try clicking the video camera icon on the edge of Chrome's address bar, you may have it set there. Try also clicking the Manage microphone access button there to see if you have it set there.If this doesn't work, please provide more detail about your problem if you want people here to be able to help you. For example, you did not even mention if the permission is currently always on, or off.", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Mar 29, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/167", "issue_status": " Closed\n", "issue_list": [{"user_name": "Ogarunite", "datetime": "Feb 11, 2016", "body": "Hi, i've tried adding the examplecode to a tampermonkey userscript on chrome. However, every couple seconds chrome's popup window appears asking for microphone access. How can this be prevented to ask only once? I noticed that if i keep saying commands to the script, it won't question. But as soon as i'm quiet for about 5 seconds, the popup comes.`// ==UserScript==\n//           22\n//      \n//    Say a message out loud to say it into chat.\n//        :_\n//        :*\n//        :*\n//        *\n//        \n//        MIT\n//         monorail\n//        0.2\n// ==/UserScript==if (annyang) {\n// Let's define our first command. First the text we expect, and then the function it should call\nvar commands = {\n'show tps report': function() {\nconsole.log('called');\n},\n'split': function() {\nconsole.log('split');\n},\n'dash': function() {\nconsole.log('dash');\n},\n'fun': function() {\nconsole.log('fun');\n}\n};// Add our commands to annyang\nannyang.addCommands(commands);// Start listening. You can call this here, or attach this call to an event, button, etc.\nannyang.start();\n}`Another thing i'm wondering is, how can i speed up the process to recognize a word? There are only 2 words i would like recognized, they are \"dash\" and \"split\". I noticed it takes about 1-2 seconds before console.log() outputs it. Is it possible to slow the speed down to perhaps 0.5 seconds or less?", "type": "commented", "related_issue": null}, {"user_name": "jamesarm97", "datetime": "Feb 11, 2016", "body": "Same here just today. From what I read if you have chrome accessing an http site it always asks for permission but if https it will remember or ask once. I was adding in this to the magic mirror project and basically ran into this and stopped for now.", "type": "commented", "related_issue": null}, {"user_name": "Ogarunite", "datetime": "Feb 11, 2016", "body": "The website it's supposed to run on doesn't contain https. There must be a way to bypass this message popping up every few seconds. Maybe inject the script directly, or something?\nStart at document-start / body / end?", "type": "commented", "related_issue": null}, {"user_name": "rmilesson", "datetime": "Feb 11, 2016", "body": "You're running into a security measure by Chrome. Here you have a bunch of different solutions . They are probably not what you're looking for, but alas, this is what you have to work with.", "type": "commented", "related_issue": null}, {"user_name": "jamesarm97", "datetime": "Feb 11, 2016", "body": "Here is another that may be related but I only see values for do not allow and prompt. Maybe 1 is always allow?DefaultMediaStreamSetting (deprecated)\nDefault mediastream settingData type:Integer [Windows:REG_DWORD]Windows registry location:Software\\Policies\\Chromium\\DefaultMediaStreamSettingMac/Linux preference name:DefaultMediaStreamSettingSupported on:\nChromium (Linux, Mac, Windows) since version 22\nChromium OS (Chromium OS) since version 22\nSupported features:Dynamic Policy Refresh: Yes, Per Profile: YesDescription:Allows you to set whether websites are allowed to get access to media capture devices. Access to media capture devices can be allowed by default, or the user can be asked every time a website wants to get access to media capture devices.If this policy is left not set, 'PromptOnAccess' will be used and the user will be able to change it.\n2 = Do not allow any site to access the camera and microphone\n3 = Ask every time a site wants to access the camera and/or microphone\nExample value:0x00000002 (Windows), 2 (Linux), 2 (Mac)", "type": "commented", "related_issue": null}, {"user_name": "jamesarm97", "datetime": "Feb 11, 2016", "body": "I even found this but it didn’t seem to help:", "type": "commented", "related_issue": null}, {"user_name": "jamesarm97", "datetime": "Feb 11, 2016", "body": "So annoying. I have tried everything. Clicking the X instead of answering allow or deny makes the box go away. Then you can click the X on the camera icon on url and say always allow which adds an exception. This works only until you refresh the page even though the exception is in my privacy settings. I don't get it.", "type": "commented", "related_issue": null}, {"user_name": "asantos00", "datetime": "Mar 5, 2016", "body": "Are you serving the file with a webserver or directly from file? Chrome blocks some APIs when you serve directly from file (sorry if it is a stupid question)", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Mar 31, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5572", "issue_status": " Closed\n", "issue_list": [{"user_name": "00Markus0", "datetime": "Jul 8, 2022", "body": "Hi, I'm new to Carla and I encountered an issue when trying to spawn a RGB camera sensor while playing the Carla Simulator in Unreal EngineWhat I did:camera_init_trans = carla.Transform(carla.Location(x=-5,y=118,z=-200))\nblueprint = world.get_blueprint_library().find('sensor.camera.rgb')\nprint(blueprint)\nblueprint.set_attribute('image_size_x', '1920')\nblueprint.set_attribute('image_size_y', '1080')\nblueprint.set_attribute('fov', '110')\ncamera = world.spawn_actor(blueprint, camera_init_trans)After the last code cell, Unreal Engine crashes.\nThe weird thing is, when I try to use the same commands in the pre-built version of Carla, it works completely fine.\nDoes anybody know what I did wrong here?CARLA version: 0.9.13\nPlatform/OS: Windows 10 21H2Update: I found the log file and it gives me the following error (which it also throws when I try to package the whole project):[2022.07.06-07.45.31:336][890]LogTemp: Loaded OpenDrive file 'D:/carlaSim/Unreal/CarlaUE4/Content/Carla/Maps/OpenDrive/Town10HD_Opt.xodr'\n[2022.07.06-07.45.41:375][920]LogCarla: Spawning actor 'sensor.camera.rgb'\n[2022.07.06-07.45.41:375][920]LogLinker: Warning: Failed to read package file summary, the file \"../../../../../carlaSim/Unreal/CarlaUE4/Plugins/Carla/Content/PostProcessingMaterials/PhysicLensDistortion.uasset\" is unversioned and we cannot safely load unversioned files in the editor.\n[2022.07.06-07.45.41:375][920]LogLinker: Warning: The file ../../../../../carlaSim/Unreal/CarlaUE4/Plugins/Carla/Content/PostProcessingMaterials/PhysicLensDistortion.uasset was saved by a previous version which is not backwards compatible with this one. Min Required Version: 214  Package Version: 0\n[2022.07.06-07.45.41:375][920]LogLinker: Warning: Failed to read package file summary, the file \"../../../../../carlaSim/Unreal/CarlaUE4/Plugins/Carla/Content/PostProcessingMaterials/PhysicLensDistortion.uasset\" is unversioned and we cannot safely load unversioned files in the editor.\n[2022.07.06-07.45.41:375][920]LogLinker: Warning: The file ../../../../../carlaSim/Unreal/CarlaUE4/Plugins/Carla/Content/PostProcessingMaterials/PhysicLensDistortion.uasset was saved by a previous version which is not backwards compatible with this one. Min Required Version: 214  Package Version: 0\n[2022.07.06-07.45.41:375][920]LogUObjectGlobals: Warning: Failed to find object 'Material /Carla/PostProcessingMaterials/PhysicLensDistortion.PhysicLensDistortion'\n[2022.07.06-07.45.41:375][920]Error: CDO Constructor (SceneCaptureCamera): Failed to find Material'/Carla/PostProcessingMaterials/PhysicLensDistortion.PhysicLensDistortion'[2022.07.06-07.45.41:375][920]LogOutputDevice: Warning:Script Stack (0 frames):[2022.07.06-07.45.41:376][920]LogWindows: Windows GetLastError: Der Vorgang wurde erfolgreich beendet. (0)", "type": "commented", "related_issue": null}, {"user_name": "00Markus0", "datetime": "Jul 19, 2022", "body": "Fixed this issue by re-building the carla simulator", "type": "commented", "related_issue": null}, {"user_name": "00Markus0", "datetime": "Jul 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5395", "issue_status": " Closed\n", "issue_list": [{"user_name": "sbv198", "datetime": "Apr 28, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "sbv198", "datetime": "Apr 28, 2022", "body": "I have used camera RGB and radar i am getting vector 3D of each pixels generated by RADAR and for that i have used the code provided in carla documentation. i want to measure distance from an object using the image captured by camera. how to do thatany help is appreciatedthanks\nSagar Varvariya", "type": "commented", "related_issue": null}, {"user_name": "PatrickFSLin", "datetime": "Apr 29, 2022", "body": "the same question here...", "type": "commented", "related_issue": null}, {"user_name": "sbv198", "datetime": "Apr 29, 2022", "body": " I used following code to measure  average distance of detected objects. the following code you will find it on carla documentation as well.refer this documents as wellgood luck", "type": "commented", "related_issue": null}, {"user_name": "AmirMz1", "datetime": "Aug 21, 2022", "body": "Hello, thank you for your answer\nI want to get the distance to each car.\nCan you please guide me how to do this, what the radar sensor detects and how to set it only for cars.", "type": "commented", "related_issue": null}, {"user_name": "sbv198", "datetime": "Aug 22, 2022", "body": " Radar will detect obstacles infront of it and this code shown will show the detected points with its velocity, azimuth, altitude and longitudinal distance from radar.You will need to do trial and error for mostly to know which points to take into consideration.I think it is not possible to get the distance particularly from cars only. but my knowledge is limited.You can see the images below for radar detection and its values\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "sbv198", "datetime": "Jun 15, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5365", "issue_status": " Closed\n", "issue_list": [{"user_name": "TR-Monash", "datetime": "Apr 13, 2022", "body": "I'm trying to simulate some near-collision instances such as two vehicles passing close by. To do so, my plan was to setup two PID-controlled ego vehicles rigged with cameras navigating through custom waypoints in two adjacent lanes. These waypoints were to based on the waypoints already on the map and modified so that one set of waypoints comes closer to the other, as shown below.However, it seems that the waypoints are static and cannot be modified (they are always right in the middle of a lane) as their transform is not affected as expected. Is there any other way of achieving this?\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "JunadMQ", "datetime": "Jun 15, 2022", "body": "\"  Having the same issue as you described. May I know how did you resolve it. This is the link to my posted issue:", "type": "commented", "related_issue": null}, {"user_name": "TR-Monash", "datetime": "Jun 26, 2022", "body": " Sorry, wasn't able to do this. Tuning the PID parameters was the best I could do.", "type": "commented", "related_issue": null}, {"user_name": "GiantSeaweed", "datetime": "Aug 20, 2022", "body": " I also have similar questions about customized waypoints. Have you solved this? Thanks!", "type": "commented", "related_issue": null}, {"user_name": "TR-Monash", "datetime": "Apr 20, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5330", "issue_status": " Closed\n", "issue_list": [{"user_name": "makaveli10", "datetime": "Apr 6, 2022", "body": "Based on the  I tried to draw bounding boxes on parked cars. As mentioned in the issue that bb for parked cars are in world coordinates so, I assumed that we need to convert those to sensor coordinates which I did(replaced _vehicle_to_sensor with _world_to_sensor) by adding the following code in  :But these doesnt draw any box on the parked vehicles instead a cluster of bounding boxes at a single location.\n   As you guys have already closed this issue any help to get this sorted out is most welcome.\nAlso, in the closed issue its mentioned that the parked vehicles can also be converted to vehicle objects. I wonder If the feature has been implemented?\nThanks", "type": "commented", "related_issue": null}, {"user_name": "truncs", "datetime": "Apr 22, 2022", "body": "The extent is not in world coordinates. you can use a similar method to convert the extent to world coordinates by doing the following", "type": "commented", "related_issue": null}, {"user_name": "TR-Monash", "datetime": "May 13, 2022", "body": "Can confirm that  suggestion works with one small change in .\n should be changed to\n.", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "May 27, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5453", "issue_status": " Closed\n", "issue_list": [{"user_name": "AOOOOOA", "datetime": "May 19, 2022", "body": "Carla-version: 0.9.13\nOS: ubuntu 18.04Hi,I have two vehicles in the Carla world, and both of them have attached a lidar.\nThis is what I want to achieve:This is my procedure:However, after finishing the whole process, I plot the transformed bbox in point cloud of vehicle 2. I found out that the bbox_v2 has a considerable deviation shown below. The vehicle in red circle is vehicle_1.Does there any error in the process or do I have any misunderstanding about the coordinate transformation in Carla?", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "May 19, 2022", "body": "I found out that is because of  typo in my code and it is ok now.\n", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "May 31, 2022", "body": " Hi, I am trying to work on creating a synthetic dataset out of CARLA in the kitti dataset. And I am stuck in this issue . However, it seems that you have been able to create a 3d object detection dataset out of CARLA. If you could give me some pointers that would be great.\nThanks", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "Jun 1, 2022", "body": "I didn't totally get your stuck point. Where did you get your dataset?  It seems like you stuck at the lidar coordinate and camera coordinate transformation. There are some tools for visualizing the KITTI dataset, they have the function to project the lidar point on the image. You can use this function to check whether your transformation matrix is right or not.", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Jun 1, 2022", "body": " Yes, lidar and camera coordinate frame. As for the code I am writing it myself so, you mean tools like meshlab? or do you have reference to any other tool that might help project lidar points on image.\nthanks", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "Jun 1, 2022", "body": "You can try this Kitti-visualization-tool to project your lidar data to the camera image to check the correctness of your transformation matrix.", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "Jun 2, 2022", "body": "  I received the email about your response but cannot see on this page. It's weired.", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Jun 2, 2022", "body": " Weird. However, I wanted to ask about if you were successfully able to train a model on syntetic CARLA dataset(kitti format). If yes, what did you use? Seems like you used OpenPCDet with PointRCNN? How were the results?", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "Jun 2, 2022", "body": "I haven't tried to train a model, but I use openpcdet to infer on my dataset. The result is normal.", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Jun 2, 2022", "body": " okay. interesting. thanks. maybe it would be a good idea then to finetune on the custom carla dataset.", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "May 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5292", "issue_status": " Closed\n", "issue_list": [{"user_name": "makaveli10", "datetime": "Mar 28, 2022", "body": "Hello,\nI am trying to create a KITTI format dataset from CARLA. So, to do that I need to build the bounding box from ,  relative rotation of vehicle to camera and the loaction of vehicle  as done in .\nWith reference to , I am able to save the  location aswhere and for the relative_rotation_y:All the 3 things are saved using  to convert to KITTI format( I assumed this is the correct way?)Now, the issue is to build the box from the available information i.e. , ,  for which I use the traditional kitti method  where I dont see any bboxes being drawn with the points returned.So, I tried to draw to project the coordinates obtained from  using the saved ,  and   using this method:which does give me bounding boxes but totally disoriented:\nThanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Mar 28, 2022", "body": "Tried building the 3d bbox from ,  and  obtained using the method mentioned in the above.Using the above method,  and the methods mentioned above you should be able to reproduce the issue.\nI think I made some progress as the bbox on the ego vehicle is a little better ?  but its still too bad:\n", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Mar 28, 2022", "body": [], "type": "issue", "related_issue": "#5054"}, {"user_name": "makaveli10", "datetime": "May 24, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5003", "issue_status": " Closed\n", "issue_list": [{"user_name": "soo4826", "datetime": "Dec 20, 2021", "body": "CARLA version: 0.9.13\nPlatform/OS: Ubuntu 18.04 LTSHi.Carla's semantic camera, lidar sensor provides only one vehicle class including all of Cars, vans, trucks, motorcycles, bikes, buses, and trains.But I need a subdivided class, so I want to subdivide the class.In addition, the pedestrian class is expressed as a single pedestrian class, including cyclists, motorcyclists, and pedestrians on the sidewalk.Likewise, pedestrian classes are intended to be subdivided.I would like to change the source code so that I can subdivide the class, and I wonder if this is possible.And I want to receive lidar and camera data using ROS. (Should I change carla-ros-bridge source code?)The questions are summarized as follows.Of course, I know that Instance segmentation is supported in this update, so it would be possible to post-process the data by class.Please reply.Thank you.", "type": "commented", "related_issue": null}, {"user_name": "tianqi-wang1996", "datetime": "May 13, 2022", "body": " Hello, have you managed to achieve this function? Since I am also looking for a solution.Best", "type": "commented", "related_issue": null}, {"user_name": "soo4826", "datetime": "May 13, 2022", "body": "Yes.\nI modify some code for subdivide semantic classThere is guideline in this repo: Ask me question if you have problem!", "type": "commented", "related_issue": null}, {"user_name": "tianqi-wang1996", "datetime": "May 28, 2022", "body": "\nHello, could you please tell me how to reorganize the newly created folder as well as the original Vehicles folder.I am thinking of separating some vehicles from the Vehicles folder to the newly created Vans, Trucks, Motorcycles, and Bicycles folder.Which files should I include in the new folders apart from the ones named as the vehicle types?  Is it necessary to keep the folder structure the same as the Vehicles folder, e.g. put all the files in the 2Wheeled or 4Wheeled folder? (I have tried to do this, the compiling process succeeded but whenever I spawn such a vehicle, the simulator crashes.)", "type": "commented", "related_issue": null}, {"user_name": "soo4826", "datetime": "May 28, 2022", "body": "HiIn my purpose, I separate 2 wheeled vehicle into motorcycle(PTW, Powered Two Wheeler), Bicycle).\nAnd separate 4 wheeled vehicle into Commercial vehicle and VehicleI do not create new folder, I hard-coded to recognize each class.Heres's my Tagger.h file.Also,I dont't and new object into simulator, i just subdivide Vehicle class into (PTW, Bicycle, Commercial Vehicle, Veicle)If you do no solve your problem, I'll opn my private repo for subdividing vehicle class!Feel free to ask your question!!Thanks.", "type": "commented", "related_issue": null}, {"user_name": "tianqi-wang1996", "datetime": "May 29, 2022", "body": "\nHello, can you share the screen capture of your Vehicles folder, and the new folders inside it, such as the Bicycle folder? Did you just delete the 2wheeled and 4wheeled folder?", "type": "commented", "related_issue": null}, {"user_name": "tianqi-wang1996", "datetime": "May 29, 2022", "body": "\nI am sorry but I cannot see your tagger.h file, I guess I need to make some change in both Tagger.h and Tagger.cpp, right?My current folder structure:\nIn Tagger.h: (how should I code it?)\nIn Tagger.cpp:\n", "type": "commented", "related_issue": null}, {"user_name": "soo4826", "datetime": "Feb 3, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4877", "issue_status": " Closed\n", "issue_list": [{"user_name": "newengineeeer", "datetime": "Nov 12, 2021", "body": "What i did i download the exe from the github repository, pip installed all the requirements.txt and then ran tutorial.py and then it gave the error:and so i tried checking which line of code was causing the error and to my surprise it wasas i did a try-except on it to see if it was working or not.\nso to fix this i tried pip installing carla and then uninstalling it and then installing it again, i tried using carla 0.9.11 but i came across the same problem, i also manually set the port to 2000 and it still didn't connect but the weirdest part was manual_control.py is working and so is generate_traffic.py", "type": "commented", "related_issue": null}, {"user_name": "lzhhh93", "datetime": "Nov 25, 2021", "body": "On next line:\nclient.set_timeout(2.0)\nchange 2.0 to 5.0 and see how it works.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "amiraliyazdan", "datetime": "Jul 19, 2022", "body": "Hi, I have almost the same problem, but my scripts just worked perfectly fine yesterday, and for nearly no reason, they don't work today. I tried to change the client.set_timeout(2.0) to 5.0, but nothing has changed.Please help me out.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 26, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4953", "issue_status": " Closed\n", "issue_list": [{"user_name": "aslirey", "datetime": "Dec 7, 2021", "body": "Hello everyone,it gave  the error below when I executed tutorial.py example.PS C:\\Program Files\\carla\\PythonAPI\\examples> python tutorial.py\ncreated vehicle.ford.crown\nINFO:  Found the required file in cache!  Carla/Maps/TM/Town10HD_Opt.bin\ncreated sensor.camera.depth\nmoved vehicle to Location(x=84.055626, y=-60.723831, z=0.398577)\ncreated vehicle.micro.microlino\nTraceback (most recent call last):\nFile \"tutorial.py\", line 90, in \ncamera.listen(lambda image: image.save_to_disk('_out/%06d.png' % image.frame, cc))How can I fix it? Thanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "aslirey", "datetime": "Dec 7, 2021", "body": "When I ran a Command Prompt as administrator, The error went away.", "type": "commented", "related_issue": null}, {"user_name": "9527huang", "datetime": "Apr 1, 2022", "body": "hello , do you fix it?", "type": "commented", "related_issue": null}, {"user_name": "aslirey", "datetime": "Dec 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4949", "issue_status": " Closed\n", "issue_list": [{"user_name": "RocaPiedra", "datetime": "Dec 6, 2021", "body": "CARLA version: 0.9.13\nPlatform/OS: Windows 10Hello everybody,I would like to access the output sensor from carla, for now, I just need the video stream from the RGB sensor. I think the easier way to do it is through ROS bridge but as far as I know, it is not compatible with windows 10. I prefer Linux for development but my new laptop's NVIDIA drivers do not work with Ubuntu (the laptop is an aero15 with the RTX 3070 and I tried version 18.04 and 20.04). I also tried to capture the window in order to stream it to my code but the generated window has no name and I haven't been able to reference it properly using win32gui.Does anybody know the easier way to stream sensor data to other codes in Windows 10? For now, I only need it locally but in the future I expect to use a Jetson AGX Xavier to assist the data processing.Thanks in advance :)", "type": "commented", "related_issue": null}, {"user_name": "roque2205", "datetime": "Dec 16, 2021", "body": "I'd suggest that you create a script where you create a client, get the world, then get the camera you want to stream from and write a routine that get's the data from buffer and sends it to your DNN. Very much like it is done with pygame in the example manual_control.py.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "RocaPiedra", "datetime": "Apr 25, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4914", "issue_status": " Closed\n", "issue_list": [{"user_name": "mawi42", "datetime": "Nov 19, 2021", "body": "CARLA version: 0.9.13\nPlatform/OS: Windows 10\nProblem you have experienced: I read raw data and understand semantic class information (red channel) and expected in blue / green channel somehow the object ID. But when I compare such data against actor ID these are different. So how can I make the link to actor information.\nAnd the color conversion does not result in different colors for different instances / objects.Thanksmawi", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 30, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5070", "issue_status": " Closed\n", "issue_list": [{"user_name": "brscholz", "datetime": "Jan 14, 2022", "body": "Hi there,I want to retrieve image data from the rgb camera in full float precision. So I created my own pixel type with 4 float channels and a camera withEnable16BitFormat(true)\nFPixelReader::SendPixelsInRenderThread(*this, true)similar to the optical flow camera. When I retrieve the data from buffer as type np.half, I get the strangest array with nan and negative float values. I also wrote a method similar to the creation of the FakeImage in the OpticalFlowCamera, that is supposed to deliver the 8bit representation of my float image, but this one crashes with SegmentationFault.Hints on what I'm doing wrong would be highly appreciated.  Maybe you can help me out here?HDRCamera.cppHDRPixel in color.hHDRImageSerializer.cppBest regards\nRobert", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Jan 14, 2022", "body": "Okay, found the mistake by myself.\nI suggest renaming the method \"WriteFloatPixelsToBuffer_Vulkan\" to \"WriteFlowValuesToBuffer\" as this is what it really does...", "type": "commented", "related_issue": null}, {"user_name": "IbrahimYang", "datetime": "Mar 8, 2022", "body": "Hi brscholz,\nI'm still struggling with rendering HDR images in Carla. May you tell me your modification is working or not? If yes, may you share this core with us? Thanks！", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Mar 11, 2022", "body": "Hi, take a look at my repository, it's linked in the issue I mentioned above.", "type": "commented", "related_issue": null}, {"user_name": "manuelmaiorb", "datetime": "Jul 7, 2022", "body": "Hello! I have a question related to the pixel values. What the range of float values per channel should be? As far as I know, for 16-bit floats, Unreal Engine outputs values between 0.0f - 65535.0f. Does this also apply in your implementation?", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Jul 7, 2022", "body": "Hi Manuel, for the HDR camera we transfer float values, not uint_16. That means, our outputs range between 0 and the maximum float value for HDR defined by UE4. I'm not 100% sure about that exakt value, but it should be much closer to 1.0f than 65535.0f. You will need to apply some kind of tone mapping.I highly recommend going through the issue mentioned earlier in this issue, that's where  and me discussed our collaboration and all questions should be addressed. :)", "type": "commented", "related_issue": null}, {"user_name": "manuelmaiorb", "datetime": "Jul 8, 2022", "body": "Thank you for your answer! Meanwhile I have taken a look at what the range of values is for ETextureRenderTargetFormat::RTF_RGBA16f (which is set for the render target). According to , even in 16-bit float format, per pixel values can get values up to ~65000. I have tested your implementation and more specifically the problem I have is that if I raise the camera exposure compensation to ~30.0 (which I need to do for the renders which I want to obtain), the pixel values are saturating at exactly (R=100.0625, G=99.9375, B=100.00). Also, for checking the values, I have captured the results by saving images directly from Carla plugin, from the corresponding render target, and not from Python API.", "type": "commented", "related_issue": null}, {"user_name": "Domsall", "datetime": "Jul 11, 2022", "body": "Do you mean the defined value in ?\nI do not even know, if this needs to be defined at that point (in , nothing is defined).The values you get from the HDR camera come from  in . Back then, I debugged those values and found out that they are always between 0 and 1. You could do the same and output the values. The values are actually not changed when sent to the Client. That is then done in the Python API.\nPlease tell us, if we done something wrong.\nIn your example: Why shouldn't the values go into saturation?!", "type": "commented", "related_issue": null}, {"user_name": "manuelmaiorb", "datetime": "Jul 11, 2022", "body": "Hi ! The values are indeed between 0.0-1.0, but when the exposure compensation is raised (or any setting which affects luminance), these exceed 1.0, and according to the UE4 documentation, they should go up to about 2^16. Also, as a sanity check I captured an image with high Exposure Compensation in Unreal Engine using Scene Capture Component, with the results written in a RTF_RGBA16f Render Target, and in the pixel values are not saturated in this case.", "type": "commented", "related_issue": null}, {"user_name": "Domsall", "datetime": "Jul 11, 2022", "body": "A few things are not clear to me:", "type": "commented", "related_issue": null}, {"user_name": "manuelmaiorb", "datetime": "Jul 11, 2022", "body": "I did not suppose that something is wrong with the implementation. I just expected it to output something different. If you say that the default tone mapping can be changed, then I think I might give a try changing something there. Could you tell me where can I access these tone mapping configurations?", "type": "commented", "related_issue": null}, {"user_name": "Domsall", "datetime": "Jul 11, 2022", "body": "You can find the implemention to transform the HDR image into an 8 bit image in . The Python-Function is called .There are still some open issues and questions with the tone mapping / post processing from Unreal. Maybe we are also outputting the wrong data (Point 2 from above).\nSo: If you find some better way to output the values or/and can solve those issues, we would love to see your solution.", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Feb 18, 2022", "body": [], "type": "issue", "related_issue": "#4379"}, {"user_name": "brscholz", "datetime": "Mar 11, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5069", "issue_status": " Closed\n", "issue_list": [{"user_name": "marcusvinicius178", "datetime": "Jan 13, 2022", "body": "Hi i am using CARLA 0.9.6 with this tutorial:  to set up CARLA with APOLLO.I have done all the steps successfully however, in the last step ( ) Apollo does not displays the Carla03Town as in the tutorial\nA zoom:In tutorial the City: Town03 appears:How do  from Apollo in order to get the images from the simulator?Thanks in advance", "type": "commented", "related_issue": null}, {"user_name": "marcusvinicius178", "datetime": "Jan 13, 2022", "body": "I have solved partially this issue dowloading the Town06 and Town07 maps from here:And I have copied them and pasted to this folder inside Apollo folder repo: Afterward, the maps have appeared in Dreamview:\n I was not able to add the Town03 map in Dreamview (From tutorial). Where do I get and download the code folder from   map?Thanks in advance", "type": "commented", "related_issue": null}, {"user_name": "ldh2020", "datetime": "Jan 17, 2022", "body": "Town06 and Town07 maps are imported successfully. Why is Town03 map failed? The method is the same.", "type": "commented", "related_issue": null}, {"user_name": "marcusvinicius178", "datetime": "Jan 17, 2022", "body": "Yes now I am doing in other machine...it worked. I guess that my old machine was using the cache instead pull the new scenarios docker images....because previously I was using LGSVL simulator for other project.But :libGL error: No matching fbConfigs or visuals found\nlibGL error: failed to load driver: swrast\nX Error of failed request:  BadValue (integer parameter out of range for operation)\nMajor opcode of failed request:  152 (GLX)\nMinor opcode of failed request:  3 (X_GLXCreateContext)\nValue in failed request:  0x0\nSerial number of failed request:  101\nCurrent serial number in output stream:  102when launching the manual_control.py script inside docker..the libGL is not found. I tried numerous workarounds, reinstall nvidia-driver, install mese-drivers, vulkan drivers, add sim links: and also tried this approach: :`docker run -it \n-v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n--privileged \n-e DISPLAY=$DISPLAY \n-v /usr/lib/nvidia:/usr/lib/nvidia \n-v /usr/lib32/nvidia:/usr/lib32/nvidia \n--device /dev/dri \ncarla_apolloexport PATH=\"/usr/lib/nvidia/bin\":${PATH}\nexport LD_LIBRARY_PATH=\"/usr/lib/nvidia/usr/lib32/nvidia\":${LD_LIBRARY_PATH}`that would fix the issue, however this command did not work in my computer, because I have other nvidia-driver. I have tried to put my nvidia-driver instead...but no success. The command does not work. I am not that good to understand the Docker images flags. Do you know how the command from this git issue could be adjusted to my machine...I am using nvidia-driver 470?Or how to fix this issue? Did you have this libGL and swrast issue when you used Apollo+CARLA in docker? How to fix it?I would like a more recent tutorial of CARLA and Apollo do you know if is it available? version 0.9.6 is too old to install locally with ubuntu 20.04This issue is better detailed here:", "type": "commented", "related_issue": null}, {"user_name": "ldh2020", "datetime": "Jan 18, 2022", "body": "I tested the method with nvidia-410 was OK, but I didn't know 470 how to do. Can you launch manual_control.py in local instead of in the container?", "type": "commented", "related_issue": null}, {"user_name": "marcusvinicius178", "datetime": "Jan 25, 2022", "body": "I am going to close this issue as I have solved the map issues here: yes I could launch this script outside docker. However the other script needed to communicate with Apollo 5.0 cannot be launched outside docker: the \"run_bridge.py\" script. Because it imports modules which are built inside docker environment and are not available outside docker. I am still struggling with this issue. I have opened a new issue here:  in the proper channel. I will be very grateful if you can help me there ", "type": "commented", "related_issue": null}, {"user_name": "reylio06", "datetime": "Mar 8, 2022", "body": "Hello .\nI've been struggling for a while to run carla with Apollo and i've seen you've also struggled with this topic. I was trying to run it locally with a prebuilt version of Carla 0.9.6 and i had the same issue while trying to run the client from carla-apollo-bridge.\nRunning the client locally started my apollo dreamview but i had no data for camera view and sensors.\nI wonder if you can actually run it from a local build or how complicated it is to modify the images/containers in order to do so.\nThank you", "type": "commented", "related_issue": null}, {"user_name": "zhouyu102030", "datetime": "Sep 15, 2022", "body": "I would like to ask where can I get the map resources of town02?\nThank you.", "type": "commented", "related_issue": null}, {"user_name": "ldh2020", "datetime": "Sep 16, 2022", "body": "Look here.  ", "type": "commented", "related_issue": null}, {"user_name": "zhouyu102030", "datetime": "Sep 21, 2022", "body": "Thank you, I solved the problem.", "type": "commented", "related_issue": null}, {"user_name": "marcusvinicius178", "datetime": "Jan 25, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4836", "issue_status": " Closed\n", "issue_list": [{"user_name": "125FaddaDLe", "datetime": "Nov 4, 2021", "body": "Hello,I would like to know how to extract 3D bounding box data in the format of Kitti Dataset from 4 different cameras for a single instance.I would like to know if it is possible to generate a bounding box for a vehicle in all 4 camera FOV and extract the data and save it in the format of Kitti dataset.Carla Version: 0.9.12", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4833", "issue_status": " Closed\n", "issue_list": [{"user_name": "snowbrood", "datetime": "Nov 3, 2021", "body": "hi thereI am trying to find the camera location in world coordinates?I find the vehicle's coordinates bythis yields -85.21098327636719 , -126.87291717529297 , -0.05492006242275238camera attached to this vehicle ,s relative position (x=2.5 y=0 z=0.7)can I just add the camera's relative position and Vehicle coordinates to find the camera's location in real world?", "type": "commented", "related_issue": null}, {"user_name": "Madecu", "datetime": "Nov 18, 2021", "body": "Hi \nYes, in order to have the world position of the camera you just need to add its relative position with the world position of its parent, in your case, the vehicle world location.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "Madecu", "datetime": "Nov 18, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "Madecu", "datetime": "Nov 18, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 29, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4826", "issue_status": " Closed\n", "issue_list": [{"user_name": "jimmyw404", "datetime": "Nov 2, 2021", "body": " 0.9.12\n Ubuntu 20.04.2 LTS\n Benchmark performance is significantly lower in 0.9.12 vs previous releases\n Benchmarking performance to be similar between 0.9.12 vs previous releases\n Run PythonAPI/util/performance_benchmark.py on different versions with the same sensor configurations.\nDuring normal (non-benchmarked) operation I empirically noticed a significant performance drop on multiple machines when switching to CARLA 0.9.12. I downloaded packages for the latest CARLA (dev commit  as of Nov 1, 2021), CARLA 0.9.12 and all the previous versions and ran the performance_benchmark.py utility on them.Comparing the results in Town03 and 4x 1920x1080 camera I get the below performance:I configured the sensor options with identical parameters:I used ClearNoon as the weather preset\nI downloaded the packages from these tags:  and I'm using an NVIDIA RTX 2060 with driver version 470.63.01 on an i7-8700 with 32GB of RAM on Ubuntu 20.Benchmark output:\n\n\n\n\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "jimmyw404", "datetime": "Nov 12, 2021", "body": "Issue appears related toScaling up cameras, especially on a PC with a powerful GPU (Testing against an RTX 3080 Ti) does not increase GPU Utilization.", "type": "commented", "related_issue": null}, {"user_name": "jimmyw404", "datetime": "Nov 12, 2021", "body": "Also related to", "type": "commented", "related_issue": null}, {"user_name": "xqrdot", "datetime": "Nov 24, 2021", "body": "Can confirm a  drop for Camera performance between  and Identical sensor configurations, Town01, 1000 Samples.Setup information:\nUbuntu 18.04, RTX 2070 Max-Q, i7-8750H CPU @ 2.20GHz × 12", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4779", "issue_status": " Closed\n", "issue_list": [{"user_name": "cicciosab97", "datetime": "Oct 23, 2021", "body": "CARLA version: CARLA 0.9.8Platform/OS: Ubuntu 18.04.3 LTSPython: 3.7Problem you have experienced: I have experienced problems using Driving Benchmark, Cexp, and Scenario Runner. I believe that there may be problems related to the simulator. In practice, it happens that during the Benchmark, collisions with non-visible vehicles occur. When I start NoCrash Benchmark, there are vehicle collisions that the RGB-camera cannot detect. The vehicles with which the collision occurs appear to be those that were previously in the collision position.\nCould it be RGB camera rendering problem, or transmission of information from the simulator?", "type": "commented", "related_issue": null}, {"user_name": "glopezdiest", "datetime": "Dec 1, 2021", "body": "Hey , could you explain in more detail what exact commands are you running? Also, I don't understand the aim of the video, as it is it a 6 second normal drive through the lane.", "type": "commented", "related_issue": null}, {"user_name": "cicciosab97", "datetime": "Dec 1, 2021", "body": "Yes.  this is NoCrash Benchmark from , but I used it in combination with CARLA 0.9.8. I also used the Scenario Runner package found from the ghithub repository. It is a matter of 6 seconds of driving along the lane, but here the episode ends by collision. The video ends at the collision. I think this is a sensor rendering issue. Effectively analyzing the non-player agents through the measurements of the scene, a vehicle is actually present at the collision point, but it is not visible in the RGB and Depth cameras. This is not an isolated case, because all the episodes end for the same reason. Therefore the Benchmark is falsified.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4778", "issue_status": " Closed\n", "issue_list": [{"user_name": "AhmedAredah", "datetime": "Oct 22, 2021", "body": "I am working on a project that requires VR through HeadMounted Display.\nI found Unreal Engine supports HMD through VR camera, my question is How to attach this VR camera to a pedestrian?Thank you\nOS: Windows 10\nCarla version 0.9.12", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "Madecu", "datetime": "Nov 18, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4777", "issue_status": " Closed\n", "issue_list": [{"user_name": "DonadiHarshith", "datetime": "Oct 22, 2021", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "DonadiHarshith", "datetime": "Oct 22, 2021", "body": "import glob\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport cv2try:\nsys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\nsys.version_info.major,\nsys.version_info.minor,\n'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\nexcept IndexError:\npassimport carlaIM_WIDTH = 640\nIM_HEIGHT = 480def image(image):\nmatrix_representational_data = np.array(image.raw_data)\nreshape_of_image = matrix_representational_data.reshape((IM_HEIGHT, IM_WIDTH, 4))\nlive_feed_from_camera = reshape_of_image[:, :, :3]\ncv2.imshow(\"\", live_feed_from_camera)\ncv2.waitKey(1)\nreturndef camera(get_blueprint_of_world):\ncamera_sensor = get_blueprint_of_world.find('sensor.camera.rgb')\ncamera_sensor.set_attribute('image_size_x', f'{IM_WIDTH}')\ncamera_sensor.set_attribute('image_size_y', f'{IM_HEIGHT}')\ncamera_sensor.set_attribute('fov', '70')\nreturn camera_sensordef car_control():data = []\nactor_list = []\ntry:\nclient = carla.Client('127.0.0.1', 2000)\nclient.set_timeout(2.0)\nworld = client.get_world()\nget_blueprint_of_world = world.get_blueprint_library()\ncar_model = get_blueprint_of_world.filter('model3')[0]\nspawn_point = (world.get_map().get_spawn_points()[1])\ndropped_vehicle = world.spawn_actor(car_model, spawn_point)finally:\nprint('destroying actors')\nfor actor in actor_list:\nactor.destroy()\nprint('done.')This is my code:-import glob\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport cv2try:\nsys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\nsys.version_info.major,\nsys.version_info.minor,\n'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\nexcept IndexError:\npassimport carlaIM_WIDTH = 640\nIM_HEIGHT = 480def image(image):\nmatrix_representational_data = np.array(image.raw_data)\nreshape_of_image = matrix_representational_data.reshape((IM_HEIGHT, IM_WIDTH, 4))\nlive_feed_from_camera = reshape_of_image[:, :, :3]\ncv2.imshow(\"\", live_feed_from_camera)\ncv2.waitKey(1)\nreturndef camera(get_blueprint_of_world):\ncamera_sensor = get_blueprint_of_world.find('sensor.camera.rgb')\ncamera_sensor.set_attribute('image_size_x', f'{IM_WIDTH}')\ncamera_sensor.set_attribute('image_size_y', f'{IM_HEIGHT}')\ncamera_sensor.set_attribute('fov', '70')\nreturn camera_sensordef car_control():data = []\nactor_list = []\ntry:\nclient = carla.Client('127.0.0.1', 2000)\nclient.set_timeout(2.0)\nworld = client.get_world()\nget_blueprint_of_world = world.get_blueprint_library()\ncar_model = get_blueprint_of_world.filter('model3')[0]\nspawn_point = (world.get_map().get_spawn_points()[1])\ndropped_vehicle = world.spawn_actor(car_model, spawn_point)finally:\nprint('destroying actors')\nfor actor in actor_list:\nactor.destroy()\nprint('done.')", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": " can you provide some more details about your aims?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4776", "issue_status": " Closed\n", "issue_list": [{"user_name": "hesittia", "datetime": "Oct 21, 2021", "body": "CARLA version:    0.9.10\nPlatform/OS:     ubuntu18.04\n\nWhat you expected to happen:  i wanna have a look at my cars as a spectator ,but it told me that it can not find 'spectator'The following is the output of the system:\nFile \"basic_api.py\", line 78, in main\nspectator = world.get_spectator()\nRuntimeError: internal error: unable to find spectatorI don not have an idea about how to solve that, install some package? or import some lib? Thank you for helping me ,please give me some suggestion.And here is the whole script i try to play:import carla\nimport os\nimport randomdef main():\nactor_list = []\nsensor_list = []if  == '':\ntry:\nmain()\nexcept KeyboardInterrupt:\nprint(' - Exited by user.')", "type": "commented", "related_issue": null}, {"user_name": "hesittia", "datetime": "Oct 21, 2021", "body": "RuntimeError: internal error: unable to find spectator (world.get_spectator()) ", "type": "commented", "related_issue": null}, {"user_name": "AOOOOOA", "datetime": "Dec 27, 2021", "body": "Hi,I have also met this problem. And when I ran some example code, it also shows this error. How do you solve this?Thanks!", "type": "commented", "related_issue": null}, {"user_name": "hesittia", "datetime": "Oct 21, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4770", "issue_status": " Closed\n", "issue_list": [{"user_name": "onyx22574", "datetime": "Oct 21, 2021", "body": "Can be used CARLA for personal robots simulation as well aka vacuum cleaners etc...", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": " yes CARLA can be used for any robotic application. CARLA simulates many of the sensor types that autonomous robots use including cameras, depth cameras, radar, lidar and collision detectors and all the data they stream is accessible through the API. CARLA's assets, the maps and all the details in them (roads, building exteriors, vehicles, etc.) are aimed at autonomous driving applications. However, it is entirely possible to create and load your own assets (for example domestic building interiors) that would be relevant to your application and load them into CARLA. That way you can exploit all of CARLA's inbuilt simulation functionality to simulate sensor data in your own environments. CARLA also provides the ROS bridge; , for integration with robotics applications. here is the information on the different types of sensors simulated by CARLA and refer to the asset and map documentation to learn how to import your own environments into CARLA. ", "type": "commented", "related_issue": null}, {"user_name": "thejeshk", "datetime": "Jan 14, 2022", "body": "Hi  I am interested in you point: Do you have any uses cases ? That would really helpHi  Were you able to figure out ?:| the usage of ROS/ROS2 with CARLA", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4764", "issue_status": " Closed\n", "issue_list": [{"user_name": "joeking11829", "datetime": "Oct 19, 2021", "body": "Hi Carla Teams,CARLA version: 0.9.12\nPlatform/OS: Ubuntu 18.04I want to retrieve data from camera.rgb when replay the record.\nthe rgb images has some strange performance.Every images retrieve from replaying have same problem.\nThe moving vehicles produce fuzzy image.Any idea to fix that ?\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": " could this be an issue with motion blur parameters?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "yuvalHG", "datetime": "Nov 30, 2021", "body": [], "type": "issue", "related_issue": "#4761"}, {"user_name": "stale", "datetime": "Apr 19, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4761", "issue_status": " Closed\n", "issue_list": [{"user_name": "mrcoee", "datetime": "Oct 18, 2021", "body": "I currently would like to generate a few thousand images of random traffic scenes. For this purpose I randomly spawn pedestrians and vehicles, make an rgb and an sematic segmantation image, and despwan every spawned objects afterwards. Unfortunately I occasionally get partially/fully distored vehicles and transparent pedestrians:The RGB sensor attributes are:\nimage_size_x :1920\nimage_size_x: 1080\nfov: 110\nsensor_tick: 1I'm using Carla 0.9.11 and 0.9.12 on Ubuntu18.04.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": " it may be that the camera catches the vehicles or the pedestrians as they are being spawned or destroyed. Some operations CARLA are multithreaded and asynchronous, including the operations that render sensor images, therefore one thread may be spawning the vehicle while another thread samples the sensor image.Are you using CARLA in synchronous mode? If not, this could help solve the problem. If you are already using synchronous mode, I would then suggest dividing your process into spawn cycles and image cycles. i.e. spawn hundreds of vehicles and pedestrians, tick() numerous times, then randomly place your cameras and take the images, then rinse and repeat.If the problem persists, please reopen this issue.", "type": "commented", "related_issue": null}, {"user_name": "yuvalHG", "datetime": "Nov 30, 2021", "body": " I work with synchronous mode (Carla 9.11), first spawning all actors and later after applying auto pilot to each, snap images from RGB camera on the Ego vehicle and same problem is observed.It seems that this issue of fuzzy images / not fully rendered vehicles is repeatedly reported, here are some examples:Is there any walkaround for this issue?\nIs it due to improper chose of camera parameters?Thanks\nYuval", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4757", "issue_status": " Closed\n", "issue_list": [{"user_name": "tamas-visy", "datetime": "Oct 16, 2021", "body": "Specifically 0.9.10Windows 10The lambda listening to a camera () receives data with a large delay (about 500-1000ms). Other sensors seem to be unaffected.A lesser delay.Spawn an RGB camera, set the lambda with , then display images as soon as they arrive. In 0.9.10, the data has a delay compared to the simulation spectator camera or other sensors' data.In other versions of CARLA (tested with 0.9.11 and 0.9.12) there is no noticeable delay.", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Oct 22, 2021", "body": "Hi, I suppose you do use synchronous mode?", "type": "commented", "related_issue": null}, {"user_name": "tamas-visy", "datetime": "Oct 24, 2021", "body": "No, it's running in asynchronous mode (with a variable time step).I have not encountered other issues with this previously, so I did not want to change the default behaviour.", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Oct 25, 2021", "body": "That's relatable... In my experience images made at the exact point in time I specify requires synch mode, but I have never encountered such large delays. For problem identification it might still be good to know if this happens in synch mode, too. :)", "type": "commented", "related_issue": null}, {"user_name": "tamas-visy", "datetime": "Oct 31, 2021", "body": "I found this piece of information in the :What I experience in  is a far longer delay than what \"a couple of frames\" would imply. Anyways, looks like synchronous mode could solve this.I won't have time to do proper testing in synch mode, but I might revisit this issue later. Until then, I'll be using version  (I don't need specifically ).", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": " the delay from sensors depends heavily on hardware configuration. The delay of a couple of frames is presuming very high performance graphics hardware (and also CPU). If you are using high performance GPUs from older generations or commodity graphics hardware, this would explain why your delay might be longer. Even though we do make the best of effort to optimise, CARLA is necessarily highly demanding on graphics hardware.Can you give us some details of your hardware configuration?", "type": "commented", "related_issue": null}, {"user_name": "tamas-visy", "datetime": "Dec 8, 2021", "body": "I'm using a regular CPU and GPU. However, please note that as per my original postThe issue exists in specifically 0.9.10, in the newer versions I tested I did not encounter a noticeable delay. However, I did not find any patch notes about a related fix or update. Also, I do not recall such a delay in a pre-0.9.10 version on my configuration either.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 25, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4682", "issue_status": " Closed\n", "issue_list": [{"user_name": "mattangus", "datetime": "Sep 25, 2021", "body": "Is there any way to force the loading of assets near a RGB camera sensor? I'm trying to generate images from  random locations but if the random location is too far away from the \"main\" camera then I get images that are like the following with no terrain or roads loaded (taken from Town06):I load the sensor like this:Am I doing something wrong here?", "type": "commented", "related_issue": null}, {"user_name": "Madecu", "datetime": "Oct 14, 2021", "body": "It seems that the camera in this image is below the ground. That is why you can't see the terrain. From \"behind\", the mesh is transparent. I would recommend setting a minimum altitude.\nYou can change the render distance in unreal engine, so the assets that are far away from the main camera are forced to be rendered regarding any auto-optimization unreal engine makes.", "type": "commented", "related_issue": null}, {"user_name": "MattRowe18", "datetime": "Nov 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1712", "issue_status": " Closed\n", "issue_list": [{"user_name": "LewsTherin511", "datetime": "Nov 3, 2021", "body": "I'm using a script to run pose estimation on the video stream from a camera, and I wanted to try improving the inference speed. Currently, I'm using:And this the code I'm using.I wanted to try a quantization of these models and I was referring to . So, I tried something like this:But it's returning the error:In the page I linked, it's specified that 'data_shapes' should be a \"List of DataDesc, required if calib_data is not provided\". How am I supposed to use it precisely?", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Feb 2, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Feb 2, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Feb 9, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1293", "issue_status": " Closed\n", "issue_list": [{"user_name": "shachargluska", "datetime": "May 10, 2020", "body": "I have a few issues/questions regarding training FCN using gluoncv:", "type": "commented", "related_issue": null}, {"user_name": "Aktcob", "datetime": "May 12, 2020", "body": "Q1: crop_size & base_size:  \nQ2: Usually, the result is close with symmetrical or asymmetrical size.\nQ3: The code is FCN32 model. If u want to use FCN16 or FCN8, u should do it by yourself. Change the code about head.", "type": "commented", "related_issue": null}, {"user_name": "bryanyzhu", "datetime": "May 26, 2020", "body": " Thank you for your answer, they are great. Just to add a few more details.Q1: As shown by Aktcob@, base size is a size used during data augmentation for random short side jittering. Crop size is the final input to the network. So for Cityscapes with a resolution of 1024x2048. The base size is the short side, which is 1024. And most people use 768x768 patches to train the network, which makes the crop size of 768.Q2: If you want to do inference on asymmetrical size, just specify height and width . Crop size only works when you didn't specify height and width.", "type": "commented", "related_issue": null}, {"user_name": "bryanyzhu", "datetime": "May 26, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bryanyzhu", "datetime": "May 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1239", "issue_status": " Closed\n", "issue_list": [{"user_name": "la-cruche", "datetime": "Apr 1, 2020", "body": "Hi,\nthe field of depth perception from images is getting traction. For example:I'd love gluoncv to make this domain of vision more accessible by providing pre-trained models and tutos. I have a slight preference for monocular models, which intuitively look easier to implement in the field; but open to learn the SOTA from you !", "type": "commented", "related_issue": null}, {"user_name": "la-cruche", "datetime": "Jul 24, 2020", "body": "Depth models seem to be making their way in gluoncv  ! big news! closing this issue", "type": "commented", "related_issue": null}, {"user_name": "la-cruche", "datetime": "Jul 24, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1038", "issue_status": " Closed\n", "issue_list": [{"user_name": "dbsxdbsx", "datetime": "Nov 13, 2019", "body": "For this tutorial: .\nI tried GPU, but failed with problems like :It seems  a problem with opencv, but why it happened when gpu is used?I am using latest gluoncv with mxnet 1.5 gpu cuda10 on win10.", "type": "commented", "related_issue": null}, {"user_name": "hetong007", "datetime": "Nov 25, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "dbsxdbsx", "datetime": "Nov 26, 2019", "body": " ，yes , for CPU, everything is OK.  And  I guess the problem is somehow related to GPU can't load multi models at same time.", "type": "commented", "related_issue": null}, {"user_name": "hetong007", "datetime": "Nov 26, 2019", "body": "I don't have much experience with Windows + GPU unfortunately. My suggestion would be", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 30, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 30, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 7, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/589", "issue_status": " Closed\n", "issue_list": [{"user_name": "Jian-Yin-Shine", "datetime": "Jan 24, 2019", "body": "I use the lst description file to make my own training set.\nThere's no problem running a loop, but then there were problems.INFO:root:Namespace(batch_size=2, dataset='camera', epochs=30, gpus='0,1', log_interval=100, lr=0.001, lr_decay=0.1, lr_decay_epoch='13,20,24', lr_warmup=-1, mixup=False, momentum=0.9, network='resnet50_v1b', no_mixup_epochs=20, num_workers=2, resume='', save_interval=1, save_prefix='faster_rcnn_resnet50_v1b_camera', seed=233, start_epoch=0, val_interval=1, verbose=False, wd=0.0005)\nINFO:root:Start training from [Epoch 0]\nINFO:root:[Epoch 0] Training cost: 19.745, RPN_Conf=0.434,RPN_SmoothL1=0.026,RCNN_CrossEntropy=0.512,RCNN_SmoothL1=0.058\nINFO:root:[Epoch 0] Validation:\ncamera=0.00146627565982\nmAP=0.00146627565982\nINFO:root:[Epoch 0] mAP 0.00146627565982 higher than current best [0] saving to faster_rcnn_resnet50_v1b_camera_best.params\nINFO:root:[Epoch 0] Saving parameters to faster_rcnn_resnet50_v1b_camera_0000_0.0015.params\nTraceback (most recent call last):\nFile \"train_faster_rcnn.py\", line 463, in \ntrain(net, train_data, val_data, eval_metric, ctx, args)\nFile \"train_faster_rcnn.py\", line 377, in train\ncls_pred, box_pred, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box)\nFile \"/home/users/xxxx/anaconda2/lib/python2.7/site-packages/mxnet/gluon/block.py\", line 541, in \nout = self.forward(*args)\nFile \"/home/users/xxxx/anaconda2/lib/python2.7/site-packages/mxnet/gluon/block.py\", line 908, in forward\nreturn self._call_cached_op(x, *args)\nFile \"/home/users/xxxx/anaconda2/lib/python2.7/site-packages/mxnet/gluon/block.py\", line 801, in _call_cached_op\nassert fmt == self._in_format, \"Invalid input format\"\nAssertionError: Invalid input format", "type": "commented", "related_issue": null}, {"user_name": "Jian-Yin-Shine", "datetime": "Jan 24, 2019", "body": "I didn't rewrite the training set and the verification set to hybridize.", "type": "commented", "related_issue": null}, {"user_name": "szza", "datetime": "Aug 31, 2019", "body": "do you resolve the problem?", "type": "commented", "related_issue": null}, {"user_name": "Jian-Yin-Shine", "datetime": "Jan 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/huihut/Facemoji/issues/8", "issue_status": " Closed\n", "issue_list": [{"user_name": "Rossichan", "datetime": "Mar 12, 2019", "body": "ArgumentException: The output Mat object has to be of the same size\nOpenCVForUnity.Utils.webCamTextureToMat (UnityEngine.WebCamTexture webCamTexture, OpenCVForUnity.Mat mat, UnityEngine.Color32[] bufferColors) (at Assets/Plugins/OpenCVForUnity/org/opencv/unity/Utils.cs:665)", "type": "commented", "related_issue": null}, {"user_name": "huihut", "datetime": "Mar 13, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "Rossichan", "datetime": "Mar 15, 2019", "body": "thanks，i got it!", "type": "commented", "related_issue": null}, {"user_name": "huihut", "datetime": "Mar 16, 2019", "body": "ok, I closed the issue.", "type": "commented", "related_issue": null}, {"user_name": "huihut", "datetime": "Mar 16, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/fairseq/issues/2570", "issue_status": " Closed\n", "issue_list": [{"user_name": "MorrisXu-Driving", "datetime": "Sep 4, 2020", "body": "I am trying to fine-tune the wav2vec_small.pt model based on my own dataset. However, as the training goes on, the wer cannot go down further as it reach 100.\n\n\n\n\nCould anyone tell me what could be the potential mistake and how to solve it?", "type": "commented", "related_issue": null}, {"user_name": "lematt1991", "datetime": "Sep 8, 2020", "body": "Can you please provide a reproducible example including the full command that you ran", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Sep 9, 2020", "body": "can you take one of the checkpoints and decode it to see what the model is predicting? your UER is also 100 which means it can't predict any letters at allyou can also hack ctc criterion and just print out some samples during validationalso if you can share the entire training log i can take a look real quick", "type": "commented", "related_issue": null}, {"user_name": "luweishuang", "datetime": "Sep 9, 2020", "body": "I encounted the same question and the \"pred_units_arr\" in fairseq/criterions/ctc.py kept  the null during training. Here is my training log\n", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Sep 9, 2020", "body": "I looked at the log and i see you are printing out pred_units_arr. this is only done during validation to make training reasonably quick (that is why its not populated during training)i guess you are working with mandarin, and have a much larger vocab (8583 units?). I am not sure how much data you are fine-tuning on, but if it some small amount it may not work with a vocab this big. You might want to use something like phones instead and then decode to words later. You can also try the seq2seq arch (wav2vec_seq2seq instead of wav2vec_ctc) but you will have to tune hyper params (and use a bigger fine-tuning dataset)", "type": "commented", "related_issue": null}, {"user_name": "haythemdhieb", "datetime": "Nov 16, 2020", "body": "Any solution. I have the same problem. The training stops when   WER  reaches 100.", "type": "commented", "related_issue": null}, {"user_name": "AdrianVandierAst", "datetime": "Dec 16, 2020", "body": "Hello :)\nWe face the same problem but with librispeech dataset. We use the default configs (base_100h or base_960h).\nWhen using base_960h with complete librispeech dataset everything is fine, the raw_wer is decreasing slowly to around 8. But when using base_100h with only test-clean-100, raw_wer is always above 100. We tried to modify the config in a lot of ways and we never succeed to get below 100.What can be the reason ? Any clue on that 100 threshold ?", "type": "commented", "related_issue": null}, {"user_name": "maggieezzat", "datetime": "Jan 24, 2021", "body": " excuse me but what does UER exactly stand for?", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Jan 24, 2021", "body": "unit error rate - the error rate for whatever units the model is predicting (letters, BPE tokens, etc whatever) as opposed to \"word error rate\" where these units are composed into words before evaluating the error rate. usually the unit error rate is significantly lower, and it is also more appropriate for languages where space tokenization is problematic (e.g. many asian languages)", "type": "commented", "related_issue": null}, {"user_name": "maggieezzat", "datetime": "Jan 27, 2021", "body": "I am facing the same problem unfortunately. Using the XLSR pretrained model to finetune on arabic dataset. The UER as well as the WER is 100", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Jan 27, 2021", "body": "try setting freeze_finetuning_updates to 0 so you can see progress quicker.", "type": "commented", "related_issue": null}, {"user_name": "muntasir2000", "datetime": "Feb 6, 2021", "body": "I am also facing the same issue when fine tuning the XLSR-53 model on Bengali Babel data (82hr). I'm using base_100h config. But the WER and and UER gets stuck at 100 after epoch 2. But the loss is decreasing.My config and log  -\nI have also tried using other parameter configurations (max_tokens, mask_channel_prob, freeze_finetune_updates to zero) etc. Any help appreciated.", "type": "commented", "related_issue": null}, {"user_name": "zxpan", "datetime": "Feb 27, 2021", "body": "Anyone have any insights how to proceed with the issue (valid wer stays 100)?", "type": "commented", "related_issue": null}, {"user_name": "zxpan", "datetime": "Feb 27, 2021", "body": "seem dict.ltr.txt matters here, how the order (letter) of symbol plays the role here?   ", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Mar 1, 2021", "body": "try printing the output of the checkpoint and see what it is producing. unfortunately it is difficult to help since i dont know your setup, you will have to debug :(", "type": "commented", "related_issue": null}, {"user_name": "zxpan", "datetime": "Mar 4, 2021", "body": "just FYI: with latest fairseq code, I am now seeing the training in right direction (loss decreasing)", "type": "commented", "related_issue": null}, {"user_name": "vigneshgig", "datetime": "Mar 13, 2021", "body": "Hi, I am also getting a valid WER of 100. And also I checked the predicted data, below as you see I am getting a single character.\nBOXER ENGINE\tO\nBOOST\tO\nBLUETOOTH\tO\nBLIND SPOT\tI\nBETWEEN\tI\nBEFORE NOON\tI\nBEFORE LUNCH\tR\nBED LINER\tE\nBASE MODEL\tI\nBASE\tO\nBALENO ZETA CVT\tR\nBALENO ZETA\tO\nBALENO SIGMA\tO\nBALENO DELTA CVT\tR\nBALENO DELTA\tE\nBALENO ALPHA CVT\tE\nBALENO ALPHA\tO\nBACKUP CAMERA\tO\nAWB\tO\n\n\nThanks", "type": "commented", "related_issue": null}, {"user_name": "vigneshgig", "datetime": "Mar 15, 2021", "body": "Hi , I installed latest fairseq and pytorch 1.7.0 with cuda 10.2 in python 3.7.9 version.\nNow the valid_wer is decreasing and also it starts from 83~ .\nand also trained loss start from 54.0 comparing  older version which starts from 117~\n", "type": "commented", "related_issue": null}, {"user_name": "vigneshgig", "datetime": "Mar 15, 2021", "body": "This wer problem is not problem of version. It's problem of freeze_finetuning_updates parameter. please read this\n", "type": "commented", "related_issue": null}, {"user_name": "MorrisXu-Driving", "datetime": "Sep 4, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "lematt1991", "datetime": "Sep 8, 2020", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "lematt1991", "datetime": "Sep 8, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "alexeib", "datetime": "Mar 29, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "facebook-github-bot", "datetime": "Dec 9, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "vedanuj", "datetime": "Jun 30, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/261", "issue_status": " Closed\n", "issue_list": [{"user_name": "brimoor", "datetime": "Jul 16, 2020", "body": "s can have fields like  that contain unstructured, unpreviewable data:We should organize fields for which visualization is not supported in a separate section in the sidebar:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 15, 2020", "body": "Addressed in v0.5.0", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jul 16, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 15, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3396", "issue_status": " Closed\n", "issue_list": [{"user_name": "wl2776", "datetime": "Jul 7, 2021", "body": "I'd like to annotate RGBD data from the Intel Realsense D435/D455 cameras.\nIntel Realsense SDK contains viewer application that can save data streams from these cameras in ROS1 Bag format.\nHowever, CVAT doesn't support it.\nI've tried uploading bag file, but have got the error \"No media data found\"Process these data in a fashion, similar to video processing.Very long pause, then error \"No media data found\"Add Realsense support using SDK", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": " , sorry. I'm not sure we are going to support Intel RealSense rosbug in the nearest future. Try to use official scripts from RealSense SDK to extract images. If we see that a lot of community members need the feature, we will consider to add the option. Also you can try to contribute the feature by yourself.", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Aug 9, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Aug 9, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Aug 9, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Nov 20, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4298", "issue_status": " Closed\n", "issue_list": [{"user_name": "davodogster", "datetime": "Feb 7, 2022", "body": "The copy paste feature is great - from one frame to another. But if the camera moves I need to rotate the polygon aswell (after pasting it). Is this possible in CVAT?You know when you drag an image into Microsoft PPT, and you are able to rotate it as much as you want.Thanks,\nSam", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 8, 2022", "body": "Hi, rotation feature was implemented only for bounding boxes and ellipses, so, you can not rotate polygons for now.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 10, 2022", "body": "I will close the issue, if you have any more questions, do not hesitate to reopen. If you with to support polygon rotation in CVAT, please submit a new issue and describe your use case in details, provide screenshots, etc.", "type": "commented", "related_issue": null}, {"user_name": "domef", "datetime": "Jul 18, 2022", "body": "any news about this feature? will it ever be implemented?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 18, 2022", "body": "Hi, thank your for the question.That is not in roadmap and looks like not really demanded.\nIf the community is ready to implement it, I am ready to help with some suggestions.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3958", "issue_status": " Closed\n", "issue_list": [{"user_name": "omarreda14", "datetime": "Nov 29, 2021", "body": "You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "omarreda14", "datetime": "Nov 29, 2021", "body": "i have a labeled frames resulted from a trained model , the frames are from a security camera video , i need to set an unique ID to each label to help in tracking  some objects over frames .\nis there any way to do that ?", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Nov 30, 2021", "body": "Hi, try to use ReID model: \nInstallation guide: ", "type": "commented", "related_issue": null}, {"user_name": "subhailams", "datetime": "Dec 10, 2021", "body": "We have annotations from the person detection model, then we are trying to run the person  and export that to Market 1501 dataset format. But we don't get the exact number of query images. For two-person in a video, when we export the data, we only receive one person query image.Could anyone please summarize how to export the data in Market1501 format after performing detection + reidentification using CVAT models??", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Nov 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Apr 7, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3315", "issue_status": " Closed\n", "issue_list": [{"user_name": "HannesReichert96", "datetime": "Jun 10, 2021", "body": "I discovered the following 3D annotation mode:\n\nHowever I'm not able to create a task with 3D data (e.g. LiDAR)\nIs this already possible already or jet to come?Apart from this I have a multiple view camera setup and wonder if it is in principle possible to simultaneous view and label synchronize frames from all cameras. in one task.If not I would be happy to contribute.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jun 11, 2021", "body": "Hi This mode is not ready to use yet. We are preparing documentation about how to use it and finishing development.\nBasically, you need to create a task with .zip archive containing point cloud data (.pcd files) or velodyne format data (.bin files).\nAn example is the Kitty dataset: Different sources (several cameras as I understand) with lidar data are not supported at the same task, probably we are not very acquainted about this use-case. It is great, if you ready to contribute. From our side, we could assist you if any help is needed.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jun 28, 2021", "body": "I'll close the issue. If you still have questions, do not hesitate reopen it.", "type": "commented", "related_issue": null}, {"user_name": "rodrigoGA", "datetime": "Jul 6, 2021", "body": "  Do you have an estimated date to launch this feature?", "type": "commented", "related_issue": null}, {"user_name": "apockill", "datetime": "Sep 28, 2021", "body": "I am also curious!", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 28, 2021", "body": "The feature is develop branch. It is not a perfect yet, but workable. We are planning to improve it continuously. Refer to documentation to create 3D task.", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Jun 16, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jun 28, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2187", "issue_status": " Closed\n", "issue_list": [{"user_name": "LeeLeman", "datetime": "Sep 16, 2020", "body": "The videos were taken with Go Pro's camera fixed upside down. The video file's metadata contains information about a 180 degree rotate. However, the images in the problem are not rotated 180 degrees and still upside down.\nBefore that, I used cvat version 0.6.1 and this worked correctly", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 16, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 16, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 16, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Marishka17", "datetime": "Sep 24, 2020", "body": [], "type": "pull", "related_issue": "#2218"}, {"user_name": "nmanovic", "datetime": "Nov 5, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Nov 5, 2020", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/503", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Oct 25, 2019", "body": "I'm not 100% sure we want to do this, but we could  the recorder tab instead of pinning it.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": "Looking at the  it says: \"Tabs that are sharing the screen, microphone or camera cannot be hidden.\"So we can't do this.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": "I'll experiment with this a bit more to see if something is possible.", "type": "commented", "related_issue": null}, {"user_name": "poperigby", "datetime": "Jan 10, 2020", "body": "Maybe just automatically close it when the user closes the FV popup.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": "With  this doesn't seem so important.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 25, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 20, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 10, 2020", "body": [], "type": "issue", "related_issue": "#823"}, {"user_name": "ianb", "datetime": "Jan 14, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 23, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 23, 2020", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 24, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/213", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Sep 7, 2019", "body": "These UX guidelines might be of some interest:  (particularly page 3)", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Sep 9, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/211", "issue_status": " Closed\n", "issue_list": [{"user_name": "savan77", "datetime": "Nov 22, 2018", "body": "Hi,\nWe are adding several features into CVAT and will be open-sourced. We might need your advice along the way, just wanted to know if you can help. Currently, we are trying to change the interpolation. As of now, interpolation just puts bounding box in the remaining frames at the same position as it is in the first frame. We are trying to change that and add tracking there. Since the code base is huge I am unable to understand the exact flow of process.For now, say instead of constant coordinates I want to shift box to right a little bit (i.e 10 pixels). I guess its trivial task. Just need your help regarding the same, if possible. Thanks", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Nov 22, 2018", "body": "\nIf I understood you right, you need \"shapes.js\" file, \"BoxModel\" class and \"_interpolatePosition\" method.\nThis method computes an interpolation for a bounding box. All shape instances consist a field \"this._frame\" which is start frame for an interpolation shape. Method has an argument \"frame\" which is frame for an interpolation. From these data you can compute right offset on each frame.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 22, 2018", "body": "Hi ,I talked with Donald and Rush. Welcome to our community!Indeed interpolation works between key frames. It is not only \"puts bounding box in the remaining frames at the same position\". If you draw a bounding box on first frame  it will be propagated as is on next frames. If you change the propagated bounding box on 10th frame all frames between 1st and 10th will be changed (linearly interpolated). It works well for DSS annotation tasks with static cameras.Tracking is a good feature which will be useful in many scenarios. How are you going to add it? Will it work on clients only using OpenCV.js or it will be a solution on server-side using a DL model?Interpolation exists in two places: client (draw bounding boxes between key frames) and server (dump interpolated boxes).", "type": "commented", "related_issue": null}, {"user_name": "savan77", "datetime": "Nov 22, 2018", "body": " Thanks for the information. Currently, we are planning to use trackers provided by OpenCV, so I guess that won't create any problems. It is shipped with opencv-contrib.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 22, 2018", "body": "Hi , does it mean that you will support all trackers which are supported by OpenCV (including GOTURN)? As far as I can understand it will be server side implementation. How are you going to enable tracking for an object? I just want to understand how it will look like for an user. Will it be a global option in settings? For example, : \"interpolation\" (by default), \"GOTURN\", \"TLD\", \"MedianFlow\", ... Or are you going to allow choose the method per track/object?", "type": "commented", "related_issue": null}, {"user_name": "rushtehrani", "datetime": "Nov 22, 2018", "body": " those are great questions.  It will be a global setting for the first phase.We are currently targeting to support GOTURN.  and I will coordinate to see what it takes to support any OpenCV compatible tracker.  Let us know if you have any ideas on a more generic implementation.", "type": "commented", "related_issue": null}, {"user_name": "savan77", "datetime": "Nov 23, 2018", "body": " Yes, as of now, we are planning to have interpolation work as a tracker. We haven't decided whether we will give an option to choose tracking method. Ideally, we'll have tracker with the best performance as a default. But you can use any opencv-tracker you want including GOTURN. Based on our experiements, GOTURN should give better results but anyhow boxes explode while using GOTURN. However, CSRT gave good results.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 21, 2019", "body": "Hi  ,Do you still want to delivery the feature? If so when?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 21, 2019", "body": "There are some implementation ideas in the duplicated request ", "type": "commented", "related_issue": null}, {"user_name": "savan77", "datetime": "Jan 25, 2019", "body": "Hi ,  ,I and  are coordinating to implement this feature. So far, we have implemented the tracking part but it is yet to be integrated with CVAT. I will discuss with  to come up with a rough timeline.\nThanks", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 25, 2019", "body": "Hi  ,Don't hesitate to discuss with us any preliminary ideas, prototype. We can test the feature internally and provide an early feedback. We implemented many annotation tools in the past and some of them had tracking feature. The main difficulty here is the client-server architecture of the app. Need to provide smooth experience for users. In general they should think that the tracking is working in real-time on the client side but it is the final goal.I have in my mind the following implementation. What do you think?On the server we should have 1 entry point:On the client we should have the following behaviour:", "type": "commented", "related_issue": null}, {"user_name": "savan77", "datetime": "Feb 1, 2019", "body": "Hi ,Thanks for the input. That looks like a potential process flow for the tracking. We are actually in the middle of something else. We won't move forward without your consent on the process flow. So, I will get back to you in few days and we can discuss more about the flow.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Mar 30, 2019", "body": "Onepanel.io is not going to implement the request for now. I will move the on the next release. Hope we will enough time to implement the feature. It should not be very complex.", "type": "commented", "related_issue": null}, {"user_name": "mistermult", "datetime": "May 20, 2019", "body": "    I hacked a first version of the tracking feature together. It works as expected. Now want to polish it up for a pull request. It has the following flow:I found this very useful in the following situation: Create a keyframe when the object enters and leaves the image. Press tracking. In the middle the bounding box is often not accurate enough, so correct it (byMachine=false). Press tracking again. The bounding boxes from the middle user defined keyframe to the last user defined keyframe are now updated. To do this, we need to know which frames are from the user or the machine, hence the new attribute byMachine on each frame.So I need the following modifications in the existing code:Code structure:I looking forward to your answers for all the question.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 11, 2019", "body": " , missed the comment. Just submit a PR and I will recommend something. First of all need to check it from user experience point of view. When it is fine I will recommend how to change code to make it better.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": "No support from community. We will implement the feature internally. Moved to next release.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 16, 2019", "body": " , for now it is planned for release 1.0 but I don't think we have enough resources to implement it in the upcoming release. If you can help and propose a PR we will be glad to review and merge.", "type": "commented", "related_issue": null}, {"user_name": "korabelnikov", "datetime": "Dec 16, 2019", "body": "Thanks for info", "type": "commented", "related_issue": null}, {"user_name": "ksenyakor", "datetime": "Jan 16, 2020", "body": "Could you please explain, how to use tracking tool in CVAT?\nThere is demo video \nBut I don't have \"Track\" in the context menu....", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 16, 2020", "body": " , the tracking feature isn't integrated yet. The PR is here: Probably it will be integrated only after our next release in February. If you like you can manually apply the patch on your local git copy.", "type": "commented", "related_issue": null}, {"user_name": "ksenyakor", "datetime": "Jan 22, 2020", "body": "Thanks for your answer!", "type": "commented", "related_issue": null}, {"user_name": "hasanahmedfaisal", "datetime": "Jan 22, 2020", "body": "Hi Cloud you please share any details on how this interpolation is done , any code or any articles , is it done using opencv , and are bounding boxes edge points used or centre point, scale , aspect ratio are used ,please please share , i would like to implement this myself with few changes i am thinking of , So please help\nAny one if you know please shareThank you", "type": "commented", "related_issue": null}, {"user_name": "korabelnikov", "datetime": "Jan 22, 2020", "body": "why don't look sources?", "type": "commented", "related_issue": null}, {"user_name": "hasanahmedfaisal", "datetime": "Jan 22, 2020", "body": "Hi    , Do you mean source code of cvat , i tried to look into it , as per my search , most code is return in JavaScript and i  donot know java script , so having hard time understandingIf you cloud point me any sources in python , please share , or any other advice please shareThank you", "type": "commented", "related_issue": null}, {"user_name": "kaustubhharapanahalli", "datetime": "Jun 5, 2020", "body": " Hi, Any progress on this?", "type": "commented", "related_issue": null}, {"user_name": "mingweihe", "datetime": "Aug 4, 2020", "body": " Thank you for the hard work. I was recently trying to deploy this function using the nuctl command \"nuctl deploy --project-name cvat --path serverless/pytorch/foolwood/siammask/nuclio\", it seems that it is not working.\nSo may I happen to know if this function is ongoing now?\nerror message is as follows:", "type": "commented", "related_issue": null}, {"user_name": "483415258", "datetime": "Sep 23, 2021", "body": "    \nHello everyone, I have encountered the same problem as mingweihe above. How can I solve itThank you for the hard work. I was recently trying to deploy this function using the nuctl command \"nuctl deploy --project-name cvat --path serverless/pytorch/foolwood/siammask/nuclio\", it seems that it is not working.\nSo may I happen to know if this function is ongoing now?\nerror message is as follows:Verifying transaction: ...working... done\nExecuting transaction: ...working... done==> WARNING: A newer version of conda exists. <==\ncurrent version: 4.8.2\nlatest version: 4.8.3Please update conda by runningRemoving intermediate container 4952a8bb92d5\n---> 18fcb57d7c72\nStep 7/20 : RUN source activate siammask\n---> Running in 4bb228f86d01\n/bin/sh: 1: source: not found\nRemoving intermediate container 4bb228f86d01stderr:\nThe command '/bin/sh -c source activate siammask' returned a non-zero code: 127Failed to build docker image\n.../pkg/containerimagebuilderpusher/docker.go:56\nFailed to build processor image\n/nuclio/pkg/processor/build/builder.go:250\nFailed to deploy function\n...//nuclio/pkg/platform/abstract/platform.go:171", "type": "commented", "related_issue": null}, {"user_name": "Teagueporter", "datetime": "May 26, 2022", "body": "Hi I'm pretty new to using cvat as a tool, and right now I'm trying to create bounding boxes for fish in an ocean, the main thing is the fish will leave the screen or hid behind something for a few frames, but the problem that I ran in to was I delete the bounding box when they left and that just deleted it for every frame, the other was I hid the box but that too hides it for all frames.\nAnd I would rather not have a box just floating around while the fish is not in the frame.By the way I'm using the open cv tracking tool", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 27, 2022", "body": "Hi, \"Outside\" feature is exactly what you need. You can hide track starting from frame N, and show it again starting from N+M. To do that please use setting: \"Show all interpolation tracks\".  Or another way is to create two independent tracks and merge them, using \"Merge\" feature.Please, refer to the documentation for details:\n\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 22, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 22, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 22, 2018", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 1, 2018", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 21, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 21, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 21, 2019", "body": [], "type": "issue", "related_issue": "#38"}, {"user_name": "nmanovic", "datetime": "Mar 30, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Mar 21, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 27, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 27, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 27, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 2, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 5, 2020", "body": [], "type": "pull", "related_issue": "#1988"}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "snyk-bot", "datetime": "Mar 24, 2022", "body": [], "type": "pull", "related_issue": "michibiki-io/cvat-subpath#5"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/187", "issue_status": " Closed\n", "issue_list": [{"user_name": "AFusco", "datetime": "Nov 7, 2018", "body": "I would like to add new images, create new tasks and dump annotations automatically through an API.I'd like to upload a new image to an existing task directly from the camera, and then retrieve the latest dump from an external server/container.I read in issue  that adding new images isn't currently supported.\nIs such an API outside the scope of this project?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2018", "body": "Hi  ,First of all such API already exists but it isn't stable and well defined. A browser (client) makes request to CVAT server using a set of end points. For example, Of course it would be much easy if CVAT provides a clear python API to do the same. I will keep the issue open. Thank you for the request.", "type": "commented", "related_issue": null}, {"user_name": "geraldmwangi", "datetime": "Jan 21, 2019", "body": "+1 for that feature", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2018", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 7, 2018", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 7, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 21, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 11, 2019", "body": [], "type": "pull", "related_issue": "#389"}, {"user_name": "nmanovic", "datetime": "Apr 23, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "snyk-bot", "datetime": "Oct 19, 2021", "body": [], "type": "pull", "related_issue": "hixio-mh/cvat#97"}]},
{"issue_url": "https://github.com/PCCoE-Hacktoberfest-21/FRIDAY/issues/85", "issue_status": " Closed\n", "issue_list": [{"user_name": "BAHETIHARSH", "datetime": "Oct 26, 2021", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "ParamTatiya02", "datetime": "Oct 27, 2021", "body": "Assign this task to me", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 27, 2021", "body": " would you like to work on this issue (as you have opened this issue) or should i assign it to someone else", "type": "commented", "related_issue": null}, {"user_name": "BAHETIHARSH", "datetime": "Oct 27, 2021", "body": "yes, I want to work.", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 27, 2021", "body": " Harsh has opened this issue and wants to work on it .\nYou may go ahead and open a new issue for a feature you want to add", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 27, 2021", "body": " go ahead", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 27, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 27, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "BAHETIHARSH", "datetime": "Oct 29, 2021", "body": [], "type": "pull", "related_issue": "#91"}, {"user_name": "rohitjoshi6", "datetime": "Oct 29, 2021", "body": [], "type": "pull", "related_issue": "#91"}, {"user_name": "rohitjoshi6", "datetime": "Oct 29, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/4642", "issue_status": " Closed\n", "issue_list": [{"user_name": "daje0601", "datetime": "Jul 1, 2022", "body": "Hello, An error occurs when installing the bart model while inference to Blenderbot 2.0, which can be searched on the Internet.\nI proceeded as follows. The Internet server downloaded the server from and used it.", "type": "commented", "related_issue": null}, {"user_name": "daje0601", "datetime": "Jul 4, 2022", "body": "I solve this problem and close the issu page~", "type": "commented", "related_issue": null}, {"user_name": "daje0601", "datetime": "Jul 4, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/1557", "issue_status": " Closed\n", "issue_list": [{"user_name": "JohannesTK", "datetime": "Mar 15, 2019", "body": "Congratulations on NAACL and a great paper!\"All of our code, pretrained models, and full chatlogs, will be released open-source\" - do you have a timeframe on that? Would love to try it out.Thanks!", "type": "commented", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Mar 15, 2019", "body": "cc ", "type": "commented", "related_issue": null}, {"user_name": "abisee", "datetime": "Mar 16, 2019", "body": "Hi , thanks for your interest!The NAACL camera-ready deadline is April 1, so we'll have finished our revised version of the paper by then. We hope to release the code, pretrained models and chatlogs shortly after that, though it usually takes a little while to clean everything up to the level that people can use it, so it might take 1-2 weeks after the camera-ready deadline.", "type": "commented", "related_issue": null}, {"user_name": "JohannesTK", "datetime": "Mar 18, 2019", "body": "Thanks for the update, .Will be looking forward to it and hope you have a successful presentation at NAACL!", "type": "commented", "related_issue": null}, {"user_name": "g-karthik", "datetime": "Apr 10, 2019", "body": " weighted decoding and conditional training are somewhat model-agnostic in that they should work with any encoder-decoder model, so would you be integrating the two approaches at the framework-level prior to the release? I noticed that you used a 2-layer LSTM with attention in your paper, but it would be ideal if your release works with any encoder-decoder model with minimal effort. Thanks!Good paper, btw!", "type": "commented", "related_issue": null}, {"user_name": "abisee", "datetime": "Apr 22, 2019", "body": "Update: Apologies, looks like my previous estimate for when this would be done was a bit over-optimistic. But I am now working on it and hope to have it done soon. Thanks for your patience!", "type": "commented", "related_issue": null}, {"user_name": "emilydinan", "datetime": "Mar 15, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "stephenroller", "datetime": "Apr 19, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "abisee", "datetime": "May 2, 2019", "body": [], "type": "pull", "related_issue": "#1680"}, {"user_name": "stephenroller", "datetime": "May 2, 2019", "body": [], "type": "issue", "related_issue": "#1690"}, {"user_name": "stephenroller", "datetime": "May 6, 2019", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/608", "issue_status": " Closed\n", "issue_list": [{"user_name": "naimulhq", "datetime": "Nov 30, 2020", "body": "Is there any ways to improve detection speeds for the custom object detection model? I was planning to use this model with a live feed from a camera to perform real time object detection but the object detection time takes too long which causes the camera to lag.", "type": "commented", "related_issue": null}, {"user_name": "naimulhq", "datetime": "Dec 9, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/512", "issue_status": " Closed\n", "issue_list": [{"user_name": "chaotianjiao", "datetime": "Mar 26, 2020", "body": "when I run \nit show my mac camera is working,but I can't get a window to see the result.\nwhat should I add into the code to open a window and get the real-time detection result?\nThanks!", "type": "commented", "related_issue": null}, {"user_name": "nickums", "datetime": "Feb 13, 2021", "body": "the documentation describes the  per_frame_function=forFrame procedure in which the frame is displayed by plt", "type": "commented", "related_issue": null}, {"user_name": "chaotianjiao", "datetime": "Feb 25, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/596", "issue_status": " Closed\n", "issue_list": [{"user_name": "GetRektBoy724", "datetime": "Oct 30, 2020", "body": "I run an example code from ImageAI\nand its showed up something like this :\n\ni use tensorflow non-avx build version 2.4.0\ni get it in here : \ni already test the tensorflow and its working\npls tell me if you want more information\npls help...any answer is appreciated :D", "type": "commented", "related_issue": null}, {"user_name": "GetRektBoy724", "datetime": "Oct 30, 2020", "body": "lmao i bypass the problem (kinda tho)\ni modified the code to\n", "type": "commented", "related_issue": null}, {"user_name": "GetRektBoy724", "datetime": "Oct 30, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/595", "issue_status": " Closed\n", "issue_list": [{"user_name": "GetRektBoy724", "datetime": "Oct 29, 2020", "body": "i have imageai install in my linux\ni have intel 7th gen celeron cpu and intel hd graphics card (it was crappy? ummm yes)\nif i run\n\ni get\n\ni already install ALL of the requirements and i install imageai using pip\ni use python3.8 if you ask\nif i look on dmesg...this thing showed up :\n\npls help\nthis is the code im trying to run\n`\n#!/usr/bin/python3\nfrom imageai.Detection import VideoObjectDetection\nimport os\nimport cv2execution_path = os.getcwd()camera = cv2.VideoCapture(0)detector = VideoObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath(os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\")) # Download the model via this link \ndetector.loadModel()video_path = detector.detectObjectsFromVideo(camera_input=camera,\noutput_file_path=os.path.join(execution_path, \"camera_detected_video\")\n, frames_per_second=20, log_progress=True, minimum_percentage_probability=30)\nprint(video_path)`\nany answer is appreciated :D", "type": "commented", "related_issue": null}, {"user_name": "GetRektBoy724", "datetime": "Oct 29, 2020", "body": "im really a beginner in the world of object detection and image recognition", "type": "commented", "related_issue": null}, {"user_name": "GetRektBoy724", "datetime": "Oct 30, 2020", "body": "yayy got it fixed but i have a new problem tho\ni use this pre-build wheel : \nfrom this : ", "type": "commented", "related_issue": null}, {"user_name": "elbriga", "datetime": "Feb 11, 2022", "body": "how do u fixed it?", "type": "commented", "related_issue": null}, {"user_name": "GetRektBoy724", "datetime": "Oct 30, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/510", "issue_status": " Closed\n", "issue_list": [{"user_name": "SamsLogic", "datetime": "Mar 23, 2020", "body": "Please add a safe kill switch for video input using camera. I have to kill the whole program to kill the video detection.PS: Thank you for this library. It is getting very handy for my project. Great work.", "type": "commented", "related_issue": null}, {"user_name": "AbimbolaOO", "datetime": "Aug 2, 2020", "body": "I think there are no-kill switch builtin. All you need to do is press Cntrl+C or Cntrl+X on your keyboard.  At times you would be required to press any of those keys twice.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 7, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/447", "issue_status": " Closed\n", "issue_list": [{"user_name": "dalisoft", "datetime": "Dec 28, 2019", "body": "Hi  and thanks for great tool, i recently found this library. I am JS developer and today only for your library i'm learning Python (v3) as Tensorflow, Keras and some tool already available in Node.js, but i found your library very flexible and easy-to-use as JS developer.Today i runned some examples from your project, at first and 3-4 tries worked, but after changing few lines it stopped working.Can you help me?\nSorry for stupid questions, but today my first day on Python world :)See diffsOS: macOS 10.15.2\nPython: 3.7\nPip: 19.3.1\nVirtual Env: virtualenv\nShell: fish shellThanks", "type": "commented", "related_issue": null}, {"user_name": "dalisoft", "datetime": "Dec 28, 2019", "body": "Sorry, it's my fault", "type": "commented", "related_issue": null}, {"user_name": "dalisoft", "datetime": "Dec 28, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "dalisoft", "datetime": "Dec 28, 2019", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/411", "issue_status": " Closed\n", "issue_list": [{"user_name": "shivam9804", "datetime": "Nov 16, 2019", "body": "I don want the video to be saved as I want to detect on a live is it possible ? and as I was trying to do it the video screen wouldnt open I using opencv to camera access.", "type": "commented", "related_issue": null}, {"user_name": "hwillard98", "datetime": "Nov 22, 2019", "body": "I believe this is what you're looking for:\n", "type": "commented", "related_issue": null}, {"user_name": "dalisoft", "datetime": "Dec 30, 2019", "body": "\n option does not work.\nI am tried this optionThis anyway saves file as  file into folder.", "type": "commented", "related_issue": null}, {"user_name": "codebugged", "datetime": "May 16, 2020", "body": "Hi \nDid it work ?I am trying your code snippet but am not able to get live detected feed.\nWhat's number:  +918004006979", "type": "commented", "related_issue": null}, {"user_name": "shivam9804", "datetime": "Jan 7, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/391", "issue_status": " Closed\n", "issue_list": [{"user_name": "rEufrazio", "datetime": "Oct 29, 2019", "body": "The code I am trying to run is the one below, and it is almost identical to the one available at the documentation for \"Video and Live-Feed Detection and Analysis\":The error is as follows:I have already installed the latest version, v.2.1.5, and tried the YOLOv3 and TinyYOLOv3 models, both returning the same error. Also tried 2 different cameras.At first I thought it might be the threading problem, as I was using the multithreaded solution from imutils, but it turns out that the same thing is happening with the cv2.VideoCapture.Could you please help me out? I have no idea what is causing this problem.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": "Could someone please help me here? I'm two weeks in and still have the same problem.I recently discovered that this just happens when I use the  in any way, even if I assign it to another variable.Already updated and downgraded the library and dependencies to every possible option, and tried using the other \"for\" methods, with no success.If anyone could help me in any way I would be very grateful.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": "I managed to solve the problem. It looks like the function will always display this message when anything wrong happens inside the  call. The error was being caused simply because I was calling the  function incorrectly and not calling  inside the method.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/361", "issue_status": " Closed\n", "issue_list": [{"user_name": "ulgacemre", "datetime": "Sep 25, 2019", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Oct 3, 2019", "body": "If you want to run detection and display results in real-time, you will need to run on a computer system with . I will recommend  or higher", "type": "commented", "related_issue": null}, {"user_name": "DeveloperRachit", "datetime": "Nov 12, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "DeveloperRachit", "datetime": "Nov 12, 2019", "body": "that Specifications i have in my GPU but training is too slow i am using 4 batch size", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Nov 12, 2019", "body": " Please note the difference between training and detecting in real time. The amount of time it takes to train depends on your dataset size.", "type": "commented", "related_issue": null}, {"user_name": "codebugged", "datetime": "May 16, 2020", "body": "Hi \nCan you please share the code for detecting and showing it in live feed", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Nov 12, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/259", "issue_status": " Closed\n", "issue_list": [{"user_name": "andreqwert", "datetime": "Jul 30, 2019", "body": "Good afternoon!Is there any possibility to launch model showing bounding boxes from web-camera? I figured out only how to get bounding boxes coordinates in command line but the videostream is not switched on at this moment and I don't understand how to switch it on?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jul 30, 2019", "body": "See this tutorial.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jul 31, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/248", "issue_status": " Closed\n", "issue_list": [{"user_name": "Stuffffffffff", "datetime": "Jul 18, 2019", "body": "For a project I am making it so a motion sensor only records when specifically a cat is there, Can I do that Live or at least have a program to delete videos without the cat? if so, how?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jul 24, 2019", "body": "", "type": "", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/271", "issue_status": " Closed\n", "issue_list": [{"user_name": "PushkarChatterji", "datetime": "Aug 6, 2019", "body": "Hi,I have trained the yolov3 on a custom data set of 200+ images with 1 class for 200 epochs.When I try to load this trained model back for video detection using your code, I get the error in the title of this post.Any suggestions?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": "Multiple models were saved during the training. Have you tried any of the other models?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": "Also, have you tried to detect the object from an image?", "type": "commented", "related_issue": null}, {"user_name": "PushkarChatterji", "datetime": "Aug 6, 2019", "body": "The saved model(s) run perfectly for images.\nBut not for video input", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": "Incredible. I will run a check on this and give you feedback. You can also send me the model as well as the  file via my email  to my work on the debug.", "type": "commented", "related_issue": null}, {"user_name": "PushkarChatterji", "datetime": "Aug 6, 2019", "body": "Got it. I'll send you a dropbox link for the model as it's too big for gmail...", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": "Hello, I just ran the video detection code using the model and json files you shared with me. It seems to work perfectly. See the Google drive link below for the results I obtained.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 6, 2019", "body": "I will advice that you do the following:", "type": "commented", "related_issue": null}, {"user_name": "PushkarChatterji", "datetime": "Aug 7, 2019", "body": "Hmm...thats interesting that it worked for you.... (the bounding boxes are like the height and the width have been switched...)Removing the spaces in the file name didn't work for me. I am getting the same error:Here are the versions:\ntensorflow-gpu = 1.13.1\nKeras = 2.2.4\nopencv-python= 4.1.0.25And here's the code I am trying to run:from imageai.Detection import VideoObjectDetection\nimport osdetector = VideoObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath(\"/models/detection_model-ex-50--loss-13.44.h5\")\ndetector.loadModel()video_path = detector.detectObjectsFromVideo(input_file_path=\"video_test_ok.avi\",\noutput_file_path=\"traffic_detected.avi\"\n, frames_per_second=24, log_progress=True)\nprint(video_path)", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 7, 2019", "body": "Now I know where the issue is. You are using the wrong class for your video object detection. : This class is for the inbuilt, pre-trained object detection model. NOT YOUR CUSTOM MODEL. : This is the class for your custom detection model. Visit the link below for all the details.", "type": "commented", "related_issue": null}, {"user_name": "PushkarChatterji", "datetime": "Aug 7, 2019", "body": "Indeed! My bad.Everything works perfectly now. Thank you!Off topic:\nWhen I use the json config file with the anchor values calculated at training time, I get the bounding boxes in the following shape in running the saved model:\nHowever, when i swap the values around in the anchor file, i get the desired bounding boxes:\nPossibly something being swapped with writing to json?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 7, 2019", "body": "Can you put in the comment the arrangement of the anchor values  and  your swap?", "type": "commented", "related_issue": null}, {"user_name": "PushkarChatterji", "datetime": "Aug 7, 2019", "body": "Here is the original:\n{\n\"labels\" : [\n\"customer\"\n],\n\"anchors\" : [\n[\n129,\n65,\n73,\n64,\n104,\n49\n],\n[\n134,\n45,\n73,\n38,\n119,\n34\n],\n[\n38,\n29,\n95,\n28,\n63,\n26\n]\n]\n}Here is the swapped:\n{\n\"labels\" : [\n\"customer\"\n],\n\"anchors\" : [\n[\n65,\n129,\n64,\n73,\n49,\n104\n],\n[\n45,\n134,\n38,\n73,\n34,\n119\n],\n[\n29,\n38,\n28,\n95,\n26,\n63\n]\n]\n}", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 7, 2019", "body": "OH MY! Bug discovered and fixed. Kindly see this update here. ", "type": "commented", "related_issue": null}, {"user_name": "achipalamwazani", "datetime": "Nov 19, 2019", "body": "I used a pretrained models provided, and am trying to run the sample code. I am using the right class: from imageai.Detection.Custom import CustomVideoObjectDetection. However, I am still running in the same error till right now, trying to detect object from video file. Has anyone managed to resolve this issue please:P.S: I am able to detect objects from a camera, however, it is detecting everything, and I would like to detect only objects of my interest", "type": "commented", "related_issue": null}, {"user_name": "ayushpandey2708", "datetime": "Oct 12, 2020", "body": "I'm having similar error when I implemented-\nyolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\nyolo_model.load_weights(\"model_data/trained_weights_final.h5\")the error is as following-InvalidArgumentError                      Traceback (most recent call last)\nc:\\users\\91884\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\n685           graph_def_version, node_def_str, input_shapes, input_tensors,\n--> 686           input_tensors_as_shapes, status)\n687   except errors.InvalidArgumentError as err:c:\\users\\91884\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in (self, type_arg, value_arg, traceback_arg)\n515             compat.as_text(c_api.TF_Message(self.status.status)),\n--> 516             c_api.TF_GetCode(self.status.status))\n517     # Delete the underlying status object from memory otherwise it stays aliveInvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 18. Shapes are [1,1,1024,75] and [18,1024,1,1]. for 'Assign_1082' (op: 'Assign') with input shapes: [1,1,1024,75], [18,1024,1,1].kindly help.", "type": "commented", "related_issue": null}, {"user_name": "ayushpandey2708", "datetime": "Oct 12, 2020", "body": "I have solved this problem by-\nyolo_model.load_weights(\"model_data/trained_weights_final.h5\",by_name=True,skip_mismatch=True)but now I'm having error in prediction,here is sample code-\ndef predict(sess, image_path):predict(sess, \"yolo/dog.jpg\")please help.", "type": "commented", "related_issue": null}, {"user_name": "ayushpandey2708", "datetime": "Oct 13, 2020", "body": "I was having this error because of missing draw_boxes and get_colors_for_classes in utils.py\nadd these from here-\nproblem solved.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 7, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Aug 9, 2019", "body": [], "type": "issue", "related_issue": "#8"}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/198", "issue_status": " Closed\n", "issue_list": [{"user_name": "chandnisharma25", "datetime": "Apr 28, 2019", "body": "Hi,I have been trying to use your video object detection code as follows:from imageai.Detection import VideoObjectDetection\nimport os\nimport cv2execution_path = os.getcwd()camera = cv2.VideoCapture(0)detector = VideoObjectDetection()\ndetector.setModelTypeAsYOLOv3()\ndetector.setModelPath(os.path.join(execution_path , \"yolo.h5\"))\ndetector.loadModel()video_path = detector.detectObjectsFromVideo(camera_input=camera,\noutput_file_path=os.path.join(execution_path, \"camera_detected_video\")\n, frames_per_second=20, log_progress=True, minimum_percentage_probability=30)print(video_path)It worked and the file saved into /home/pi/objectdetection/traffic_detected.avi  successfully. However, I have been trying to use ffmpeg to convert from avi to mp4 and I have had no success. I tried to use this command to convert:avconv -i traffic_detected.avi -c:v libx264 -c:a copy trafficdetection.mp4and got this error:[avi @ 0x24af5c0] Could not find codec parameters for stream 0 (Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown)): unspecified size\nConsider increasing the value for the 'analyzeduration' and 'probesize' options\nInput #0, avi, from 'traffic_detected.avi':\nDuration: N/A, start: 0.000000, bitrate: N/A\nStream #0:0: Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown), 20 fps, 20 tbr, 20 tbn, 20 tbc\nOutput #0, mp4, to 'trafficdetection.mp4':\nOutput file #0 does not contain any streami tried this code to change analyzeduration and probesize \" ffmpeg -analyzeduration 2147483647 -probesize 2147483647 -i /home/pi/objectdetection/traffic_detected.avi\n\" Tried again and got this:[avi @ 0x21e85c0] Could not find codec parameters for stream 0 (Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown)): unspecified size\nConsider increasing the value for the 'analyzeduration' and 'probesize' options\nInput #0, avi, from '/home/pi/objectdetection/traffic_detected.avi':\nDuration: N/A, start: 0.000000, bitrate: N/A\nStream #0:0: Video: mjpeg (MJPG / 0x47504A4D), none(bt470bg/unknown/unknown), 20 fps, 20 tbr, 20 tbn, 20 tbc\nAt least one output file must be specifiedCan you help me ?Thank you,Chandni.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": "Kindly refer to the link below on resolving this.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/195", "issue_status": " Closed\n", "issue_list": [{"user_name": "PranoveAB", "datetime": "Apr 23, 2019", "body": "Hey!,Till now I was able to connect to an IP camera and get live feed input, which saves it as an .avi file.\nThis saved file is not efficient enough. Is there a way to get a output video as a stream like the input stream?", "type": "commented", "related_issue": null}, {"user_name": "PranoveAB", "datetime": "Apr 24, 2019", "body": "Anybody with the same question please, perhaps with a solution? :)", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Apr 26, 2019", "body": "You can use the sample code linked below to display the detected video stream.Also study extensively the sample codes provided in the documentation linked below.", "type": "commented", "related_issue": null}, {"user_name": "PranoveAB", "datetime": "Apr 23, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "May 30, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/191", "issue_status": " Closed\n", "issue_list": [{"user_name": "erdalalim", "datetime": "Apr 16, 2019", "body": "Hi everyone, I want to run video analysis per second..\nIam trying to import:\nfrom imageai.Detection import VideoObjectDetection\nbut  Iam using Anaconda/Jupyter notebook.\nI install ImageAI via Anaconda prompt , pip install.I  check ImageAI folder,there is no any VideoObjectDetection.py script..Also I downloaded and ImageAI.zip from here (github) but even there is no ..\nfrom where I can import?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": "Kindly follow the tutorial linked below on resolving this.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/188", "issue_status": " Closed\n", "issue_list": [{"user_name": "iqratoheed", "datetime": "Apr 6, 2019", "body": "hi i am using imageAI on google colab everything is working good but my camera doesnot starts for object recognition. Can anyone help regarding this issue?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": "Hello, you have to run the code on your computer to connect with your camera. Colab notebooks doesn't provide functionalities for you to connect with your system camera", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/185", "issue_status": " Closed\n", "issue_list": [{"user_name": "Anouk04", "datetime": "Apr 3, 2019", "body": "When I run detectCustumObjectsFromVideo() I get the error: detectCustomObjectsFromVideo() got an unexpected keyword argument 'return_detected_frame'This is my full code:from imageai.Detection import VideoObjectDetection\nimport osexecution_path = os.getcwd()detector = VideoObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\ndetector.loadModel()custom_objects = detector.CustomObjects(cup=True, spoon=True, bottle=True)video_path = detector.detectCustomObjectsFromVideo(custom_objects=custom_objects, input_file_path=os.path.join(execution_path, \"tea-making-part_Trim_Trim.mp4\"), output_file_path=os.path.join(execution_path, \"cups_detected\"), frames_per_second=20, log_progress=True, return_detected_frame=True)\nprint(video_path)TypeError: detectCustomObjectsFromVideo() got an unexpected keyword argument 'return_detected_frame'I also get the same error when trying to run the per_second_function (or per_frame_function)from imageai.Detection import VideoObjectDetection\nimport osdef forSeconds(second_number, output_arrays, count_arrays, average_output_count):\nprint(\"SECOND : \", second_number)\nprint(\"Array for the outputs of each frame \", output_arrays)\nprint(\"Array for output count for unique objects in each frame : \", count_arrays)\nprint(\"Output average count for unique objects in the last second: \", average_output_count)\nprint(\"------------END OF A SECOND --------------\")video_detector = VideoObjectDetection()\nvideo_detector.setModelTypeAsRetinaNet()\nvideo_detector.setModelPath(os.path.join(execution_path, \"resnet50_coco_best_v2.0.1.h5\"))\nvideo_detector.loadModel()video_detector.detectObjectsFromVideo(input_file_path=os.path.join(execution_path, \"tea-making-part_Trim_Trim.mp4\"), output_file_path=os.path.join(execution_path, \"video_second_analysis\") ,  frames_per_second=20, per_second_function=forSeconds,  minimum_percentage_probability=30, return_detected_frame=True)TypeError: detectObjectsFromVideo() got an unexpected keyword argument 'per_second_function'Can you help me with this?\nThank you!", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Apr 26, 2019", "body": "In the  function, set ", "type": "commented", "related_issue": null}, {"user_name": "nex0ma", "datetime": "Jul 4, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "suraj9741", "datetime": "May 22, 2020", "body": "how we can accept element inside the def forFrame", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Apr 26, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/173", "issue_status": " Closed\n", "issue_list": [{"user_name": "nswaglerevs", "datetime": "Mar 24, 2019", "body": "TypeError                                 Traceback (most recent call last)\n in ()\n16 video_path = detector.detectObjectsFromVideo(camera_input=camera,\n17                                 output_file_path=os.path.join(execution_path, \"camera_detected_video\")\n---> 18                                 , frames_per_second=20, log_progress=True, minimum_percentage_probability=40)TypeError: detectObjectsFromVideo() got an unexpected keyword argument 'camera_input'", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Mar 27, 2019", "body": "Post your full code here.", "type": "commented", "related_issue": null}, {"user_name": "nswaglerevs", "datetime": "Mar 27, 2019", "body": "from imageai.Detection import VideoObjectDetection\nimport os\nimport cv2execution_path = os.getcwd()camera = cv2.VideoCapture(0)detector = VideoObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath(os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\ndetector.loadModel()video_path = detector.detectObjectsFromVideo(camera_input=camera,\noutput_file_path=os.path.join(execution_path, \"camera_detected_video\")\n, frames_per_second=20, log_progress=True, minimum_percentage_probability=40)Image prediction and video prediction code is working but this live streaming code isn't  working..please kindly help me with this..\nThank you..", "type": "commented", "related_issue": null}, {"user_name": "ayadav10491", "datetime": "Jun 5, 2019", "body": "Facing the same issue", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": "Please visit the tutorial linked below on resolving this.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/181", "issue_status": " Closed\n", "issue_list": [{"user_name": "MaryBD", "datetime": "Apr 2, 2019", "body": "Hi all, I try to detect the closest margins to the camera, and as shown it returns me the false detection everywhile. Is there something wrong with the depth camera? Does it need calibration? Thanks.\nThe object is about half meter from camera, and camera returns very often some dots or a circle like object in the distance about 20 cm from camera (as shown).\n", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 13, 2019", "body": "Did you use ImageAI's Object Detection for this ?", "type": "commented", "related_issue": null}, {"user_name": "MaryBD", "datetime": "Jun 17, 2019", "body": "No I detect the closest points (withithin a range) to the camera using depth camera.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 17, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/177", "issue_status": " Closed\n", "issue_list": [{"user_name": "prashantsondkar", "datetime": "Mar 29, 2019", "body": "Hello\nI used the code to perform the object detection in live feed (camera). But output video runs very very fast.from imageai.Detection import VideoObjectDetection\nimport os\nimport cv2execution_path = os.getcwd()camera = cv2.VideoCapture(0)detector = VideoObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath(os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\ndetector.loadModel()video_path = detector.detectObjectsFromVideo(camera_input=camera,\noutput_file_path=os.path.join(execution_path, \"camera_detected_video\")\n, frames_per_second=40, log_progress=True, minimum_percentage_probability=40)", "type": "commented", "related_issue": null}, {"user_name": "prajintst", "datetime": "Apr 25, 2019", "body": "change the fps  according to the detection speed. decrease your fps to 15 or less.", "type": "commented", "related_issue": null}, {"user_name": "prashantsondkar", "datetime": "Mar 29, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "prashantsondkar", "datetime": "Mar 29, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jun 12, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/161", "issue_status": " Closed\n", "issue_list": [{"user_name": "tiru1930", "datetime": "Feb 25, 2019", "body": "HiIt is a nice library, I want to build a software where I want to use real-time object detection where the object is within a certain range from the camera. How can I achieve this", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jul 30, 2019", "body": "See ", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Jul 30, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/120", "issue_status": " Closed\n", "issue_list": [{"user_name": "Devraj004", "datetime": "Dec 9, 2018", "body": "Hi Ola,\nI am trying to run object detection on my raspberry pi and getting the \"Segmentation fault\" error while the same script is running fine on my laptop. any help please ?Using TensorFlow backend.\nSegmentation faultHere is the script.from imageai.Detection import VideoObjectDetection\nimport os\nimport cv2execution_path = os.getcwd()\ncamera = cv2.VideoCapture(0)def forFrame(frame_number, output_array, output_count):\nprint(\"FOR FRAME \" , frame_number)\nprint(\"Output for each object : \", output_array)\nprint(\"Output count for unique objects : \", output_count)\nprint(\"------------END OF A FRAME --------------\")video_detector = VideoObjectDetection()\nvideo_detector.setModelTypeAsYOLOv3()\nvideo_detector.setModelPath(os.path.join(execution_path, \"yolo.h5\"))\nvideo_detector.loadModel()video_detector.detectObjectsFromVideo(camera_input=camera,   frames_per_second=20, per_frame_function=forFrame,  minimum_percentage_probability=30, save_detected_video=False)", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 14, 2018", "body": "Please uninstall and re-install the ImageAI and it's dependencies on the Raspberry Pi. You can also follow the link below for more clarity on this.", "type": "commented", "related_issue": null}, {"user_name": "Devraj004", "datetime": "Dec 16, 2018", "body": "I tested the code line by line and identified the issue with tensorflow library when running in python virtual environment.\nSo we can close this case for now.Thank you John and Moses. I have read your book and found very useful. I am really grateful for you both to share such an excellent deep learning and Computer vision book with us.Many thanks\nDevraj", "type": "commented", "related_issue": null}, {"user_name": "Devraj004", "datetime": "Dec 16, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/115", "issue_status": " Closed\n", "issue_list": [{"user_name": "rw86347", "datetime": "Dec 1, 2018", "body": "I found two very similar bottles of alcohol.  I then tied a string to their necks and then took video of them from all sides.  This produced something like 3000 images of each of the two bottles.  I then trained until the accuracy hit 100%.  (epoch 25).  I then wrote a python script that would use the webcam and held the bottles in front of the camera.  I  the predictions were right 25% of the time.  But more concerning was when I held nothing in front of the camera. In that case it would still guess 90% chance it was one of the bottles of alcohol.Why won't it recognize nothing as low probability for both bottles?Is there some trick I can do for super fine accuracy?from imageai.Prediction.Custom import ModelTrainingmodel_trainer = ModelTraining()\nmodel_trainer.setModelTypeAsResNet()\nmodel_trainer.setDataDirectory(\"products\")model_trainer.trainModel(num_objects=2, num_experiments=100, enhance_data=True, batch_size=12, show_network_summary=True)", "type": "commented", "related_issue": null}, {"user_name": "jameswellsiv", "datetime": "Dec 4, 2018", "body": "I think you might need to include images where there are no bottles so the classifier can learn what it would look like if the object wasn't in the image", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 6, 2018", "body": "Exactly... The model was to recognize 2 bottle types, which means it assumes everything else it sees falls into these 2 categories.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 6, 2018", "body": "And considering the bottles are very similar, and you took their pictures of the same objects from different sides, your  images most likely amount to just  unique view of the bottles which led to your model overfitting.  was probably why your model achieved 100% accuracy but failed in test. You will need more truly diverse images of the bottles.", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 14, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/101", "issue_status": " Closed\n", "issue_list": [{"user_name": "gujiachen", "datetime": "Nov 4, 2018", "body": "when i run camera_feed_detection, I got the error：“TypeError: detectObjectsFromVideo() got an unexpected keyword argument 'camera_input'”.  So how can i fix it?", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Nov 9, 2018", "body": "Kindly comment your code.", "type": "commented", "related_issue": null}, {"user_name": "joyarx", "datetime": "Feb 18, 2019", "body": "see the source of Detection.py line 620:                 input_video = cv2.VideoCapture(input_file_path)\nso the right code should be:\nvideo_path = detector.detectObjectsFromVideo(\ninput_file_path=0,", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Nov 9, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Nov 9, 2018", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 14, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8303", "issue_status": " Closed\n", "issue_list": [{"user_name": "chitoku", "datetime": "Aug 8, 2022", "body": "This is similar to Issue , but is happening on Matek F405 based board.\nI built , which comes with F405 based FC with MPU6000 and BMP280.The virtual drone model moves, and Gyro, Accel and Baro buttons lit in blue, like shown in .\nI could not fix this and need help.When flashing BetaFlight 4.3.1 ( selected), and custom defaults applied, Gyro and Accel buttons lit up and the virtual drone model moves as FC wiggled.\nIn  tab,  is actually turned on.\nOn BetaFlight CLI", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Aug 8, 2022", "body": " you using wrong target.\n\"Matek F405 based\" FC is not Matek F405 FC.\nYou need to ask manufacturer for support, not INAV, as INAV developers don't have this FC and have no clue what it is.", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Aug 8, 2022", "body": "Additionally: I just looked at BF target (MATEKF405STD) and compared with INAV (MATEKF405), from the first look they are more or less the same, so, at least Gyro/Acc should be detected, as long as OSD, SD Card/Flash and UARTs should work.\nDo you selected Full chip erase during flashing your FC with INAV?", "type": "commented", "related_issue": null}, {"user_name": "chitoku", "datetime": "Aug 8, 2022", "body": "Hi  , thank you so much for sharing your insights.\nYes, I selected .\nIt's great news/confirmation for me that Betaflight target definition is more or less the same.\nI will try to see what's happening to the SPI and I2C communication on INAV.\nAfter all, Joshua Bardwell got to fly this Tyro 119 with INAV 2.4, so I'm hoping there is a way.", "type": "commented", "related_issue": null}, {"user_name": "chitoku", "datetime": "Aug 8, 2022", "body": "BTW, when I try to flash the old INAV 2.4.0 using INAV configurator 2.4.1, I cannot connect to the FC after flashing done.", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Aug 8, 2022", "body": "I believe that there is nothing to see what happens on SPI buses, only reason for that if you decided to start \"fixing\" code, building and etc. But you can try, ofc.Regarding INAV 2.4, it is unsupported and again, whitout having this exact FC on a bench I have nothing to check. Unfortunately.", "type": "commented", "related_issue": null}, {"user_name": "chitoku", "datetime": "Aug 14, 2022", "body": "It seems that Eachine started shipping their FC now with a different IMU. Mine had BM270 as hinted in their update note.\nIn my local copy, I defined a new target and I made some changes based on   to see if built firmware would work on the Eachine FC.\nThe result was that I got to make Gyro and Accel work along with Magnetometer and the GPS, so push that in my fork.\nThe BMP280 barometer on my board seems to be defective as it did not work on BetaFlighter either.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Aug 15, 2022", "body": "This is the reason it does not work: ", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Aug 15, 2022", "body": "According what you described I don't see issue in INAV.", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Aug 15, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8232", "issue_status": " Closed\n", "issue_list": [{"user_name": "joneszee69", "datetime": "Jul 15, 2022", "body": "I loaded Inav 5.0 on my matek 722 WPX Target FC and I cannot get the OSD to populate. When I  first turn on OSD menu there are a few things that appear on screen and in my goggles  like voltage,amp,RSSI and timer. As soon as I try to set up my custom values and hit save all of the OSD characters show on the screen but go away in my goggles  and I have nothing but The video feed with no OSD characters at all in goggles (I am using analog video system ) . I am using the default OSD setting and in configuration I have OSD tab turned on. I even tried it in auto , Pal and NTSD. I even tried moving the Characters around the screen and still nothing. I also tried to reload firmware, still nothing. I since downgraded to 4.0 and have no problem in OSD. Thanks", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jul 15, 2022", "body": "Just a thought. As it's the WPX. Is the FrSky OSD set on UART 6 on the ports page?", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jul 15, 2022", "body": "The graphical OSD's can sometimes be tricky. Make sure you have updated the graphical OSD to at least v2.0 AND the font before playing around in iNAV's osd page. \nLike Darren said the port has to be activated as well and don't use AUTO settings for the camera detection. Especially Matek products don't like that. Just set what you use all the way throught -> Cam-FC-(VTX-VRX)-Goggles", "type": "commented", "related_issue": null}, {"user_name": "joneszee69", "datetime": "Jul 15, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "joneszee69", "datetime": "Jul 15, 2022", "body": "Thanks guys for all your help the problem was I didn’t have the frisky OSD protocol selected, I must’ve tried to put something on that UART 6 and never reset it. Thank you Darren for your recommendation to check  the UART and for all the other guys suggestions…👍🏻", "type": "commented", "related_issue": null}, {"user_name": "joneszee69", "datetime": "Jul 15, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "JulioCesarMatias", "datetime": "Jul 22, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8143", "issue_status": " Closed\n", "issue_list": [{"user_name": "AndreaCCIE", "datetime": "Jun 21, 2022", "body": "On my Matek F411 WTE, flashed with either 5.0 Rc3 (F411te) or 4.1, the Lipo is plugged but the battery voltage on the configurator shows 0v, Cell count shows 1 and from the CLI:\nVoltage: 0.00V (1S battery - NOT PRESENT)The motor spins up at a limited pace, servos work (5v), camera and vtx (on vbat) are ok.\nTelemetry doesn't also report any bat voltage.Please note I'm not sure whether this is a software or a hardware issue. This FC is quite recent, I don't have another one available nor know anyone else running it.Battery is recognised and correct voltage shown.CLI Dump:\n", "type": "commented", "related_issue": null}, {"user_name": "AndreaCCIE", "datetime": "Jun 21, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "julianschweizer", "datetime": "Jun 22, 2022", "body": "I have two FCs here, for both the voltage/current meter worked fine. But one bricked with some other/unknown hardware issue (FC not booting up anymore) ... . Seems like bad quality from matek ... . Version\n INAV/MATEKF411TE 5.0.0 Jun 10 2022 / 09:25:02 ()\n GCC-10.2.1 20201103 (release)\n", "type": "commented", "related_issue": null}, {"user_name": "AndreaCCIE", "datetime": "Jun 22, 2022", "body": "According to Matek, there is a missing 1k resistor on this board. Not a software issue, I'm closing this one.\nI leave the reference pic below in case anyone else has got the same problem.", "type": "commented", "related_issue": null}, {"user_name": "MATEKSYS", "datetime": "Jun 23, 2022", "body": "Be careful with the resistor on the right of solder iron when soldering the Dupont pins\n", "type": "commented", "related_issue": null}, {"user_name": "AndreaCCIE", "datetime": "Jun 22, 2022", "body": [], "type": "issue", "related_issue": "#8147"}, {"user_name": "AndreaCCIE", "datetime": "Jun 22, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/125", "issue_status": " Closed\n", "issue_list": [{"user_name": "wassimgoodar", "datetime": "Dec 17, 2018", "body": "Hi,\nThe object detection function works well when used in while loop.\nHowever, when i try to run the same function using threading in python, it does not works.\nThe error led me to things like session ends,invalid input image path..... but I could not resolve it.\nIs it because we cannot use functions of imageai in multithreading in python or is there some more parameters to set.\nErrors i got:\nException in thread Thread-2:\nTraceback (most recent call last):\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1050, in _run\nsubfeed, allow_tensor=True, allow_operation=False)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3488, in as_graph_element\nreturn self._as_graph_element_locked(obj, allow_tensor, allow_operation)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3567, in _as_graph_element_locked\nraise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\nValueError: Tensor Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool) is not an element of this graph.During handling of the above exception, another exception occurred:Traceback (most recent call last):\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/imageai/Detection/.py\", line 777, in detectCustomObjectsFromImage\nK.learning_phase(): 0\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 887, in run\nrun_metadata_ptr)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1053, in _run\n'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool) is not an element of this graph.During handling of the above exception, another exception occurred:Traceback (most recent call last):\nFile \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\nself.run()\nFile \"/usr/lib/python3.5/threading.py\", line 862, in run\nself._target(*self._args, **self._kwargs)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/threadfinal.py\", line 53, in read\nobjectDetect(imageInProcess)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/singularObjectDetection.py\", line 13, in objectDetect\ndetections = detector.detectCustomObjectsFromImage(custom_objects=custom, input_image=os.path.join(execution_path,filename), output_image_path=os.path.join(execution_path, \"image3new-custom.jpg\"), minimum_percentage_probability=30)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/imageai/Detection/.py\", line 860, in detectCustomObjectsFromImage\n\"Ensure you specified correct input image, input type, output type and/or output image path \")\nValueError: Ensure you specified correct input image, input type, output type and/or output image pathMy function\nexecution_path = os.getcwd()\ndetector = ObjectDetection()\ndetector.setModelTypeAsTinyYOLOv3()\ndetector.setModelPath(os.path.join(execution_path, \"yolo-tiny.h5\"))\ndetector.loadModel(detection_speed=\"flash\")\ncustom = detector.CustomObjects(person=True, dog=True)\ndef objectDetect(filename):\nhumanDetected=False\ndetections = detector.detectCustomObjectsFromImage(custom_objects=custom, input_image=os.path.join(execution_path,filename), output_image_path=os.path.join(execution_path, \"image3new-custom.jpg\"), minimum_percentage_probability=30)\nfor eachObject in detections:\nprint(eachObject[\"name\"], \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"])\nprint(\"--------------------------------\")\nif(eachObject[\"name\"]==\"person\"):\nif(humanDetected==False):\nhumanDetected=True\nbreakprint(\"Finish detecting photo \"+ str(humanDetected))Thread calls the above functionq=queue.Queue()\nt1=threading.Thread(target=write,args=(q,),daemon=True)\n#the write function calls the object detection function", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Dec 18, 2018", "body": "When you are using multithreading, ensure you load the model [ detector.loadModel() ]in the same thread that you want to use for the detection. Or better still, you can load the model outside the thread and parse the \"detector\" object into the thread, from where you will call the detection function. ", "type": "commented", "related_issue": null}, {"user_name": "wassimgoodar", "datetime": "Dec 18, 2018", "body": "I already try calling the detector.loadModel() in the same thread and even outside the thread yet no positive result was obtained.\nI print the path of where the thread was being executed which was same as the path of the main program but yet i got the same error\nI did not parse the detector object though,\"how to do that\"???", "type": "commented", "related_issue": null}, {"user_name": "wassimgoodar", "datetime": "Dec 18, 2018", "body": "here is part of my script\nthis is my function that the thread t2 calls:\ndef read():\nprint(\"read \"+ os.getcwd())\nexecution_path = os.getcwd()\ndetector = ObjectDetection()\ndetector.setModelTypeAsTinyYOLOv3()\ndetector.setModelPath(os.path.join(execution_path, \"yolo-tiny.h5\"))\ndetector.loadModel(detection_speed=\"flash\")\ncustom = detector.CustomObjects(person=True, dog=True)\nwhile True:\nobjectDetect(\"image1.jpg\")And in main:t2=threading.Thread(target=read,daemon=True)\nt2.start()", "type": "commented", "related_issue": null}, {"user_name": "cconst04", "datetime": "Dec 20, 2018", "body": "I use django and your library works perfectly when i use the --nothreading option when launching the server. But when i remove that option, the first time when an http request for image analysis is sent the analysis works perfectly.\nBut then every time when an http request is sent to the django server for image analyzing i get the following errors.\nErrors:\nA summary of my code:def analyze(self):\nself.detector = ObjectDetection()\nself.detector.setModelTypeAsRetinaNet()\nself.detector.setModelPath( os.path.join(execution_path,\"resnet50_coco_best_v2.0.1.h5\"))\nself.detector.loadModel()\ndetections = self.detector.detectObjectsFromImage(input_image=self.input_image,\noutput_image_path=self.output_image)Please note that i don't use anywhere in my code threads, i don't know how django organizes my code in threads, i've tried putting the whole function in a thread but still the same error occurs", "type": "commented", "related_issue": null}, {"user_name": "KLiFF2606", "datetime": "Jan 16, 2019", "body": "Hi, is there any news about this issue? I have the same problem.Here is my thread class:I get this error:The same code works fine when it is inside a while true loop. All my files are in the same folder and I have checked the video and model path are correct.", "type": "commented", "related_issue": null}, {"user_name": "wassimgoodar", "datetime": "Jan 19, 2019", "body": "Hi,\ni am sure about it but maybe that the size allocated for each thread(bearing in mind that all the threads same the same memory space) is not big enough to load the model iand maybe that's what is causing the error.(something to do with threading.stack_size())I did get around my problem by using threads to do the other functions but call the object detection model from the main thread itself and works fine until now.", "type": "commented", "related_issue": null}, {"user_name": "buroa", "datetime": "Mar 12, 2019", "body": "Tried to give them an email to reply ... looks like their email server is even jacked. Opps.", "type": "commented", "related_issue": null}, {"user_name": "photogaff", "datetime": "May 8, 2019", "body": "I'm experiencing a similar issue while trying to run capture in one thread and imageai processing in another thread:As soon as I take the same code out of the thread/function, it processes as it should. I've tried passing the object and also used a global declaration and both produce the same error above. It doesn't seem to want to play.", "type": "commented", "related_issue": null}, {"user_name": "Angus1996", "datetime": "May 16, 2019", "body": "When I was using the imageai in my pyqt5 project, I met the same problem.However I solved that by laoding the model in the function \"run()\" of my thread. A part of code is like below. Hoping this can be helpful.", "type": "commented", "related_issue": null}, {"user_name": "rola93", "datetime": "Oct 7, 2019", "body": "I think  should solve this issue.Please, upgrade ImageAI () and check if it is solved now.If this is the case, please close this issue", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Oct 8, 2019", "body": "thanks for the comment This issue has been solved in the latest version.", "type": "commented", "related_issue": null}, {"user_name": "skoroneos", "datetime": "May 20, 2020", "body": "Greetings.\nAfter spending a couple of days on trying to get imageAI to work with Flask, it was made obvious that the threading issue is not resolved.\nTo make a long story short if the prediction is executed on a different thread that the model was loaded it fails with the following errorValueError: Tensor Tensor(\"predictions/Softmax:0\", shape=(?, 56), dtype=float32) is not an element of this graph.which is the same problem described in this issue and is related to this issue in KerasI have tried many different solution proposed but nothing worked.\nThe only way that \"worked\" is to force Flask not to use threading\ni.e\napp.run(host='0.0.0.0',threaded = False)I have not looked into the ImageAI code extensively but for custom image detection using InceptionV3 its 100% sure it does not work with threads.", "type": "commented", "related_issue": null}, {"user_name": "jasondalycan", "datetime": "Sep 25, 2020", "body": "Same issue as .  I'm using ImageAI 2.1.5.", "type": "commented", "related_issue": null}, {"user_name": "PIYUSH9090", "datetime": "Nov 21, 2020", "body": "Not solved in latest version still getting error.  I'm using ImageAI 2.1.5.", "type": "commented", "related_issue": null}, {"user_name": "1chimaruGin", "datetime": "Jan 21, 2021", "body": "Same issue here.", "type": "commented", "related_issue": null}, {"user_name": "M-Zubair10", "datetime": "Mar 27, 2022", "body": "did the issue solved?\nme getting same error when running with thread in selenium", "type": "commented", "related_issue": null}, {"user_name": "OlafenwaMoses", "datetime": "Oct 8, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7813", "issue_status": " Closed\n", "issue_list": [{"user_name": "audio98", "datetime": "Feb 8, 2022", "body": "ADC for both current and Vbat give correct values after \"save and reboot\"  but are static and do not change until the next \"save and reboot\"\nAll other features GPS, Magnetometer, LED lights, camera tilt servo, Barometer. motors work properly.", "type": "commented", "related_issue": null}, {"user_name": "audio98", "datetime": "Feb 8, 2022", "body": "Works in iNav 3.02 but not in 4.0", "type": "commented", "related_issue": null}, {"user_name": "terryfritz99", "datetime": "Feb 17, 2022", "body": "This has fixed the problem I had with no Current value displayed in OSD in 4.0.0 Thank you for the fix.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 10, 2022", "body": [], "type": "pull", "related_issue": "#7816"}, {"user_name": "DzikuVx", "datetime": "Feb 11, 2022", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 11, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "YellowMan87", "datetime": "Apr 23, 2022", "body": [], "type": "issue", "related_issue": "#7982"}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8010", "issue_status": " Closed\n", "issue_list": [{"user_name": "sdellava", "datetime": "May 7, 2022", "body": "Currently blackbox collect all data provided by the FC. This lead to quicky fill the blackbox memory, even selecting  low logging rate.\nSome fligth data, like i.e. the gryo/accelerometer, are required to elaborate video recorded with an oboard camera missing of gyro/acc.Due to the huge ammount of data collecte by default, the collection last no more than few minutes, making it impossible to elaborate a long video footage.Selecting only some data allow to extend the usable video length.Filter which data to collect into the BC memory, according with the configuration.Add a multichose menu in the configurator to select whick kind of data heve to be collected.\nFilter the data collection according the configuration.Everyone need tha gyro/acc. data only.", "type": "commented", "related_issue": null}, {"user_name": "stronnag", "datetime": "May 7, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "May 7, 2022", "body": "Also see Blackbox improvements section of INAV 4.0 release notes", "type": "commented", "related_issue": null}, {"user_name": "sdellava", "datetime": "May 7, 2022", "body": "Thanks . I totally missed it.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "May 7, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7971", "issue_status": " Closed\n", "issue_list": [{"user_name": "francis916", "datetime": "Apr 17, 2022", "body": "iNav 4.1 on Matek F411-WSE won't arm without giving the reason.System Uptime: 337 seconds\nCurrent Time: 2022-04-16T13:19:53.796+00:00\nVoltage: 10.92V (3S battery - OK)\nCPU Clock=96MHz, GYRO=MPU6000, ACC=MPU6000, BARO=SPL06\nSTM32 system clocks:\nSYSCLK = 96 MHz\nHCLK   = 96 MHz\nPCLK1  = 48 MHz\nPCLK2  = 96 MHz\nSensor status: GYRO=OK, ACC=OK, MAG=NONE, BARO=OK, RANGEFINDER=NONE, OPFLOW=NONE, GPS=OK, IMU2=NONE\nStack size: 6144, Stack address: 0x20020000, Heap available: 1728\nI2C Errors: 0, config size: 8808, max available config: 16384\nADC channel usage:\nBATTERY : configured = ADC 1, used = ADC 1\nRSSI : configured = ADC 3, used = none\nCURRENT : configured = ADC 2, used = ADC 2\nAIRSPEED : configured = ADC 4, used = none\nSystem load: 15, cycle time: 1027, PID rate: 973, RX rate: 90, System rate: 9\nArming disabled flags: CLI\nVTX: band: R, chan: 5, power: 1 (25 mW), freq: 5806 MHzHi. I setup iNav 4.1 on Matek F411-WSE but it won't arm without giving the reason.I think it's a bug because it is not arming giving a reason. I hope it's not a stupid mistake of mine.Thank you.--------Components\nFC Matek F411-WSE\nESC AtomRC 30A 2~4S BLS ESC\nModel AtmoRC Mobula 650mm Delta Wing\nRX Spektrum SPM4650\nTX Spektrum NX8\nGPS Beitian BN-220\nVTX Goku VTX625\nCam None\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "francis916", "datetime": "Apr 17, 2022", "body": "I added a capacitor to the FC, soldered at the 2nd motor tab. Could it be the cause by any chance? Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Jetrell", "datetime": "Apr 21, 2022", "body": "As the OSD messages show, its not seeing low throttle.\nYou don't want to touch the radio trims. Try setting the  to 0%... Idle is for multi-rotor use.\nAlso try adjusting the , possibly lower.", "type": "commented", "related_issue": null}, {"user_name": "francis916", "datetime": "Apr 21, 2022", "body": "Thanks! Will try.", "type": "commented", "related_issue": null}, {"user_name": "francis916", "datetime": "Apr 21, 2022", "body": "Solved! I tried rxrange, Motor idle power to 0%. Thank you!", "type": "commented", "related_issue": null}, {"user_name": "Jetrell", "datetime": "Apr 21, 2022", "body": " That's good to hear. Please close this help request now.  Thanks", "type": "commented", "related_issue": null}, {"user_name": "francis916", "datetime": "Apr 22, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7958", "issue_status": " Closed\n", "issue_list": [{"user_name": "cgrrty", "datetime": "Apr 12, 2022", "body": "Hi\nI'm trying to build electronic equipment indoors.\nOmnibus F4 Pro   1\nServos             2\nESC                 1\nsbus Receiver  1\nHow to set up the main motor to rotate without a gps receiver\nThe hardware connection is shown below\n", "type": "commented", "related_issue": null}, {"user_name": "cgrrty", "datetime": "Apr 12, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "cgrrty", "datetime": "Apr 12, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "iNavFlight", "datetime": "Apr 12, 2022", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Apr 12, 2022", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7878", "issue_status": " Closed\n", "issue_list": [{"user_name": "clemmmdawg", "datetime": "Mar 6, 2022", "body": "My Matek F405-WSE flight controller will only enter a safe to arm state when connected with USB and disconnected from battery voltage. Once the safe to arm state has been achieved, I am able to connect battery voltage, unplug the model from USB, arm, launch and fly. Below is the procedure I've come up with to get my model into an armed state and flying.Model should enter safe to arm state with battery plugged in and have no need for USB connection.A few Discord users suggested the problem may have to do with the HDOP value being too high. After some testing I was still able to reproduce the problem with an HDOP value between 1.5 and 2.0. This model has been flown in both GPS and stabilization modes with this convoluted arming procedure, so a hardware failure seems unlikely.I'm also unable to switch out of manual mode before the pre-arming checks are completed.", "type": "commented", "related_issue": null}, {"user_name": "Jetrell", "datetime": "Mar 6, 2022", "body": "I've experienced the same thing with one of my quads. Being that it has more electrical noise produced from the CRSF, VTX and camera, all being in close proximity to the GPS.\nGetting a GPS lock by USB, prevents these devices from being powered. Which helps it pickup weaker satellites, that are drowned out by the noise of those devices.There is a difference between getting 6 satellites for a general fix. And getting the required amount of satellites for a 3D fix.\nIt can sometimes require more than 8 Satellites to get a 3D fix in a noisy install.  While on a low electrical noise installation, a 3D fix can occur with only 4 satellites.\nWithout a 3D fix, the home position will not be recorded at Arming.", "type": "commented", "related_issue": null}, {"user_name": "clemmmdawg", "datetime": "Mar 6, 2022", "body": "The RX and GPS are both powered via 4v5 pins so they are on with either USB power or VBAT. My VTX is powered from the 8v pin that only turns on with VBAT. Physically, the RX and GPS are pretty close together, but both are on the opposite side of my model from the VTX. So the only thing that would interfere with the GPS would be the RX, both of which power on from USB. However, that is the only configuration I am able to arm in.Getting a 3D fix is not the problem as far as I can tell. My  of the failed pre-arm checks shows a 3D fix.", "type": "commented", "related_issue": null}, {"user_name": "Jetrell", "datetime": "Mar 6, 2022", "body": "You can try setting \nThis will allow the FC to arm without a GPS fix. And may help you track down the problem, by seeing if it records the home position when it arms.\nThis may prove if its a GPS issue, that is stopping it from arming.", "type": "commented", "related_issue": null}, {"user_name": "breadoven", "datetime": "Mar 7, 2022", "body": "It's the runtime calibration that's causing the problem, I'm guessing Gyro (or something related to Gravity possibly) failing to calibrate. This will cause the Navigation Safety issue (invalid Position status) and also the Manual mode issue since Manual mode is forced on until runtime calibration has completed.Not sure why having the battery connected would cause the calibration issue however other than it being FC board related.", "type": "commented", "related_issue": null}, {"user_name": "clemmmdawg", "datetime": "Mar 7, 2022", "body": "I'll give this a shot in the next day or two.I was drawing the same conclusions, I just don't know enough about how INAV is coded to be sure. I have a feeling you're right about it being a problem with this specific board though since I have two others with zero issues. I guess if the nav_extra_arming_safety doesn't work, I'll be swapping out the FC.", "type": "commented", "related_issue": null}, {"user_name": "clemmmdawg", "datetime": "Mar 18, 2022", "body": "After a FC swap, everything worked fine. Must have been a hardware issue. Maybe we could get more verbose messages in the OSD or an error code alluding to the problem instead of \"waiting for GPS fix\" when navigation/runtime calibrations are unsafe.Thanks for the help!", "type": "commented", "related_issue": null}, {"user_name": "clemmmdawg", "datetime": "Mar 18, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7820", "issue_status": " Closed\n", "issue_list": [{"user_name": "access3424", "datetime": "Feb 11, 2022", "body": "See attached screenshot. Graphical OSD artificial horizon does not match Airplane pitch attitude. In level flight raw pitch attitude shows 3.4 degrees nose up (Which is correct) and graphical horizon showing 6 degrees nose down. There is also  fw_level_pitch_trim =  4.000 and  align_board_roll = 0     align_board_pitch = 25    align_board_yaw = 900The above settings show the airplane level on the bench. The fw_level_pitch_trim is for angle mode to work better\n1.\nAll attitude references to match. ie. the graphical pitch indicator to match reality.INAV/MATEKF722WPX 4.0.0 Dec 15 2021 / 15:09:49 ()", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Feb 11, 2022", "body": "try to adjust with osd_ahi_vertical_offset", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 11, 2022", "body": "This is the angle of your camera. iNav doesn’t have any control over that. The CLI command that StuWe posted will sort your problem.", "type": "commented", "related_issue": null}, {"user_name": "access3424", "datetime": "Feb 11, 2022", "body": "I wasn’t referring to the difference between the camera horizon and the artificial horizon line. I was referring to the indicated angle (dot between 0 and -10) but i will try that cli command see what happens. 👍🏻", "type": "commented", "related_issue": null}, {"user_name": "access3424", "datetime": "Feb 11, 2022", "body": "I used osd_ahi_vertical_offset. it was by default -18 no idea why. After reseting to 0 i got the desired result", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 11, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 11, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "access3424", "datetime": "Feb 11, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8034", "issue_status": " Closed\n", "issue_list": [{"user_name": "MartinHugh", "datetime": "May 12, 2022", "body": "Following a maiden of a wing with iNav 4.01, and carrying out auto level and autotune, the aircraft flew well in all modes configured (Horizon/Acro/Manual).  Checking for flying level using LOS the aircraft looked fine, but through FPV, the AHI was well below the horizon.  I initally suspected that this could be down to camera tilt, but I only have a tiny amount set normally. This could be a contributing factor to the exaggerated downward angle, but the OSD is not vertically centred anyway.When reviewing frames from the FPV recording, and expecting the vertical centre of the OSD display to be the vertical centre of the image, I found it was about 55-60% of the way down.The OSD should be vertically and horizontally centred.  If the aircraft is flying level, the camera is straight and the OSD is centred, the levelled AHI and aircraft symbol should be close to the real horizon rather than pointing downIs it possible to vertically centre the OSD?\nAre there any CLI commands I can enter to adjust its offsets ?Using HDZero Whoop VTX with latest firmware and HDZero Nano Micro V1 camera\nosd_video_system = HDINAV/MATEKF405SE 4.1.0 Feb 11 2022 / 10:52:32 ()\nGCC-10.2.1 20201103 (release)Attached image is in Horizon mode, with Auto Level ganged to that mode selectionCLI dump pasted to :\n", "type": "commented", "related_issue": null}, {"user_name": "Jetrell", "datetime": "May 13, 2022", "body": "The analog character OSD has a similar offset, which makes sense, because of it even number of character positions. Maybe the HDzero rendering is based on it.\nThere are some AHI offset commands, that work on the analog Character and Pixel OSD.. I haven't tried them on HDzero. But they may work.\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 13, 2022", "body": " thanks for your post.\nI have tried changing the followingosd_horizon_offset\nto various values between -2 and +2\nNo changeosd_row_shiftdown\nto 0 and 1\nNo changeosd_ahi_vertical_offset\nto +40,-40,-18\nNo changeobviously in each case I have confirmed the value set properly by calling \"get\" on the parameter and then running \"save\" to save and reboot the FC.Nothing is moving in the display.Am I missing something?\nCould it be that the HDZero generation of OSD is not subject to these parameters?", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "May 13, 2022", "body": "I can only see parameters for the analogue OSD and the FrSky Pixel OSD. There’s nothing specific for HD. Maybe  could let us know if the analogue parameters should move the AHI on HDZero?", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 13, 2022", "body": "I have not done anything special with the HDZero canvas mode. It should be the same as analogue, just with a larger canvas. If you think there is definitely an issue, I can have a look over the next few days...", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 13, 2022", "body": " can you move the OSD within the image using the commands I have tried.  It is possible there is something I am missing here. Thanks", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 14, 2022", "body": "\"osd_ahi_vertical_offset\"\n\"osd_ahi_height\"These are pixel OSD only.", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 14, 2022", "body": "\"osd_horizon_offset\"\n\"To vertically adjust the whole OSD and AHI and scrolling bars\"I have tried this and it does work. +/- 2 lines from the center.\nNote that the configurator does not show this offset when set.", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 14, 2022", "body": "\"osd_sidebar_horizontal_offset\"\n\"Sidebar horizontal offset from default position. Positive values move the sidebars closer to the edges.\"I have tested this and the sidebars move as expected.\nAs above, the configurator does not reflect this offset.", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 14, 2022", "body": "Current iNav locks the width and height of the AHI to 9 rows by 11 chars.\nThe location is also fixed to the center of the display size, adjustable only by the \"osd_horizon_offset\" parameter. Unfortunately, this is locked to +/-2.When I implemented \"HD\" mode, I didn't want to change any of the AHI code as it would have just complicated things. I thought that this area could be re-visited at a later date.", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 14, 2022", "body": " thank you for trying these.  I tried this one in particular and got no change on the actual OSD.\nSo there is some hope then that I am doing something wrong.I did not repower the FC, I assumed the reboot after saving the CLI setting would have been enough, but no change to the display.I will try again.  If you can see anything obvious I may have done wrong I would appreciate a steer.", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 14, 2022", "body": " , I realise you are stating that the dimensions of the AHI above, but in relation to this I note in info from HDZero, they reserve space for the VTX OSD at the top (not sure if they do the same at the bottom) and this might explain why there is a gap at the top before the first iNav OSD row.[Edit 720-648 = 72 which is enough height for two 36px chars.  Does this mean they are reserving 1 row at the top and one at the bottom, or two at the top? Either way, in the display there is no bottom margin shown.]The area that HDZero render is 50x18 chars (1200px * 648px)The glaring problem there, which was alluded to by  in an earlier post is that if there are an even number of rows, how can you have a centre one.", "type": "commented", "related_issue": null}, {"user_name": "geoffsim", "datetime": "May 14, 2022", "body": "The VRX retains one line at the top and one at the bottom. The 18 lines for the OSD should be central. Try it by placing widgets at the extreme edges of the OSD. Note there is also a margin left and right.", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 14, 2022", "body": " that sounds right.  The 18 rows should be central, the point I was making was that with an even number of rows there is no way any particular row (like the centre of the AHI) can ever be vertically central anyway.I hope to retry the osd_horizon_offset very soon and will report back\nThank you.", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 14, 2022", "body": "set osd_horizon_offset=-2\nsaveDoes indeed move the AHI up 2 chars.\nWhether we need scope for more movement is another topic.Sorry for the finger trouble on this.  I think perhaps I had misunderstood and was expecting the whole osd to move and the bottom row stayed put.  But in fact this does what I need.Thank you for your help with this  and thank you for the work on the HDZero OSD feature itself.", "type": "commented", "related_issue": null}, {"user_name": "MartinHugh", "datetime": "May 14, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/8000", "issue_status": " Closed\n", "issue_list": [{"user_name": "flyingzzy", "datetime": "May 2, 2022", "body": "Mini Ar wing, all conditions are good.\nwhen about 5km far and 540m high, RC link got lost and the the Failsafe rth was trigered,but it doesnt just turn around and fly back ,instead, it loitered a few cycles and finally decided to emergency landing.\nin the video we can see when rth trigered,zhe plane began to turn back ,but the yaw angle seemed to be \"frozen\",that caused the FC to make more cycles, but the angle was still very strange , so the FC decided to land.\nthe sensor was always good,even in earlly phase of this flight and in the following flights,it seems that somewhere hides a bug?\nnote that when i switched to cruise mode and fly towards home direction,fs rth happened again,but this time it doesnt do any strange behaviors. so why?1.FLY\n2.far enough to lost rc link,live home behind\n3.see what happens.when rc lost,the plane should go straight back homeNot sure.", "type": "commented", "related_issue": null}, {"user_name": "breadoven", "datetime": "May 3, 2022", "body": "The emergency landing was caused by Sanity checking, i.e. it flew 500m further away from home after the RTH was triggered. See .Not sure why the RTH failed to go home though, difficult to say without a log. Were you manually correcting the CRUZ direction when the failsafe occurred ?", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 4, 2022", "body": "i have found the reason.\nif rth is trigered while the plane is flying against home, fc try to turn around and fly abck,but the gyroscope seems working wrong,the yaw  angle changes very slow, maybe the sensor was affected by the wires of the VTX.i have changed the wire routing,and will do a test later", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 4, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "breadoven", "datetime": "May 4, 2022", "body": "I doubt yaw related stuff is the reason. Far more likely related to this bug .If you were moving the pitch/roll stick when the failsafe occurred RTH Nav control would have become stuck in manual RC adjustment mode preventing RTH from navigating back home.", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 5, 2022", "body": "\nI have done a test with nothing  changed.\nset vtx at full power, pick up the plane and turn a few circles, left and right. the gyroscope work very well.\nThe reason seems to be a code  bug, as you have found.\nI have met twice rth error recent days.When signal was low, i was doing controls(moving sticks).", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 5, 2022", "body": "Here is another video, pay attention to the home arrow and the yaw direction change, it seems to be frozen at some moment.\nAt first i was flying at cruise mode and adjusting yaw direction, the rc signal was good. then i switch to RTH mode", "type": "commented", "related_issue": null}, {"user_name": "breadoven", "datetime": "May 5, 2022", "body": "The bug mentioned above kicks in only if you're moving the controls at the moment the failsafe happens.Looking at the recent video I'd say there is a problem with horizon drift. When the horizon in the camera view is level the roll angle is around 10 degrees when it should be zero obviously. That drunken rolling movement is what you see when horizon drift is affecting navigation. There is also maybe some drift on the pitch also, but not so obvious. A known problem with INAV, possibly worse on flying wings if they suffer from pitch oscillations.", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 6, 2022", "body": "I understand the reason of the first video,its a code bug which has been found and will be solved in the following release.The horizon drift is another puzzle, it happens in every flight.If u continuous move the roll stick toward one direction,the  horizon line will go wrong. To correct it,u can move the stick toward the other direction for a while, or u can switch to cruise mode to let the fc correct it. Will this problem be solved in the following release?", "type": "commented", "related_issue": null}, {"user_name": "breadoven", "datetime": "May 6, 2022", "body": "Horizon drift is a long standing issue with INAV and won't get fixed until someone works out how to fix it. All you can do to minimise it is make sure the plane is mechanically well trimmed and PIDs well tuned to try and minimise pitch/roll oscillations, pitch oscillations in particular on a flying wing.", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 6, 2022", "body": "\nHere  is another question, in early release i used to turn on altitude sidebar scroll arrow, but i found the move direction was wrong:if u dive,the sidebar arrow scrolls down. On real plane HUD it should scroll up.\nDo u know if this has been fixed or not?\n", "type": "commented", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 5, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "flyingzzy", "datetime": "May 9, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7801", "issue_status": " Closed\n", "issue_list": [{"user_name": "j-watkiss", "datetime": "Feb 5, 2022", "body": "set rssi_scale = 200 returns ### ERROR: Invalid nameRSSI scale should be set to 200batch startmmix resetmmix 0  1.000  0.000  0.000  0.000smix resetsmix 0 2 0 -100 0 -1\nsmix 1 3 1 50 0 -1\nsmix 2 3 2 50 0 -1\nsmix 3 4 1 -50 0 -1\nsmix 4 4 2 50 0 -1servo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature MOTOR_STOP\nfeature GPS\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSDbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDblackbox -NAV_ACC\nblackbox NAV_POS\nblackbox NAV_PID\nblackbox MAG\nblackbox ACC\nblackbox ATTI\nblackbox RC_DATA\nblackbox RC_COMMAND\nblackbox MOTORSmap AETRserial 20 1 115200 115200 0 115200\nserial 0 2048 115200 115200 0 115200\nserial 1 2 115200 115200 0 115200\nserial 2 0 115200 115200 0 115200\nserial 4 0 115200 115200 0 115200\nserial 5 64 115200 115200 0 115200\nserial 6 0 115200 115200 0 115200\nserial 7 1024 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 3 1700 2100\naux 1 2 2 1300 1700\naux 2 12 2 1700 2100\naux 3 10 0 1700 2100\naux 4 53 1 1300 1700\naux 5 11 0 1300 1700\naux 6 3 1 1300 1700\naux 7 21 4 1700 2100\naux 8 37 2 1300 2100\naux 9 54 4 1300 1700\naux 10 40 7 1700 2100\naux 11 27 6 1700 2100\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0\nwp 60 0 0 0 0 0 0 0 0\nwp 61 0 0 0 0 0 0 0 0\nwp 62 0 0 0 0 0 0 0 0\nwp 63 0 0 0 0 0 0 0 0\nwp 64 0 0 0 0 0 0 0 0\nwp 65 0 0 0 0 0 0 0 0\nwp 66 0 0 0 0 0 0 0 0\nwp 67 0 0 0 0 0 0 0 0\nwp 68 0 0 0 0 0 0 0 0\nwp 69 0 0 0 0 0 0 0 0\nwp 70 0 0 0 0 0 0 0 0\nwp 71 0 0 0 0 0 0 0 0\nwp 72 0 0 0 0 0 0 0 0\nwp 73 0 0 0 0 0 0 0 0\nwp 74 0 0 0 0 0 0 0 0\nwp 75 0 0 0 0 0 0 0 0\nwp 76 0 0 0 0 0 0 0 0\nwp 77 0 0 0 0 0 0 0 0\nwp 78 0 0 0 0 0 0 0 0\nwp 79 0 0 0 0 0 0 0 0\nwp 80 0 0 0 0 0 0 0 0\nwp 81 0 0 0 0 0 0 0 0\nwp 82 0 0 0 0 0 0 0 0\nwp 83 0 0 0 0 0 0 0 0\nwp 84 0 0 0 0 0 0 0 0\nwp 85 0 0 0 0 0 0 0 0\nwp 86 0 0 0 0 0 0 0 0\nwp 87 0 0 0 0 0 0 0 0\nwp 88 0 0 0 0 0 0 0 0\nwp 89 0 0 0 0 0 0 0 0\nwp 90 0 0 0 0 0 0 0 0\nwp 91 0 0 0 0 0 0 0 0\nwp 92 0 0 0 0 0 0 0 0\nwp 93 0 0 0 0 0 0 0 0\nwp 94 0 0 0 0 0 0 0 0\nwp 95 0 0 0 0 0 0 0 0\nwp 96 0 0 0 0 0 0 0 0\nwp 97 0 0 0 0 0 0 0 0\nwp 98 0 0 0 0 0 0 0 0\nwp 99 0 0 0 0 0 0 0 0\nwp 100 0 0 0 0 0 0 0 0\nwp 101 0 0 0 0 0 0 0 0\nwp 102 0 0 0 0 0 0 0 0\nwp 103 0 0 0 0 0 0 0 0\nwp 104 0 0 0 0 0 0 0 0\nwp 105 0 0 0 0 0 0 0 0\nwp 106 0 0 0 0 0 0 0 0\nwp 107 0 0 0 0 0 0 0 0\nwp 108 0 0 0 0 0 0 0 0\nwp 109 0 0 0 0 0 0 0 0\nwp 110 0 0 0 0 0 0 0 0\nwp 111 0 0 0 0 0 0 0 0\nwp 112 0 0 0 0 0 0 0 0\nwp 113 0 0 0 0 0 0 0 0\nwp 114 0 0 0 0 0 0 0 0\nwp 115 0 0 0 0 0 0 0 0\nwp 116 0 0 0 0 0 0 0 0\nwp 117 0 0 0 0 0 0 0 0\nwp 118 0 0 0 0 0 0 0 0\nwp 119 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 2 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 3 V\nosd_layout 0 12 1 4 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 0 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 H\nosd_layout 0 23 1 1 H\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 2 10 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 H\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 0 125 0 0 H\nosd_layout 0 126 0 0 H\nosd_layout 0 127 0 0 H\nosd_layout 0 128 0 0 H\nosd_layout 0 129 0 10 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 1 125 0 0 H\nosd_layout 1 126 0 0 H\nosd_layout 1 127 0 0 H\nosd_layout 1 128 0 0 H\nosd_layout 1 129 0 10 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 2 125 0 0 H\nosd_layout 2 126 0 0 H\nosd_layout 2 127 0 0 H\nosd_layout 2 128 0 0 H\nosd_layout 2 129 0 10 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 H\nosd_layout 3 125 0 0 H\nosd_layout 3 126 0 0 H\nosd_layout 3 127 0 0 H\nosd_layout 3 128 0 0 H\nosd_layout 3 129 0 10 Hset looptime = 1000\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_main_lpf_hz = 25\nset gyro_main_lpf_type = BIQUAD\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 30\nset gyro_to_use = 0\nset setpoint_kalman_enabled = ON\nset setpoint_kalman_q = 100\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = ICM42605\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = -7\nset acczero_y = 9\nset acczero_z = 4\nset accgain_x = 4074\nset accgain_y = 4092\nset accgain_z = 4081\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = NONE\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = SPL06\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = FPORT\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 1\nset blackbox_device = SDCARD\nset sdcard_detect_inverted = OFF\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 400\nset motor_pwm_protocol = STANDARD\nset motor_poles = 14\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = LAND\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 2700\nset vbat_meter_type = ADC\nset vbat_scale = 2100\nset current_meter_scale = 150\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = AIRPLANE\nset has_flaps = OFF\nset model_preview_type = 29\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 1000\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 5000\nset imu_dcm_ki_mag = 0\nset small_angle = 180\nset imu_acc_ignore_rate = 9\nset imu_acc_ignore_slope = 5\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 3\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = AUTO\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = OFF\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = STICK_CENTER_ONCE\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 80\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 5000\nset nav_wp_safe_distance = 10000\nset nav_wp_mission_restart = RESUME\nset nav_wp_multi_mission_index = 1\nset nav_auto_speed = 300\nset nav_max_auto_speed = 1000\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_fw_soaring_motor_stop = OFF\nset nav_fw_soaring_pitch_deadband = 5\nset nav_rth_climb_first = ON\nset nav_rth_climb_first_stage_mode = AT_LEAST\nset nav_rth_climb_first_stage_altitude = 0\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = FS_ONLY\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 5000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mission_planner_reset = ON\nset nav_mc_bank_angle = 30\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 2\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 39\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_stats_page_auto_swap_time = 3\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_airspeed_alarm_min =  0.000\nset osd_airspeed_alarm_max =  0.000\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_esc_rpm_precision = 3\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_smartaudio_alternate_softserial_method = ON\nset vtx_band = 5\nset vtx_channel = 8\nset vtx_power = 1\nset vtx_low_power_disarm = UNTIL_FIRST_ARM\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dji_message_speed_source = 3D\nset dji_rssi_source = RSSI\nset dji_use_adjustments = OFF\nset dji_cn_alternating_duration = 30\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset beeper_pwm_mode = ON\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 40\nset mc_i_pitch = 30\nset mc_d_pitch = 23\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 30\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 85\nset mc_i_yaw = 45\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 15\nset fw_i_pitch = 5\nset fw_d_pitch = 5\nset fw_ff_pitch = 80\nset fw_p_roll = 15\nset fw_i_roll = 3\nset fw_d_roll = 7\nset fw_ff_roll = 50\nset fw_p_yaw = 20\nset fw_i_yaw = 0\nset fw_d_yaw = 0\nset fw_ff_yaw = 100\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 450\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 10\nset dterm_lpf_type = PT2\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = PT1\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  0.500\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 15\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 5\nset nav_fw_pos_xy_p = 60\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_min =  1.000\nset d_boost_max =  1.000\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  1.000\nset antigravity_accelerator =  1.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 30\nset rc_yaw_expo = 30\nset roll_rate = 18\nset pitch_rate = 9\nset yaw_rate = 3\nset manual_rc_expo = 35\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0\nset rate_dynamics_center_sensitivity = 100\nset rate_dynamics_end_sensitivity = 100\nset rate_dynamics_center_correction = 10\nset rate_dynamics_end_correction = 10\nset rate_dynamics_center_weight = 0\nset rate_dynamics_end_weight = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAH\nset controlrate_profile = 0\nset throttle_scale =  1.000\nset throttle_idle =  5.000\nset turtle_mode_power_factor = 55\nset failsafe_throttle = 1000\nset fw_min_throttle_down_pitch = 0\nset nav_mc_hover_thr = 1500\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_pitch2thr = 10\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0batch end", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 5, 2022", "body": "There is no  CLI command in iNav. You use  and  to set the range.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 5, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7797", "issue_status": " Closed\n", "issue_list": [{"user_name": "HerpDerpula", "datetime": "Feb 3, 2022", "body": "The FlywooF411V2 used in the Flywoo Explorer LR is causing some issues on iNav (iNav 3.0.1. and 4.0.0 were tested). A BMP280 barometer and Matek M8Q-5883 GPS/Magnetometer were added to the board with the baro and mag wired in parallel on the I2C bus via the SDA/SDL pads.Behavior: iNav intermittently/randomly reports compass and barometer failures. This appears to lead to the flight controller glitching out and causing there to be no Vtx connection in the form of 1) no signal lock at all or 2) The OSD being present without the camera.Lowering the I2C bus speed appears to make the issue worse. A second different Flywoo Explorer LR flashed on a custom user modified/compiled version of iNav 2.6 does not have this issue and performs flawlessly. It's possible that this is a hardware issue but we believe it to be software related.", "type": "commented", "related_issue": null}, {"user_name": "iNavFlight", "datetime": "Feb 7, 2022", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 7, 2022", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7791", "issue_status": " Closed\n", "issue_list": [{"user_name": "RickzFPV", "datetime": "Jan 31, 2022", "body": "I am curious if in the Rover profile, things could be changed to either accommodate 4 motor inputs or 4 servo inputs.  I know it is possible because the quadcopter profile, gives access to 4 motors.  I am working on a STEM Project for and with my son, for his Robotics Class.  I will include some pictures, The car runs on a omnibus F4 aio, and I can set up 2 motor inputs and 2 servo inputs but with however I try and do the mixing I just can't get the control of the car consistent.  The end goal with this is to be able to add a gps, and fpv camera to it so it 1 can be driven from the transmitter manually with fpv, and also be able to have it do waypoint missions autonomously.  I really want to be able to achieve this with INAV as it is not possible to do this with any other firmware that I can find.  thank you for taking the time to consider this.  Here are a couple of photos of the project at this point.Rick\n\n", "type": "commented", "related_issue": null}, {"user_name": "iNavFlight", "datetime": "Feb 23, 2022", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 23, 2022", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7789", "issue_status": " Closed\n", "issue_list": [{"user_name": "DataStorageCS", "datetime": "Jan 30, 2022", "body": "HDZero connected to H743 Wing V2. OSD elements in goggles don't seem to be aligning what is selected in the configurator. For example, average cell voltage is selected and centered in the OSD tab in the configurator but what shows on the output is a combination of km and sats.Expected behavior would be OSD elements appearing in the goggles according to what they should be in the configurator.\ndumpbatch startmmix resetmmix 0  1.000  0.000  0.000  0.000\nmmix 1  1.000  0.000  0.000  0.000smix resetsmix 0 3 0 50 0 -1\nsmix 1 3 1 50 0 -1\nsmix 2 4 0 -50 0 -1\nsmix 3 4 1 50 0 -1servo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1503 -100\nservo 4 1000 2000 1503 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature MOTOR_STOP\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSD\nfeature FW_LAUNCH\nfeature FW_AUTOTRIMbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDblackbox -NAV_ACC\nblackbox NAV_POS\nblackbox NAV_PID\nblackbox MAG\nblackbox ACC\nblackbox ATTI\nblackbox RC_DATA\nblackbox RC_COMMAND\nblackbox MOTORSmap TAERserial 20 1 115200 115200 0 115200\nserial 0 1 115200 115200 0 115200\nserial 1 0 115200 115200 0 115200\nserial 2 33554432 115200 115200 0 115200\nserial 3 0 115200 115200 0 115200\nserial 5 64 115200 115200 0 115200\nserial 6 0 115200 115200 0 115200\nserial 7 0 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 1625 2100\naux 1 1 1 1700 2100\naux 2 12 1 900 1300\naux 3 10 3 1600 2100\naux 4 53 2 1700 2100\naux 5 21 2 1300 1700\naux 6 54 1 1700 2100\naux 7 0 0 900 900\naux 8 0 0 900 900\naux 9 0 0 900 900\naux 10 0 0 900 900\naux 11 0 0 900 900\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0\nwp 60 0 0 0 0 0 0 0 0\nwp 61 0 0 0 0 0 0 0 0\nwp 62 0 0 0 0 0 0 0 0\nwp 63 0 0 0 0 0 0 0 0\nwp 64 0 0 0 0 0 0 0 0\nwp 65 0 0 0 0 0 0 0 0\nwp 66 0 0 0 0 0 0 0 0\nwp 67 0 0 0 0 0 0 0 0\nwp 68 0 0 0 0 0 0 0 0\nwp 69 0 0 0 0 0 0 0 0\nwp 70 0 0 0 0 0 0 0 0\nwp 71 0 0 0 0 0 0 0 0\nwp 72 0 0 0 0 0 0 0 0\nwp 73 0 0 0 0 0 0 0 0\nwp 74 0 0 0 0 0 0 0 0\nwp 75 0 0 0 0 0 0 0 0\nwp 76 0 0 0 0 0 0 0 0\nwp 77 0 0 0 0 0 0 0 0\nwp 78 0 0 0 0 0 0 0 0\nwp 79 0 0 0 0 0 0 0 0\nwp 80 0 0 0 0 0 0 0 0\nwp 81 0 0 0 0 0 0 0 0\nwp 82 0 0 0 0 0 0 0 0\nwp 83 0 0 0 0 0 0 0 0\nwp 84 0 0 0 0 0 0 0 0\nwp 85 0 0 0 0 0 0 0 0\nwp 86 0 0 0 0 0 0 0 0\nwp 87 0 0 0 0 0 0 0 0\nwp 88 0 0 0 0 0 0 0 0\nwp 89 0 0 0 0 0 0 0 0\nwp 90 0 0 0 0 0 0 0 0\nwp 91 0 0 0 0 0 0 0 0\nwp 92 0 0 0 0 0 0 0 0\nwp 93 0 0 0 0 0 0 0 0\nwp 94 0 0 0 0 0 0 0 0\nwp 95 0 0 0 0 0 0 0 0\nwp 96 0 0 0 0 0 0 0 0\nwp 97 0 0 0 0 0 0 0 0\nwp 98 0 0 0 0 0 0 0 0\nwp 99 0 0 0 0 0 0 0 0\nwp 100 0 0 0 0 0 0 0 0\nwp 101 0 0 0 0 0 0 0 0\nwp 102 0 0 0 0 0 0 0 0\nwp 103 0 0 0 0 0 0 0 0\nwp 104 0 0 0 0 0 0 0 0\nwp 105 0 0 0 0 0 0 0 0\nwp 106 0 0 0 0 0 0 0 0\nwp 107 0 0 0 0 0 0 0 0\nwp 108 0 0 0 0 0 0 0 0\nwp 109 0 0 0 0 0 0 0 0\nwp 110 0 0 0 0 0 0 0 0\nwp 111 0 0 0 0 0 0 0 0\nwp 112 0 0 0 0 0 0 0 0\nwp 113 0 0 0 0 0 0 0 0\nwp 114 0 0 0 0 0 0 0 0\nwp 115 0 0 0 0 0 0 0 0\nwp 116 0 0 0 0 0 0 0 0\nwp 117 0 0 0 0 0 0 0 0\nwp 118 0 0 0 0 0 0 0 0\nwp 119 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 H\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 2 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 3 V\nosd_layout 0 12 1 4 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 3 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 V\nosd_layout 0 23 0 6 V\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 13 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 2 V\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 0 7 V\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 0 8 V\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 0 V\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 0 125 0 0 H\nosd_layout 0 126 0 0 H\nosd_layout 0 127 0 0 H\nosd_layout 0 128 0 0 H\nosd_layout 0 129 0 10 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 1 125 0 0 H\nosd_layout 1 126 0 0 H\nosd_layout 1 127 0 0 H\nosd_layout 1 128 0 0 H\nosd_layout 1 129 0 10 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 2 125 0 0 H\nosd_layout 2 126 0 0 H\nosd_layout 2 127 0 0 H\nosd_layout 2 128 0 0 H\nosd_layout 2 129 0 10 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 H\nosd_layout 3 125 0 0 H\nosd_layout 3 126 0 0 H\nosd_layout 3 127 0 0 H\nosd_layout 3 128 0 0 H\nosd_layout 3 129 0 10 Hset looptime = 1000\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_main_lpf_hz = 25\nset gyro_main_lpf_type = BIQUAD\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 30\nset gyro_to_use = 0\nset setpoint_kalman_enabled = ON\nset setpoint_kalman_q = 100\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = -5\nset acczero_y = 7\nset acczero_z = -102\nset accgain_x = 4057\nset accgain_y = 4083\nset accgain_z = 4081\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = NONE\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = SPL06\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = CRSF\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 1\nset blackbox_device = SDCARD\nset sdcard_detect_inverted = OFF\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 400\nset motor_pwm_protocol = STANDARD\nset motor_poles = 14\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = RTH\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 250\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = AIRPLANE\nset has_flaps = OFF\nset model_preview_type = 8\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 1000\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 5000\nset imu_dcm_ki_mag = 0\nset small_angle = 180\nset imu_acc_ignore_rate = 9\nset imu_acc_ignore_slope = 5\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 3\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX7\nset gps_sbas_mode = WAAS\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = ON\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = STICK_CENTER_ONCE\nset airmode_throttle_threshold = 1150\nset fw_autotune_min_stick = 50\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 80\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 5000\nset nav_wp_safe_distance = 10000\nset nav_wp_mission_restart = RESUME\nset nav_wp_multi_mission_index = 1\nset nav_auto_speed = 300\nset nav_max_auto_speed = 1000\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_fw_soaring_motor_stop = OFF\nset nav_fw_soaring_pitch_deadband = 5\nset nav_rth_climb_first = ON\nset nav_rth_climb_first_stage_mode = AT_LEAST\nset nav_rth_climb_first_stage_altitude = 0\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = FS_ONLY\nset nav_rth_alt_mode = AT_LEAST_LINEAR_DESCENT\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 10000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mission_planner_reset = ON\nset nav_mc_bank_angle = 30\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_bank_angle = 45\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 2\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1600\nset nav_fw_launch_max_angle = 60\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 250\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 10000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = ON\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_stats_page_auto_swap_time = 3\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_rssi_dbm_max = -30\nset osd_rssi_dbm_min = -120\nset osd_temp_label_align = LEFT\nset osd_airspeed_alarm_min =  0.000\nset osd_airspeed_alarm_max =  0.000\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE2\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_esc_rpm_precision = 3\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_smartaudio_alternate_softserial_method = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dji_message_speed_source = 3D\nset dji_rssi_source = RSSI\nset dji_use_adjustments = OFF\nset dji_cn_alternating_duration = 30\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset beeper_pwm_mode = ON\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 40\nset mc_i_pitch = 30\nset mc_d_pitch = 23\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 30\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 85\nset mc_i_yaw = 45\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 15\nset fw_i_pitch = 5\nset fw_d_pitch = 5\nset fw_ff_pitch = 204\nset fw_p_roll = 15\nset fw_i_roll = 3\nset fw_d_roll = 7\nset fw_ff_roll = 79\nset fw_p_yaw = 20\nset fw_i_yaw = 0\nset fw_d_yaw = 0\nset fw_ff_yaw = 100\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 550\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 10\nset dterm_lpf_type = PT2\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = PT1\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  0.200\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 15\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 5\nset nav_fw_pos_xy_p = 60\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_min =  1.000\nset d_boost_max =  1.000\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  1.000\nset antigravity_accelerator =  1.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 30\nset rc_yaw_expo = 30\nset roll_rate = 16\nset pitch_rate = 5\nset yaw_rate = 3\nset manual_rc_expo = 35\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0\nset rate_dynamics_center_sensitivity = 100\nset rate_dynamics_end_sensitivity = 100\nset rate_dynamics_center_correction = 10\nset rate_dynamics_end_correction = 10\nset rate_dynamics_center_weight = 0\nset rate_dynamics_end_weight = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAH\nset controlrate_profile = 0\nset throttle_scale =  1.000\nset throttle_idle =  5.000\nset turtle_mode_power_factor = 55\nset failsafe_throttle = 1000\nset fw_min_throttle_down_pitch = 0\nset nav_mc_hover_thr = 1500\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_pitch2thr = 10\nset nav_fw_launch_thr = 1800\nset nav_fw_launch_idle_thr = 1500\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0batch end", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 30, 2022", "body": "You haven’t set the video format to HD.", "type": "commented", "related_issue": null}, {"user_name": "joshfisk3", "datetime": "Jan 30, 2022", "body": "Whoops posted that with my work account. Is that this setting? set osd_video_system", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 30, 2022", "body": "Just use the radio buttons on the top left of the OSD page ", "type": "commented", "related_issue": null}, {"user_name": "joshfisk3", "datetime": "Jan 30, 2022", "body": "gotcha just saw it, thank you for the help", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 30, 2022", "body": "No problem 👍🏻", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Jan 31, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7626", "issue_status": " Closed\n", "issue_list": [{"user_name": "IvanBiv", "datetime": "Nov 23, 2021", "body": "The crosshair symbol and home arrow are not at the same horizontal position, but inav configurator is fine.  A similar problem was previously here: \n", "type": "commented", "related_issue": null}, {"user_name": "erstec", "datetime": "Nov 23, 2021", "body": "\nCan you provide your dump?I don't see anything wrong, but will try with your configuration\n", "type": "commented", "related_issue": null}, {"user_name": "IvanBiv", "datetime": "Nov 23, 2021", "body": "batch startmmix resetmmix 0  1.000  0.000  0.000  0.000smix resetsmix 0 3 1 100 0 -1\nsmix 1 4 0 100 0 -1\nsmix 2 5 2 100 0 -1servo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature MOTOR_STOP\nfeature GPS\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSD\nfeature FW_AUTOTRIMbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap AETRserial 20 1 115200 115200 0 115200\nserial 0 2 115200 115200 0 115200\nserial 1 64 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 1475 2100\naux 1 1 1 1350 1525\naux 2 12 1 900 1125\naux 3 10 1 1850 2100\naux 4 11 1 1675 1850\naux 5 53 1 1500 1675\naux 6 54 3 1300 1700\naux 7 21 3 1700 2100\naux 8 13 4 1725 2100\naux 9 42 6 1225 1625\naux 10 43 6 1625 2100\naux 11 47 5 1300 2100\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 24 1 V\nosd_layout 0 1 13 1 V\nosd_layout 0 2 0 0 V\nosd_layout 0 3 8 6 V\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 3 H\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 8 V\nosd_layout 0 12 1 10 V\nosd_layout 0 13 23 5 V\nosd_layout 0 14 18 1 V\nosd_layout 0 15 23 6 V\nosd_layout 0 16 8 11 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 3 0 V\nosd_layout 0 21 16 0 V\nosd_layout 0 22 14 10 V\nosd_layout 0 23 23 8 V\nosd_layout 0 24 13 3 V\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 V\nosd_layout 0 27 3 5 H\nosd_layout 0 28 22 10 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 11 V\nosd_layout 0 31 4 1 V\nosd_layout 0 32 13 2 V\nosd_layout 0 33 1 7 V\nosd_layout 0 34 10 3 H\nosd_layout 0 35 2 9 V\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 23 9 V\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 12 4 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 1 8 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 5 6 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 1 0 25 0 V\nosd_layout 1 1 11 0 V\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 11 V\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 16 0 V\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 7 V\nosd_layout 1 17 2 8 V\nosd_layout 1 18 2 9 V\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 1 V\nosd_layout 1 30 1 12 V\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 3 3 V\nosd_layout 1 34 18 2 H\nosd_layout 1 35 3 4 V\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 11 1 V\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 10 V\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 3 0 V\nosd_layout 1 63 3 1 V\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 3 2 V\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 13 2 V\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 24 0 V\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 Hset looptime = 1000\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_main_lpf_hz = 25\nset gyro_main_lpf_type = BIQUAD\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 30\nset gyro_abg_alpha =  0.000\nset gyro_abg_boost =  0.350\nset gyro_abg_half_life =  0.500\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 48\nset acczero_y = -9\nset acczero_z = -488\nset accgain_x = 4092\nset accgain_y = 4071\nset accgain_z = 4012\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = NONE\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = DPS310\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = CRSF\nset serialrx_inverted = OFF\nset spektrum_sat_bind = 0\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 1\nset blackbox_device = SERIAL\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 400\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = STANDARD\nset throttle_scale =  1.000\nset throttle_idle =  5.000\nset motor_poles = 14\nset turtle_mode_power_factor = 55\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle = 1000\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = RTH\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = -2\nset align_board_pitch = -2\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 423\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = AIRPLANE\nset has_flaps = OFF\nset model_preview_type = 14\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 180\nset imu_acc_ignore_rate = 9\nset imu_acc_ignore_slope = 5\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 3\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = AUTO\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = ON\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = STICK_CENTER_ONCE\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_p_to_d_gain = 0\nset fw_autotune_ff_to_i_tc = 600\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 90\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = OFF\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 1500\nset nav_wp_safe_distance = 10000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = ALWAYS\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 5000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mc_bank_angle = 30\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_cruise_thr = 1350\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1850\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 5000\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 5\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 180\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 1000\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 20000\nset nav_fw_launch_max_altitude = 120\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = ON\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = ON\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 255\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset beeper_pwm_mode = OFF\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 40\nset mc_i_pitch = 30\nset mc_d_pitch = 23\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 30\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 85\nset mc_i_yaw = 45\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 15\nset fw_i_pitch = 10\nset fw_d_pitch = 0\nset fw_ff_pitch = 80\nset fw_p_roll = 10\nset fw_i_roll = 8\nset fw_d_roll = 0\nset fw_ff_roll = 40\nset fw_p_yaw = 20\nset fw_i_yaw = 5\nset fw_d_yaw = 0\nset fw_ff_yaw = 100\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 350\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 10\nset dterm_lpf_type = BIQUAD\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = BIQUAD\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  0.500\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 20\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 5\nset nav_fw_pos_xy_p = 60\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.000\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  1.000\nset antigravity_accelerator =  1.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = OFF\nset setpoint_kalman_q = 100\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset fw_level_pitch_trim = -2.012\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 30\nset rc_yaw_expo = 30\nset roll_rate = 18\nset pitch_rate = 9\nset yaw_rate = 3\nset manual_rc_expo = 70\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 300\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAHbatch end", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Nov 23, 2021", "body": "The configurator and OSD never lined up since the OSD positioning was changed in 2.6. This is fine in 4.0. I checked the elements when the new fonts were checked.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Nov 23, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7503", "issue_status": " Closed\n", "issue_list": [{"user_name": "jag3957", "datetime": "Oct 10, 2021", "body": "With betaflight and SucceX-D F7 V2.2 TwinG Flight Controller is possible change motor out to servo out. the links are here to check and if possible add this feature to INAV.I want to add camera tilt servo using motor out M5 in this FC with betaflight is possible with remap source with INAV I understand I have to compile my own firmware, that is really complex for me and mean every time INAV get updated I must compile again.so if anybody can add this feature to INAV or make a video or something explaining how to make it easy I will really appreciate.adding a camera servo tilt using motor out: ", "type": "commented", "related_issue": null}, {"user_name": "SuiX187", "datetime": "Oct 11, 2021", "body": "Hello,\nsimilar Problem here.\ni only have 2 UART´s at jhemcu play f4 (1x CRSF; 1x GPS) but i use SmartAudio via IRC Tramp on LED Pin.\nin BF i easy can remap the source but i always read \"No resource remapping in INAV\"\nAt this point i cant use Inav for my projekt (BF has not enought GPS Features)", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7468", "issue_status": " Closed\n", "issue_list": [{"user_name": "crayZyjoe", "datetime": "Sep 21, 2021", "body": "batch startmmix resetmmix 0  1.000 -1.000  1.000 -1.000\nmmix 1  1.000 -1.000 -1.000  1.000\nmmix 2  1.000  1.000  1.000  1.000\nmmix 3  1.000  1.000 -1.000 -1.000smix resetservo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature GPS\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSDbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap AETRserial 20 1 115200 115200 0 115200\nserial 0 2 115200 115200 0 115200\nserial 1 64 115200 115200 0 115200\nserial 2 2048 115200 115200 0 115200\nserial 3 32 115200 115200 0 115200\nserial 5 262144 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 1300 1700\naux 1 51 1 1300 1700\naux 2 10 2 900 1200\naux 3 11 2 1300 1700\naux 4 3 2 1300 1700\naux 5 28 3 1300 1700\naux 6 13 11 1300 1700\naux 7 47 9 1300 1700\naux 8 0 0 900 900\naux 9 0 0 900 900\naux 10 0 0 900 900\naux 11 0 0 900 900\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 2 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 3 V\nosd_layout 0 12 1 4 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 0 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 H\nosd_layout 0 23 1 1 H\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 13 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 H\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 Hset looptime = 500\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_main_lpf_hz = 110\nset gyro_main_lpf_type = PT1\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 120\nset gyro_to_use = 0\nset gyro_abg_alpha =  0.000\nset gyro_abg_boost =  0.350\nset gyro_abg_half_life =  0.500\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 101\nset acczero_y = -20\nset acczero_z = -359\nset accgain_x = 4097\nset accgain_y = 4065\nset accgain_z = 4018\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = QMC5883\nset mag_declination = 0\nset magzero_x = 77\nset magzero_y = -20\nset magzero_z = -44\nset maggain_x = 1168\nset maggain_y = 1095\nset maggain_z = 1023\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = DPS310\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 12\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = SBUS\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = ON\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 2\nset blackbox_device = SDCARD\nset sdcard_detect_inverted = OFF\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 16000\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = DSHOT600\nset throttle_scale =  1.000\nset throttle_idle =  15.000\nset motor_poles = 14\nset turtle_mode_power_factor = 55\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle = 1000\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = RTH\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 179\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = MULTIROTOR\nset has_flaps = OFF\nset model_preview_type = 3\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 25\nset imu_acc_ignore_rate = 0\nset imu_acc_ignore_slope = 0\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 2\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = AUTO\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = ON\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = THROTTLE_THRESHOLD\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_p_to_d_gain = 0\nset fw_autotune_ff_to_i_tc = 600\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 90\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 100\nset nav_wp_safe_distance = 10000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = ALWAYS\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 1000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mc_bank_angle = 30\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 0\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset beeper_pwm_mode = OFF\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 44\nset mc_i_pitch = 75\nset mc_d_pitch = 25\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 60\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 35\nset mc_i_yaw = 80\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 5\nset fw_i_pitch = 7\nset fw_d_pitch = 0\nset fw_ff_pitch = 50\nset fw_p_roll = 5\nset fw_i_roll = 7\nset fw_d_roll = 0\nset fw_ff_roll = 50\nset fw_p_yaw = 6\nset fw_i_yaw = 10\nset fw_d_yaw = 0\nset fw_ff_yaw = 60\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 300\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 110\nset dterm_lpf_type = PT1\nset dterm_lpf2_hz = 170\nset dterm_lpf2_type = PT1\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  1.000\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 40\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 10\nset nav_fw_pos_xy_p = 75\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.500\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  2.000\nset antigravity_accelerator =  5.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = ON\nset setpoint_kalman_q = 200\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 20\nset tpa_breakpoint = 1200\nset fw_tpa_time_constant = 0\nset rc_expo = 70\nset rc_yaw_expo = 70\nset roll_rate = 70\nset pitch_rate = 70\nset yaw_rate = 60\nset manual_rc_expo = 70\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 6\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAHbatch end", "type": "commented", "related_issue": null}, {"user_name": "iNavFlight", "datetime": "Sep 24, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Sep 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7433", "issue_status": " Closed\n", "issue_list": [{"user_name": "Velvethsaw", "datetime": "Sep 9, 2021", "body": "OSD Information not reaching the DJI VTX. Have tried various settings to no availe.Would expect to see any OSD information be passed along. I appears as nothing is getting passed to the VTX.Make Matek H743 board work with DJI VTXbatch startmmix resetmmix 0  1.000  0.000  0.000  0.000smix resetsmix 0 3 6 100 0 -1\nsmix 1 4 5 100 0 -1\nsmix 2 5 4 100 0 -1\nsmix 3 6 0 100 0 -1\nsmix 4 7 0 100 0 -1\nsmix 5 8 0 100 0 -1servo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature MOTOR_STOP\nfeature SOFTSERIAL\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSDbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap AETRserial 20 1 115200 115200 0 115200\nserial 0 1 115200 115200 0 115200\nserial 1 2 115200 115200 0 115200\nserial 2 2097152 115200 115200 0 115200\nserial 3 0 115200 115200 0 115200\nserial 5 64 115200 115200 0 115200\nserial 6 0 115200 115200 0 115200\nserial 7 0 115200 115200 0 115200\nserial 30 2097152 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 900 900\naux 1 0 0 900 900\naux 2 0 0 900 900\naux 3 0 0 900 900\naux 4 0 0 900 900\naux 5 0 0 900 900\naux 6 0 0 900 900\naux 7 0 0 900 900\naux 8 0 0 900 900\naux 9 0 0 900 900\naux 10 0 0 900 900\naux 11 0 0 900 900\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 13 6 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 12 7 V\nosd_layout 0 12 11 8 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 0 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 H\nosd_layout 0 23 1 1 H\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 13 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 H\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 Hset looptime = 1000\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_main_lpf_hz = 25\nset gyro_main_lpf_type = BIQUAD\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 30\nset gyro_to_use = 0\nset gyro_abg_alpha =  0.000\nset gyro_abg_boost =  0.350\nset gyro_abg_half_life =  0.500\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 74\nset acczero_y = -15\nset acczero_z = -514\nset accgain_x = 4143\nset accgain_y = 4057\nset accgain_z = 4027\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = QMC5883\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = DPS310\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = MS4525\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = SBUS\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 1\nset blackbox_device = SERIAL\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 400\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = STANDARD\nset throttle_scale =  1.000\nset throttle_idle =  5.000\nset motor_poles = 14\nset turtle_mode_power_factor = 55\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle = 1000\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = SET-THR\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 250\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = AIRPLANE\nset has_flaps = OFF\nset model_preview_type = 14\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 100\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 180\nset imu_acc_ignore_rate = 10\nset imu_acc_ignore_slope = 0\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 3\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = NONE\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = OFF\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = STICK_CENTER_ONCE\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_p_to_d_gain = 0\nset fw_autotune_ff_to_i_tc = 600\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 90\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 100\nset nav_wp_safe_distance = 10000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = FS_ONLY\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 5000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mc_bank_angle = 30\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 2\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = IMPERIAL\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = AIRCRAFT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = SPEED\nset osd_right_sidebar_scroll = ALTITUDE\nset osd_sidebar_scroll_arrows = ON\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 12\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = OFF\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 40\nset mc_i_pitch = 30\nset mc_d_pitch = 23\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 30\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 85\nset mc_i_yaw = 45\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 15\nset fw_i_pitch = 10\nset fw_d_pitch = 0\nset fw_ff_pitch = 60\nset fw_p_roll = 10\nset fw_i_roll = 8\nset fw_d_roll = 0\nset fw_ff_roll = 40\nset fw_p_yaw = 20\nset fw_i_yaw = 5\nset fw_d_yaw = 0\nset fw_ff_yaw = 100\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 350\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 40\nset dterm_lpf_type = BIQUAD\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = BIQUAD\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  0.500\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 20\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 5\nset nav_fw_pos_xy_p = 50\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.000\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  1.000\nset antigravity_accelerator =  1.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = OFF\nset setpoint_kalman_q = 100\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 30\nset rc_yaw_expo = 30\nset roll_rate = 18\nset pitch_rate = 9\nset yaw_rate = 3\nset manual_rc_expo = 70\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAHbatch end", "type": "commented", "related_issue": null}, {"user_name": "iNavFlight", "datetime": "Sep 13, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Sep 13, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7384", "issue_status": " Closed\n", "issue_list": [{"user_name": "NathanEllis888", "datetime": "Aug 27, 2021", "body": "Issue trying on both INAV 2.6.1 and 3.0.1I am trying to bench test my motors before installing in my fixed wing model however cannot get the red failsafe parachute icon to go away nor the orange arming icon to light up. Arming switch has been setup, failsafe is set to land as no GPS has been installed yet and can get all servos to move from the transmitter input and motor works when throttle applied using the outputs tab in INAV configurator.Have tried multiple tx pins (TX4 & TX6) for the signal wire from the SPM4650 serial SRXL2 receiver, reflashing 2.6.1 and 3.0.1 (currently using 2.6.1) has shown same results.When in CLI status, the only arming flags are CLI and RX and cannot get the RX flag to go away.\nI can flick the arm switch top show the ARMSW flag on and off in CLI as well as move the throttle on the DX7 when in CLI to induce the THR flag but they are not the issue it would seem.All transmitter ranges are set from 1000-2000 (some slightly more or less +- 1 or 2)\nESC has already been calibrated also.I did get it to work one single time controlling the motor from the transmitter but next power on I was getting these issues and have not been able to solve them whether plugged into the configurator or not.Unsure if perhaps could be order I connect power to the board or when I arm the model but uncertain what is causing the failsafe mode to be engaged no matter if the receiver and transmitter are on or not.CLI STATUS:System Uptime: 233 seconds\nCurrent Time: 2041-06-28T01:04:00.000+00:00\nVoltage: 24.23V (6S battery - OK)\nCPU Clock=216MHz, GYRO=MPU6000, ACC=MPU6000, BARO=BMP280, PITOT=ADC\nSTM32 system clocks:\nSYSCLK = 216 MHz\nHCLK   = 216 MHz\nPCLK1  = 54 MHz\nPCLK2  = 108 MHz\nSensor status: GYRO=OK, ACC=OK, MAG=NONE, BARO=OK, RANGEFINDER=NONE, OPFLOW=NONE, GPS=NONE\nSD card: Startup failed\nStack size: 6144, Stack address: 0x20020000, Heap available: 1716\nI2C Errors: 50, config size: 6538, max available config: 32768\nADC channel usage:\nBATTERY : configured = ADC 1, used = ADC 1\nRSSI : configured = ADC 3, used = none\nCURRENT : configured = ADC 2, used = ADC 2\nAIRSPEED : configured = ADC 4, used = ADC 4\nSystem load: 2, cycle time: 1007, PID rate: 993, RX rate: 88, System rate: 9\nArming disabled flags: RX CLI\nVTX: not detectedCLI Failsafe variablesfailsafe_delay = 5\nAllowed range: 0 - 200failsafe_recovery_delay = 5\nAllowed range: 0 - 200failsafe_off_delay = 200\nAllowed range: 0 - 200failsafe_throttle = 1000\nAllowed range: 1000 - 2000failsafe_throttle_low_delay = 0\nAllowed range: 0 - 300failsafe_procedure = SET-THR\nAllowed values: SET-THR, DROP, RTH, NONEfailsafe_stick_threshold = 50\nAllowed range: 0 - 500failsafe_fw_roll_angle = -200\nAllowed range: -800 - 800failsafe_fw_pitch_angle = 100\nAllowed range: -800 - 800failsafe_fw_yaw_rate = -45\nAllowed range: -1000 - 1000failsafe_min_distance = 0\nAllowed range: 0 - 65000failsafe_min_distance_procedure = DROP\nAllowed values: SET-THR, DROP, RTH, NONEfailsafe_mission = OFF\nAllowed values: OFF, ONosd_failsafe_switch_layout = OFF\nAllowed values: OFF, ONCLI DUMP:batch startmmix resetmmix 0  1.000  0.000  0.000  0.000\nmmix 1  1.000  0.000  0.000  0.000smix resetsmix 0 2 1 100 0 -1\nsmix 1 3 0 100 0 -1\nsmix 2 3 14 100 0 -1\nsmix 3 4 0 100 0 -1\nsmix 4 4 14 -100 0 -1\nsmix 5 5 2 100 0 -1servo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature VBAT\nfeature TX_PROF_SEL\nfeature BAT_PROF_AUTOSWITCH\nfeature MOTOR_STOP\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSDbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap TAERserial 20 1 115200 115200 0 115200\nserial 0 1 115200 115200 0 115200\nserial 1 0 115200 115200 0 115200\nserial 2 0 115200 115200 0 115200\nserial 3 64 115200 115200 0 115200\nserial 5 0 115200 115200 0 115200\nserial 6 0 115200 115200 0 115200\nserial 7 0 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 1775 1900\naux 1 12 1 900 1200\naux 2 0 0 900 900\naux 3 0 0 900 900\naux 4 0 0 900 900\naux 5 0 0 900 900\naux 6 0 0 900 900\naux 7 0 0 900 900\naux 8 0 0 900 900\naux 9 0 0 900 900\naux 10 0 0 900 900\naux 11 0 0 900 900\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 V\nosd_layout 0 4 8 6 V\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 2 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 3 V\nosd_layout 0 12 1 4 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 0 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 H\nosd_layout 0 23 1 1 H\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 13 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 H\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 24 12 H\nosd_layout 0 110 24 11 H\nosd_layout 0 111 25 9 H\nosd_layout 0 112 25 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 24 12 H\nosd_layout 1 110 24 11 H\nosd_layout 1 111 25 9 H\nosd_layout 1 112 25 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 24 12 H\nosd_layout 2 110 24 11 H\nosd_layout 2 111 25 9 H\nosd_layout 2 112 25 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 24 12 H\nosd_layout 3 110 24 11 H\nosd_layout 3 111 25 9 H\nosd_layout 3 112 25 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 Hset looptime = 1000\nset gyro_sync = ON\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 188HZ\nset gyro_lpf_hz = 60\nset gyro_lpf_type = BIQUAD\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_stage2_lowpass_hz = 0\nset gyro_stage2_lowpass_type = BIQUAD\nset dynamic_gyro_notch_enabled = OFF\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 120\nset dynamic_gyro_notch_min_hz = 150\nset gyro_to_use = 0\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 25\nset acczero_y = 26\nset acczero_z = -15\nset accgain_x = 4089\nset accgain_y = 4069\nset accgain_z = 4066\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset align_mag = DEFAULT\nset mag_hardware = NONE\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = BMP280\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = ADC\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = SRXL2\nset serialrx_inverted = OFF\nset rx_spi_rf_channel_count = 0\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 1\nset blackbox_device = SDCARD\nset sdcard_detect_inverted = OFF\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 400\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = STANDARD\nset throttle_scale =  1.000\nset throttle_idle =  15.000\nset motor_poles = 14\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle = 1000\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = SET-THR\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = OFF\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 250\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = AIRPLANE\nset has_flaps = OFF\nset model_preview_type = 14\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 180\nset imu_acc_ignore_rate = 0\nset imu_acc_ignore_slope = 0\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset applied_defaults = 3\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = NONE\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = OFF\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset mc_airmode_type = STICK_CENTER\nset mc_airmode_threshold = 1300\nset fw_autotune_overshoot_time = 100\nset fw_autotune_undershoot_time = 200\nset fw_autotune_threshold = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_ff_to_i_tc = 600\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_radius = 3000\nset nav_wp_safe_distance = 10000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_landing_speed = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = FS_ONLY\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_rth_altitude = 5000\nset nav_rth_home_altitude = 0\nset nav_mc_bank_angle = 30\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 0\nset nav_fw_pitch2thr_threshold = 0\nset nav_fw_loiter_radius = 5000\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 2\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = f\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32768\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset ledstrip_visual_beeper = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_temp_label_align = LEFT\nset osd_artificial_horizon_reverse_roll = OFF\nset osd_artificial_horizon_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_home_position_arm_screen = ON\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESCprofile 1set mc_p_pitch = 40\nset mc_i_pitch = 30\nset mc_d_pitch = 23\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 30\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 85\nset mc_i_yaw = 45\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 5\nset fw_i_pitch = 7\nset fw_ff_pitch = 50\nset fw_p_roll = 5\nset fw_i_roll = 7\nset fw_ff_roll = 50\nset fw_p_yaw = 6\nset fw_i_yaw = 10\nset fw_ff_yaw = 60\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 300\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 40\nset dterm_lpf_type = BIQUAD\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = BIQUAD\nset yaw_lpf_hz = 30\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1000.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  1.000\nset fw_iterm_limit_stick_position =  0.500\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 40\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 10\nset nav_fw_pos_xy_p = 75\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.250\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  1.000\nset antigravity_accelerator =  1.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = OFF\nset setpoint_kalman_q = 100\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 30\nset rc_yaw_expo = 20\nset roll_rate = 20\nset pitch_rate = 15\nset yaw_rate = 9\nset manual_rc_expo = 30\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 430\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 3000\nset battery_capacity_warning = 1500\nset battery_capacity_critical = 750\nset battery_capacity_unit = MAHbatch end", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7717", "issue_status": " Closed\n", "issue_list": [{"user_name": "StuweFPV", "datetime": "Jan 4, 2022", "body": "I'd like to suggest the creation of a OSD expansion so that it is possible to display a switch setting in the OSD. In order to keep it simple I suggest to have given channels where this applies and users will have to change their servos accordingliy i.e.:CH7-CH9 2-position switch\nCH10-CH12 3-position swichThis would allow users to visualize special functions in OSD like Flaps, Gear, Cargo Door, etc.\nA simple graph could then be displayed in OSD and you have visual control of your setting without having to touch the switch", "type": "commented", "related_issue": null}, {"user_name": "a1axx", "datetime": "Jan 4, 2022", "body": "This is cool,There are characters that could be used already, I am sorry, but can't remember what these are!\n", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 4, 2022", "body": "can't see any characters on your pic that would serve the purpose.. looks more like AHI to me :)\neven better would be if there are some letter attached to it like F=Flaps, G=Gear in front of the symbol. Similar to what we now have when displaying the PID profile selection with P.\nEven if there are usable characters available i never saw any switch in the OSD to turn either a given function like Flaps on or a given channel. how would you display and assign your characters in the OSD?\ncheers mate", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 4, 2022", "body": "Yep, thats the AHI. I think you’d only need the icons for the 3 position switch. They would only show the high and low positions for 2 way switches.It would be simpler to implement the channel number and the switch icon. Once you start adding letters, the setup becomes more complex. Perhaps a couple of options for flaps, gear, doors, then select the channel number? It would need to be simple for setup on the OSD page though.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 4, 2022", "body": "yes i see the complexity if you want to start customizing with letters and variables. hence the predefined set. but it would be cool if there was a basic set of letters like C=Cam rec, D=door, F=flaps, G=gear. If the channel are selectable that would be delux. I could live with predefined and just swap my servos accordingly. I really \"need\" the 2-pos indicator - almost more than the 3-pos for the gear. this is vital when landing and it makes it more realistic :)\ncheers mate for looking into it", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 5, 2022", "body": "Also have to be considered how many OSD spaces have to be used for the switch display. Here is the available list:1 OSD char - 3 new icons needed - economical on OSD space, but probably the image will be too small?\n2 OSD char - 6 new icons needed - middle size which might be good but icon hungry.\n3 OSD char - 2 new icons needed - highly visible, fewer icons needed but eats some OSD space.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 5, 2022", "body": "good point thank you. i guess that kicks out the option of a letter in front of it. I think most users will use max. 3x this feature and it should be fairly \"detectable\" by the icon itself. i.e. gear is the one with only 2 pos. flaps the one with 3. and the other one, somewhere stacked i a special corner of the osd would be the \"special function\" whatever that might be. cam recording or cargo bay open.\nI would actually have to see and test before making a final call which size is best. i understand the limit of the char in osd available by the font but form your reading option  would probably be the nice middle ground :)", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 6, 2022", "body": "I did some icons for 2 character which I also think is a good compromise of space usage and visibility on the OSD. You can scroll through the different positions to see how it looks which I think is quite good. I began with separators which look good for individual switches but when indicators are stacked they look messy, so I eliminate them. You can have a look at both stacks with and w/o separators to understand what I mean. Simbols are oversized so they serve equally well for 2 and 3 position switches. Grey zone is of course transparent.\n\n\n\n\n\n.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "wow!! that's very good work! I guess they could work but to be honest i would prefer just a filled square like in my drawing. I found that fine stuff is hard to see sometimes especially when video reception is not at its best. just keep them in the corner of my eye and not really looking at it or just a very brief glance. however this is sure going to work. i like the circle if you use up/down. for flaps that would be weird. which font is that in/based on? convertable to any font? I use clarity medium.\ncheers mate!!", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 6, 2022", "body": "It is not based on an existing font but can fit any. Initially I was about to do the squares but preferred the signs for several reasons:The circle uneqivocaly identifies the center position, of course can be changed for a dash or a square. The signes does not identify flaps, gear, etc, they just gives you the position of the switches. The triangles I will probably enlarge a bit to increase visibility, don't worry about size they are larger than most of the icons available.Here is an example with squares and with signs to compare:\n", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "yes two very valid points. i was not planning on stacking them. i would probably only use two of those symbols since my plane only has one set of gear :) but offering different version would give users a whole range of options they could use. if it's no biggie to change that then i would prefer for the 2-pos switch arrow-up and arrow-down. for the 3-pos the circle is not ideal if the middle position represents a active setting. maybe the circle at the top, in the middle a 1 and at the bottom a 2? representing the flap positions.\nif you could merge them into clarity medium i'd be very happy to test them in real conditions and provide a feedback/video.\nthanks!P.S. how do you make these graphs and integrate them? i'd love to be able to play around with icons modifying my osd.. is there a tool for that? could not find anything in the wiki", "type": "commented", "related_issue": null}, {"user_name": "OptimusTi", "datetime": "Jan 6, 2022", "body": "Something more like this:\nThis could be cool for Programming logic too. Global vars?", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 6, 2022", "body": "When designing features for Inav you have to think in general so that the feature fits anyone and given the scarce free chars available its impossible to fit every case eg: flaps, gears, cameras, lights, custom functions... some wont stack the indicators, others will. In short you have to design the features globaly. At any time you can change the icons you want and compile them for your own needs.I don't remember where I got the font compiler, it was some time ago but probably  can help you as he reworked the fonts recently.I have done one more set making the triangles bigger and adding a square in place of the circl, so now the same icons fit nicely 3 and 3 position switches.The big problem you have now is to find someone who wants to write the code for Inav and the configurator... :-)\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "I love that!!! super cool!! I wish we had color...", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "that's always hard to please everyone and everyone's needs.. especially in osd - the liking is so individual. ok I'll ask Darren if he can help me out with the designer. yes the switch is another issue.. I thought just a font upload.. but you're right. i have to be able to activate it.. I think Darren did something on LQ's. I'll ask. thanks for your effort.", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 6, 2022", "body": "Don't worry, if this feature sees the light I will make a set of icons for you exactly as you wish ;)", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "ok thanks. so compiling is the issue now... let's see what Darren says. he's super busy and probably many other priorities. maybe i'll start learning to compile myself.. who knows. it's cool if you can do your own stuff", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 6, 2022", "body": "That would be pretty cool and only need three characters. I like  icons with the triangles too.  But being 2 OSD squares high, it would need 6 characters. But also I think it would make it less attractive from a usability perspective. I think L S (Letter Symbol) inline, taking up 2 OSD blocks would be more usable and better for OSD real estate. But the triangles and circle looks cool.Compiling the fonts is a piece of cake. You just run a file and it does everything. Can’t remember the name of the file, as I set it up on my batch file.I did start looking at code. I think I have an implementation in my head now. the first 4 GVars can already be shown on the OSD.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 6, 2022", "body": "Hurra MrD-RC added it as a feature request - problem solved :) thank you for that!! I think many people will use this.\nFor simplicity I think Optimus's icon would be a good start even though the arrows also have their advantages. A letter in front would be perfect so it's clear what it is but not mandatory. I'm sure you have a great idea in your head.no worries about the compiler - you're much faster and better. I'd rather be your test pilot :)\ncheers mate!!", "type": "commented", "related_issue": null}, {"user_name": "OptimusTi", "datetime": "Jan 6, 2022", "body": "Yes, using 5 characters but I see what you mean by 3 characters. As for gvars I was thinking about operating the symbols with them but nah. It would be simpler operating them by assigning a channel and checking the values.", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 7, 2022", "body": "OptimusTi icon looks cute but its misleading, it has color which improves visibility, its not dimensioned and being a single character is not easy to spot on the OSD in difficult conditions as StuweFPV asked. And its 5 font chars vs 6 font chars. Thats why the double character was preferred although its font expensive. Here are both versions in real size. Of course they should be tested on the OSD to check how well they actually look. ", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 7, 2022", "body": "i'm sure the colors are misleading - would've been so nice though :) in the option you show i think the smaller one is too small on here but we have to see it in the goggles before making a call. I like the idea when you can see the \"empty\" fields as well - making it clear which position active. framed squares either filled or empty would just be good enough if you don't use the arrows.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 7, 2022", "body": " it would be interesting to see how well the first option with the squares shows up on the OSD. To me, that is the better solution as it uses less font characters and can be used horizontally. I think with the 2 screen character version, it would have to use a vertical orientation, with the function character above the switch indicator . Which would be a little odd, and could make positioning more difficult.i have an idea for the characters, but won’t be able to draw it up until tonight. It may not work though. Was thinking about having the active vs inactive as different proportions. So what’s happening may be more visible with a single screen character.", "type": "commented", "related_issue": null}, {"user_name": "OptimusTi", "datetime": "Jan 7, 2022", "body": "That's not color. Blue is transparent. Based off existing symbols.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "Here are a couple of thoughts for switches. The grey colour is the transparency.\n \nSeeing them next to each other, I quite like the 2nd one. It looks more like a transmitter switch. But would need to see how it looks in the goggles. What do you guys think?Examples with function:\n Flaps\n Gear", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "Another example, using the triangles and circle on a single character\n", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 8, 2022", "body": "I like the second one either. It has the advantage of shape change which coupled with the black and white vertical line gives a clear indication of the position. The F and G are nice but limiting to two functions only and need 3 icons for each letter.On the last one I understand what you have done extending the range of the 3D stick but clipping the shape does not work well: the stick looks much smaller on the edges, and if taken as a triangle (when not in motion) it points to the opposite direction of the stick position which is counterintuitive. Not sure about the visibility of the black dashes but the vertical line on the upper icons make an easier guess on the sticks position and better contrast. Good job Darren ", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "Yeah, I think the 2nd one looks the best. I’ll try it on the OSD. But based on size next to the letters, I think it will be fine for visibility. I don’t get what you mean by the letters limiting to 2 functions, or needing 3 icons for each letter? The F in the example is just to show that the switch is for the flaps. So you would have the F followed by the switch indicator. The switch indicator would be mapped to an RC channel. My thought is to have both the letter and RC channel selectable by the pilot. Maybe have 4 switch indicators that could be enabled. This should have plenty of scope.", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 8, 2022", "body": " excuse me my bad, On the phone looked like the letters had a frame but its just a regular font. Great", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "The plan is going ok at the moment. Hit a bit of a snag with the configurator. The data-setting method seems to hang when the data type is char, so I need to look in to that. But this is the plan.\nOn the left, the regular toggles to enable or disable the OSD elements.\nThen on the right, the settings area to choose a letter and channel that the switch is on.On thing's for sure, I need to sort out that alignment with the inputs vs select boxes on the right side of the OSD page. Don't know how I've not noticed that before. It looks pretty crap with that misalignment.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "Sorted the MSP stuff. Strings weren't being processed at all, so it just threw an exception  The configurator side is done. Just the OSD stuff, then we can see how it looks.\n", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 8, 2022", "body": "wow now that's some nice work in no time! well done Darren! looks super cool and the option to assign your own channel and letter opens this up for many users with all sorts of applications.\nI really like the 2-pos switch. indicating nicely with the pear shaped form up and down.\non the 3-pos switch i'd say this is good for a regular application where you have off in the middle and up/down represent a function. for flaps however it is less ideal as the neutral position is at the top. should be 0 there and then the pear shape forms in the middle and at the bottom. now this is very specific and would almost require a special flaps switch meaning create a 3rd type which will probably complicate the whole thing. on one side i say leave it as it is. on the other hand i think flaps is probably the most widely used \"additional\" function out there. even if it's only flaperon.. i'll have that on about 70% of my planes whereby gear only on 10% these days. tough call - up to you.\ngreat work an i'm happy to test up in the air to see how visible and useful this really is!", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": "I get what you’re saying about flaps. But this has to be universal. It’s would add a whole load more to get that done, as well as take away more resources. Even though the icons may not be exactly what would fit that specific purpose. It would still be easy to see the status of the flaps as it is.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 8, 2022", "body": "yeah fully agree. let's keep them as universal as you can so users will use it. it surely gets the job done! nice work\nnow lets test up in the air. i need F405/722 targets - clarity medium :) :)", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 8, 2022", "body": "Excellent job Darren! I have to admit I like the result very much! Now wondering for blank space instead of letter is it enough to left the field blank? And in case its blank is the space usable? ThanksEdit: what are the ranges for each position?StuweFPV this is not a specific visual style switch, its just shows the switches position on your TX regardless of their function. But you definitely can exchange them for any icon you like, you can take my rectangle icon above, separate in three and substitute the ones Darren did.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Jan 8, 2022", "body": "\nok thank you understood. i still need Darren to produce the hex and the configurator :) I have planes ready waiting and can test tomorrow up in the air. feedback video incl :)", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 19, 2022", "body": " The ranges are the same as in the programming:I don't quite follow about the blank spaces. Back then, there was only one character. Now you can have up to 4 characters. The switch is on the right, and the text is right aligned. So that the characters will be closest to the switch. Maybe I could add an alignment option, though this would not be represented in the configurator.  Possibly could be, but I'd have to have a little think.", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 19, 2022", "body": " thanks, yes the range is as expected. The blank space question was simply if a letter is not used can multiple switches be put close together side by side? I guess the configurator will show the overlapping positions in red but not a big deal if that works.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 19, 2022", "body": "Perhaps that’s something that can be checked on the firmware side. To only write the switch to the OSD if there is no text.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 19, 2022", "body": "The Configurator side works. I could potentially make the OSD field a variable length; As it can be updated on the OSD, so the layout will always be right.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 20, 2022", "body": "Got it working. Nice and customisable now. Ignore the slight glitch when removing the LT. This will only happen during editing, and wouldn't ever be visible in flight. To be honest. Unless people have a live OSD while editing, it would never appear.", "type": "commented", "related_issue": null}, {"user_name": "Mateyhv", "datetime": "Jan 20, 2022", "body": "Excellent!!! ", "type": "commented", "related_issue": null}, {"user_name": "a1axx", "datetime": "Jan 21, 2022", "body": "Awesome work. Il be finding a use for these!!", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Feb 12, 2022", "body": "So sad to see that this new feature did not make it into 4.1 as it is all ready to go.. I could've made good use of that.", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 12, 2022", "body": "There are font changes, so needs a major version increase with the new version numbering system. Plus 4.1 has broken it  I need to get it fixed....fixed now.", "type": "commented", "related_issue": null}, {"user_name": "StuweFPV", "datetime": "Feb 12, 2022", "body": "I see.. still can't wait to use that feature. just did 2 flights where this would've been very useful. you surely make the best changes / improvements on inav for sure :) keep up the good work!", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Feb 12, 2022", "body": "Thanks mate, I make the easy changes. But it allows the real devs to work on things that need an understanding of complex maths or electronics. I wish I knew more of both, but thats the way it is.I spent a couple of hours (well, about 8) working on the units conversions. Vertical speed will be in m/s or ft/s going forwards.", "type": "commented", "related_issue": null}, {"user_name": "happy12", "datetime": "Mar 26, 2022", "body": "This feature will be so awesome! Thanks for making it!", "type": "commented", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 6, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "MrD-RC", "datetime": "Jan 8, 2022", "body": [], "type": "pull", "related_issue": "#7734"}, {"user_name": "MrD-RC", "datetime": "Apr 19, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7370", "issue_status": " Closed\n", "issue_list": [{"user_name": "SilentZen", "datetime": "Aug 23, 2021", "body": "crosshair does not move based on heatrackingHave crosshair move down when looking up, move left when looking right and vice versa. Also have method to calibrate amount of movement.People who use headtrackers on front of planes and need assistance on identifying forward.", "type": "commented", "related_issue": null}, {"user_name": "SilentZen", "datetime": "Nov 1, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "SilentZen", "datetime": "Nov 1, 2021", "body": "This impacts all plane pilots that mount their pan/tilt cameras on front of planes with no way of identifying centre forward.\nThis info is so important lot of pilots purposely mount pan/tilt cameras further back to see aircraft to identify centre position of camera.  The crosshair going off the screen is not an issue it can even disappear before edge of screen. With pan setups going way past cam FOV that would be expected, you cannot have centre forward been displayed when you are looking backwards.  You seem to be too preoccupied  with the extents and accuracy when looking up down left right that you seem to miss the important part of identifying centre and seeing deviation from centre. You also seem to be focused on yourself and your experiences rather than the communities.You even stated you have your reset your drifting  headtracker multiple times during a flight, maybe if you knew centre forward and could see it you would not have to. Maybe you are just clicking reset on headtracker to make sure you are looking centre because you just didn't know.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7330", "issue_status": " Closed\n", "issue_list": [{"user_name": "StrawHatSam92", "datetime": "Aug 6, 2021", "body": "In the motor outputs tab, iNav target for Foxeer F722 V2 has an error where motor signal S2 does not spin the motor and causes the ESC to beep. Motor S1 does spin the motor, but it causes ESC beeps. Motor signals S3 and S4 seem to work fine.Here is a video on YT showing the issue in more detail:\nversion\nINAV/FOXEERF722V2 3.0.1 Jul  4 2021 / 13:46:18 ()\nGCC-9.3.1 20200408 (release)", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Aug 7, 2021", "body": "Does the same happens with Multishot protocol? Care to attach a full config dump?", "type": "commented", "related_issue": null}, {"user_name": "StrawHatSam92", "datetime": "Aug 7, 2021", "body": "Hi Pawel,I did not think to test multishot protocol unfortunately before I replaced the Foxeer FC with a Matek so I cannot confirm whether it works or not with multishot. I'm am not sure how to attach things in Github so here is the dump below:`# versionbatch startmmix resetmmix 0  1.000 -1.000  1.000 -1.000\nmmix 1  1.000 -1.000 -1.000  1.000\nmmix 2  1.000  1.000  1.000  1.000\nmmix 3  1.000  1.000 -1.000 -1.000smix resetservo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature TX_PROF_SEL\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLEbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap AETRserial 20 1 115200 115200 0 115200\nserial 0 64 115200 115200 0 115200\nserial 1 0 115200 115200 0 115200\nserial 2 0 115200 115200 0 115200\nserial 3 0 115200 115200 0 115200\nserial 4 0 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 900 900\naux 1 0 0 900 900\naux 2 0 0 900 900\naux 3 0 0 900 900\naux 4 0 0 900 900\naux 5 0 0 900 900\naux 6 0 0 900 900\naux 7 0 0 900 900\naux 8 0 0 900 900\naux 9 0 0 900 900\naux 10 0 0 900 900\naux 11 0 0 900 900\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 0 900 900 0 0\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 23 0 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 12 V\nosd_layout 0 8 20 2 H\nosd_layout 0 9 1 2 V\nosd_layout 0 10 8 6 H\nosd_layout 0 11 2 3 V\nosd_layout 0 12 1 4 V\nosd_layout 0 13 23 1 H\nosd_layout 0 14 0 11 V\nosd_layout 0 15 1 0 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 18 12 H\nosd_layout 0 21 0 12 H\nosd_layout 0 22 14 11 H\nosd_layout 0 23 1 1 H\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 V\nosd_layout 0 29 23 12 H\nosd_layout 0 30 1 13 V\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 H\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 0 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 Hset looptime = 500\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_main_lpf_hz = 110\nset gyro_main_lpf_type = PT1\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 250\nset dynamic_gyro_notch_min_hz = 120\nset gyro_abg_alpha =  0.000\nset gyro_abg_boost =  0.350\nset gyro_abg_half_life =  0.500\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 0\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 0\nset acczero_y = 0\nset acczero_z = 0\nset accgain_x = 4096\nset accgain_y = 4096\nset accgain_z = 4096\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW270FLIP\nset mag_hardware = NONE\nset mag_declination = 0\nset magzero_x = 0\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1024\nset maggain_y = 1024\nset maggain_z = 1024\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = NONE\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1100\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 0\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = SBUS\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 2\nset blackbox_device = SPIFLASH\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 8000\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = DSHOT300\nset throttle_scale =  1.000\nset throttle_idle =  6.000\nset motor_poles = 14\nset turtle_mode_power_factor = 55\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 200\nset failsafe_throttle = 1000\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = SET-THR\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 0\nset align_board_pitch = 0\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 400\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = OFF\nset platform_type = MULTIROTOR\nset has_flaps = OFF\nset model_preview_type = 3\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 50\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 25\nset imu_acc_ignore_rate = 0\nset imu_acc_ignore_slope = 0\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 2\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = NONE\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = OFF\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = THROTTLE_THRESHOLD\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_p_to_d_gain = 0\nset fw_autotune_ff_to_i_tc = 600\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 90\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.100\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = OFF\nset nav_extra_arming_safety = ON\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 100\nset nav_wp_safe_distance = 10000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 500\nset nav_land_slowdown_maxalt = 2000\nset nav_emerg_landing_speed = 500\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = OFF\nset nav_rth_allow_landing = ALWAYS\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 1000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mc_bank_angle = 30\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_cruise_thr = 1400\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_bank_angle = 35\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 0\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = AUTO\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_speed_source = GROUND\nset i2c_speed = 400KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name =\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_band = 4\nset vtx_channel = 1\nset vtx_power = 1\nset vtx_low_power_disarm = OFF\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dshot_beeper_enabled = ON\nset dshot_beeper_tone = 1\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 1set mc_p_pitch = 44\nset mc_i_pitch = 75\nset mc_d_pitch = 25\nset mc_cd_pitch = 60\nset mc_p_roll = 40\nset mc_i_roll = 60\nset mc_d_roll = 23\nset mc_cd_roll = 60\nset mc_p_yaw = 35\nset mc_i_yaw = 80\nset mc_d_yaw = 0\nset mc_cd_yaw = 60\nset mc_p_level = 20\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 5\nset fw_i_pitch = 7\nset fw_d_pitch = 0\nset fw_ff_pitch = 50\nset fw_p_roll = 5\nset fw_i_roll = 7\nset fw_d_roll = 0\nset fw_ff_roll = 50\nset fw_p_yaw = 6\nset fw_i_yaw = 10\nset fw_d_yaw = 0\nset fw_ff_yaw = 60\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 300\nset max_angle_inclination_pit = 300\nset dterm_lpf_hz = 110\nset dterm_lpf_type = PT1\nset dterm_lpf2_hz = 170\nset dterm_lpf2_type = PT1\nset yaw_lpf_hz = 0\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  1.000\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 90\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 40\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 10\nset nav_fw_pos_xy_p = 75\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RP\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.500\nset d_boost_max_at_acceleration =  7500.000\nset d_boost_gyro_delta_lpf_hz = 80\nset antigravity_gain =  2.000\nset antigravity_accelerator =  5.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = ON\nset setpoint_kalman_q = 200\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 20\nset tpa_breakpoint = 1200\nset fw_tpa_time_constant = 0\nset rc_expo = 70\nset rc_yaw_expo = 70\nset roll_rate = 70\nset pitch_rate = 70\nset yaw_rate = 60\nset manual_rc_expo = 70\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAH`", "type": "commented", "related_issue": null}, {"user_name": "StrawHatSam92", "datetime": "Aug 14, 2021", "body": "Hi Pawel, I reinstalled a different Foxeer F722 V2 onto my drone and the same problem persists, so I think that shows it's a firmware issue not a hardware issue. In addition, I took the time to see if the issue happens when using MULTISHOT. With MULTISHOT enabled, there seem to be no issues. Both when spinning up the motors in the configurator and when arming the motors with my radio (not connected to the PC), none of the motors shut down and it all works as expected.In addition, I tried DSHOT300 outside of the configurator, just arming the quad with my radio and all four motors spin up, BUT as soon as I raise the throttle and give it any kind of stick input, the S2 motor shuts down and the S2 ESC reboots. So I don't think it's a configurator issue since it happens on the bench too. I will fly the quad anyway on MULTISHOT because I am also testing BlHeli 32.81 with variable PWM enabled (24-48kHz) to see how that goes.", "type": "commented", "related_issue": null}, {"user_name": "prasanthsoman86", "datetime": "Aug 30, 2021", "body": "I face the same issue with my F722 V2, and I quit Inav and flashed betaflight", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Aug 7, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "giacomo892", "datetime": "Aug 31, 2021", "body": [], "type": "pull", "related_issue": "#7410"}, {"user_name": "DzikuVx", "datetime": "Aug 31, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7292", "issue_status": " Closed\n", "issue_list": [{"user_name": "Inv3nt0r1", "datetime": "Jul 22, 2021", "body": "Hello,\nI need advice on changing the PINIO in the latest INAV.\nI have FC MatekF722mini, which has two PINIOBOX defined:\nC08: PINIO1 OUT\nC09: PINIO2 OUT\nThe PINIO is used as follows:\n\nI want to add an external PINIO to trigger the camera (triggers on rising edge): I connected PINIO physically to pin MOTOR5 (A15) and modified it in FW - target.h, it worked (trigger the camera) but it also turned off VTX and I don't want it because I use IRC Tramp.\n\nCan you please advise me where to change in code (other than the target.h) so that it will not power off the VTX, ie. it will not disconnect the VBAT power supply for VTX, because I want VBAT to be enabled permanently and use both the PINIOs to trigger the camera.\nThank you.", "type": "commented", "related_issue": null}, {"user_name": "Inv3nt0r1", "datetime": "Jul 25, 2021", "body": "Issue resolved. I defined pinio3_pin and pinio4_pin and assigned the default pinio pins (vtx vbat and cam switcher) to them.", "type": "commented", "related_issue": null}, {"user_name": "Inv3nt0r1", "datetime": "Jul 25, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/7216", "issue_status": " Closed\n", "issue_list": [{"user_name": "Kuzon24", "datetime": "Jul 4, 2021", "body": "Post Hold is drifting when you move the quad and come to a stop it'll have a rubberband effect. It's not locked in at all. In inav 2.4 this problem was not as bad as inav 3.0 How can this be solved? Is it pids? Breaking options?I don't expect it to be as sharp and stable as a dji quad but hopefully it can be used as a safetey feature when hitting angle mode + Post hold the quad would come to a stop on the spot.Info:batch startmmix resetmmix 0  1.000 -1.000  1.000 -1.000\nmmix 1  1.000 -1.000 -1.000  1.000\nmmix 2  1.000  1.000  1.000  1.000\nmmix 3  1.000  1.000 -1.000 -1.000smix resetservo 0 1000 2000 1500 100\nservo 1 1000 2000 1500 100\nservo 2 1000 2000 1500 100\nservo 3 1000 2000 1500 100\nservo 4 1000 2000 1500 100\nservo 5 1000 2000 1500 100\nservo 6 1000 2000 1500 100\nservo 7 1000 2000 1500 100\nservo 8 1000 2000 1500 100\nservo 9 1000 2000 1500 100\nservo 10 1000 2000 1500 100\nservo 11 1000 2000 1500 100\nservo 12 1000 2000 1500 100\nservo 13 1000 2000 1500 100\nservo 14 1000 2000 1500 100\nservo 15 1000 2000 1500 100safehome 0 0 0 0\nsafehome 1 0 0 0\nsafehome 2 0 0 0\nsafehome 3 0 0 0\nsafehome 4 0 0 0\nsafehome 5 0 0 0\nsafehome 6 0 0 0\nsafehome 7 0 0 0logic 0 0 -1 0 0 0 0 0 0\nlogic 1 0 -1 0 0 0 0 0 0\nlogic 2 0 -1 0 0 0 0 0 0\nlogic 3 0 -1 0 0 0 0 0 0\nlogic 4 0 -1 0 0 0 0 0 0\nlogic 5 0 -1 0 0 0 0 0 0\nlogic 6 0 -1 0 0 0 0 0 0\nlogic 7 0 -1 0 0 0 0 0 0\nlogic 8 0 -1 0 0 0 0 0 0\nlogic 9 0 -1 0 0 0 0 0 0\nlogic 10 0 -1 0 0 0 0 0 0\nlogic 11 0 -1 0 0 0 0 0 0\nlogic 12 0 -1 0 0 0 0 0 0\nlogic 13 0 -1 0 0 0 0 0 0\nlogic 14 0 -1 0 0 0 0 0 0\nlogic 15 0 -1 0 0 0 0 0 0\nlogic 16 0 -1 0 0 0 0 0 0\nlogic 17 0 -1 0 0 0 0 0 0\nlogic 18 0 -1 0 0 0 0 0 0\nlogic 19 0 -1 0 0 0 0 0 0\nlogic 20 0 -1 0 0 0 0 0 0\nlogic 21 0 -1 0 0 0 0 0 0\nlogic 22 0 -1 0 0 0 0 0 0\nlogic 23 0 -1 0 0 0 0 0 0\nlogic 24 0 -1 0 0 0 0 0 0\nlogic 25 0 -1 0 0 0 0 0 0\nlogic 26 0 -1 0 0 0 0 0 0\nlogic 27 0 -1 0 0 0 0 0 0\nlogic 28 0 -1 0 0 0 0 0 0\nlogic 29 0 -1 0 0 0 0 0 0\nlogic 30 0 -1 0 0 0 0 0 0\nlogic 31 0 -1 0 0 0 0 0 0gvar 0 0 -32768 32767\ngvar 1 0 -32768 32767\ngvar 2 0 -32768 32767\ngvar 3 0 -32768 32767\ngvar 4 0 -32768 32767\ngvar 5 0 -32768 32767\ngvar 6 0 -32768 32767\ngvar 7 0 -32768 32767pid 0 0 0 0 0 0 0 0 0 0\npid 1 0 0 0 0 0 0 0 0 0\npid 2 0 0 0 0 0 0 0 0 0\npid 3 0 0 0 0 0 0 0 0 0feature -THR_VBAT_COMP\nfeature -VBAT\nfeature -TX_PROF_SEL\nfeature -BAT_PROF_AUTOSWITCH\nfeature -MOTOR_STOP\nfeature -SOFTSERIAL\nfeature -GPS\nfeature -RPM_FILTERS\nfeature -TELEMETRY\nfeature -CURRENT_METER\nfeature -REVERSIBLE_MOTORS\nfeature -RSSI_ADC\nfeature -LED_STRIP\nfeature -DASHBOARD\nfeature -BLACKBOX\nfeature -TRANSPONDER\nfeature -AIRMODE\nfeature -SUPEREXPO\nfeature -VTX\nfeature -PWM_OUTPUT_ENABLE\nfeature -OSD\nfeature -FW_LAUNCH\nfeature -FW_AUTOTRIM\nfeature VBAT\nfeature TX_PROF_SEL\nfeature BAT_PROF_AUTOSWITCH\nfeature GPS\nfeature TELEMETRY\nfeature CURRENT_METER\nfeature BLACKBOX\nfeature AIRMODE\nfeature PWM_OUTPUT_ENABLE\nfeature OSDbeeper RUNTIME_CALIBRATION\nbeeper HW_FAILURE\nbeeper RX_LOST\nbeeper RX_LOST_LANDING\nbeeper DISARMING\nbeeper ARMING\nbeeper ARMING_GPS_FIX\nbeeper BAT_CRIT_LOW\nbeeper BAT_LOW\nbeeper GPS_STATUS\nbeeper RX_SET\nbeeper ACTION_SUCCESS\nbeeper ACTION_FAIL\nbeeper READY_BEEP\nbeeper MULTI_BEEPS\nbeeper DISARM_REPEAT\nbeeper ARMED\nbeeper SYSTEM_INIT\nbeeper ON_USB\nbeeper LAUNCH_MODE\nbeeper LAUNCH_MODE_LOW_THROTTLE\nbeeper LAUNCH_MODE_IDLE_START\nbeeper CAM_CONNECTION_OPEN\nbeeper CAM_CONNECTION_CLOSEDmap AETRserial 20 1 115200 115200 0 115200\nserial 0 64 115200 115200 0 115200\nserial 1 2 115200 115200 0 115200\nserial 2 1 9600 115200 0 115200\nserial 3 2097152 115200 115200 0 115200\nserial 5 32 115200 115200 0 115200led 0 0,0::C:0\nled 1 0,0::C:0\nled 2 0,0::C:0\nled 3 0,0::C:0\nled 4 0,0::C:0\nled 5 0,0::C:0\nled 6 0,0::C:0\nled 7 0,0::C:0\nled 8 0,0::C:0\nled 9 0,0::C:0\nled 10 0,0::C:0\nled 11 0,0::C:0\nled 12 0,0::C:0\nled 13 0,0::C:0\nled 14 0,0::C:0\nled 15 0,0::C:0\nled 16 0,0::C:0\nled 17 0,0::C:0\nled 18 0,0::C:0\nled 19 0,0::C:0\nled 20 0,0::C:0\nled 21 0,0::C:0\nled 22 0,0::C:0\nled 23 0,0::C:0\nled 24 0,0::C:0\nled 25 0,0::C:0\nled 26 0,0::C:0\nled 27 0,0::C:0\nled 28 0,0::C:0\nled 29 0,0::C:0\nled 30 0,0::C:0\nled 31 0,0::C:0color 0 0,0,0\ncolor 1 0,255,255\ncolor 2 0,0,255\ncolor 3 30,0,255\ncolor 4 60,0,255\ncolor 5 90,0,255\ncolor 6 120,0,255\ncolor 7 150,0,255\ncolor 8 180,0,255\ncolor 9 210,0,255\ncolor 10 240,0,255\ncolor 11 270,0,255\ncolor 12 300,0,255\ncolor 13 330,0,255\ncolor 14 0,0,0\ncolor 15 0,0,0mode_color 0 0 1\nmode_color 0 1 11\nmode_color 0 2 2\nmode_color 0 3 13\nmode_color 0 4 10\nmode_color 0 5 3\nmode_color 1 0 5\nmode_color 1 1 11\nmode_color 1 2 3\nmode_color 1 3 13\nmode_color 1 4 10\nmode_color 1 5 3\nmode_color 2 0 10\nmode_color 2 1 11\nmode_color 2 2 4\nmode_color 2 3 13\nmode_color 2 4 10\nmode_color 2 5 3\nmode_color 3 0 8\nmode_color 3 1 11\nmode_color 3 2 4\nmode_color 3 3 13\nmode_color 3 4 10\nmode_color 3 5 3\nmode_color 4 0 7\nmode_color 4 1 11\nmode_color 4 2 3\nmode_color 4 3 13\nmode_color 4 4 10\nmode_color 4 5 3\nmode_color 5 0 9\nmode_color 5 1 11\nmode_color 5 2 2\nmode_color 5 3 13\nmode_color 5 4 10\nmode_color 5 5 3\nmode_color 6 0 6\nmode_color 6 1 10\nmode_color 6 2 1\nmode_color 6 3 0\nmode_color 6 4 0\nmode_color 6 5 2\nmode_color 6 6 3\nmode_color 6 7 6\nmode_color 6 8 0\nmode_color 6 9 0\nmode_color 6 10 0aux 0 0 0 900 1300\naux 1 51 5 1300 1700\naux 2 1 2 900 1725\naux 3 10 4 1825 2100\naux 4 11 3 1300 1600\naux 5 3 2 1350 1625\naux 6 5 3 1825 2100\naux 7 28 4 1300 1700\naux 8 30 1 1700 2100\naux 9 52 7 1700 2100\naux 10 13 5 1300 2100\naux 11 27 5 1775 2100\naux 12 0 0 900 900\naux 13 0 0 900 900\naux 14 0 0 900 900\naux 15 0 0 900 900\naux 16 0 0 900 900\naux 17 0 0 900 900\naux 18 0 0 900 900\naux 19 0 0 900 900\naux 20 0 0 900 900\naux 21 0 0 900 900\naux 22 0 0 900 900\naux 23 0 0 900 900\naux 24 0 0 900 900\naux 25 0 0 900 900\naux 26 0 0 900 900\naux 27 0 0 900 900\naux 28 0 0 900 900\naux 29 0 0 900 900\naux 30 0 0 900 900\naux 31 0 0 900 900\naux 32 0 0 900 900\naux 33 0 0 900 900\naux 34 0 0 900 900\naux 35 0 0 900 900\naux 36 0 0 900 900\naux 37 0 0 900 900\naux 38 0 0 900 900\naux 39 0 0 900 900adjrange 0 0 7 900 950 0 7\nadjrange 1 0 0 900 900 0 0\nadjrange 2 0 0 900 900 0 0\nadjrange 3 0 0 900 900 0 0\nadjrange 4 0 0 900 900 0 0\nadjrange 5 0 0 900 900 0 0\nadjrange 6 0 0 900 900 0 0\nadjrange 7 0 0 900 900 0 0\nadjrange 8 0 0 900 900 0 0\nadjrange 9 0 0 900 900 0 0\nadjrange 10 0 0 900 900 0 0\nadjrange 11 0 0 900 900 0 0\nadjrange 12 0 0 900 900 0 0\nadjrange 13 0 0 900 900 0 0\nadjrange 14 0 0 900 900 0 0\nadjrange 15 0 0 900 900 0 0\nadjrange 16 0 0 900 900 0 0\nadjrange 17 0 0 900 900 0 0\nadjrange 18 0 0 900 900 0 0\nadjrange 19 0 0 900 900 0 0rxrange 0 1000 2000\nrxrange 1 1000 2000\nrxrange 2 1000 2000\nrxrange 3 1000 2000temp_sensor 0 0 0 0 0 0\ntemp_sensor 1 0 0 0 0 0\ntemp_sensor 2 0 0 0 0 0\ntemp_sensor 3 0 0 0 0 0\ntemp_sensor 4 0 0 0 0 0\ntemp_sensor 5 0 0 0 0 0\ntemp_sensor 6 0 0 0 0 0\ntemp_sensor 7 0 0 0 0 0#wp 0 invalid\nwp 0 0 0 0 0 0 0 0 0\nwp 1 0 0 0 0 0 0 0 0\nwp 2 0 0 0 0 0 0 0 0\nwp 3 0 0 0 0 0 0 0 0\nwp 4 0 0 0 0 0 0 0 0\nwp 5 0 0 0 0 0 0 0 0\nwp 6 0 0 0 0 0 0 0 0\nwp 7 0 0 0 0 0 0 0 0\nwp 8 0 0 0 0 0 0 0 0\nwp 9 0 0 0 0 0 0 0 0\nwp 10 0 0 0 0 0 0 0 0\nwp 11 0 0 0 0 0 0 0 0\nwp 12 0 0 0 0 0 0 0 0\nwp 13 0 0 0 0 0 0 0 0\nwp 14 0 0 0 0 0 0 0 0\nwp 15 0 0 0 0 0 0 0 0\nwp 16 0 0 0 0 0 0 0 0\nwp 17 0 0 0 0 0 0 0 0\nwp 18 0 0 0 0 0 0 0 0\nwp 19 0 0 0 0 0 0 0 0\nwp 20 0 0 0 0 0 0 0 0\nwp 21 0 0 0 0 0 0 0 0\nwp 22 0 0 0 0 0 0 0 0\nwp 23 0 0 0 0 0 0 0 0\nwp 24 0 0 0 0 0 0 0 0\nwp 25 0 0 0 0 0 0 0 0\nwp 26 0 0 0 0 0 0 0 0\nwp 27 0 0 0 0 0 0 0 0\nwp 28 0 0 0 0 0 0 0 0\nwp 29 0 0 0 0 0 0 0 0\nwp 30 0 0 0 0 0 0 0 0\nwp 31 0 0 0 0 0 0 0 0\nwp 32 0 0 0 0 0 0 0 0\nwp 33 0 0 0 0 0 0 0 0\nwp 34 0 0 0 0 0 0 0 0\nwp 35 0 0 0 0 0 0 0 0\nwp 36 0 0 0 0 0 0 0 0\nwp 37 0 0 0 0 0 0 0 0\nwp 38 0 0 0 0 0 0 0 0\nwp 39 0 0 0 0 0 0 0 0\nwp 40 0 0 0 0 0 0 0 0\nwp 41 0 0 0 0 0 0 0 0\nwp 42 0 0 0 0 0 0 0 0\nwp 43 0 0 0 0 0 0 0 0\nwp 44 0 0 0 0 0 0 0 0\nwp 45 0 0 0 0 0 0 0 0\nwp 46 0 0 0 0 0 0 0 0\nwp 47 0 0 0 0 0 0 0 0\nwp 48 0 0 0 0 0 0 0 0\nwp 49 0 0 0 0 0 0 0 0\nwp 50 0 0 0 0 0 0 0 0\nwp 51 0 0 0 0 0 0 0 0\nwp 52 0 0 0 0 0 0 0 0\nwp 53 0 0 0 0 0 0 0 0\nwp 54 0 0 0 0 0 0 0 0\nwp 55 0 0 0 0 0 0 0 0\nwp 56 0 0 0 0 0 0 0 0\nwp 57 0 0 0 0 0 0 0 0\nwp 58 0 0 0 0 0 0 0 0\nwp 59 0 0 0 0 0 0 0 0osd_layout 0 0 24 1 V\nosd_layout 0 1 12 0 V\nosd_layout 0 2 0 0 H\nosd_layout 0 3 8 6 H\nosd_layout 0 4 8 6 H\nosd_layout 0 5 23 8 H\nosd_layout 0 6 23 9 H\nosd_layout 0 7 13 13 V\nosd_layout 0 8 14 14 V\nosd_layout 0 9 1 2 H\nosd_layout 0 10 8 6 H\nosd_layout 0 11 26 6 V\nosd_layout 0 12 1 7 H\nosd_layout 0 13 26 7 V\nosd_layout 0 14 0 0 V\nosd_layout 0 15 0 6 V\nosd_layout 0 16 2 10 H\nosd_layout 0 17 2 11 H\nosd_layout 0 18 2 12 H\nosd_layout 0 19 15 1 H\nosd_layout 0 20 0 11 V\nosd_layout 0 21 0 12 V\nosd_layout 0 22 14 2 V\nosd_layout 0 23 0 1 V\nosd_layout 0 24 12 2 H\nosd_layout 0 25 23 5 H\nosd_layout 0 26 24 7 H\nosd_layout 0 27 3 5 H\nosd_layout 0 28 23 11 H\nosd_layout 0 29 23 12 H\nosd_layout 0 30 0 15 H\nosd_layout 0 31 0 10 H\nosd_layout 0 32 12 1 V\nosd_layout 0 33 6 2 H\nosd_layout 0 34 18 2 H\nosd_layout 0 35 1 5 H\nosd_layout 0 36 1 5 H\nosd_layout 0 37 1 6 H\nosd_layout 0 38 1 7 H\nosd_layout 0 39 1 5 H\nosd_layout 0 40 1 2 H\nosd_layout 0 41 1 8 H\nosd_layout 0 42 1 7 H\nosd_layout 0 43 0 0 H\nosd_layout 0 44 0 0 H\nosd_layout 0 45 0 0 H\nosd_layout 0 46 3 6 H\nosd_layout 0 47 3 7 H\nosd_layout 0 48 23 7 H\nosd_layout 0 49 23 6 H\nosd_layout 0 50 0 0 H\nosd_layout 0 51 12 2 H\nosd_layout 0 52 12 2 H\nosd_layout 0 53 12 1 H\nosd_layout 0 54 12 1 H\nosd_layout 0 55 1 8 H\nosd_layout 0 56 2 12 H\nosd_layout 0 57 2 12 H\nosd_layout 0 58 2 12 H\nosd_layout 0 59 2 12 H\nosd_layout 0 60 2 12 H\nosd_layout 0 61 2 12 H\nosd_layout 0 62 2 10 H\nosd_layout 0 63 2 11 H\nosd_layout 0 64 2 12 H\nosd_layout 0 65 2 12 H\nosd_layout 0 66 2 12 H\nosd_layout 0 67 2 12 H\nosd_layout 0 68 2 12 H\nosd_layout 0 69 2 12 H\nosd_layout 0 70 2 12 H\nosd_layout 0 71 2 12 H\nosd_layout 0 72 2 12 H\nosd_layout 0 73 2 12 H\nosd_layout 0 74 2 12 H\nosd_layout 0 75 2 12 H\nosd_layout 0 76 2 12 H\nosd_layout 0 77 2 12 H\nosd_layout 0 78 0 0 H\nosd_layout 0 79 2 12 H\nosd_layout 0 80 2 12 H\nosd_layout 0 81 2 12 H\nosd_layout 0 82 2 12 H\nosd_layout 0 83 2 12 H\nosd_layout 0 84 2 12 H\nosd_layout 0 85 23 1 H\nosd_layout 0 86 19 2 H\nosd_layout 0 87 19 3 H\nosd_layout 0 88 19 4 H\nosd_layout 0 89 19 5 H\nosd_layout 0 90 19 6 H\nosd_layout 0 91 19 7 H\nosd_layout 0 92 19 8 H\nosd_layout 0 93 19 9 H\nosd_layout 0 94 19 10 H\nosd_layout 0 95 19 11 H\nosd_layout 0 96 0 0 H\nosd_layout 0 97 1 12 H\nosd_layout 0 98 0 0 H\nosd_layout 0 99 0 0 H\nosd_layout 0 100 12 4 H\nosd_layout 0 101 12 5 H\nosd_layout 0 102 12 6 H\nosd_layout 0 103 12 7 H\nosd_layout 0 104 0 0 H\nosd_layout 0 105 3 5 H\nosd_layout 0 106 1 2 H\nosd_layout 0 107 1 3 H\nosd_layout 0 108 2 12 H\nosd_layout 0 109 23 12 H\nosd_layout 0 110 23 11 H\nosd_layout 0 111 24 9 H\nosd_layout 0 112 24 10 H\nosd_layout 0 113 1 1 H\nosd_layout 0 114 1 2 H\nosd_layout 0 115 1 3 H\nosd_layout 0 116 1 4 H\nosd_layout 0 117 0 0 H\nosd_layout 0 118 0 0 H\nosd_layout 0 119 0 0 H\nosd_layout 0 120 0 0 H\nosd_layout 0 121 3 4 H\nosd_layout 0 122 3 5 H\nosd_layout 0 123 3 6 H\nosd_layout 0 124 23 2 H\nosd_layout 1 0 23 0 H\nosd_layout 1 1 12 0 H\nosd_layout 1 2 0 0 H\nosd_layout 1 3 8 6 H\nosd_layout 1 4 8 6 H\nosd_layout 1 5 23 8 H\nosd_layout 1 6 23 9 H\nosd_layout 1 7 13 12 H\nosd_layout 1 8 20 2 H\nosd_layout 1 9 1 2 H\nosd_layout 1 10 8 6 H\nosd_layout 1 11 2 3 H\nosd_layout 1 12 1 4 H\nosd_layout 1 13 23 1 H\nosd_layout 1 14 0 11 H\nosd_layout 1 15 1 0 H\nosd_layout 1 16 2 10 H\nosd_layout 1 17 2 11 H\nosd_layout 1 18 2 12 H\nosd_layout 1 19 15 1 H\nosd_layout 1 20 18 12 H\nosd_layout 1 21 0 12 H\nosd_layout 1 22 14 11 H\nosd_layout 1 23 1 1 H\nosd_layout 1 24 12 2 H\nosd_layout 1 25 23 5 H\nosd_layout 1 26 24 7 H\nosd_layout 1 27 3 5 H\nosd_layout 1 28 23 11 H\nosd_layout 1 29 23 12 H\nosd_layout 1 30 1 13 H\nosd_layout 1 31 0 10 H\nosd_layout 1 32 12 1 H\nosd_layout 1 33 6 2 H\nosd_layout 1 34 18 2 H\nosd_layout 1 35 1 5 H\nosd_layout 1 36 1 5 H\nosd_layout 1 37 1 6 H\nosd_layout 1 38 1 7 H\nosd_layout 1 39 1 5 H\nosd_layout 1 40 1 2 H\nosd_layout 1 41 1 8 H\nosd_layout 1 42 1 7 H\nosd_layout 1 43 0 0 H\nosd_layout 1 44 0 0 H\nosd_layout 1 45 0 0 H\nosd_layout 1 46 3 6 H\nosd_layout 1 47 3 7 H\nosd_layout 1 48 23 7 H\nosd_layout 1 49 23 6 H\nosd_layout 1 50 0 0 H\nosd_layout 1 51 12 2 H\nosd_layout 1 52 12 2 H\nosd_layout 1 53 12 1 H\nosd_layout 1 54 12 1 H\nosd_layout 1 55 1 8 H\nosd_layout 1 56 2 12 H\nosd_layout 1 57 2 12 H\nosd_layout 1 58 2 12 H\nosd_layout 1 59 2 12 H\nosd_layout 1 60 2 12 H\nosd_layout 1 61 2 12 H\nosd_layout 1 62 2 10 H\nosd_layout 1 63 2 11 H\nosd_layout 1 64 2 12 H\nosd_layout 1 65 2 12 H\nosd_layout 1 66 2 12 H\nosd_layout 1 67 2 12 H\nosd_layout 1 68 2 12 H\nosd_layout 1 69 2 12 H\nosd_layout 1 70 2 12 H\nosd_layout 1 71 2 12 H\nosd_layout 1 72 2 12 H\nosd_layout 1 73 2 12 H\nosd_layout 1 74 2 12 H\nosd_layout 1 75 2 12 H\nosd_layout 1 76 2 12 H\nosd_layout 1 77 2 12 H\nosd_layout 1 78 0 0 H\nosd_layout 1 79 2 12 H\nosd_layout 1 80 2 12 H\nosd_layout 1 81 2 12 H\nosd_layout 1 82 2 12 H\nosd_layout 1 83 2 12 H\nosd_layout 1 84 2 12 H\nosd_layout 1 85 23 1 H\nosd_layout 1 86 19 2 H\nosd_layout 1 87 19 3 H\nosd_layout 1 88 19 4 H\nosd_layout 1 89 19 5 H\nosd_layout 1 90 19 6 H\nosd_layout 1 91 19 7 H\nosd_layout 1 92 19 8 H\nosd_layout 1 93 19 9 H\nosd_layout 1 94 19 10 H\nosd_layout 1 95 19 11 H\nosd_layout 1 96 0 0 H\nosd_layout 1 97 0 12 H\nosd_layout 1 98 0 0 H\nosd_layout 1 99 0 0 H\nosd_layout 1 100 12 4 H\nosd_layout 1 101 12 5 H\nosd_layout 1 102 12 6 H\nosd_layout 1 103 12 7 H\nosd_layout 1 104 0 0 H\nosd_layout 1 105 3 5 H\nosd_layout 1 106 1 2 H\nosd_layout 1 107 1 3 H\nosd_layout 1 108 2 12 H\nosd_layout 1 109 23 12 H\nosd_layout 1 110 23 11 H\nosd_layout 1 111 24 9 H\nosd_layout 1 112 24 10 H\nosd_layout 1 113 1 1 H\nosd_layout 1 114 1 2 H\nosd_layout 1 115 1 3 H\nosd_layout 1 116 1 4 H\nosd_layout 1 117 0 0 H\nosd_layout 1 118 0 0 H\nosd_layout 1 119 0 0 H\nosd_layout 1 120 0 0 H\nosd_layout 1 121 3 4 H\nosd_layout 1 122 3 5 H\nosd_layout 1 123 3 6 H\nosd_layout 1 124 23 2 H\nosd_layout 2 0 23 0 H\nosd_layout 2 1 12 0 H\nosd_layout 2 2 0 0 H\nosd_layout 2 3 8 6 H\nosd_layout 2 4 8 6 H\nosd_layout 2 5 23 8 H\nosd_layout 2 6 23 9 H\nosd_layout 2 7 13 12 H\nosd_layout 2 8 20 2 H\nosd_layout 2 9 1 2 H\nosd_layout 2 10 8 6 H\nosd_layout 2 11 2 3 H\nosd_layout 2 12 1 4 H\nosd_layout 2 13 23 1 H\nosd_layout 2 14 0 11 H\nosd_layout 2 15 1 0 H\nosd_layout 2 16 2 10 H\nosd_layout 2 17 2 11 H\nosd_layout 2 18 2 12 H\nosd_layout 2 19 15 1 H\nosd_layout 2 20 18 12 H\nosd_layout 2 21 0 12 H\nosd_layout 2 22 14 11 H\nosd_layout 2 23 1 1 H\nosd_layout 2 24 12 2 H\nosd_layout 2 25 23 5 H\nosd_layout 2 26 24 7 H\nosd_layout 2 27 3 5 H\nosd_layout 2 28 23 11 H\nosd_layout 2 29 23 12 H\nosd_layout 2 30 1 13 H\nosd_layout 2 31 0 10 H\nosd_layout 2 32 12 1 H\nosd_layout 2 33 6 2 H\nosd_layout 2 34 18 2 H\nosd_layout 2 35 1 5 H\nosd_layout 2 36 1 5 H\nosd_layout 2 37 1 6 H\nosd_layout 2 38 1 7 H\nosd_layout 2 39 1 5 H\nosd_layout 2 40 1 2 H\nosd_layout 2 41 1 8 H\nosd_layout 2 42 1 7 H\nosd_layout 2 43 0 0 H\nosd_layout 2 44 0 0 H\nosd_layout 2 45 0 0 H\nosd_layout 2 46 3 6 H\nosd_layout 2 47 3 7 H\nosd_layout 2 48 23 7 H\nosd_layout 2 49 23 6 H\nosd_layout 2 50 0 0 H\nosd_layout 2 51 12 2 H\nosd_layout 2 52 12 2 H\nosd_layout 2 53 12 1 H\nosd_layout 2 54 12 1 H\nosd_layout 2 55 1 8 H\nosd_layout 2 56 2 12 H\nosd_layout 2 57 2 12 H\nosd_layout 2 58 2 12 H\nosd_layout 2 59 2 12 H\nosd_layout 2 60 2 12 H\nosd_layout 2 61 2 12 H\nosd_layout 2 62 2 10 H\nosd_layout 2 63 2 11 H\nosd_layout 2 64 2 12 H\nosd_layout 2 65 2 12 H\nosd_layout 2 66 2 12 H\nosd_layout 2 67 2 12 H\nosd_layout 2 68 2 12 H\nosd_layout 2 69 2 12 H\nosd_layout 2 70 2 12 H\nosd_layout 2 71 2 12 H\nosd_layout 2 72 2 12 H\nosd_layout 2 73 2 12 H\nosd_layout 2 74 2 12 H\nosd_layout 2 75 2 12 H\nosd_layout 2 76 2 12 H\nosd_layout 2 77 2 12 H\nosd_layout 2 78 0 0 H\nosd_layout 2 79 2 12 H\nosd_layout 2 80 2 12 H\nosd_layout 2 81 2 12 H\nosd_layout 2 82 2 12 H\nosd_layout 2 83 2 12 H\nosd_layout 2 84 2 12 H\nosd_layout 2 85 23 1 H\nosd_layout 2 86 19 2 H\nosd_layout 2 87 19 3 H\nosd_layout 2 88 19 4 H\nosd_layout 2 89 19 5 H\nosd_layout 2 90 19 6 H\nosd_layout 2 91 19 7 H\nosd_layout 2 92 19 8 H\nosd_layout 2 93 19 9 H\nosd_layout 2 94 19 10 H\nosd_layout 2 95 19 11 H\nosd_layout 2 96 0 0 H\nosd_layout 2 97 0 12 H\nosd_layout 2 98 0 0 H\nosd_layout 2 99 0 0 H\nosd_layout 2 100 12 4 H\nosd_layout 2 101 12 5 H\nosd_layout 2 102 12 6 H\nosd_layout 2 103 12 7 H\nosd_layout 2 104 0 0 H\nosd_layout 2 105 3 5 H\nosd_layout 2 106 1 2 H\nosd_layout 2 107 1 3 H\nosd_layout 2 108 2 12 H\nosd_layout 2 109 23 12 H\nosd_layout 2 110 23 11 H\nosd_layout 2 111 24 9 H\nosd_layout 2 112 24 10 H\nosd_layout 2 113 1 1 H\nosd_layout 2 114 1 2 H\nosd_layout 2 115 1 3 H\nosd_layout 2 116 1 4 H\nosd_layout 2 117 0 0 H\nosd_layout 2 118 0 0 H\nosd_layout 2 119 0 0 H\nosd_layout 2 120 0 0 H\nosd_layout 2 121 3 4 H\nosd_layout 2 122 3 5 H\nosd_layout 2 123 3 6 H\nosd_layout 2 124 23 2 H\nosd_layout 3 0 23 0 H\nosd_layout 3 1 12 0 H\nosd_layout 3 2 0 0 H\nosd_layout 3 3 8 6 H\nosd_layout 3 4 8 6 H\nosd_layout 3 5 23 8 H\nosd_layout 3 6 23 9 H\nosd_layout 3 7 13 12 H\nosd_layout 3 8 20 2 H\nosd_layout 3 9 1 2 H\nosd_layout 3 10 8 6 H\nosd_layout 3 11 2 3 H\nosd_layout 3 12 1 4 H\nosd_layout 3 13 23 1 H\nosd_layout 3 14 0 11 H\nosd_layout 3 15 1 0 H\nosd_layout 3 16 2 10 H\nosd_layout 3 17 2 11 H\nosd_layout 3 18 2 12 H\nosd_layout 3 19 15 1 H\nosd_layout 3 20 18 12 H\nosd_layout 3 21 0 12 H\nosd_layout 3 22 14 11 H\nosd_layout 3 23 1 1 H\nosd_layout 3 24 12 2 H\nosd_layout 3 25 23 5 H\nosd_layout 3 26 24 7 H\nosd_layout 3 27 3 5 H\nosd_layout 3 28 23 11 H\nosd_layout 3 29 23 12 H\nosd_layout 3 30 1 13 H\nosd_layout 3 31 0 10 H\nosd_layout 3 32 12 1 H\nosd_layout 3 33 6 2 H\nosd_layout 3 34 18 2 H\nosd_layout 3 35 1 5 H\nosd_layout 3 36 1 5 H\nosd_layout 3 37 1 6 H\nosd_layout 3 38 1 7 H\nosd_layout 3 39 1 5 H\nosd_layout 3 40 1 2 H\nosd_layout 3 41 1 8 H\nosd_layout 3 42 1 7 H\nosd_layout 3 43 0 0 H\nosd_layout 3 44 0 0 H\nosd_layout 3 45 0 0 H\nosd_layout 3 46 3 6 H\nosd_layout 3 47 3 7 H\nosd_layout 3 48 23 7 H\nosd_layout 3 49 23 6 H\nosd_layout 3 50 0 0 H\nosd_layout 3 51 12 2 H\nosd_layout 3 52 12 2 H\nosd_layout 3 53 12 1 H\nosd_layout 3 54 12 1 H\nosd_layout 3 55 1 8 H\nosd_layout 3 56 2 12 H\nosd_layout 3 57 2 12 H\nosd_layout 3 58 2 12 H\nosd_layout 3 59 2 12 H\nosd_layout 3 60 2 12 H\nosd_layout 3 61 2 12 H\nosd_layout 3 62 2 10 H\nosd_layout 3 63 2 11 H\nosd_layout 3 64 2 12 H\nosd_layout 3 65 2 12 H\nosd_layout 3 66 2 12 H\nosd_layout 3 67 2 12 H\nosd_layout 3 68 2 12 H\nosd_layout 3 69 2 12 H\nosd_layout 3 70 2 12 H\nosd_layout 3 71 2 12 H\nosd_layout 3 72 2 12 H\nosd_layout 3 73 2 12 H\nosd_layout 3 74 2 12 H\nosd_layout 3 75 2 12 H\nosd_layout 3 76 2 12 H\nosd_layout 3 77 2 12 H\nosd_layout 3 78 0 0 H\nosd_layout 3 79 2 12 H\nosd_layout 3 80 2 12 H\nosd_layout 3 81 2 12 H\nosd_layout 3 82 2 12 H\nosd_layout 3 83 2 12 H\nosd_layout 3 84 2 12 H\nosd_layout 3 85 23 1 H\nosd_layout 3 86 19 2 H\nosd_layout 3 87 19 3 H\nosd_layout 3 88 19 4 H\nosd_layout 3 89 19 5 H\nosd_layout 3 90 19 6 H\nosd_layout 3 91 19 7 H\nosd_layout 3 92 19 8 H\nosd_layout 3 93 19 9 H\nosd_layout 3 94 19 10 H\nosd_layout 3 95 19 11 H\nosd_layout 3 96 0 0 H\nosd_layout 3 97 0 12 H\nosd_layout 3 98 0 0 H\nosd_layout 3 99 0 0 H\nosd_layout 3 100 12 4 H\nosd_layout 3 101 12 5 H\nosd_layout 3 102 12 6 H\nosd_layout 3 103 12 7 H\nosd_layout 3 104 0 0 H\nosd_layout 3 105 3 5 H\nosd_layout 3 106 1 2 H\nosd_layout 3 107 1 3 H\nosd_layout 3 108 2 12 H\nosd_layout 3 109 23 12 H\nosd_layout 3 110 23 11 H\nosd_layout 3 111 24 9 H\nosd_layout 3 112 24 10 H\nosd_layout 3 113 1 1 H\nosd_layout 3 114 1 2 H\nosd_layout 3 115 1 3 H\nosd_layout 3 116 1 4 H\nosd_layout 3 117 0 0 H\nosd_layout 3 118 0 0 H\nosd_layout 3 119 0 0 H\nosd_layout 3 120 0 0 H\nosd_layout 3 121 3 4 H\nosd_layout 3 122 3 5 H\nosd_layout 3 123 3 6 H\nosd_layout 3 124 23 2 Hset looptime = 500\nset align_gyro = DEFAULT\nset gyro_hardware_lpf = 256HZ\nset gyro_anti_aliasing_lpf_hz = 250\nset gyro_anti_aliasing_lpf_type = PT1\nset moron_threshold = 32\nset gyro_notch_hz = 0\nset gyro_notch_cutoff = 1\nset gyro_main_lpf_hz = 100\nset gyro_main_lpf_type = PT1\nset gyro_use_dyn_lpf = OFF\nset gyro_dyn_lpf_min_hz = 200\nset gyro_dyn_lpf_max_hz = 500\nset gyro_dyn_lpf_curve_expo = 5\nset dynamic_gyro_notch_enabled = ON\nset dynamic_gyro_notch_range = MEDIUM\nset dynamic_gyro_notch_q = 175\nset dynamic_gyro_notch_min_hz = 110\nset gyro_to_use = 0\nset gyro_abg_alpha =  0.000\nset gyro_abg_boost =  0.350\nset gyro_abg_half_life =  0.500\nset vbat_adc_channel = 1\nset rssi_adc_channel = 3\nset current_adc_channel = 2\nset airspeed_adc_channel = 4\nset acc_notch_hz = 0\nset acc_notch_cutoff = 1\nset align_acc = DEFAULT\nset acc_hardware = MPU6000\nset acc_lpf_hz = 15\nset acc_lpf_type = BIQUAD\nset acczero_x = 7\nset acczero_y = -8\nset acczero_z = -141\nset accgain_x = 4105\nset accgain_y = 4098\nset accgain_z = 4049\nset rangefinder_hardware = NONE\nset rangefinder_median_filter = OFF\nset opflow_hardware = NONE\nset opflow_scale =  10.500\nset align_opflow = CW0FLIP\nset imu2_hardware = NONE\nset imu2_use_for_osd_heading = OFF\nset imu2_use_for_osd_ahi = OFF\nset imu2_use_for_stabilized = OFF\nset imu2_align_roll = 0\nset imu2_align_pitch = 0\nset imu2_align_yaw = 0\nset imu2_gain_acc_x = 0\nset imu2_gain_acc_y = 0\nset imu2_gain_acc_z = 0\nset imu2_gain_mag_x = 0\nset imu2_gain_mag_y = 0\nset imu2_gain_mag_z = 0\nset imu2_radius_acc = 0\nset imu2_radius_mag = 0\nset align_mag = CW0FLIP\nset mag_hardware = QMC5883\nset mag_declination = 0\nset magzero_x = 136\nset magzero_y = 0\nset magzero_z = 0\nset maggain_x = 1545\nset maggain_y = 1478\nset maggain_z = 1555\nset mag_calibration_time = 30\nset align_mag_roll = 0\nset align_mag_pitch = 0\nset align_mag_yaw = 0\nset baro_hardware = BMP280\nset baro_median_filter = ON\nset baro_cal_tolerance = 150\nset pitot_hardware = NONE\nset pitot_lpf_milli_hz = 350\nset pitot_scale =  1.000\nset receiver_type = SERIAL\nset min_check = 1050\nset max_check = 1900\nset rssi_source = AUTO\nset rssi_channel = 16\nset rssi_min = 0\nset rssi_max = 100\nset sbus_sync_interval = 3000\nset rc_filter_frequency = 50\nset serialrx_provider = SBUS\nset serialrx_inverted = OFF\nset srxl2_unit_id = 1\nset srxl2_baud_fast = ON\nset rx_min_usec = 885\nset rx_max_usec = 2115\nset serialrx_halfduplex = AUTO\nset blackbox_rate_num = 1\nset blackbox_rate_denom = 2\nset blackbox_device = SDCARD\nset sdcard_detect_inverted = OFF\nset max_throttle = 1850\nset min_command = 1000\nset motor_pwm_rate = 16000\nset motor_accel_time = 0\nset motor_decel_time = 0\nset motor_pwm_protocol = DSHOT600\nset throttle_scale =  1.000\nset throttle_idle =  6.000\nset motor_poles = 14\nset turtle_mode_power_factor = 55\nset failsafe_delay = 5\nset failsafe_recovery_delay = 5\nset failsafe_off_delay = 50\nset failsafe_throttle = 1200\nset failsafe_throttle_low_delay = 0\nset failsafe_procedure = RTH\nset failsafe_stick_threshold = 50\nset failsafe_fw_roll_angle = -200\nset failsafe_fw_pitch_angle = 100\nset failsafe_fw_yaw_rate = -45\nset failsafe_min_distance = 0\nset failsafe_min_distance_procedure = DROP\nset failsafe_mission = ON\nset align_board_roll = 10\nset align_board_pitch = 4\nset align_board_yaw = 0\nset vbat_meter_type = ADC\nset vbat_scale = 1100\nset current_meter_scale = 179\nset current_meter_offset = 0\nset current_meter_type = ADC\nset bat_voltage_src = RAW\nset cruise_power = 0\nset idle_power = 0\nset rth_energy_margin = 5\nset thr_comp_weight =  1.000\nset motor_direction_inverted = ON\nset platform_type = MULTIROTOR\nset has_flaps = OFF\nset model_preview_type = 3\nset fw_min_throttle_down_pitch = 0\nset 3d_deadband_low = 1406\nset 3d_deadband_high = 1514\nset 3d_neutral = 1460\nset servo_protocol = PWM\nset servo_center_pulse = 1500\nset servo_pwm_rate = 60\nset servo_lpf_hz = 20\nset flaperon_throw_offset = 200\nset tri_unarmed_servo = ON\nset servo_autotrim_rotation_limit = 15\nset reboot_character = 82\nset imu_dcm_kp = 2500\nset imu_dcm_ki = 50\nset imu_dcm_kp_mag = 10000\nset imu_dcm_ki_mag = 0\nset small_angle = 25\nset imu_acc_ignore_rate = 0\nset imu_acc_ignore_slope = 0\nset fixed_wing_auto_arm = OFF\nset disarm_kill_switch = ON\nset switch_disarm_delay = 250\nset prearm_timeout = 10000\nset applied_defaults = 2\nset rpm_gyro_filter_enabled = OFF\nset rpm_gyro_harmonics = 1\nset rpm_gyro_min_hz = 100\nset rpm_gyro_q = 500\nset gps_provider = UBLOX\nset gps_sbas_mode = NONE\nset gps_dyn_model = AIR_1G\nset gps_auto_config = ON\nset gps_auto_baud = ON\nset gps_ublox_use_galileo = ON\nset gps_min_sats = 6\nset deadband = 5\nset yaw_deadband = 5\nset pos_hold_deadband = 10\nset control_deadband = 10\nset alt_hold_deadband = 50\nset 3d_deadband_throttle = 50\nset airmode_type = THROTTLE_THRESHOLD\nset airmode_throttle_threshold = 1300\nset fw_autotune_min_stick = 50\nset fw_autotune_ff_to_p_gain = 10\nset fw_autotune_p_to_d_gain = 0\nset fw_autotune_ff_to_i_tc = 600\nset fw_autotune_rate_adjustment = AUTO\nset fw_autotune_max_rate_deflection = 90\nset inav_auto_mag_decl = ON\nset inav_gravity_cal_tolerance = 5\nset inav_use_gps_velned = ON\nset inav_use_gps_no_baro = OFF\nset inav_allow_dead_reckoning = OFF\nset inav_reset_altitude = FIRST_ARM\nset inav_reset_home = FIRST_ARM\nset inav_max_surface_altitude = 200\nset inav_w_z_surface_p =  3.500\nset inav_w_z_surface_v =  6.100\nset inav_w_xy_flow_p =  1.000\nset inav_w_xy_flow_v =  2.000\nset inav_w_z_baro_p =  0.350\nset inav_w_z_gps_p =  0.200\nset inav_w_z_gps_v =  0.090\nset inav_w_xy_gps_p =  1.000\nset inav_w_xy_gps_v =  2.000\nset inav_w_z_res_v =  0.500\nset inav_w_xy_res_v =  0.500\nset inav_w_xyz_acc_p =  1.000\nset inav_w_acc_bias =  0.010\nset inav_max_eph_epv =  1000.000\nset inav_baro_epv =  100.000\nset nav_disarm_on_landing = OFF\nset nav_use_midthr_for_althold = ON\nset nav_extra_arming_safety = OFF\nset nav_user_control_mode = ATTI\nset nav_position_timeout = 5\nset nav_wp_load_on_boot = OFF\nset nav_wp_radius = 50\nset nav_wp_safe_distance = 5000\nset nav_auto_speed = 300\nset nav_auto_climb_rate = 500\nset nav_manual_speed = 500\nset nav_manual_climb_rate = 200\nset nav_land_minalt_vspd = 50\nset nav_land_maxalt_vspd = 200\nset nav_land_slowdown_minalt = 400\nset nav_land_slowdown_maxalt = 500\nset nav_emerg_landing_speed = 250\nset nav_min_rth_distance = 500\nset nav_overrides_motor_stop = ALL_NAV\nset nav_rth_climb_first = ON\nset nav_rth_climb_ignore_emerg = OFF\nset nav_rth_tail_first = ON\nset nav_rth_allow_landing = ALWAYS\nset nav_rth_alt_mode = AT_LEAST\nset nav_rth_alt_control_override = OFF\nset nav_rth_abort_threshold = 50000\nset nav_max_terrain_follow_alt = 100\nset nav_max_altitude = 0\nset nav_rth_altitude = 1000\nset nav_rth_home_altitude = 0\nset safehome_max_distance = 20000\nset safehome_usage_mode = RTH\nset nav_mc_bank_angle = 40\nset nav_mc_hover_thr = 1500\nset nav_mc_auto_disarm_delay = 2000\nset nav_mc_braking_speed_threshold = 100\nset nav_mc_braking_disengage_speed = 75\nset nav_mc_braking_timeout = 2000\nset nav_mc_braking_boost_factor = 100\nset nav_mc_braking_boost_timeout = 750\nset nav_mc_braking_boost_speed_threshold = 150\nset nav_mc_braking_boost_disengage_speed = 100\nset nav_mc_braking_bank_angle = 40\nset nav_mc_pos_deceleration_time = 120\nset nav_mc_pos_expo = 10\nset nav_mc_wp_slowdown = ON\nset nav_fw_cruise_thr = 1399\nset nav_fw_min_thr = 1200\nset nav_fw_max_thr = 1700\nset nav_fw_bank_angle = 34\nset nav_fw_climb_angle = 20\nset nav_fw_dive_angle = 15\nset nav_fw_pitch2thr = 10\nset nav_fw_pitch2thr_smoothing = 6\nset nav_fw_pitch2thr_threshold = 50\nset nav_fw_loiter_radius = 7500\nset nav_fw_cruise_speed = 0\nset nav_fw_control_smoothness = 0\nset nav_fw_land_dive_angle = 2\nset nav_fw_launch_velocity = 300\nset nav_fw_launch_accel = 1863\nset nav_fw_launch_max_angle = 45\nset nav_fw_launch_detect_time = 40\nset nav_fw_launch_thr = 1700\nset nav_fw_launch_idle_thr = 1000\nset nav_fw_launch_idle_motor_delay = 0\nset nav_fw_launch_motor_delay = 500\nset nav_fw_launch_spinup_time = 100\nset nav_fw_launch_end_time = 3000\nset nav_fw_launch_min_time = 0\nset nav_fw_launch_timeout = 5000\nset nav_fw_launch_max_altitude = 0\nset nav_fw_launch_climb_angle = 18\nset nav_fw_cruise_yaw_rate = 20\nset nav_fw_allow_manual_thr_increase = OFF\nset nav_use_fw_yaw_control = OFF\nset nav_fw_yaw_deadband = 0\nset telemetry_switch = OFF\nset telemetry_inverted = OFF\nset frsky_default_latitude =  0.000\nset frsky_default_longitude =  0.000\nset frsky_coordinates_format = 0\nset frsky_unit = METRIC\nset frsky_vfas_precision = 0\nset frsky_pitch_roll = OFF\nset report_cell_voltage = OFF\nset hott_alarm_sound_interval = 5\nset telemetry_halfduplex = ON\nset smartport_fuel_unit = MAH\nset ibus_telemetry_type = 0\nset ltm_update_rate = NORMAL\nset sim_ground_station_number =\nset sim_pin = 0000\nset sim_transmit_interval = 60\nset sim_transmit_flags = 2\nset acc_event_threshold_high = 0\nset acc_event_threshold_low = 0\nset acc_event_threshold_neg_x = 0\nset sim_low_altitude = -32767\nset mavlink_ext_status_rate = 2\nset mavlink_rc_chan_rate = 5\nset mavlink_pos_rate = 2\nset mavlink_extra1_rate = 10\nset mavlink_extra2_rate = 2\nset mavlink_extra3_rate = 1\nset mavlink_version = 2\nset ledstrip_visual_beeper = OFF\nset osd_telemetry = OFF\nset osd_video_system = PAL\nset osd_row_shiftdown = 0\nset osd_units = METRIC\nset osd_stats_energy_unit = MAH\nset osd_stats_min_voltage_unit = BATTERY\nset osd_rssi_alarm = 20\nset osd_time_alarm = 10\nset osd_alt_alarm = 100\nset osd_dist_alarm = 1000\nset osd_neg_alt_alarm = 5\nset osd_current_alarm = 0\nset osd_gforce_alarm =  5.000\nset osd_gforce_axis_alarm_min = -5.000\nset osd_gforce_axis_alarm_max =  5.000\nset osd_imu_temp_alarm_min = -200\nset osd_imu_temp_alarm_max = 600\nset osd_esc_temp_alarm_max = 900\nset osd_esc_temp_alarm_min = -200\nset osd_baro_temp_alarm_min = -200\nset osd_baro_temp_alarm_max = 600\nset osd_snr_alarm = 4\nset osd_link_quality_alarm = 70\nset osd_rssi_dbm_alarm = 0\nset osd_temp_label_align = LEFT\nset osd_ahi_reverse_roll = OFF\nset osd_ahi_max_pitch = 20\nset osd_crosshairs_style = DEFAULT\nset osd_crsf_lq_format = TYPE1\nset osd_horizon_offset = 0\nset osd_camera_uptilt = 0\nset osd_ahi_camera_uptilt_comp = OFF\nset osd_camera_fov_h = 135\nset osd_camera_fov_v = 85\nset osd_hud_margin_h = 3\nset osd_hud_margin_v = 3\nset osd_hud_homing = OFF\nset osd_hud_homepoint = OFF\nset osd_hud_radar_disp = 0\nset osd_hud_radar_range_min = 3\nset osd_hud_radar_range_max = 4000\nset osd_hud_radar_nearest = 0\nset osd_hud_wp_disp = 0\nset osd_left_sidebar_scroll = NONE\nset osd_right_sidebar_scroll = NONE\nset osd_sidebar_scroll_arrows = OFF\nset osd_main_voltage_decimals = 1\nset osd_coordinate_digits = 9\nset osd_estimations_wind_compensation = ON\nset osd_failsafe_switch_layout = OFF\nset osd_plus_code_digits = 11\nset osd_plus_code_short = 0\nset osd_ahi_style = DEFAULT\nset osd_force_grid = OFF\nset osd_ahi_bordered = OFF\nset osd_ahi_width = 132\nset osd_ahi_height = 162\nset osd_ahi_vertical_offset = -18\nset osd_sidebar_horizontal_offset = 0\nset osd_left_sidebar_scroll_step = 0\nset osd_right_sidebar_scroll_step = 0\nset osd_sidebar_height = 3\nset osd_home_position_arm_screen = ON\nset osd_pan_servo_index = 0\nset osd_pan_servo_pwm2centideg = 0\nset osd_speed_source = GROUND\nset i2c_speed = 800KHZ\nset debug_mode = NONE\nset throttle_tilt_comp_str = 0\nset name = Kuzon GPS\nset mode_range_logic_operator = OR\nset stats = OFF\nset stats_total_time = 0\nset stats_total_dist = 0\nset stats_total_energy = 0\nset tz_offset = 0\nset tz_automatic_dst = OFF\nset display_force_sw_blink = OFF\nset vtx_halfduplex = ON\nset vtx_smartaudio_early_akk_workaround = ON\nset vtx_band = 3\nset vtx_channel = 3\nset vtx_power = 2\nset vtx_low_power_disarm = ON\nset vtx_pit_mode_chan = 1\nset vtx_max_power_override = 0\nset pinio_box1 = 47\nset pinio_box2 = 48\nset pinio_box3 = 255\nset pinio_box4 = 255\nset log_level = ERROR\nset log_topics = 0\nset esc_sensor_listen_only = OFF\nset smartport_master_halfduplex = ON\nset smartport_master_inverted = OFF\nset dji_workarounds = 1\nset dji_use_name_for_messages = ON\nset dji_esc_temp_source = ESC\nset dshot_beeper_enabled = OFF\nset dshot_beeper_tone = 4\nset limit_cont_current = 0\nset limit_burst_current = 0\nset limit_burst_current_time = 0\nset limit_burst_current_falldown_time = 0\nset limit_cont_power = 0\nset limit_burst_power = 0\nset limit_burst_power_time = 0\nset limit_burst_power_falldown_time = 0\nset limit_pi_p = 100\nset limit_pi_i = 100\nset limit_attn_filter_cutoff =  1.200profile 3set mc_p_pitch = 37\nset mc_i_pitch = 70\nset mc_d_pitch = 22\nset mc_cd_pitch = 100\nset mc_p_roll = 31\nset mc_i_roll = 50\nset mc_d_roll = 21\nset mc_cd_roll = 100\nset mc_p_yaw = 50\nset mc_i_yaw = 70\nset mc_d_yaw = 10\nset mc_cd_yaw = 100\nset mc_p_level = 30\nset mc_i_level = 15\nset mc_d_level = 75\nset fw_p_pitch = 5\nset fw_i_pitch = 7\nset fw_d_pitch = 0\nset fw_ff_pitch = 50\nset fw_p_roll = 5\nset fw_i_roll = 7\nset fw_d_roll = 0\nset fw_ff_roll = 50\nset fw_p_yaw = 6\nset fw_i_yaw = 10\nset fw_d_yaw = 0\nset fw_ff_yaw = 60\nset fw_p_level = 20\nset fw_i_level = 5\nset fw_d_level = 75\nset max_angle_inclination_rll = 450\nset max_angle_inclination_pit = 450\nset dterm_lpf_hz = 90\nset dterm_lpf_type = PT1\nset dterm_lpf2_hz = 0\nset dterm_lpf2_type = PT1\nset yaw_lpf_hz = 30\nset fw_iterm_throw_limit = 165\nset fw_loiter_direction = RIGHT\nset fw_reference_airspeed =  1500.000\nset fw_turn_assist_yaw_gain =  1.000\nset fw_turn_assist_pitch_gain =  1.000\nset fw_iterm_limit_stick_position =  0.500\nset fw_yaw_iterm_freeze_bank_angle = 0\nset pidsum_limit = 500\nset pidsum_limit_yaw = 350\nset iterm_windup = 50\nset rate_accel_limit_roll_pitch = 0\nset rate_accel_limit_yaw = 10000\nset heading_hold_rate_limit = 45\nset nav_mc_pos_z_p = 50\nset nav_mc_vel_z_p = 100\nset nav_mc_vel_z_i = 50\nset nav_mc_vel_z_d = 10\nset nav_mc_pos_xy_p = 65\nset nav_mc_vel_xy_p = 40\nset nav_mc_vel_xy_i = 15\nset nav_mc_vel_xy_d = 100\nset nav_mc_vel_xy_ff = 40\nset nav_mc_heading_p = 60\nset nav_mc_vel_xy_dterm_lpf_hz =  2.000\nset nav_mc_vel_xy_dterm_attenuation = 90\nset nav_mc_vel_xy_dterm_attenuation_start = 10\nset nav_mc_vel_xy_dterm_attenuation_end = 60\nset nav_fw_pos_z_p = 40\nset nav_fw_pos_z_i = 5\nset nav_fw_pos_z_d = 10\nset nav_fw_pos_xy_p = 75\nset nav_fw_pos_xy_i = 5\nset nav_fw_pos_xy_d = 8\nset nav_fw_heading_p = 60\nset nav_fw_pos_hdg_p = 30\nset nav_fw_pos_hdg_i = 2\nset nav_fw_pos_hdg_d = 0\nset nav_fw_pos_hdg_pidsum_limit = 350\nset mc_iterm_relax = RPY\nset mc_iterm_relax_cutoff = 15\nset d_boost_factor =  1.500\nset d_boost_max_at_acceleration =  5000.000\nset d_boost_gyro_delta_lpf_hz = 65\nset antigravity_gain =  2.000\nset antigravity_accelerator =  5.000\nset antigravity_cutoff_lpf_hz = 15\nset pid_type = AUTO\nset mc_cd_lpf_hz = 30\nset setpoint_kalman_enabled = OFF\nset setpoint_kalman_q = 100\nset setpoint_kalman_w = 4\nset setpoint_kalman_sharpness = 100\nset fw_level_pitch_trim =  0.000\nset smith_predictor_strength =  0.500\nset smith_predictor_delay =  0.000\nset smith_predictor_lpf_hz = 50\nset fw_level_pitch_gain =  5.000\nset thr_mid = 50\nset thr_expo = 0\nset tpa_rate = 0\nset tpa_breakpoint = 1500\nset fw_tpa_time_constant = 0\nset rc_expo = 70\nset rc_yaw_expo = 70\nset roll_rate = 70\nset pitch_rate = 70\nset yaw_rate = 60\nset manual_rc_expo = 70\nset manual_rc_yaw_expo = 20\nset manual_roll_rate = 100\nset manual_pitch_rate = 100\nset manual_yaw_rate = 100\nset fpv_mix_degrees = 0battery_profile 1set bat_cells = 0\nset vbat_cell_detect_voltage = 425\nset vbat_max_cell_voltage = 420\nset vbat_min_cell_voltage = 330\nset vbat_warning_cell_voltage = 350\nset battery_capacity = 0\nset battery_capacity_warning = 0\nset battery_capacity_critical = 0\nset battery_capacity_unit = MAH", "type": "commented", "related_issue": null}, {"user_name": "JulioCesarMatias", "datetime": "Jul 5, 2021", "body": "switch autopilot mode from  to  in . And yes, inav has a pos-hold as good as the DJI, you just need to know how to configure your equipment.", "type": "commented", "related_issue": null}, {"user_name": "Kuzon24", "datetime": "Jul 5, 2021", "body": "I will try that, thank you! It can really be that stable? It's holding it's position fine but when you move the sticks and let go it takes time to get locked into position. Can you please give me some tips on things I can try to make it perfect?", "type": "commented", "related_issue": null}, {"user_name": "JulioCesarMatias", "datetime": "Jul 5, 2021", "body": "like i said above, toggle autopilot mode.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Jul 6, 2021", "body": "The MAG calibration seems to be little off", "type": "commented", "related_issue": null}, {"user_name": "Kuzon24", "datetime": "Jul 6, 2021", "body": "I always calibrate compass before every flight. But I use the speedybee app for it. Maybe this is giving me wrong calibration? I'll try and use the quick sticks command this time.Thank you for taking time to check it Pawel ", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Jul 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Feb 3, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/342", "issue_status": " Closed\n", "issue_list": [{"user_name": "pprp", "datetime": "Aug 11, 2018", "body": "", "type": "commented", "related_issue": null}, {"user_name": "humandotlearning", "datetime": "Aug 17, 2018", "body": "When a picture is taken with some camera, it is recorded as a landscape direction picture and it is specified in the EXIF file of the picture that it was a portrait direction picture.\nlabelImg isn't handling the rotation needed through that EXIF file.You have to rotate and save the pictures before using them in labelImg.You can useOn Linux :\nexiftran -ai *.jpeg\nexiftran\nOn Windows:\nJPEG Autorotate", "type": "commented", "related_issue": null}, {"user_name": "pprp", "datetime": "Aug 18, 2018", "body": "Thank you very much!^_^", "type": "commented", "related_issue": null}, {"user_name": "pprp", "datetime": "Aug 18, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "zjZSTU", "datetime": "Jul 10, 2020", "body": [], "type": "issue", "related_issue": "#198"}]},
{"issue_url": "https://github.com/keras-team/keras/issues/13148", "issue_status": " Closed\n", "issue_list": [{"user_name": "python1995", "datetime": "Jul 24, 2019", "body": "I used the code and everything from the here.\n\nI was at the point were I went to run the code and see if the machine learning has worked and mz car can  go through a path detecting the linesHowever I'm getting this(env) pi@raspberrypi:~/donkey2 $ python drive.py --model model_2019-07-24__18-43\nusing donkey version: 2.5.8 ...\n/home/pi/.local/lib/python3.5/site-packages/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\nreturn f(*args, **kwds)\n/home/pi/.local/lib/python3.5/site-packages/env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\nreturn f(*args, **kwds)\nloading config file: /home/pi/donkey2/config.py\nconfig loaded\npygame 1.9.6\nHello from the pygame community. \nWebcam loaded.. .warming camera\nStarting Donkey Server...\nTraceback (most recent call last):\nFile \"drive.py\", line 142, in \ndrive(cfg, model_path=args['--model'])\nFile \"drive.py\", line 52, in drive\nkl.load(model_path)\nFile \"/home/pi/.local/lib/python3.5/site-packages/env/lib/python3.5/site-packages/donkeypart_keras_behavior_cloning/part.py\", line 17, in load\nself.model = load_model(model_path)\nFile \"/home/pi/.local/lib/python3.5/site-packages/env/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 187, in load_model\nraise ImportError(' requires h5py.')\nImportError:  requires h5py.", "type": "commented", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": "\nJust install necessary packagesThis should work.\nI am closing the issue. But if you are still facing the issue, you can reopen it again. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "python1995", "datetime": "Jul 24, 2019", "body": "\nOne quick question. As u saw I was working in an environment.\nDo I still use  sudo when installing  the packages _", "type": "commented", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 25, 2019", "body": " Please refer to this  which has been beautifully answered.", "type": "commented", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "gowthamkpr", "datetime": "Jul 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/159", "issue_status": " Closed\n", "issue_list": [{"user_name": "Kiathui", "datetime": "Sep 10, 2017", "body": "All my portrait images loaded into labelimg is flipped horizontal and i have no ways of rotating it in the program. Coordinates appear weird because of this.", "type": "commented", "related_issue": null}, {"user_name": "iraadit", "datetime": "Sep 28, 2017", "body": "I have exactly the same problem", "type": "commented", "related_issue": null}, {"user_name": "iraadit", "datetime": "Oct 13, 2017", "body": ", I found what was the problem !\nWhen a picture is taken with some camera, it is recorded as a landscape direction picture and it is specified in the EXIF file of the picture that it was a portrait direction picture.\nlabelImg isn't handling the rotation needed through that EXIF file.You have to rotate and save the pictures before using them in labelImg.You can use", "type": "commented", "related_issue": null}, {"user_name": "Kiathui", "datetime": "Oct 14, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#180"}]},
{"issue_url": "https://github.com/keras-team/keras/issues/13077", "issue_status": " Closed\n", "issue_list": [{"user_name": "dunfrey", "datetime": "Jul 6, 2019", "body": "I trained a CNN and everything occurs well, except the loss validation of one kind of output (training loss decreases but the validation loss not decrease, is too high and almost constant). I'd tried everything: more dropouts, decrease learning rate, increase and decrease the complexity of the nn, regularizers...Nothing solves, so I just wait for the end of the training step. On the test, is everything predicted close to the correct label but the lasts are every predicted wrong.EXAMPLE:For example, I have 100 images. The first 75 images are predicted close to the ground truth, but the last 25% data is predicted really far from the correct label (100 images - last 25%). So, I remove this last 25% of data from the test, and now, of the new set data to test (75 images), the last 25% is predicted wrong.When I set to 100 epochs, the results look like this:Sometimes the validation loss changes but not so far that the result that was shown and is very far to the training validation.", "type": "commented", "related_issue": null}, {"user_name": "dabasajay", "datetime": "Jul 7, 2019", "body": "If possible, can you provide the source code? Also, did you try to shuffle the dataset?", "type": "commented", "related_issue": null}, {"user_name": "dunfrey", "datetime": "Jul 7, 2019", "body": "Yes. The network is PoseNet, that is basically a GoogLeNet without the intermediate outputs and with two outputs on the end. The objective is use a image as input and predict the position of the camera (it is the label). Attahed the NN:The problem occours with the output \nThis values espected have the range of:\n[[ -4.26460441668e-05 , 10.4308511841 ] , [ -9.26670750683e-06 , 3.92646981481 ]]To train I'm using:To image preprocesses I just resizing the image:The accuracy in training is changing as weel the loss too, but in validation the accuracy is the same every time and the validation changes but is too high yet:\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Aug 2, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/15993", "issue_status": " Closed\n", "issue_list": [{"user_name": "softwareTR-Can", "datetime": "Feb 1, 2022", "body": "Exception in Tkinter callback\nTraceback (most recent call last):\nFile \"C:\\python\\lib\\tkinter__.py\", line 1921, in \nreturn self.func(*args)\nFile \"C:\\Users\\umutc\\PycharmProjects\\pythonProject2\\main.py\", line 178, in \nclassify_b = Button(top, text=\"Görseli Sınıflandır\", command=lambda: classify(file_path), padx=10, pady=10)\nFile \"C:\\Users\\umutc\\PycharmProjects\\pythonProject2\\main.py\", line 167, in classify\npred = model.predict_classes([image])[0]\nAttributeError: 'Sequential' object has no attribute 'predict_classes'", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Feb 2, 2022", "body": " Can you please share a simple standalone code to reproduce the issue? I tried running your code but see different error (may be due to some formatting issue). Please either update the attached colab gist or share a *.py file with the code.  is a gist for reference. Thanks was deprecated. Please use the following approach to get the class index.Please check the  for an example with  data. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Feb 9, 2022", "body": "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Feb 16, 2022", "body": "Closing as stale. Please reopen if you'd like to work on this further.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Feb 16, 2022", "body": "Are you satisfied with the resolution of your issue?\n\n", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Feb 2, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Feb 2, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Feb 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Feb 16, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Feb 19, 2022", "body": [], "type": "issue", "related_issue": "#16089"}]},
{"issue_url": "https://github.com/keras-team/keras/issues/14386", "issue_status": " Closed\n", "issue_list": [{"user_name": "ewwll", "datetime": "Jan 21, 2021", "body": "I now compress a model, but why is the compressed model bigger than before? I use the official tutorial example\nclass Distiller(keras.Model):\ndef (self, student, teacher):\nsuper(Distiller, self).()\nself.teacher = teacher\nself.student = studentif forwhat_num==0:\nteacher=load_model(\"./model_bak/letter_acc93_epoch8.h5\")\nelif forwhat_num==1:\nteacher=load_model(\"./model_bak/weight-number-07-0.95.h5\")\nelse:\nteacher=load_model(\"./model_bak/symbol_acc96_epoch15.h5\")student = Sequential()\nstudent.add(Flatten(input_shape=(480,640,1)))\nstudent.add(Dense(128, activation='relu'))\nstudent.add(Dense(len(type_name), activation='softmax'))student_scratch = keras.models.clone_model(student)distiller = Distiller(student=student, teacher=teacher)\ndistiller.compile(\noptimizer=keras.optimizers.Adam(),\nmetrics=[keras.metrics.SparseCategoricalAccuracy()],\nstudent_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\ndistillation_loss_fn=keras.losses.KLDivergence(),\nalpha=0.1,\ntemperature=10,\n)checkpoint_path = \"./weight-distiller-\"+forwhat[forwhat_num]+\".h5\"\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=False, monitor='val_acc', save_best_only=True, verbose=1)callbacks_list = [cp_callback]distiller.fit(imgall, label, epochs=10,validation_data=(testimgall,testlabel),callbacks=callbacks_list)\ndistiller.evaluate(testimgall, testlabel)\nstudent.save(\"my_model.h5\")\nstudent.save_weights(\"my_wight.h5\")\ndistiller.save_Model()\ndistiller.save_Weight()\ndistiller.save_weights(\"d.h5\")tf.saved_model.save(distiller,\"distiller_{}_graph\".format(forwhat[forwhat_num]))\npres=distiller(testimgall,training=False)\nout=[]\nm=np.size(pres, axis=0)\npres=pres.tolist()\nprint(pres)for i in range(m):\nitem=pres[i]\nindex=item.index(np.max(pres))\nout.append(index)\nprint(out)\nWhy is the compressed model bigger than before?", "type": "commented", "related_issue": null}, {"user_name": "ariG23498", "datetime": "Mar 3, 2021", "body": "Hey  could you share a colab instead of the what you have given here! Right now it is not readable or executable. After you share the code, I can reproduce and check for any issues.", "type": "commented", "related_issue": null}, {"user_name": "ewwll", "datetime": "Mar 16, 2021", "body": "#!/usr/bin/env python3import sys\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\nfrom keras import applications\ntrainname=os.listdir(\"./camera/image/\")\ntestname=os.listdir(\"./camera/label/\")\nprint(trainname)\nprint(len(trainname))\nprint(testname)\nprint(len(testname))\ntrain=[]\ntest=[]\ntrain_label=[]\ntest_label=[]\nn=0\nfor i in trainname:\nif(n<200):\nimg=cv2.imread(\"./camera/image/\"+i)\nimg=cv2.resize(img,(252,189))\nimg=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\nimg=img/255\ntrain.append(img)\nj=0\nnum=[]\nwhile(j<6):\nimg1=cv2.imread(\"./camera/label/\"+str(n)+''+str(j)+\".jpg\")\nimg1=cv2.resize(img1,(252,180))\nimg1=cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY)\nimg1=img1/255\nimg1=img1.transpose()\nnum.append(img1)\nj=j+1\nprint(len(num))\nnum=np.array(num)\nnum=num.transpose()\nprint(num.shape)\ntest_label.append(num)\nn=n+1\nplt.imshow(train[0])\nplt.show()\nplt.imshow(test[0])\nplt.show()\nprint(train_label[0].shape)\nproject_name = \"fcn_segment\"\nchannels = 1\nstd_shape = (189, 252, channels)\nmodel = Sequential(name = project_name)model.add(applications.VGG19(input_shape=std_shape,include_top=None,weights=None))\nmodel.add(BatchNormalization())\nmodel.add(UpSampling2D(size = (6, 6), interpolation = \"nearest\", name = \"upsamping_6\"))\nmodel.add(Conv2D(3, kernel_size = (5, 5), activation = \"sigmoid\",padding = \"same\", name = \"conv_6\"))\nmodel.add(UpSampling2D(size = (6, 6), interpolation = \"nearest\", name = \"upsamping_7\"))\nmodel.add(Conv2D(6, kernel_size = (3, 3), activation = \"sigmoid\",padding = \"same\", name = \"conv_7\"))\nmodel.compile(optimizer = \"adam\",loss = \"mean_squared_error\",metrics = [\"accuracy\"])\nmodel.summary()\ntrain=np.array(train)\ntest=np.array(test)\ntrain_label=np.array(train_label)\ntest_label=np.array(test_label)\nprint(train.shape)\nprint(test.shape)\nprint(train_label.shape)\nprint(test_label.shape)\ntrain=np.expand_dims(train,axis=3)\ntest=np.expand_dims(test,axis=3)\nprint(train.shape)\nprint(test.shape)\nfilepath = \"best_weights.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=0, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\nhistory = model.fit(train, train_label,batch_size =6,epochs = 40, shuffle=True,verbose = 1,validation_data=(test,test_label),callbacks = callbacks_list)\nacc = history.history['accuracy']\nloss = history.history['loss']\nepochs = range(1, len(acc) + 1)\nplt.title('Accuracy and Loss')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, loss, 'blue', label='Validation loss')\nplt.legend()\nplt.show()", "type": "commented", "related_issue": null}, {"user_name": "gadagashwini", "datetime": "Apr 21, 2022", "body": ",\nCreate 3x smaller models from pruning, you can use pruning method to compress the model.\nFor more information read ", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Apr 28, 2022", "body": "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "May 5, 2022", "body": "Closing as stale. Please reopen if you'd like to work on this further.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "May 5, 2022", "body": "Are you satisfied with the resolution of your issue?\n\n", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Jan 21, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "saikumarchalla", "datetime": "Feb 4, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "sushreebarsa", "datetime": "Nov 18, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "gadagashwini", "datetime": "Apr 21, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "gadagashwini", "datetime": "Apr 21, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sushreebarsa", "datetime": "Apr 21, 2022", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Apr 28, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "May 5, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/76", "issue_status": " Closed\n", "issue_list": [{"user_name": "Gavin-Huang", "datetime": "Apr 7, 2017", "body": "when open label xml file console print error\" AttributeError: \"LabelFile \" object has no attribute 'load'\"\nxml detail\n\nwin7 64 bit\nlabelImg v1.2.2", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "May 18, 2017", "body": "It is fixed in the latest release", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "May 18, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/14293", "issue_status": " Closed\n", "issue_list": [{"user_name": "bgyooPtr", "datetime": "Dec 8, 2020", "body": "I made a generator by inheriting the sequence class.\nI know that class overrides the  function to return data.\nMy model is not a classifier, and when I load the data I load the input and ground truth as well as some parameters needed to calculate the loss.So, the return values of , X and y, have the following shape.\n is a numpy object and its shape is (batch_size, 120, 120, 6).\n has 4 kinds of data including ground truth. (gt, a,b,c) -> {gt:(3,3,1), a:(3,3,1), b:(3,1), c:(4,4)}\nTherefore, it is composed of a list.\nThe length of the list is batch_size.After that, when I ran fit_generator, the following error occurred.What did I do wrong and how can I fix it?", "type": "commented", "related_issue": null}, {"user_name": "daniloguimaraes71", "datetime": "Dec 14, 2020", "body": "can you share your code so I can reproduce it?how are you outputting x and y?\nhave you tried using:yield X, {'gt': gt, 'a':a, 'b':b, 'c':c}this shape works for me.", "type": "commented", "related_issue": null}, {"user_name": "bgyooPtr", "datetime": "Dec 15, 2020", "body": " Please refer to the sample code below.\nWhat I've found is that I can't configure multiple outputs.\nThis is because it can be confused with (input, targets, sample_weight) when calling the generator's  in the fit function.\nBut I don't know how to reference the parameters P1 and P2 in the train_step function.", "type": "commented", "related_issue": null}, {"user_name": "sushreebarsa", "datetime": "Jul 13, 2021", "body": " Moving this issue to closed status as there has been no recent activity.In case you still face the error please create a new issue,we will get you the right help.Thanks!", "type": "commented", "related_issue": null}, {"user_name": "saikumarchalla", "datetime": "Dec 15, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sushreebarsa", "datetime": "Jul 13, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "sushreebarsa", "datetime": "Jul 13, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/198", "issue_status": " Closed\n", "issue_list": [{"user_name": "Pitt64", "datetime": "Nov 11, 2017", "body": "Could you please add \"rotate image\" feature. I'm not sure if the application takes into account the exif metadata of the image; in my case the image is 480x640 and is always rotated to landscape format 640x480 to the left after loading.", "type": "commented", "related_issue": null}, {"user_name": "Adnation", "datetime": "Feb 22, 2018", "body": "I am also facing similar issue, my images are also rotated automatically and hence It gives wrong annotation coordinates.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 30, 2018", "body": "Weird, I haven't experienced this issue myself. Do you mind posting an example for us to test w/?", "type": "commented", "related_issue": null}, {"user_name": "Pitt64", "datetime": "May 30, 2018", "body": "Here are some examples of the problem:\nI managed to find a solution to put the images right by modifying the exif metadata with this command: \nThis command will change the exif metadata of all jpg files in the current directory to turn them 90° right.", "type": "commented", "related_issue": null}, {"user_name": "ArvinCY", "datetime": "Jun 17, 2018", "body": "I also realized this issue, which makes my trained model incorrect. Here is my example of this issue ,the two images are the same actually.\n", "type": "commented", "related_issue": null}, {"user_name": "ArvinCY", "datetime": "Jun 17, 2018", "body": " hi，did  you take these pictures with your phone？ I take my photos with my iphone6 and I find that if I upload the photos with data cable，the image in labelImg always takes the left side of my phone as its bottom. But when I upload photos with wechat(a chinese social software), there is no rotation when I use labelImg. I just wonder if your situation is the same as mine", "type": "commented", "related_issue": null}, {"user_name": "humandotlearning", "datetime": "Aug 17, 2018", "body": "When a picture is taken with some camera, it is recorded as a landscape direction picture and it is specified in the EXIF file of the picture that it was a portrait direction picture.\nlabelImg isn't handling the rotation needed through that EXIF file.You have to rotate and save the pictures before using them in labelImg.You can useOn Linux :\nexiftran -ai *.jpeg\nexiftran\nOn Windows:\nJPEG Autorotate", "type": "commented", "related_issue": null}, {"user_name": "Sasha1296", "datetime": "Dec 22, 2018", "body": "I have a similar issue. I decided to go through every image and right click and rotate either left or right depending on what that image needed until all images were orientated the correct way, however this didn't seem to work and labelImg is still showing them in the old rotations. I even restarted my computer just to be sure the orientation wasn't getting updated or something like that but no, when I view the images as \"extra large icons\" they are rotated in the correct fashion, but when I re-open labelImg it is still showing the wrong orientations. I have almost a thousand images, some of them need a clockwise rotation, others need a counterclockwise rotation so I can't rotate them all in one direction with one command. I have no idea why manually rotating images in the file didn't work", "type": "commented", "related_issue": null}, {"user_name": "Sasha1296", "datetime": "Dec 26, 2018", "body": "update, I seem to have found something that sorta works just by combining bits and pieces of code I found everywhere(assuming you already manually went through and rotated each image). For some reason it gives errors for some images, but for now the try statement just ignores those, I'll try to figure out why later:`import os\nimport glob\nimport PIL\nimport sys\nfrom PIL import Image, ExifTags\norig_dir = \"C:\\Users\\path_to_origional_images\"\nconv_dir = \"C:\\Users\\path_to_new_folder_to_store_images\\\" # make sure to have \\ at the end\n#for some reason when I post the comment the code only has 1 slash when it should have two\nphotos = glob.glob(orig_dir + '/*.jpg')\ntotal_pictures = len(photos)\nprint(\"total number of photos is: \" + str(total_pictures))\nk = 0\nfor photo in photos:\ntry:\nk = k+1\nprint(photo)\n# still check if the photo is there. it may have moved\nif not os.path.isfile(photo):\ncontinue\nim = Image.open(photo)\nfor orientation in ExifTags.TAGS.keys():\nif ExifTags.TAGS[orientation] == \"Orientation\":\nbreak\nexif=dict(im._getexif().items())\nif exif[orientation] == 3:\nim=im.rotate(180, expand=True)\nelif exif[orientation] == 6:\nim=im.rotate(270, expand=True)\nelif exif[orientation] == 8:\nim=im.rotate(90, expand=True)", "type": "commented", "related_issue": null}, {"user_name": "stephenramos", "datetime": "Jan 3, 2019", "body": "I opened each image in MS Paint, saved it, and closed it. That seemed to properly orient the image in labelimg", "type": "commented", "related_issue": null}, {"user_name": "mcorange1997", "datetime": "Jan 26, 2019", "body": "A good solution was given by  above and it perfectly solves the rotation problem in LabelImg. For more detailed info, you can refer to the two links about how to access ExifTags and rotate photos.\n\n", "type": "commented", "related_issue": null}, {"user_name": "zjZSTU", "datetime": "Jul 10, 2020", "body": "I meet the same problem, the picture changed when i open it in labelImg. beside this issue, i find another closed issue\nthe key of this problem is because the jpeg image have the metadata , some camera equipment will save the direction sensor's info and some tools will use this meta. obviously labelImg do itone solution is mentioned above  , use exiftran to change itanother solution is to change the jpeg image to png before you anno itby the way, i tried several tools to open the jpeg image , some use it and some do not use it. for example, labelImg and vscode parse the exif meta; but opencv (cv2.imread) and ubuntu's image tool do not parse", "type": "commented", "related_issue": null}, {"user_name": "sannykhan3777", "datetime": "Sep 21, 2020", "body": " , I found what was the problem !\nWhen a picture is taken with some camera, it is recorded as a landscape direction picture and it is specified in the EXIF file of the picture that it was a portrait direction picture.\nlabelImg isn't handling the rotation needed through that EXIF file.You have to rotate and save the pictures before using them in labelImg.\nFollowing are some links to rotate pics to portrait.You can useOn Linux :\n\nOn Windows:\n", "type": "commented", "related_issue": null}, {"user_name": "HridoyHazard", "datetime": "Jun 17, 2022", "body": "how did you use that command ? mine not working from gitbash on that folder", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Nov 17, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "keides2", "datetime": "Oct 12, 2018", "body": [], "type": "issue", "related_issue": "#376"}, {"user_name": "AnhPC03", "datetime": "Aug 19, 2020", "body": [], "type": "issue", "related_issue": "#633"}, {"user_name": "euihyun-lee", "datetime": "Sep 23, 2020", "body": [], "type": "pull", "related_issue": "#650"}, {"user_name": "tzutalin", "datetime": "Sep 26, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lllukehuang", "datetime": "Jan 12, 2021", "body": [], "type": "issue", "related_issue": "cgvict/roLabelImg#18"}]},
{"issue_url": "https://github.com/keras-team/keras/issues/12196", "issue_status": " Closed\n", "issue_list": [{"user_name": "flyingduck92", "datetime": "Feb 3, 2019", "body": "I downloaded Keras weight VGG16 (vgg16_weights_tf_dim_ordering_tf_kernels.h5) from here: The training is working, using this code:\nOn the image database I got 10 Images faces from P1 to P10, but\nWhen I tested with OpenCV i got wrong prediction anybody know what wrong with my code?\nWhen I scan P10 picture i got constant value of P8", "type": "commented", "related_issue": null}, {"user_name": "serengil", "datetime": "Feb 21, 2019", "body": "You feed pictures of 10 people and design the network consisting of 10 outputs. This means that you applied face recognition instead of face verification. Similarity check is the task of face verification.In this case, you defined the problem as regular classification problem. Loss seems to be decreasing. This means that everything goes well. You should not check the similarities here. You can just check the model output.w_dist variable stores the distribution for P1 to P10. The maximum index states the recognized face.This works for a small set of faces. You need to re-design the network when the number of people corpus increased (Output layer consists of 10 nodes for this case, this should be increased).Face recognition applications are mostly handled by face verification tasks in production. You represent images in a reduced vector space. For example, VGG-Face has 2622 output nodes. You feed two different photos of a same person and check the similarity for 2622 dimensional vector. You should find out \"siamese network\", \"one shot learning\" and \"face verification\" concepts.", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 3, 2019", "body": "Is this resolved? If not please fill issue template.\nPlease fill the issue . Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. . Thanks!", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 8, 2019", "body": "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "gabrieldemarmiesse", "datetime": "Feb 3, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 3, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 8, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/11911", "issue_status": " Closed\n", "issue_list": [{"user_name": "wassimgoodar", "datetime": "Dec 20, 2018", "body": "`import tensorflow as tf#import for threading\nimport threading\nimport queue\n#import for PIR\nimport time\nimport RPi.GPIO as GPIO\nimport os\n#import for camera\nfrom picamera import PiCamera\nfrom PIL import Image\n#import function objectDetect function from singularObjectDetection\nfrom imageai.Detection import ObjectDetection\nfrom singularObjectDetection import objectDetect###function calling objectdetection function\ndef read():\nprint(\"read \"+ os.getcwd())\nexecution_path = os.getcwd()\ndetector = ObjectDetection()\ndetector.setModelTypeAsTinyYOLOv3()\ndetector.setModelPath(os.path.join(execution_path, \"yolo-tiny.h5\"))\ndetector.loadModel(detection_speed=\"flash\")\ncustom = detector.CustomObjects(person=True, dog=True)\nwhile True:\nobjectDetect(\"image1.jpg\")print(\"in main\")\nq=queue.Queue()\nt2=threading.Thread(target=read,daemon=True)\nt1.start()errors\n`Exception in thread Thread-1:\nTraceback (most recent call last):\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1050, in _run\nsubfeed, allow_tensor=True, allow_operation=False)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3488, in as_graph_element\nreturn self._as_graph_element_locked(obj, allow_tensor, allow_operation)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3567, in _as_graph_element_locked\nraise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\nValueError: Tensor Tensor(\"Placeholder_12:0\", shape=(64,), dtype=float32) is not an element of this graph.During handling of the above exception, another exception occurred:Traceback (most recent call last):\nFile \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\nself.run()\nFile \"/usr/lib/python3.5/threading.py\", line 862, in run\nself._target(*self._args, **self._kwargs)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/threadfinal.py\", line 57, in read\ndetector.loadModel(detection_speed=\"flash\")\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/imageai/Detection/.py\", line 213, in loadModel\nmodel.load_weights(self.modelPath)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/keras/engine/network.py\", line 1166, in load_weights\nf, self.layers, reshape=reshape)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/keras/engine/saving.py\", line 1058, in load_weights_from_hdf5_group\nK.batch_set_value(weight_value_tuples)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2470, in batch_set_value\nget_session().run(assign_ops, feed_dict=feed_dict)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 887, in run\nrun_metadata_ptr)\nFile \"/home/pi/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1053, in _run\n'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder_12:0\", shape=(64,), dtype=float32) is not an element of this graph`", "type": "commented", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Dec 20, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Dec 20, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/11599", "issue_status": " Closed\n", "issue_list": [{"user_name": "ShiroDXD", "datetime": "Nov 7, 2018", "body": "I fine tuned Mobilenets and attempted to classify images to a live webcam feed using OpenCV but I cant seem to find a way to get predictions. How can I also get the labels from the retrain process when there is no option in keras to save .pbtxtI tried loading a fine tuned model saved as a .h5 file to this script but it just kept loading in Jupyter Notebook. What is the correct argument to use so I can deploy the model on a real time camera feed?Heres my code:`from keras.preprocessing import image\nfrom keras import load_model\nimport argparse\nimport cv2\nimport numpy as np\nimport os\nimport random\nimport sysimport threadinglabel = ''frame = Noneclass MyThread(threading.Thread):\ndef (self):\nthreading.Thread.(self)def run(self):\nglobal labeldef predict(self, frame):\nimage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)\nimage = image.transpose((2, 0, 1))\nimage = image.reshape((1,) + image.shape)cap = cv2.VideoCapture(0)if (cap.isOpened()):\nprint(\"Camera OK\")\nelse:\ncap.open()keras_thread = MyThread()keras_thread.start()while (True):\nret, original = cap.read()frame = cv2.resize(original, (224, 224))cv2.putText(original, \"Label: {}\".format(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\ncv2.imshow(\"Classification\", original)if (cv2.waitKey(1) & 0xFF == ord('q')):\nbreak;cap.release()\nframe = None\ncv2.destroyAllWindows()\nsys.exit()`", "type": "commented", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 12, 2018", "body": "  -  Hi, does this problem exist when tf.keras is used ?", "type": "commented", "related_issue": null}, {"user_name": "ShiroDXD", "datetime": "Nov 13, 2018", "body": " I haven't tried it on tf.keras though but can you refer me to any blogs or website that has successfully applied a fine-tuned model. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 13, 2018", "body": "  - Sure. Please take a look into the below links which help you to fine-tune a model.\n", "type": "commented", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 28, 2018", "body": "  Any update on this please ?", "type": "commented", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Dec 17, 2018", "body": "Closing this as it is in \"awaiting response\" status for more than a week. Feel free to repost your comments and we will reopen. Thanks !", "type": "commented", "related_issue": null}, {"user_name": "gabrieldemarmiesse", "datetime": "Nov 7, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 12, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 12, 2018", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Dec 17, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/9162", "issue_status": " Closed\n", "issue_list": [{"user_name": "ayush1999", "datetime": "Jan 23, 2018", "body": "Running:and then(running it in the same directory):Gives:Can someone tell me why is this happening? Also, whenever I say  , where does this HDF5 file get stored?", "type": "commented", "related_issue": null}, {"user_name": "anj-s", "datetime": "Jan 23, 2018", "body": "I am unable to reproduce this issue. The file should be saved in the same directory from where you are running your model(since you are not specifying a path). Can you specify an explicit path to save your model such as \"/tmp/modd.h5\"? You can then verify the existence of the file, the right permissions for access and load the model if the previous two checks pass.", "type": "commented", "related_issue": null}, {"user_name": "Dref360", "datetime": "Jan 23, 2018", "body": "From your output :\n\nYou do not provide the same name", "type": "commented", "related_issue": null}, {"user_name": "ayush1999", "datetime": "Jan 24, 2018", "body": " Whenever I save a model in the current directory, I should be able to see a modd.hdf5 file right? But no such file is created.", "type": "commented", "related_issue": null}, {"user_name": "ayush1999", "datetime": "Jan 24, 2018", "body": " I'm saving the model as modd.hdf5 and then loading the same model. Why is this leading to a model not found error?", "type": "commented", "related_issue": null}, {"user_name": "AndreyGurevich", "datetime": "Jan 31, 2018", "body": "Hi all!I had issue with load_model() when path was containing tilda symbol: ~Path with tilda, error:\nCheck the path, use path withour tilda. No error:\n", "type": "commented", "related_issue": null}, {"user_name": "Jeayea", "datetime": "Dec 12, 2018", "body": "In my case, I  just wrote a wrong path name.", "type": "commented", "related_issue": null}, {"user_name": "akshay-kap", "datetime": "Jul 9, 2019", "body": "Hi, I am facing the same issue, it would be of great help if someone can explain it clearly", "type": "commented", "related_issue": null}, {"user_name": "ayush1999", "datetime": "Jun 10, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/8294", "issue_status": " Closed\n", "issue_list": [{"user_name": "RohanSaphal", "datetime": "Oct 29, 2017", "body": "Traceback (most recent call last):\nFile \"/home/rohan/PycharmProjects/Comma.ai/train_steering_model.py\", line 68, in \nmodel.load_weights('/home/rohan/PycharmProjects/Comma.ai/outputs/steering_model/steering_angle.keras')\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 727, in load_weights\ntopology.load_weights_from_hdf5_group(f, layers)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 3008, in load_weights_from_hdf5_group\nK.batch_set_value(weight_value_tuples)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2184, in batch_set_value\nassign_op = x.assign(assign_placeholder)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 512, in assign\nreturn state_ops.assign(self._variable, value, use_locking=use_locking)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\nvalidate_shape=validate_shape)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\nuse_locking=use_locking, name=name)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\nop_def=op_def)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2338, in create_op\nset_shapes_for_outputs(ret)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1719, in set_shapes_for_outputs\nshapes = shape_func(op)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1669, in call_with_requiring\nreturn call_cpp_shape_fn(op, require_shape_fn=True)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\ndebug_python_shape_fn, require_shape_fn)\nFile \"/home/rohan/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\nraise ValueError(err.message)\nValueError: Dimension 2 in both shapes must be equal, but are 320 and 16 for 'Assign' (op: 'Assign') with input shapes: [8,8,320,16], [8,8,16,3].`\ndef get_model(time_len=1):\nch, row, col = 3, 160, 320  # camera formatmodel = Sequential()model.add(Lambda(lambda x: x/127.5 - 1.,\ninput_shape=(ch, row, col),\noutput_shape=(ch, row, col)))\nmodel.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\nmodel.add(ELU())\nmodel.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\nmodel.add(ELU())\nmodel.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\nmodel.add(Flatten())\nmodel.add(Dropout(.2))\nmodel.add(ELU())\nmodel.add(Dense(512))\nmodel.add(Dropout(.5))\nmodel.add(ELU())\nmodel.add(Dense(1))model.compile(optimizer=\"adam\", loss=\"mse\")return modelif  == \"\":\nparser = argparse.ArgumentParser(description='Steering angle model trainer')\nparser.add_argument('--host', type=str, default=\"localhost\", help='Data server ip address.')\nparser.add_argument('--port', type=int, default=5557, help='Port of server.')\nparser.add_argument('--val_port', type=int, default=5556, help='Port of server for validation dataset.')\nparser.add_argument('--batch', type=int, default=64, help='Batch size.')\nparser.add_argument('--epoch', type=int, default=20, help='Number of epochs.')\nparser.add_argument('--epochsize', type=int, default=10000, help='How many frames per epoch.')\nparser.add_argument('--skipvalidate', dest='skipvalidate', action='store_true', help='Multiple path output.')\nparser.set_defaults(skipvalidate=False)\nparser.set_defaults(loadweights=False)\nargs = parser.parse_args()model = get_model()\nmodel.fit_generator(\ngen(20, args.host, port=args.port),\nsamples_per_epoch=1000,\nnb_epoch=args.epoch,\nvalidation_data=gen(20, args.host, port=args.val_port),\nnb_val_samples=1000\n)if not os.path.exists(\"./outputs/steering_model\"):\nos.makedirs(\"./outputs/steering_model\")with open('./outputs/steering_model/steering_angle.json', 'w') as outfile:\njson.dump(model.to_json(), outfile)`", "type": "commented", "related_issue": null}, {"user_name": "yuuuumi", "datetime": "Dec 9, 2017", "body": "hi, I can the same error when I tried to load the weights. Did you solve this problem?", "type": "commented", "related_issue": null}, {"user_name": "adrian-botta", "datetime": "Feb 22, 2018", "body": "Hey 2 things worth trying:", "type": "commented", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/6987", "issue_status": " Closed\n", "issue_list": [{"user_name": "tejareddy-b", "datetime": "Jun 14, 2017", "body": "", "type": "commented", "related_issue": null}, {"user_name": "tejareddy-b", "datetime": "Jun 17, 2017", "body": "", "type": "commented", "related_issue": null}, {"user_name": "SreehariRamMohan", "datetime": "Aug 12, 2017", "body": "I am having the same issue. Can you explain what you removed and why it was causing an error?", "type": "commented", "related_issue": null}, {"user_name": "ashleylid", "datetime": "Oct 31, 2017", "body": "I would like an answer to this too - though it doesn't work for me. If I remove the scaling the prediction gets worse.", "type": "commented", "related_issue": null}, {"user_name": "tejareddy-b", "datetime": "Jun 17, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/6111", "issue_status": " Closed\n", "issue_list": [{"user_name": "kootenpv", "datetime": "Apr 2, 2017", "body": "It appears there is no way to alter exposure / contrast with the image generator? Is there a reason this is not included? It would seem very useful to be part of generation of artificial images.", "type": "commented", "related_issue": null}, {"user_name": "sneakers-the-rat", "datetime": "Apr 4, 2017", "body": "I suppose the reason might be that that level of image manipulation would be out of the scope of keras. Did you mean randomly adjust exposure/contrast for the sake of data augmentation? If so, both exposure and contrast are linear adjustments and don't add any new information: exposure is just a scale change so your (assumedly convolutional) network shouldn't care, and increases/decreases in contrast should wash out to neutral during training. If anything you'll be decreasing the information in the image through saturation/rounding errors. The  random jittering values across channels already implemented instead actually increases the information in the dataset since the relationship between the channels is changed (recall that a filter in a conv. network is actually LxWxC, or a 3 '2D kernels' per 'filter'). If you still want to randomly expose/contrast your images, you can pass a function that takes a 3-dimensional numpy array to preprocessing_function, or define a function to do so in the preprocessing/image.py file and add it to the ImagePreprocessing class and the relevant .next() call in the iterator classes,edit: I suppose it would be good to add the preprocessing_function argument to the docs, I don't see it mentioned there, just in the source.If you just mean a global adjustment for the sake of  the dataset rather than augmenting it, you can do that:\nYou can adjust 'exposure' (although that's not really a well-defined term in digital images) in a way that's similar but not exactly the same as photoshop using the 'rescale' argument. Digital exposure adjustment is essentially a multiplying channel intensities (in RGB space) or the value (in HSV space) by a constant - higher starting values grow/shrink by more than lower values, as opposed to a brightness adjustment which adjusts all values by the same amount. Photoshop does a bit of interpolation but that's about all that's different afaik.Contrast is also a linear change, essentially doing the same thing as exposure adjustment except you subtract the median pixel value before doing the multiplication and then add it back in afterwards (see here: ). You can do this with a keras function (and actually just scaling it should be all the same to the network if you're centering/scaling the data) not sure why you would want to do that unless the light/dark values are intrinsically meaningful though.", "type": "commented", "related_issue": null}, {"user_name": "kootenpv", "datetime": "Apr 5, 2017", "body": "Thank you so much for such an extensive answer @crimedude22.I do believe that we will \"throw away\" data when do over-exposure/over-contrast, so in fact the network would become more robust when it learns when the distribution between variables changes (at 255, some pixels cannot go higher, so they are capped, while other pixels can still increase)?I'm mostly interested in it since I assume my data to sometimes be pretty dark, sometimes not, and I hoped that creating more (artificial) data would increase the robustness. You think this would not hold as it is a linear transformation?", "type": "commented", "related_issue": null}, {"user_name": "kootenpv", "datetime": "Apr 6, 2017", "body": "Okay, I experimented with the transformations, and now have a better understanding.Since there is a cap of 255, the transformation for increase cannot be a constant.E.g. you can divide the values by a constant (e.g. 2) and it will always still be between 0-255.\nWhen you multiply, you cannot multiply by 2, because they would go outside the possible values (clipping is not the right way).Instead, the formula should be:Increasing values:Decreasing values:This value 2 should instead be randomly sampled per image I believe.It does not really seem like a purely linear transformation?But as I understand convolution, it will be pretty much like weights learned for a sliding window . In this regard, I'm very confused.MaxPooling will be weird when just simply randomly one image has more intense pixels and will thus be picked up by maxpooling? It seems maxpooling wouldn't really work with images with different intensities?I really hope that by random augmentations the network would be more robust to the brightness received by a camera, but you're (@crimedude22) saying this probably won't help?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 5, 2017", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "type": "commented", "related_issue": null}, {"user_name": "kootenpv", "datetime": "Apr 2, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 5, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Aug 12, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/12400", "issue_status": " Closed\n", "issue_list": [{"user_name": "twinanda", "datetime": "Mar 5, 2019", "body": "I am currently on Keras 2.2.4 and Tensorflow 1.12.0. This issue was also observed on Keras 2.1.6 with TF 1.8.0.So I have a UNet with batchnorm trained on my dataset. After done training, I use the model to predict segmentation output from unseen images.Sometimes I pass multiple images at a time, but sometimes it could be just one image. I notice that the segmentation output of image A differs between two cases: (1) if I pass image A with other images; and (2) if I pass only image A.With other images:\nOn its own:\nIt might not be too obvious here, but the values of the pixels are different. Also, please excuse the performance of the network. It was trained only on a few images with a few iterations, but this is not a problem. The inconsistency is observed on well-trained networks too.Here are some other things that I have experimented on:\n(1) Removing batch normalization from the network remedies this issue. The segmentation output is consistent from both scenarios. So I think I can safely say that the source of the issue is the BatchNorm layer. However, not using BtachNorm is not an option.\n(2) I have also tried to set  for all layers in my inference_model, to no avail.\n(3) Also tried to set  to all BatchNorm layers in inference_model, still no avail.\n(4) Setting  when calling the BatchNorm layers in  makes the network gives all 1.0 or 0.0 output.If anybody could give me an idea of how to solve this problem, it would be much much appreciated. This issue is really annoying because it makes evaluation and putting the model into production very difficult.", "type": "commented", "related_issue": null}, {"user_name": "jmgo", "datetime": "Mar 22, 2019", "body": "I have a similar issue. When I use predict or evaluate with different batch_sizes I get different results.  ( e.g.,  using predict  for batch_size =16 I got dice of 0.88 and for batch_size = 32 I got a dice of 0.51).Also, I noticed that the behaviour between evaluate and predict give distinct results, (e.g. for batch_size = 32,  predict gives dice of 0.51 and evaluate gives 0.35). I would expect some minor difference due to truncation errors but not in this range.Another issue is that if I run predict or evaluate more than one time on the same data I get slightly different results.Without BatchNormalization these beahviours do not happen (apart from slight difference between evaluate and predict), so it must be related to BatchNorm.", "type": "commented", "related_issue": null}, {"user_name": "harshitAgr", "datetime": "Mar 22, 2019", "body": "With what batch size did you train the model?", "type": "commented", "related_issue": null}, {"user_name": "jmgo", "datetime": "Mar 25, 2019", "body": "I trained the model with a batch_size of 20.I written an simple example to replicate the issue (batch_size of 40), the full code is attached in\n (I am using Keras 2.2.0)I tested with  , a UNET-like architecture with BatchNorm and  which is the same network without BatchNorm.The output with  wasand the output with   was:In this code the differences are small, but I have a more complex network that the differences are larger (0.1 - 0.5 in accuracy on similar dummy data)If anyone could help me out that would be great!", "type": "commented", "related_issue": null}, {"user_name": "twinanda", "datetime": "Apr 18, 2019", "body": "Any updates on this? I am surprised how this inconsistency does not bother too many people. Or is this inconsistency caused by BatchNorm is something that is inherent in BatchNorm itself? I wonder whether other frameworks show this behavior too.", "type": "commented", "related_issue": null}, {"user_name": "thekoshkina", "datetime": "May 8, 2019", "body": "any updates on this?", "type": "commented", "related_issue": null}, {"user_name": "rajasrijan", "datetime": "May 18, 2019", "body": "I'm having same issue along with  which seems to be keras bug rather than tf. Is there any keras version where this is working? Need it asap.", "type": "commented", "related_issue": null}, {"user_name": "twinanda", "datetime": "May 21, 2019", "body": "Still no updates from my side. This issue is very pronounced when you are doing segmentation on an image in which the interesting structure is only a small part of the image. It seems like it is trying to normalize the output, which results in a high false positive rate.", "type": "commented", "related_issue": null}, {"user_name": "aeweiwi", "datetime": "May 28, 2019", "body": "I am also haveing the same issue here :(", "type": "commented", "related_issue": null}, {"user_name": "JasonLi-TMT", "datetime": "May 31, 2019", "body": "same here, accuracy never improves", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Jun 8, 2019", "body": "+1 ", "type": "commented", "related_issue": null}, {"user_name": "pokeabear", "datetime": "Jun 12, 2019", "body": "I have noticed this recently - predictions on the exact same data are very different at test time for segmentation problems.", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Jun 23, 2019", "body": "I resolved my problem by standardizing a sample (in a batch of 32) which I inadvertently left non-standardized, with  - which had severely disrupted the  layers; to suggest a debug method,  s (via e.g. heatmaps) throughout training, and see if they drastically change after any particular train iteration. Also, mind extreme outliers, and other anomalous behaviors.", "type": "commented", "related_issue": null}, {"user_name": "LukeBolly", "datetime": "Jun 25, 2019", "body": "I've run into this issue as well.Re-using an identical sample for training and validation, the model gets 100% accuracy while training but validation accuracy oscillates around 60%. Setting momentum to 0.0001 so that the \"moving mean/variance\" == \"last batch mean/variance\" did not fix the validation accuracy, so BatchNorm must be doing something else which is modifying the data at Validation time.I'm using a TimeDistributed layer which might make my case unique, but here's the issue anyway with a reproducible example just in case:\nI'm using tensorflow-gpu 1.13, Keras 2.2.4", "type": "commented", "related_issue": null}, {"user_name": "Mushoz", "datetime": "Jun 25, 2019", "body": "Exact same issue for me. Would love to hear an update regarding this.", "type": "commented", "related_issue": null}, {"user_name": "isaacgerg", "datetime": "Sep 13, 2019", "body": "I just discovered this same issue.  I looked at the closed solution but I dont think it addresses the issue.  Batch norm simply shifts and scales the data by a fixed amount derived from the exponential moving averages.  This should be fixed at test time and indepdent of the batch contents.See: ", "type": "commented", "related_issue": null}, {"user_name": "isaacgerg", "datetime": "Sep 15, 2019", "body": "All,I have created a very small test fixture which reproduces the error.  Can you verify you get the bug? , ", "type": "commented", "related_issue": null}, {"user_name": "twinanda", "datetime": "Sep 16, 2019", "body": "Yes, I can reproduce the bug with this code. The rows are not equal. I have also commented on your original thread on TF. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "isaacgerg", "datetime": "Sep 16, 2019", "body": " Thanks.  Have you noticed any patterns to your observations from yoru original reconstruction experiment?  Does it happen on the test set AND training set data?  If you move the image under test to different parts of the batch, do you have the same issue?", "type": "commented", "related_issue": null}, {"user_name": "twinanda", "datetime": "Sep 17, 2019", "body": " I have noticed this in my experiment. Let's assume a person segmentation problem. Assume image A has 1 person and image B has 5 persons; and my training data has an average of 4-6 persons in the image. I did the following scenarios:Note that image A and B are from a static camera (background is the same). So, it does not really make sense why in the second case the network segments background as people, while for image A, the network performs super well.I have not really compared the result whether I pass testing or training data, neither the position of the image in the batch.", "type": "commented", "related_issue": null}, {"user_name": "liketheflower", "datetime": "Oct 17, 2019", "body": "I found the similar problem. I though it is the batch normalization's problem as my previous batch size is too small. When I changed the batch size to 8 (it is not that big also), I still encounter the similar problem.\nThere is a significant inconsistent between training and test.\nHere you can see two experiments:\nThe red one without_BN: I do not use BN\nthe dark green on is with BN.\nThe bottom curve are the total training loss and for training both of these two curves converge.\nHowever, if we check the real time loss, which is based on loss in test mode, there is a huge difference between without_BN and with_BN. Please check the upper curves.\nI set the learning phase as 0 to make it behave in test mode.Because of this, I got a super bad performance for the test data by using batch normalization.Hope the info I provided here can help fix the problem. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "depthwise", "datetime": "Oct 30, 2019", "body": "Wow, not even setting  fixes the issue. No wonder I can't replicate my PyTorch results.", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Nov 1, 2019", "body": "I've began to look at the issue, but not yet fully investigating. Observations: 's \"all rows equal\" code does  reproduce the error in TensorFlow 2.0.0 + Keras 2.3.0/2.3.1; all rows  identical.  In TensorFlow 1.14.0 + Keras 2.2.5, however, the rows aren't identical.Let me know if this solves all parts of the issue.", "type": "commented", "related_issue": null}, {"user_name": "depthwise", "datetime": "Nov 1, 2019", "body": "Here's TF 2.0.0 CPU release version, and its bundled Keras. The bug is evident.This is with , of course.", "type": "commented", "related_issue": null}, {"user_name": "depthwise", "datetime": "Nov 1, 2019", "body": "Here's \"standalone\" Keras 2.3.1:Bug is evident as well. I'm not sure how you were testing , please share details of your set-up.", "type": "commented", "related_issue": null}, {"user_name": "depthwise", "datetime": "Nov 1, 2019", "body": "Here's the same test on the GPU version of TF 2.0:", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Nov 1, 2019", "body": "Uh... I could've sworn they were the same, but now they aren't - I either got lucky, or visual cortex misfired.Alright, I'm game - will investigate sometime.", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Nov 11, 2019", "body": "I believe I've figured it out, but no one will like the verdict, myself included: the problem is , and  has nothing to do with it. It's a work in progress, but I've narrowed down the exact operation responsible:  - you can follow the work . Will update later with all relevant testing code, and a solution. I've also made OP's images into a gif for direct comparison:(P.S. removing  in your tests may \"solve\" the problem, but it's  BN that's problematic, but how it transforms input tensor dimensionalities - will clarify later)", "type": "commented", "related_issue": null}, {"user_name": "isaacgerg", "datetime": "Nov 11, 2019", "body": "I was able to produce a similar error without using batch norm on a very simply network. ", "type": "commented", "related_issue": null}, {"user_name": "twinanda", "datetime": "Nov 15, 2019", "body": " thanks for pursuing it much further than I ever could. I have another issue which might lead to a clue for the issue, or maybe to a completely different problem.\nUnfortunatlye, I cannot send you my dataset because it is private.Now riddle me this. Imagine you want to segment a white cat in front of a black background. Your training data includes cats passing by the background and images with just the background (no cats). Simple stuff. Your network converges, and your evaluation shows you have > 0.95 dice score. You are happy.\nYou pass test images with cats on the image, and your network performs amazingly.. until you call .predict and pass a single background image (with no cats on it). Suddenly, you see that the network tries to segment the dust or any random structures on the image as a cat. The false positive is strong on this one. ", "type": "commented", "related_issue": null}, {"user_name": "OverLordGoldDragon", "datetime": "Nov 15, 2019", "body": " You're welcome. On cats - sounds like overfitting, contrary to intuition;  of your neural net. Part of ability to generalize includes  - which is often omitted from explicit testing. It's what roots \"adversarial attacks\" - in the extreme example, the \"one-pixel attack\", where a single pixel in an image is (intelligently) manipulated to trick the (well-trained) NN to think that a cat is a motorboat.... or it's a bug. Can't tell much without model code and at least dataset info (shapes, quality, noise, etc). If you'd like, I can have a look if you post a minimally-reproducible example on StackOverflow.In the meantime, I have a request specific to you, which does sort of \"leak\" the crux of my investigation: right after importing backend as , run , and rerun the exact code used to generate your original greyscale image. Let me know if the ultimate difference is nearly as dramatic.", "type": "commented", "related_issue": null}, {"user_name": "ymodak", "datetime": "Mar 6, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "twinanda", "datetime": "Mar 6, 2019", "body": [], "type": "issue", "related_issue": "#3423"}, {"user_name": "moondra2017", "datetime": "Jun 8, 2019", "body": [], "type": "issue", "related_issue": "#9522"}, {"user_name": "LukeBolly", "datetime": "Jul 4, 2019", "body": [], "type": "issue", "related_issue": "tensorflow/tensorflow#30109"}, {"user_name": "isaacgerg", "datetime": "Sep 15, 2019", "body": [], "type": "issue", "related_issue": "tensorflow/tensorflow#32544"}, {"user_name": "OverLordGoldDragon", "datetime": "Nov 15, 2019", "body": [], "type": "issue", "related_issue": "numpy/numpy#14917"}, {"user_name": "SimonZhao777", "datetime": "Mar 14, 2020", "body": [], "type": "issue", "related_issue": "#6977"}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/4385", "issue_status": " Closed\n", "issue_list": [{"user_name": "MrXu", "datetime": "Nov 15, 2016", "body": "I am trying to use 2 different models in predict the label of objects on an image captured by video camera. The code is as following:However, when I run this code, it throw the following error:My guess is that the opencv creates a new thread for the video capture and run the  in a different thread. Since Tensorflow runs in a session, the second model  throws the error.\nIf I need to run multiple models with Keras using Tensorflow backend, what should I do to make this work?Thanks a lot.Please make sure that the boxes below are checked before you submit your issue. Thank you!", "type": "commented", "related_issue": null}, {"user_name": "hengguan", "datetime": "Mar 30, 2017", "body": "i face the same problem as yours,if you have solved it,please inform me, thank you in advance", "type": "commented", "related_issue": null}, {"user_name": "George-Zhu", "datetime": "Apr 27, 2017", "body": " this issue may help", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 26, 2017", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 26, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Aug 25, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/3461", "issue_status": " Closed\n", "issue_list": [{"user_name": "BoltzmannBrain", "datetime": "Aug 13, 2016", "body": "I have a generator to pass video data frame-by-frame to a Sequential model. To train the model I'm using  :(I've added .)Below is my generator, that will accumulate 200 frames worth of data into  and  -- i.e., batch size = 200.With this setup  -- i.e., . Then for subsequent epochs,  would reinitialize the generator such that we begin training again from the start of the video. Yet this is not the case. The printed output below shows a couple oddities:If there is something incorrect in my understanding of  please explain. I've gone through the documentation, this , and these  . When I get my project running I'll be more than happy to include this as an example for running video data I'm using Keras v1.0.7 with the TensorFlow backend.", "type": "commented", "related_issue": null}, {"user_name": "BoltzmannBrain", "datetime": "Aug 13, 2016", "body": "I've also posted this on .", "type": "commented", "related_issue": null}, {"user_name": "BoltzmannBrain", "datetime": "Aug 14, 2016", "body": "As a temporary fix I'm manually iterating over the epochs and calling model.fit(), as shown in .", "type": "commented", "related_issue": null}, {"user_name": "almathie", "datetime": "Sep 6, 2016", "body": "The data generator and the fitting are run in parallel. It's important to note that the generator is just that : a generator that has no idea how the data it generates is going to be used and at what epoch. It should just keep generating data forever as needed.What happens in your log is :Everything seems to be working as intended. I suggest to close the issue.", "type": "commented", "related_issue": null}, {"user_name": "juliohm", "datetime": "Nov 16, 2017", "body": "I am having a hard time trying to decipher the hidden conventions in Keras. This is basically a problem with the Python programming language that doesn't distinguish row, column vectors, and that doesn't restrict dimensions very well. Some functions in Keras expect lists, some expect Numpy arrays, this inconsistency is quite hard to grasp.Suppose I have a feature vector of size n (a time series) and a response of size m (another time series), what the generator is expected to output?", "type": "commented", "related_issue": null}, {"user_name": "BoltzmannBrain", "datetime": "Aug 14, 2016", "body": [], "type": "issue", "related_issue": "#107"}, {"user_name": "stale", "datetime": "May 23, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 22, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/3094", "issue_status": " Closed\n", "issue_list": [{"user_name": "Fheanor", "datetime": "Jun 28, 2016", "body": "Hey !First, I am really a super beginner in Deep Learning so I might not be to use correct term, I am sorry.I built a model with 3 layers that work with pictures as datas and int as labels. It is a supervised model.\nTo load the datas, I build a very larg matrix and each line is a picture.\nThen I use the function 'fit' to train the model.Example :However, when my matrix is too big, I have a memory Error problem when I use \"np_utils.to_categorical(..).\nSo I would like to split my matrix in 2 matrix and then it might work. But I will need to call the \"fit\" function two times.\nDoes anyone have any idea if it could work or not ?Thanks a lot and have a good day", "type": "commented", "related_issue": null}, {"user_name": "Fheanor", "datetime": "Jun 28, 2016", "body": "For information :Each image is associated with an integer.I have at this time So because I need , tries to create a , that it seems it way too big.I saw that that instead of using \"fit\", you can use \"train_on_bashes\" function.\nBut the problem is that I don't know how to use it in this case. My last layer need to  because of there are 34074 differents labels.I tried  to use \"train_on_bashes\" function with 500 datas image at the same time in a loop. But my \"Y_train\" matrix has a 500x500 dimension so it can't work because my output layer need a 34074x34074 matrix...It is a bit confused to explain because english is not my first langage, so if you need any more details, I would be happy to answer you.\nAnd I am sorry if I say really stupid things, but I just start DeepLearning.Best regards.", "type": "commented", "related_issue": null}, {"user_name": "AdityaGudimella", "datetime": "Jun 28, 2016", "body": "Why would you have 34074 labels? What are you trying to achieve by using the NNet? Are you trying to classify the pictures into categories? Generally, the number of labels is much lesser than the number of data points (images) that you have. Are you sure there are 34074 labels?", "type": "commented", "related_issue": null}, {"user_name": "Fheanor", "datetime": "Jun 29, 2016", "body": "In fact, I work in gesture recognitions. And at this time, our aim is to build a 3D hand that looks like the same that the user is doing in front of the camera.\nTo do this, we give to the model lot of images of different positions of a hand and I want each image to be associated with a label.\nThis label is a number that allow me to find the parameters needed to build a 3D hand.\nIn my example, I work with 34074 different hand positions so I need to be able to generate 34074 3D hands, so I need 34074 labels...\nSo there is one category for each image.", "type": "commented", "related_issue": null}, {"user_name": "jmhessel", "datetime": "Jun 29, 2016", "body": "I think that your learning task is not well formulated, deep learning aside.If you have as many discrete labels as you have training examples, it's going to be very difficult for any model to generalize to new examples. The core of the problem is that you only have a single example for each class. There are proper ways to do \"one shot\" learning, but treating each image as it's own class is probably not one of them.In my view, you should potentially discard the supervised learning framework you have in mind and look into something unsupervised.Have you considered using an autoencoder? In short, an auto-encoder is a model that predicts its own inputs, but through an informational bottleneck. For example, if you input is 12288 dimensional, you could train a model that maps from 12288 dimensions to 200 dimensions, and then back to the 12288 dimensions. Your inputs and outputs could be the same. This will force your network to learn a 200-D compression of your input data. You can then take your new training examples, put them into your learned encoder, and find nearest-neighbors in the 200-D space.", "type": "commented", "related_issue": null}, {"user_name": "Fheanor", "datetime": "Jul 1, 2016", "body": "Thank you for your short explanation, the classification solution cannot work in my case I think.\nI never considered to use auto encoder but I will check how it work !", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 23, 2017", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 23, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 22, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/2863", "issue_status": " Closed\n", "issue_list": [{"user_name": "cemsaz", "datetime": "May 31, 2016", "body": "I have Mac OS 10.11.4. When running the examples, I receive Illegal instruction: 4 error. Is there a problem with the new releases of OS?", "type": "commented", "related_issue": null}, {"user_name": "sfilatye", "datetime": "Jul 10, 2016", "body": "I am getting the same problem when I add a dropout layer", "type": "commented", "related_issue": null}, {"user_name": "sfilatye", "datetime": "Jul 10, 2016", "body": "Here is the diagnostics I get:Process:               Python [43750]\nPath:                  /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\nIdentifier:            Python\nVersion:               2.7.12 (2.7.12)\nCode Type:             X86-64 (Native)\nParent Process:        bash [39585]\nResponsible:           Terminal [513]\nUser ID:               501Date/Time:             2016-07-10 12:03:55.469 -0400\nOS Version:            Mac OS X 10.11.5 (15F34)\nReport Version:        11\nAnonymous UUID:        A43486B1-1739-4D8E-911F-30C81C25A0E6Time Awake Since Boot: 87000 secondsSystem Integrity Protection: enabledCrashed Thread:        0  Dispatch queue: com.apple.main-threadException Type:        EXC_BAD_INSTRUCTION (SIGILL)\nException Codes:       0x0000000000000001, 0x0000000000000000Thread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   cdb0b986639740d2acf156f042fe37d2.so 0x0000000101f41d4b instantiate(, ) + 59 (mod.cpp:990)\n1   org.python.python               0x00000001000c70ba PyEval_EvalFrameEx + 35834\n2   org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n3   org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n4   org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n5   org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n6   org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n7   org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n8   org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n9   org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n10  org.python.python               0x00000001000c6736 PyEval_EvalFrameEx + 33398\n11  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n12  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n13  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n14  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n15  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n16  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n17  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n18  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n19  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n20  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n21  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n22  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n23  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n24  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n25  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n26  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n27  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n28  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n29  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n30  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n31  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n32  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n33  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n34  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n35  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n36  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n37  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n38  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n39  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n40  org.python.python               0x000000010003ef30 function_call + 176\n41  org.python.python               0x000000010000cd72 PyObject_Call + 98\n42  org.python.python               0x000000010001f52d instancemethod_call + 365\n43  org.python.python               0x000000010000cd72 PyObject_Call + 98\n44  org.python.python               0x000000010007b3da slot_tp_call + 74\n45  org.python.python               0x000000010000cd72 PyObject_Call + 98\n46  org.python.python               0x00000001000c2bc0 PyEval_EvalFrameEx + 18176\n47  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n48  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n49  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n50  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n51  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n52  org.python.python               0x00000001000c6673 PyEval_EvalFrameEx + 33203\n53  org.python.python               0x00000001000c9d23 PyEval_EvalCodeEx + 2115\n54  org.python.python               0x00000001000c9e46 PyEval_EvalCode + 54\n55  org.python.python               0x00000001000eecde PyRun_FileExFlags + 174\n56  org.python.python               0x00000001000eef7a PyRun_SimpleFileExFlags + 458\n57  org.python.python               0x000000010010614d Py_Main + 3101\n58  org.python.python               0x0000000100000f14 0x100000000 + 3860Thread 0 crashed with X86 Thread State (64-bit):\nrax: 0x000000010078e150  rbx: 0x0000000109cf35f0  rcx: 0x0000000000001e00  rdx: 0x0000000000000000\nrdi: 0x000000010025d800  rsi: 0x0000000000000040  rbp: 0x00007fff5fbfa6c0  rsp: 0x00007fff5fbfa6b0\nr8: 0x0000000000000005   r9: 0x0000000000000000  r10: 0x0000000000000006  r11: 0x0000000101f41d10\nr12: 0x0000000100306c80  r13: 0x00000001089e4440  r14: 0x0000000107990210  r15: 0x0000000107f4f3c4\nrip: 0x0000000101f41d4b  rfl: 0x0000000000010202  cr2: 0x0000000109cf1440Logical CPU:     3\nError Code:      0x00000000\nTrap Number:     6Binary Images:\n0x100000000 -        0x100000fff +org.python.python (2.7.12 - 2.7.12)  /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\n0x100003000 -        0x100175ff7 +org.python.python (2.7.12, [c] 2001-2016 Python Software Foundation. - 2.7.12) <831DC7C1-B842-23D7-69C8-73A7D5E5574C> /Library/Frameworks/Python.framework/Versions/2.7/Python\n0x1002fa000 -        0x1002fcff7 +_locale.so (???) <53986AC4-ACA1-2D91-18B0-D82D415A3A23> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_locale.so\n0x101a00000 -        0x101a05fe7 +math.so (???) <630A9AF7-CA15-F304-56AE-1BFC4D4E1B20> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/math.so\n0x101a0c000 -        0x101b5afff +multiarray.so (???) <64B8A892-1966-35A0-A230-EDDB4BD80022> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/multiarray.so\n0x101c18000 -        0x101c26ff7 +datetime.so (???) <82C1F450-2466-249B-B638-BB06FD2997BE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/datetime.so\n0x101c32000 -        0x101cb2ff7 +umath.so (???) <5D0EB613-CE78-302C-AE57-C6FF1A0289C1> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/umath.so\n0x101ced000 -        0x101cf1ff7 +_struct.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_struct.so\n0x101cf9000 -        0x101cfafff +_heapq.so (???) <71697426-5211-AEBF-F5D0-D32452547F9E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_heapq.so\n0x101e00000 -        0x101e16ff7 +_ctypes.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_ctypes.so\n0x101e27000 -        0x101e2bfff +operator.so (???) <198AB272-F92F-F09D-86DB-4DC804FB50E3> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/operator.so\n0x101e32000 -        0x101e36fff +_collections.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_collections.so\n0x101e3c000 -        0x101e43ff7 +itertools.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/itertools.so\n0x101e4e000 -        0x101e5eff7 +cPickle.so (???) <3A5B9595-9170-BC22-3557-FD54327D10C0> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/cPickle.so\n0x101e66000 -        0x101e67fff +cStringIO.so (???) <4F4158C8-40AC-BD52-5585-747EF47FA628> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/cStringIO.so\n0x101e6c000 -        0x101e6dff7 +_functools.so (???) <9D740A66-E497-C8B0-12A5-172FB6BFC1F1> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload//cdb0b986639740d2acf156f042fe37d2.so\n0x101fb1000 -        0x101fb3ff7 +time.so (???) <0D2E7145-66AD-2D3C-66E4-1B9ADC5C7A59> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/time.so\n0x101ff8000 -        0x101ff8fff +grp.so (???) <1836D3C3-CCED-5E40-5695-812612F6E84D> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/grp.so\n0x101ffb000 -        0x101ffcfff +_hashlib.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_hashlib.so\n0x102800000 -        0x102815ff7 +_io.so (???) <9FA7A71E-88D4-8909-3F82-BD0AC812C3E5> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_io.so\n0x10282d000 -        0x10282ffef +binascii.so (???) <1B2157C5-3275-D9B6-D20F-076434EFFC93> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/binascii.so\n0x102833000 -        0x102834fff +_random.so (???) <9AD51EBD-D930-95AC-DD39-A396715FD4AC> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_random.so\n0x102837000 -        0x102838ff7 +fcntl.so (???) <8034A386-5C9A-5BFB-B43D-5CDC23C97208> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/fcntl.so\n0x1028bb000 -        0x1028bcff7 +lapack_lite.so (???) <56A16A48-5EF2-31F2-A5DB-8E00954A69AD> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/linalg/lapack_lite.so\n0x1028c0000 -        0x1028d5fff +_umath_linalg.so (???) <8E325D66-16C0-3BD2-B1E6-9B9EB3F106CA> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/linalg/_umath_linalg.so\n0x1028e3000 -        0x1028e3fff +future_builtins.so (???) <1CD0DBFE-57BD-3343-72DC-DA3AE14396AE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/future_builtins.so\n0x102926000 -        0x10292eff7 +fftpack_lite.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/fft/fftpack_lite.so\n0x102972000 -        0x102a0bfff +mtrand.so (???) <33458E8F-A7DC-31CB-A131-6A5C70A35150> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/random/mtrand.so\n0x102b03000 -        0x102c3bfff +cv2.so (0) <86265899-26F0-3398-8BA0-7344308966EB> /Library/Python/2.7/site-packages/cv2.so\n0x102c90000 -        0x102d52fff +libopencv_contrib.2.4.dylib (0) <7EF87EBA-7E49-34AF-B550-0D97F8FAE2EE> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_contrib.2.4.dylib\n0x102d85000 -        0x102db1ff7 +libopencv_nonfree.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_nonfree.2.4.dylib\n0x102dc0000 -        0x102dcffff +libopencv_gpu.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_gpu.2.4.dylib\n0x102de2000 -        0x102e97ff3 +libopencv_legacy.2.4.dylib (0) <622F3281-5710-3C2A-8AE0-CFDE04B5E507> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_legacy.2.4.dylib\n0x102ecd000 -        0x102ee5ffb +libopencv_photo.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_photo.2.4.dylib\n0x102eeb000 -        0x103084ff3 +libopencv_ocl.2.4.dylib (0) <8BB297B7-E0F6-341F-8A10-F7C98E14777F> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_ocl.2.4.dylib\n0x1030b3000 -        0x10317fff7 +libopencv_calib3d.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_calib3d.2.4.dylib\n0x103195000 -        0x10320affb +libopencv_features2d.2.4.dylib (0) <16E49BAF-7359-335C-9E8C-892777FD4737> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_features2d.2.4.dylib\n0x103230000 -        0x10326cffb +libopencv_flann.2.4.dylib (0) <9E41B994-C936-3040-ADC4-81DB1B75964F> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_flann.2.4.dylib\n0x103297000 -        0x1032eefff +libopencv_ml.2.4.dylib (0) <3A6B364B-D9BF-3812-B3A9-55D83E12724E> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_ml.2.4.dylib\n0x103309000 -        0x10334aff7 +libopencv_video.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_video.2.4.dylib\n0x103355000 -        0x1033affff +libopencv_objdetect.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_objdetect.2.4.dylib\n0x1033c9000 -        0x1033f1fff +libopencv_highgui.2.4.dylib (0) <28E9B85B-00D7-3B2D-A397-E3B209C25AD0> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_highgui.2.4.dylib\n0x103408000 -        0x10359effb +libopencv_imgproc.2.4.dylib (0) <3558151F-E69A-3B3D-8152-DA13EA9FF549> /usr/local/Cellar/opencv/2.4.13/lib/libopencv_imgproc.2.4.dylib\n0x103683000 -        0x103840ff7 +libopencv_core.2.4.dylib (0)  /usr/local/Cellar/opencv/2.4.13/lib/libopencv_core.2.4.dylib\n0x103897000 -        0x1038c3ff7 +libjpeg.8.dylib (0) <5DDAB24B-59F9-3E62-9905-39876B715B6A> /usr/local/opt/jpeg/lib/libjpeg.8.dylib\n0x1038ca000 -        0x1038edffb +libpng16.16.dylib (0)  /usr/local/opt/libpng/lib/libpng16.16.dylib\n0x1038f6000 -        0x10394fff7 +libtiff.5.dylib (0)  /usr/local/opt/libtiff/lib/libtiff.5.dylib\n0x10395d000 -        0x103967fff +libImath-2_2.12.dylib (0)  /usr/local/opt/ilmbase/lib/libImath-2_2.12.dylib\n0x10396d000 -        0x103c0bfff +libIlmImf-2_2.22.dylib (0) <8A5AC913-B655-32BC-A619-A0DD4D915D4E> /usr/local/opt/openexr/lib/libIlmImf-2_2.22.dylib\n0x103c65000 -        0x103c6aff7 +libIex-2_2.12.dylib (0) <909DC08A-F410-328B-878B-5EBDD46B4D89> /usr/local/opt/ilmbase/lib/libIex-2_2.12.dylib\n0x103c78000 -        0x103cb9ff7 +libHalf.12.dylib (0) <9A5C3CD6-0B65-3440-BF4A-BE9CEFA91698> /usr/local/opt/ilmbase/lib/libHalf.12.dylib\n0x103cbc000 -        0x103cbfff7 +libIlmThread-2_2.12.dylib (0)  /usr/local/opt/ilmbase/lib/libIlmThread-2_2.12.dylib\n0x103cc4000 -        0x103cc5fff +libIexMath-2_2.12.dylib (0)  /usr/local/lib/libIexMath-2_2.12.dylib\n0x103d78000 -        0x103dc5fff +hashtable.so (???) <1E741A79-682E-3119-B6FD-76C5305EAE53> /Library/Python/2.7/site-packages/pandas/hashtable.so\n0x103de6000 -        0x103f29ff7 +tslib.so (???) <4F735093-3E33-31AB-891D-EB0BB178DC95> /Library/Python/2.7/site-packages/pandas/tslib.so\n0x103fd6000 -        0x103fd9ff7 +strop.so (???) <40B05D3E-1DED-ED4E-6436-D230D8105431> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/strop.so\n0x10401e000 -        0x1040b2fef +unicodedata.so (???) <773468C9-2D9B-1067-F9F0-D11D6EBA58AD> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/unicodedata.so\n0x1040c9000 -        0x1040cffff +array.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/array.so\n0x1040d7000 -        0x1040e0ff7 +_socket.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_socket.so\n0x10412c000 -        0x104138ff7 +_ssl.so (???) <67642C4F-FE1A-F9DE-42D6-223956B3E492> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_ssl.so\n0x104244000 -        0x104247ff7 +zlib.so (???) <81CFA8F1-F57F-9B2B-0869-A68AAD20F1DD> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/zlib.so\n0x1042cc000 -        0x1042ccfff +_scproxy.so (???) <4CD48C63-3C54-0B58-E4E4-7BD3AC309AB2> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_scproxy.so\n0x1043cf000 -        0x104403fff +pyexpat.so (???) <73E5EF54-D737-14C2-01B1-48D5A80FE467> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/pyexpat.so\n0x104455000 -        0x104455fff +_bisect.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_bisect.so\n0x104458000 -        0x104532ff7 +lib.so (???)  /Library/Python/2.7/site-packages/pandas/lib.so\n0x10467f000 -        0x1046adfff +index.so (???) <6C2B2182-F348-384B-A1E8-252B9240DC1F> /Library/Python/2.7/site-packages/pandas/index.so\n0x104701000 -        0x104704ff7 +_csv.so (???) <111138F7-C0D3-6E7E-D0F3-BFC3F2B78561> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_csv.so\n0x1047cc000 -        0x1047cffff +select.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/select.so\n0x1047d5000 -        0x1047dbff7 +_json.so (???) <1C92C9C7-6057-E37C-7321-0999973EF548> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_json.so\n0x1047e0000 -        0x1047f2ff7 +json.so (???)  /Library/Python/2.7/site-packages/pandas/json.so\n0x105000000 -        0x10523aff7 +algos.so (???) <7C357B32-4534-3B34-86BF-62E626419602> /Library/Python/2.7/site-packages/pandas/algos.so\n0x1052a5000 -        0x1052f1ff7 +_period.so (???)  /Library/Python/2.7/site-packages/pandas/_period.so\n0x1053d2000 -        0x1053dcfff +_packer.so (???) <11FE38AA-2D42-3F9A-9B8C-4FE9856A59F4> /Library/Python/2.7/site-packages/pandas/msgpack/_packer.so\n0x1053e5000 -        0x1053f2fff +_unpacker.so (???) <8FCA7B13-8D7A-3558-96E0-6737D2B2880C> /Library/Python/2.7/site-packages/pandas/msgpack/_unpacker.so\n0x105540000 -        0x105576fff +_sparse.so (???)  /Library/Python/2.7/site-packages/pandas/_sparse.so\n0x105d49000 -        0x105d6cff7 +_path.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/_path.so\n0x105d7f000 -        0x105dcdff7  libstdc++.6.dylib (104.1) <76E2AFB1-07E5-3F19-B692-F6E21B7E470D> /usr/lib/libstdc++.6.dylib\n0x105ff6000 -        0x105ff7fff +_check_build.so (0) <86022FAB-12C8-3533-BAC0-DE807856FCB9> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so\n0x105ffb000 -        0x105ffcff7 +_zeros.so (???) <47FF5193-DE55-35FD-ACF0-85CF05515F06> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_zeros.so\n0x106100000 -        0x10614cff7 +parser.so (???) <4BD01EEA-D2AC-3534-8A0A-0469FF83872D> /Library/Python/2.7/site-packages/pandas/parser.so\n0x1062ab000 -        0x1062b7ff7 +_testing.so (???) <83D3648A-6B1A-3342-834A-3FECF23D1B2A> /Library/Python/2.7/site-packages/pandas/_testing.so\n0x1062bf000 -        0x10655cff7 +_sparsetools.so (???) <59AF01B7-9708-32A2-80EA-8FC4D246E134> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/_sparsetools.so\n0x106687000 -        0x1066efff7 +_csparsetools.so (???) <29CC8BAF-1061-3460-B1F6-8CAA0D68931D> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/_csparsetools.so\n0x106709000 -        0x106740ff7 +_shortest_path.so (???) <2251A770-6F58-393B-B81A-7C7770B872BD> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csgraph/_shortest_path.so\n0x106791000 -        0x1067aeff7 +_tools.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csgraph/_tools.so\n0x1067bb000 -        0x1067dbff7 +_traversal.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csgraph/_traversal.so\n0x1067e7000 -        0x106804fff +_min_spanning_tree.so (???) <3CCC2F8F-8F23-383D-9B12-A236DC49C832> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csgraph/_min_spanning_tree.so\n0x106815000 -        0x106850fff +_reordering.so (???) <099C367A-48D5-318D-91B0-23431BDD752E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csgraph/_reordering.so\n0x106867000 -        0x106878ff7 +murmurhash.so (0) <6942EF50-6949-3D25-B0C6-5C535AF7F6B6> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/murmurhash.so\n0x106881000 -        0x1069abfe7 +_ufuncs.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/special/_ufuncs.so\n0x1069ff000 -        0x106ab5fcf +libgfortran.2.0.0.dylib (3)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/.dylibs/libgfortran.2.0.0.dylib\n0x106afa000 -        0x106b06fb1 +libgcc_s.1.dylib (1)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/.dylibs/libgcc_s.1.dylib\n0x106b0f000 -        0x106b20ff7 +_ufuncs_cxx.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/special/_ufuncs_cxx.so\n0x106b6b000 -        0x106c1dfef +specfun.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/special/specfun.so\n0x106c2e000 -        0x106c67ff7 +_fblas.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/_fblas.so\n0x106c8d000 -        0x106d0fff7 +_flapack.so (???) <33A1C2F3-B566-3813-983E-E9A39F98267F> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/_flapack.so\n0x106da6000 -        0x106db0ff7 +_flinalg.so (???) <43F4D36E-F9AD-3587-A8E5-DC430940A2DE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/_flinalg.so\n0x106db8000 -        0x106ddffff +_solve_toeplitz.so (???) <46848CC9-1A1F-342A-8E41-8F8535420DB4> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/_solve_toeplitz.so\n0x106e32000 -        0x106e72fff +_decomp_update.so (???) <59FD9B9D-2B3B-3AC0-B99B-7821AFE62F70> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/_decomp_update.so\n0x106e88000 -        0x106eb3fff +cython_blas.so (???) <6983E821-2C12-39AC-8EAC-5AD0A02D7062> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/cython_blas.so\n0x106ecd000 -        0x106f48ff7 +cython_lapack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/cython_lapack.so\n0x106f95000 -        0x106fa2ff7 +_ellip_harm_2.so (???) <742B6AA0-90DD-37AE-B90C-BC690DA89CD8> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/special/_ellip_harm_2.so\n0x106fab000 -        0x106fccfe7 +_odepack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/integrate/_odepack.so\n0x106fd1000 -        0x106feafe7 +_quadpack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/integrate/_quadpack.so\n0x10702e000 -        0x107056fff +vode.so (???) <5B96AB84-1955-3B5C-8530-C7DD7E67AFCE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/integrate/vode.so\n0x10705d000 -        0x107070ff7 +_dop.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/integrate/_dop.so\n0x107076000 -        0x10709afef +lsoda.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/integrate/lsoda.so\n0x1070a0000 -        0x1070ccfff +_iterative.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/linalg/isolve/_iterative.so\n0x1070e1000 -        0x1070e4fff +_multiprocessing.so (???) <689447D4-A856-EA35-B746-7AD878D1AA38> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_multiprocessing.so\n0x1070e9000 -        0x1070ecfff +mmap.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/mmap.so\n0x1070f0000 -        0x1070f6fff +minpack2.so (???) <00719268-1389-33CA-B6C2-E5D815D8FF73> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/minpack2.so\n0x107240000 -        0x10728afff +_superlu.so (???) <2F4DC8AF-44C9-3F3F-9CC8-A5926E5299E4> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/linalg/dsolve/_superlu.so\n0x1072a5000 -        0x107328fff +_arpack.so (???) <61D77FE2-7BE9-3EE8-B714-F39CC1FFCD87> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/_arpack.so\n0x107580000 -        0x107599ff7 +_lbfgsb.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_lbfgsb.so\n0x10759e000 -        0x1075a6fff +moduleTNC.so (???) <0A415AD5-5B21-3983-8707-3587370DBBCF> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/moduleTNC.so\n0x1075a9000 -        0x1075bfff7 +_cobyla.so (???) <80E2C4A7-9A50-335B-938B-D925B35A055C> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_cobyla.so\n0x1075c4000 -        0x1075d6fff +_slsqp.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_slsqp.so\n0x1075db000 -        0x1075f3ff7 +_minpack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_minpack.so\n0x107637000 -        0x107656ff7 +_group_columns.so (???) <27C2A1BF-EEB4-32E7-9E32-DDF7A7F06057> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_group_columns.so\n0x107665000 -        0x10767bff7 +givens_elimination.so (???) <0CBCF1AB-25AC-3F52-BBEB-27638340AB37> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_lsq/givens_elimination.so\n0x1076ca000 -        0x1076d2ff7 +_nnls.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_nnls.so\n0x107716000 -        0x107723ff7 +vonmises_cython.so (???) <02758875-732E-3D4F-B68C-1CE58FFCCF1E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/stats/vonmises_cython.so\n0x10786b000 -        0x107874ff7 +_rank.so (???) <7C53C52C-A05F-3FEB-A7E1-CA57D0528492> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/stats/_rank.so\n0x1078bb000 -        0x1078c2fff +statlib.so (???) <37F13EFE-4879-37AA-95FC-FA581FE7969A> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/stats/statlib.so\n0x1078c7000 -        0x1078e2ff7 +sparsefuncs_fast.so (0) <0D87B58C-1996-38F3-B935-F673EAA2AD3E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/sparsefuncs_fast.so\n0x1078ee000 -        0x1078f7ff7 +_logistic_sigmoid.so (0) <267F4A12-9702-3148-A7BF-D581E9CFF2A6> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/_logistic_sigmoid.so\n0x107a40000 -        0x107a4eff7 +mvn.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/stats/mvn.so\n0x107b4b000 -        0x107bc2fff +ckdtree.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/spatial/ckdtree.so\n0x107be3000 -        0x107c9dfff +qhull.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/spatial/qhull.so\n0x107d0e000 -        0x107d18ff7 +_distance_wrap.so (???) <23A270AE-EE08-3ADF-9574-D6BB5957948F> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/spatial/_distance_wrap.so\n0x107d21000 -        0x107d36fff +expected_mutual_info_fast.so (0)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/cluster/expected_mutual_info_fast.so\n0x107d3f000 -        0x107d41ff7 +lgamma.so (0) <3B85DD41-989C-3D67-B48A-89B7E34C3523> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/lgamma.so\n0x107d45000 -        0x107d73ff7 +pairwise_fast.so (0) <5A40524C-7DED-3A86-9367-C97DE961B6CD> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/pairwise_fast.so\n0x10804a000 -        0x108057ff7 +_curses.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload//lazylinker_ext.so\n0x10838f000 -        0x108392ff3 +cutils_ext.so (0) <6F2652D7-38B2-3648-91AC-7076347BBD3F> /Users/USER/*/cutils_ext.so\n0x1083d6000 -        0x1083e5fff +sigtools.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/signal/sigtools.so\n0x1083eb000 -        0x1083f1ff7 +spline.so (???) <16BB7949-EF56-394D-ADBA-2A43948DB93D> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/signal/spline.so\n0x108640000 -        0x10865afff +_max_len_seq_inner.so (???) <65DBA4B1-B02F-399E-8C6B-543260086962> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/signal/_max_len_seq_inner.so\n0x1086a9000 -        0x1086d8ff7 +_fitpack.so (???) <7FC9E302-4135-3F81-86D7-C660E66090A9> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/interpolate/_fitpack.so\n0x1086dc000 -        0x108730ff7 +dfitpack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/interpolate/dfitpack.so\n0x10873d000 -        0x108776fff +_ppoly.so (???) <56046084-4C74-3C94-9084-BA63D7AF6C49> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/interpolate/_ppoly.so\n0x10878e000 -        0x1087cdff7 +interpnd.so (???) <1EC588BD-8028-3499-A02C-D78A359EC1AE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/interpolate/interpnd.so\n0x108828000 -        0x10886aff7 +_fftpack.so (???)  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/fftpack/_fftpack.so\n0x10887b000 -        0x10888dfff +convolve.so (???) <8F014EED-9E4E-3A91-967F-54D21C12EBB1> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/fftpack/convolve.so\n0x108894000 -        0x1088acff7 +_nd_image.so (???) <1992B1E8-C791-3F69-8E3A-5884F8DADDAA> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/_nd_image.so\n0x1088f2000 -        0x108935ff7 +_ni_label.so (???) <8904FC15-AFF0-39B2-AE92-2AEF7FD3D632> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/ndimage/_ni_label.so\n0x10894d000 -        0x108956ff7 +_spectral.so (???) <5EB02E65-C3F9-3E3B-8DA8-B85F80718533> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/signal/_spectral.so\n0x7fff6f8e1000 -     0x7fff6f91825f  dyld (360.22)  /usr/lib/dyld\n0x7fff8b67b000 -     0x7fff8b67bff7  libunc.dylib (29)  /usr/lib/system/libunc.dylib\n0x7fff8b835000 -     0x7fff8b843fff  libxar.1.dylib (302) <03207F66-2C4A-3DBD-8D81-70F4C85903C4> /usr/lib/libxar.1.dylib\n0x7fff8b90b000 -     0x7fff8bd39fff  com.apple.vision.FaceCore (3.3.1 - 3.3.1)  /System/Library/PrivateFrameworks/FaceCore.framework/Versions/A/FaceCore\n0x7fff8bfe8000 -     0x7fff8bff3fff  libkxld.dylib (3248.50.21) <99195052-038E-3490-ACF8-76F9AC43897E> /usr/lib/system/libkxld.dylib\n0x7fff8bff4000 -     0x7fff8c02eff7  com.apple.DebugSymbols (132 - 132) <23A42C53-B941-3871-9EE2-4C87A46005B5> /System/Library/PrivateFrameworks/DebugSymbols.framework/Versions/A/DebugSymbols\n0x7fff8c606000 -     0x7fff8c65bfff  com.apple.AE (701 - 701)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE\n0x7fff8c6a8000 -     0x7fff8c6d6ff7  libsandbox.1.dylib (460.50.4)  /usr/lib/libsandbox.1.dylib\n0x7fff8c6d7000 -     0x7fff8c6dafff  libCoreVMClient.dylib (119.5) <560D70FB-709F-3030-96C9-F249FCB7DA6D> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib\n0x7fff8c6db000 -     0x7fff8c6e1fff  com.apple.XPCService (2.0 - 1) <5E2122D6-FFA2-3552-BF16-9FD3F36B40DB> /System/Library/PrivateFrameworks/XPCService.framework/Versions/A/XPCService\n0x7fff8c8ae000 -     0x7fff8c936fff  com.apple.CoreSymbolication (3.1 - 58048.1) <4730422E-4178-34F9-8550-BB92F2A4F44B> /System/Library/PrivateFrameworks/CoreSymbolication.framework/Versions/A/CoreSymbolication\n0x7fff8ce31000 -     0x7fff8ce8ffff  com.apple.CoreServices.OSServices (728.12 - 728.12) <776EBD4F-7052-377F-A70D-E2FDBD465A5E> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices\n0x7fff8ce90000 -     0x7fff8ce98fff  libsystem_networkextension.dylib (385.40.36) <66095DC7-6539-38F2-95EE-458F15F6D014> /usr/lib/system/libsystem_networkextension.dylib\n0x7fff8ceaf000 -     0x7fff8ceb0fff  com.apple.TrustEvaluationAgent (2.0 - 25) <0239494E-FEFE-39BC-9FC7-E251BA5128F1> /System/Library/PrivateFrameworks/TrustEvaluationAgent.framework/Versions/A/TrustEvaluationAgent\n0x7fff8cec9000 -     0x7fff8ced5ff7  com.apple.OpenDirectory (10.11 - 194) <31A67AD5-5CC2-350A-96D7-821DF4BC4196> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory\n0x7fff8d070000 -     0x7fff8d07eff7  libbz2.1.0.dylib (38) <28E54258-C0FE-38D4-AB76-1734CACCB344> /usr/lib/libbz2.1.0.dylib\n0x7fff8d13f000 -     0x7fff8d1aefff  com.apple.SearchKit (1.4.0 - 1.4.0)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit\n0x7fff8d1af000 -     0x7fff8d2befe7  libvDSP.dylib (563.5) <9AB6CA3C-4F0E-35E6-9184-9DF86E7C3DAD> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib\n0x7fff8d305000 -     0x7fff8d37cfeb  libcorecrypto.dylib (335.50.1)  /usr/lib/system/libcorecrypto.dylib\n0x7fff8d39a000 -     0x7fff8d3dfff3  libFontRegistry.dylib (155.2)  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib\n0x7fff8d3f0000 -     0x7fff8d3f0fff  com.apple.audio.units.AudioUnit (1.13 - 1.13) <93C1D642-37D4-3692-AD35-DCAD04F9610B> /System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit\n0x7fff8d3f1000 -     0x7fff8d442fff  com.apple.audio.CoreAudio (4.3.0 - 4.3.0)  /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio\n0x7fff8d6c4000 -     0x7fff8d80eff7  com.apple.coreui (2.1 - 366.1) <8138636F-A0A7-31C7-896C-5F5747FA1B2A> /System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI\n0x7fff8d812000 -     0x7fff8d839fff  com.apple.ChunkingLibrary (167 - 167)  /System/Library/PrivateFrameworks/ChunkingLibrary.framework/Versions/A/ChunkingLibrary\n0x7fff8e17c000 -     0x7fff8e26eff7  libiconv.2.dylib (44)  /usr/lib/libiconv.2.dylib\n0x7fff8e26f000 -     0x7fff8e26ffff  libmetal_timestamp.dylib (600.0.44.1) <6576F284-BACA-332A-A6E7-FA1C347636E3> /System/Library/PrivateFrameworks/GPUCompiler.framework/libmetal_timestamp.dylib\n0x7fff8e270000 -     0x7fff8e271ffb  libSystem.B.dylib (1226.10.1)  /usr/lib/libSystem.B.dylib\n0x7fff8e477000 -     0x7fff8e4d3fff  libTIFF.dylib (1450) <14EB7C03-7DDA-3276-BAC5-D597913AC9C4> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib\n0x7fff8e4d4000 -     0x7fff8e869fdb  com.apple.vImage (8.0 - 8.0) <4BAC9B6F-7482-3580-8787-AB0A5B4D331B> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage\n0x7fff8e8a8000 -     0x7fff8e8abffb  libScreenReader.dylib (426.42) <16FC79D1-4573-3E90-945F-CBA22D5185FD> /usr/lib/libScreenReader.dylib\n0x7fff8e916000 -     0x7fff8e91fff3  libsystem_notify.dylib (150.40.1)  /usr/lib/system/libsystem_notify.dylib\n0x7fff8e920000 -     0x7fff8ecf8fef  com.apple.CoreAUC (214.0.0 - 214.0.0)  /System/Library/PrivateFrameworks/CoreAUC.framework/Versions/A/CoreAUC\n0x7fff8ecf9000 -     0x7fff8ed0cfff  com.apple.CoreBluetooth (1.0 - 1)  /System/Library/Frameworks/CoreBluetooth.framework/Versions/A/CoreBluetooth\n0x7fff8ed0d000 -     0x7fff8ed5dff7  com.apple.Symbolication (1.4 - 58044)  /System/Library/PrivateFrameworks/Symbolication.framework/Versions/A/Symbolication\n0x7fff8ed5e000 -     0x7fff8ed67ff7  com.apple.CommonAuth (4.0 - 2.0) <4B8673E1-3697-3FE2-8D30-AC7AC5D4F8BF> /System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth\n0x7fff8ed68000 -     0x7fff8ef73fff  libFosl_dynamic.dylib (16.24) <5F9DB82D-FD4B-3952-8531-CE020F93ED49> /usr/lib/libFosl_dynamic.dylib\n0x7fff8ef74000 -     0x7fff8ef7cfff  libGFXShared.dylib (12.1) <5A0C2493-200C-30BE-97D5-8E8C0B8E604D> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib\n0x7fff8ef87000 -     0x7fff8f194fff  libicucore.A.dylib (551.51.3) <5BC80F94-C90D-3175-BD96-FF1DC222EC9C> /usr/lib/libicucore.A.dylib\n0x7fff8f196000 -     0x7fff8f196fff  com.apple.Accelerate (1.10 - Accelerate 1.10) <185EC96A-5AF0-3620-A4ED-4D3654D25B39> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate\n0x7fff8f197000 -     0x7fff8f220ff7  com.apple.PerformanceAnalysis (1.0 - 1) <2064F7E8-5C3D-3E18-8029-2D832D13E2A2> /System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis\n0x7fff8f3fc000 -     0x7fff8f45ffff  libAVFAudio.dylib (161.2) <1A98DBF3-490B-37FB-928A-AB1E36E6E5DD> /System/Library/Frameworks/AVFoundation.framework/Versions/A/Resources/libAVFAudio.dylib\n0x7fff8f460000 -     0x7fff8f7dbffb  com.apple.VideoToolbox (1.0 - 1731.15.204) <2B21F9B2-66A2-3900-84A5-0AB66F8056E4> /System/Library/Frameworks/VideoToolbox.framework/Versions/A/VideoToolbox\n0x7fff8f7e6000 -     0x7fff8f7ebff7  libmacho.dylib (875.1) <318264FA-58F1-39D8-8285-1F6254EE410E> /usr/lib/system/libmacho.dylib\n0x7fff8f812000 -     0x7fff8f865ff7  libc++.1.dylib (120.1) <8FC3D139-8055-3498-9AC5-6467CB7F4D14> /usr/lib/libc++.1.dylib\n0x7fff8fad4000 -     0x7fff8fad4fff  libOpenScriptingUtil.dylib (169.1)  /usr/lib/libOpenScriptingUtil.dylib\n0x7fff8fae8000 -     0x7fff8fbc1fff  com.apple.CoreMedia (1.0 - 1731.15.204) <4BFDD68E-9411-3358-8679-BB3EDA94F9A2> /System/Library/Frameworks/CoreMedia.framework/Versions/A/CoreMedia\n0x7fff8fbcf000 -     0x7fff8fbcfff7  libkeymgr.dylib (28) <8371CE54-5FDD-3CE9-B3DF-E98C761B6FE0> /usr/lib/system/libkeymgr.dylib\n0x7fff90331000 -     0x7fff903a0fff  com.apple.datadetectorscore (7.0 - 460)  /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore\n0x7fff903a1000 -     0x7fff903bdff7  libextension.dylib (78)  /usr/lib/libextension.dylib\n0x7fff903be000 -     0x7fff90916ff7  com.apple.MediaToolbox (1.0 - 1731.15.204)  /System/Library/Frameworks/MediaToolbox.framework/Versions/A/MediaToolbox\n0x7fff90917000 -     0x7fff9091aff7  com.apple.help (1.3.3 - 46) <35DA4D48-0BC2-35A1-8D7C-40905CDF4F64> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help\n0x7fff90a88000 -     0x7fff90d22ff3  com.apple.security (7.0 - 57337.50.23) <8B6CF71D-A63E-34C9-9227-0AACAB643584> /System/Library/Frameworks/Security.framework/Versions/A/Security\n0x7fff9140f000 -     0x7fff91418ff7  libsystem_pthread.dylib (138.10.4) <3DD1EF4C-1D1B-3ABF-8CC6-B3B1CEEE9559> /usr/lib/system/libsystem_pthread.dylib\n0x7fff91419000 -     0x7fff914bdfff  com.apple.Bluetooth (4.4.5 - 4.4.5f3) <141F2C36-70B6-32D3-A556-7A605832CDB3> /System/Library/Frameworks/IOBluetooth.framework/Versions/A/IOBluetooth\n0x7fff914e6000 -     0x7fff914f1fff  com.apple.CrashReporterSupport (10.11 - 718) <05892B57-F2CD-3C84-B984-0417F6B361DB> /System/Library/PrivateFrameworks/CrashReporterSupport.framework/Versions/A/CrashReporterSupport\n0x7fff9161e000 -     0x7fff91649ffb  libarchive.2.dylib (33.20.2) <6C370A21-63FD-3A68-B4B3-5333F24B770B> /usr/lib/libarchive.2.dylib\n0x7fff9180c000 -     0x7fff9184aff7  libGLImage.dylib (12.1)  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib\n0x7fff918a8000 -     0x7fff918a8fff  com.apple.ApplicationServices (48 - 48)  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices\n0x7fff918ba000 -     0x7fff919e7ff3  com.apple.CoreText (352.0 - 494.11) <08E8640E-6602-3A00-BC28-94235FD311B4> /System/Library/Frameworks/CoreText.framework/Versions/A/CoreText\n0x7fff91a84000 -     0x7fff91a8fff7  libcommonCrypto.dylib (60075.50.1) <93732261-34B4-3914-B7A2-90A81A182DBA> /usr/lib/system/libcommonCrypto.dylib\n0x7fff91a90000 -     0x7fff91aacfff  com.apple.GenerationalStorage (2.0 - 239.1) <8C821448-4294-3736-9CEF-467C93785CB9> /System/Library/PrivateFrameworks/GenerationalStorage.framework/Versions/A/GenerationalStorage\n0x7fff91ac7000 -     0x7fff91ac7fff  com.apple.CoreServices (728.12 - 728.12)  /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices\n0x7fff91ac8000 -     0x7fff91acdfff  com.apple.TCC (1.0 - 1)  /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC\n0x7fff91afc000 -     0x7fff91d92fff  libmecabra.dylib (696.5)  /usr/lib/libmecabra.dylib\n0x7fff91e0d000 -     0x7fff91e36ff7  libxslt.1.dylib (14.2) <6E8D0F06-9086-32D3-9D87-3870A1CE9E99> /usr/lib/libxslt.1.dylib\n0x7fff91fb0000 -     0x7fff91fb2fff  libCVMSPluginSupport.dylib (12.1)  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib\n0x7fff91fc7000 -     0x7fff91febfff  libJPEG.dylib (1450) <1775E59E-D82C-3F7A-8E4F-B0C13F88F691> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib\n0x7fff9287c000 -     0x7fff92893fff  libmarisa.dylib (4)  /usr/lib/libmarisa.dylib\n0x7fff92b66000 -     0x7fff92b6afff  com.apple.CommonPanels (1.2.6 - 96) <4AE7E5AE-55B3-37FA-9BDE-B23147ADA2E9> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels\n0x7fff92bba000 -     0x7fff92c4ffff  com.apple.ink.framework (10.9 - 214) <1F76CF36-3F79-36B8-BC37-C540AF34B338> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink\n0x7fff92c54000 -     0x7fff92c76ff7  com.apple.Sharing (442.13.6 - 442.13.6)  /System/Library/PrivateFrameworks/Sharing.framework/Versions/A/Sharing\n0x7fff92d0f000 -     0x7fff92d27fef  libcompression.dylib (28)  /usr/lib/libcompression.dylib\n0x7fff92d28000 -     0x7fff92d39ff7  libsystem_trace.dylib (201.10.3)  /usr/lib/system/libsystem_trace.dylib\n0x7fff92d3a000 -     0x7fff9302ffff  com.apple.HIToolbox (2.1.1 - 807.2) <36413C45-36AF-34EF-9C0E-F18B31D1E565> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox\n0x7fff930c3000 -     0x7fff931e8fff  com.apple.LaunchServices (728.12 - 728.12)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices\n0x7fff93213000 -     0x7fff93303fff  libJP2.dylib (1450)  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib\n0x7fff93394000 -     0x7fff933a0fff  com.apple.SpeechRecognitionCore (2.2.7 - 2.2.7) <6BA06290-D4A3-351C-87F9-B61EF61FF055> /System/Library/PrivateFrameworks/SpeechRecognitionCore.framework/Versions/A/SpeechRecognitionCore\n0x7fff933a1000 -     0x7fff93448fff  com.apple.LanguageModeling (1.0 - 1) <58C18A47-BDE7-3CBE-81C0-797029D170A1> /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling\n0x7fff93df0000 -     0x7fff93e64ff3  com.apple.securityfoundation (6.0 - 55126) <130656AE-2711-3914-8736-D8B021C93FE0> /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation\n0x7fff93e65000 -     0x7fff93e9dff7  com.apple.RemoteViewServices (2.0 - 101)  /System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices\n0x7fff93e9e000 -     0x7fff93ebcff7  libsystem_kernel.dylib (3248.50.21) <78E54D59-D2B0-3F54-9A4A-0A68D671F253> /usr/lib/system/libsystem_kernel.dylib\n0x7fff93f02000 -     0x7fff93f06fff  libpam.2.dylib (20)  /usr/lib/libpam.2.dylib\n0x7fff945e5000 -     0x7fff945edfff  com.apple.NetFS (6.0 - 4.0) <842A5346-24C3-3F22-9ECF-E586A10EA1F2> /System/Library/Frameworks/NetFS.framework/Versions/A/NetFS\n0x7fff947e6000 -     0x7fff94802ff7  libsystem_malloc.dylib (67.40.1) <5748E8B2-F81C-34C6-8B13-456213127678> /usr/lib/system/libsystem_malloc.dylib\n0x7fff94803000 -     0x7fff94825fff  com.apple.IconServices (68.1 - 68.1)  /System/Library/PrivateFrameworks/IconServices.framework/Versions/A/IconServices\n0x7fff94827000 -     0x7fff9482fffb  libsystem_dnssd.dylib (625.50.5) <4D10E12B-59B5-386F-82DA-326F18028F0A> /usr/lib/system/libsystem_dnssd.dylib\n0x7fff94a40000 -     0x7fff94a51fff  libSparseBLAS.dylib (1162.2)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib\n0x7fff95baa000 -     0x7fff96020fff  com.apple.CoreFoundation (6.9 - 1258.1) <943A1383-DA6A-3DC0-ABCD-D9AEB3D0D34D> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n0x7fff96031000 -     0x7fff96036fff  com.apple.MediaAccessibility (1.0 - 79)  /System/Library/Frameworks/MediaAccessibility.framework/Versions/A/MediaAccessibility\n0x7fff9610f000 -     0x7fff961a5fff  com.apple.ColorSync (4.9.0 - 4.9.0) <8FC37E20-6579-3CB2-9D49-BC39FC38DF87> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync\n0x7fff96f9f000 -     0x7fff96f9fff7  liblaunch.dylib (765.50.8) <834ED605-5114-3641-AA4D-ECF31B801C50> /usr/lib/system/liblaunch.dylib\n0x7fff96fc1000 -     0x7fff97071fff  com.apple.backup.framework (1.7.4 - 1.7.4)  /System/Library/PrivateFrameworks/Backup.framework/Versions/A/Backup\n0x7fff97072000 -     0x7fff97088fff  com.apple.CoreMediaAuthoring (2.2 - 953)  /System/Library/PrivateFrameworks/CoreMediaAuthoring.framework/Versions/A/CoreMediaAuthoring\n0x7fff97089000 -     0x7fff970b8ff7  com.apple.DictionaryServices (1.2 - 250.3) <30250542-CBAA-39C1-91AA-B57A5DE17594> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices\n0x7fff970b9000 -     0x7fff970bbff7  libsystem_configuration.dylib (802.40.13) <3DEB7DF9-6804-37E1-BC83-0166882FF0FF> /usr/lib/system/libsystem_configuration.dylib\n0x7fff97542000 -     0x7fff97544ff7  com.apple.securityhi (9.0 - 55006) <1E7BE52B-97EA-371A-AECA-1EE2AD246D8A> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI\n0x7fff975cc000 -     0x7fff976f0fff  libsqlite3.dylib (216.4) <280D67B8-F93D-3587-A146-19F36C817548> /usr/lib/libsqlite3.dylib\n0x7fff97719000 -     0x7fff97a7bf3f  libobjc.A.dylib (680) <7489D2D6-1EFD-3414-B18D-2AECCCC90286> /usr/lib/libobjc.A.dylib\n0x7fff97a7c000 -     0x7fff97a95fff  com.apple.openscripting (1.7.1 - 169.1) <36EBF6A7-334A-3197-838F-E8C7B27FCDBB> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting\n0x7fff97c42000 -     0x7fff97c47fff  com.apple.DiskArbitration (2.7 - 2.7)  /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration\n0x7fff97c48000 -     0x7fff97c50fff  libMatch.1.dylib (27) <3AC0BFB8-7E69-3DBE-A175-7F3946FC4554> /usr/lib/libMatch.1.dylib\n0x7fff97cc2000 -     0x7fff97cdfff7  com.apple.AppleVPAFramework (2.1.2 - 2.1.2) <41378C0B-B56A-3A73-9BD0-E06FA1F87B8C> /System/Library/PrivateFrameworks/AppleVPA.framework/Versions/A/AppleVPA\n0x7fff97ce0000 -     0x7fff98034fff  com.apple.Foundation (6.9 - 1259) <71A9D3A0-0B1F-3E3A-86F3-1486365A6EF2> /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation\n0x7fff98035000 -     0x7fff98431fff  libLAPACK.dylib (1162.2) <987E42B0-5108-3065-87F0-9DF7616A8A06> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib\n0x7fff985c2000 -     0x7fff9876efff  com.apple.avfoundation (2.0 - 1046.9.11) <399D8273-E3CD-3358-9B80-64E6A5CBE278> /System/Library/Frameworks/AVFoundation.framework/Versions/A/AVFoundation\n0x7fff9876f000 -     0x7fff98772fff  libsystem_sandbox.dylib (460.50.4) <150A9D3D-F69E-32F7-8C7B-8E72CAAFF7E4> /usr/lib/system/libsystem_sandbox.dylib\n0x7fff9890d000 -     0x7fff98910fff  com.apple.IOSurface (108.2.1 - 108.2.1)  /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface\n0x7fff98926000 -     0x7fff98927ff3  com.apple.print.framework.Print (10.0 - 266) <3E85F70C-D7D4-34E1-B88A-C1F503F99CDA> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print\n0x7fff989a5000 -     0x7fff98a1afff  com.apple.framework.IOKit (2.0.2 - 1179.50.2)  /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit\n0x7fff98a1b000 -     0x7fff98a67ffb  com.apple.HIServices (1.22 - 550) <6B76B41C-CF5A-34C4-89F4-EFD7CA3D1C9D> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices\n0x7fff98a68000 -     0x7fff98a82ff3  liblzma.5.dylib (10)  /usr/lib/liblzma.5.dylib\n0x7fff98b64000 -     0x7fff98b68fff  libcache.dylib (75) <9548AAE9-2AB7-3525-9ECE-A2A7C4688447> /usr/lib/system/libcache.dylib\n0x7fff98bb6000 -     0x7fff98bbcff7  com.apple.speech.recognition.framework (5.1.1 - 5.1.1) <9E5A980A-F455-32D5-BBEE-3BD6018CC45E> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition\n0x7fff98f31000 -     0x7fff98f3dfff  com.apple.speech.synthesis.framework (5.4.12 - 5.4.12) <71DA00B8-5EA2-326B-8814-59DB25512F65> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis\n0x7fff98f3e000 -     0x7fff98f40fff  libsystem_coreservices.dylib (19.2) <1B3F5AFC-FFCD-3ECB-8B9A-5538366FB20D> /usr/lib/system/libsystem_coreservices.dylib\n0x7fff98f41000 -     0x7fff98f45fff  libGIF.dylib (1450)  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib\n0x7fff98f4e000 -     0x7fff98f84fff  libssl.0.9.8.dylib (59.40.2) <523FEBFA-4BF7-3A69-83B7-164265BE7F4D> /usr/lib/libssl.0.9.8.dylib\n0x7fff98f85000 -     0x7fff98f8afff  com.apple.ImageCapture (9.0 - 9.0)  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture\n0x7fff98f8c000 -     0x7fff98fddff7  libcups.2.dylib (435.2) <91584A40-214D-33E8-A613-CE22289037C8> /usr/lib/libcups.2.dylib\n0x7fff98fde000 -     0x7fff99024ff7  libauto.dylib (186) <999E610F-41FC-32A3-ADCA-5EC049B65DFB> /usr/lib/libauto.dylib\n0x7fff9919b000 -     0x7fff991acfff  libcmph.dylib (6)  /usr/lib/libcmph.dylib\n0x7fff991d2000 -     0x7fff991d4ff7  libquarantine.dylib (80) <0F4169F0-0C84-3A25-B3AE-E47B3586D908> /usr/lib/system/libquarantine.dylib\n0x7fff991d5000 -     0x7fff99227fff  com.apple.AppleVAFramework (5.0.32 - 5.0.32) <271ED7A9-73E5-3595-A8D6-28594C9F3C9D> /System/Library/PrivateFrameworks/AppleVA.framework/Versions/A/AppleVA\n0x7fff99442000 -     0x7fff99444ff7  com.apple.xpc.ServiceManagement (1.0 - 1)  /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement\n0x7fff99478000 -     0x7fff994b9ff7  libGLU.dylib (12.1)  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib\n0x7fff994c7000 -     0x7fff9968dfe7  com.apple.ImageIO.framework (3.3.0 - 1450) <18ABA1F4-43EC-3990-9777-C91FD3D6AF71> /System/Library/Frameworks/ImageIO.framework/Versions/A/ImageIO\n0x7fff99cd1000 -     0x7fff99cdcfff  com.apple.AppSandbox (4.0 - 261.40.2) <52766210-B6EB-3B73-AB1B-42E0A9AD2EE8> /System/Library/PrivateFrameworks/AppSandbox.framework/Versions/A/AppSandbox\n0x7fff99d81000 -     0x7fff99d83fff  com.apple.SecCodeWrapper (4.0 - 261.40.2) <1F832591-59A8-3B3F-943F-D6D827463782> /System/Library/PrivateFrameworks/SecCodeWrapper.framework/Versions/A/SecCodeWrapper\n0x7fff99da7000 -     0x7fff99decff7  com.apple.coreservices.SharedFileList (24.4 - 24.5) <1D2AD77B-778F-3253-A295-3D0A32A8121C> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList\n0x7fff99df2000 -     0x7fff99e02fff  libbsm.0.dylib (34) <7E14504C-A8B0-3574-B6EB-5D5FABC72926> /usr/lib/libbsm.0.dylib\n0x7fff99e03000 -     0x7fff99e08ff7  libheimdal-asn1.dylib (453.40.10) <981DE40B-FA16-36F7-BE92-8C8A115D6CD9> /usr/lib/libheimdal-asn1.dylib\n0x7fff99e09000 -     0x7fff99f70fff  libBLAS.dylib (1162.2)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\n0x7fff99f71000 -     0x7fff99f72fff  liblangid.dylib (122) <9CC4F0D1-5C51-3B69-BC8F-EE3A51FD0822> /usr/lib/liblangid.dylib\n0x7fff99fa1000 -     0x7fff99fcafff  libsystem_info.dylib (477.50.4)  /usr/lib/system/libsystem_info.dylib\n0x7fff9a009000 -     0x7fff9a058ff7  com.apple.opencl (2.7.0 - 2.7.0)  /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL\n0x7fff9a0df000 -     0x7fff9a12bfff  com.apple.print.framework.PrintCore (11.2 - 472.2) <5AE8AA6B-CE09-397D-B0D4-0F9CCBF1F77D> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore\n0x7fff9a19d000 -     0x7fff9a1a0fff  libspindump.dylib (197.1) <48F4C673-9F0C-38BE-B550-88241E812518> /usr/lib/libspindump.dylib\n0x7fff9a1a1000 -     0x7fff9a1bcff7  libCRFSuite.dylib (34) <078B4CD8-6A8C-3067-B2BA-0C2A0BAB8AC3> /usr/lib/libCRFSuite.dylib\n0x7fff9a2b4000 -     0x7fff9a2befff  com.apple.NetAuth (6.0 - 6.0)  /System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth\n0x7fff9a3d7000 -     0x7fff9a464fff  libsystem_c.dylib (1082.50.1)  /usr/lib/system/libsystem_c.dylib\n0x7fff9a465000 -     0x7fff9a465fff  com.apple.Accelerate.vecLib (3.10 - vecLib 3.10) <054DFE32-737D-3211-9A14-0FC5E1A880E3> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib\n0x7fff9a466000 -     0x7fff9a466fff  com.apple.Cocoa (6.11 - 22) <807787AB-D231-3F51-A99B-A9314623C571> /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa\n0x7fff9aca2000 -     0x7fff9ace4ff7  com.apple.Metal (56.6 - 56.6) <2B2C0F78-20B8-3878-B9B1-DE18BB92919D> /System/Library/Frameworks/Metal.framework/Versions/A/Metal\n0x7fff9ace5000 -     0x7fff9ace5fff  libenergytrace.dylib (10.40.1) <0A491CA7-3451-3FD5-999A-58AB4362682B> /usr/lib/libenergytrace.dylib\n0x7fff9ace6000 -     0x7fff9ae64fff  com.apple.UIFoundation (1.0 - 436.1)  /System/Library/PrivateFrameworks/UIFoundation.framework/Versions/A/UIFoundation\n0x7fff9aef4000 -     0x7fff9b17bff3  com.apple.CFNetwork (760.5.1 - 760.5.1)  /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork\n0x7fff9b540000 -     0x7fff9b62ffff  libxml2.2.dylib (29.7) <32BBF51E-B084-3FC2-AE9C-C008BE84102B> /usr/lib/libxml2.2.dylib\n0x7fff9b64d000 -     0x7fff9b650fff  com.apple.Mangrove (1.0 - 1) <2D86B3AD-64C3-3BB4-BC66-1CFD0C90E844> /System/Library/PrivateFrameworks/Mangrove.framework/Versions/A/Mangrove\n0x7fff9b651000 -     0x7fff9b6c5ff7  com.apple.Heimdal (4.0 - 2.0) <5D365381-8B5E-3259-8867-FC4A7D307BDE> /System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal\n0x7fff9b6df000 -     0x7fff9b708ff7  libxpc.dylib (765.50.8) <54D1328E-054E-3DAA-89E2-375722F9D18F> /usr/lib/system/libxpc.dylib\n0x7fff9b709000 -     0x7fff9b71fff7  libLinearAlgebra.dylib (1162.2)  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib\n0x7fff9bbfb000 -     0x7fff9bbfcffb  libremovefile.dylib (41) <552EF39E-14D7-363E-9059-4565AC2F894E> /usr/lib/system/libremovefile.dylib\n0x7fff9bc09000 -     0x7fff9bc3dff7  com.apple.CoreVideo (1.8 - 191.3) <1AA24A1B-CB84-3F6B-B6DE-11494542649C> /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo\n0x7fff9bc3e000 -     0x7fff9bca9ff7  com.apple.framework.CoreWLAN (11.0 - 1101.20) <3B35C543-7FCE-333F-80C1-432FA41DDCDE> /System/Library/Frameworks/CoreWLAN.framework/Versions/A/CoreWLAN\n0x7fff9c4f1000 -     0x7fff9c636fff  com.apple.QTKit (7.7.3 - 2943.10) <9D0EA81D-7BDE-3B47-B300-5C53C5EE4846> /System/Library/Frameworks/QTKit.framework/Versions/A/QTKit\n0x7fff9c637000 -     0x7fff9c64eff7  libsystem_asl.dylib (323.50.1) <41F8E11F-1BD0-3F1D-BA3A-AA1577ED98A9> /usr/lib/system/libsystem_asl.dylib\n0x7fff9c6f6000 -     0x7fff9c725ffb  libsystem_m.dylib (3105) <08E1A4B2-6448-3DFE-A58C-ACC7335BE7E4> /usr/lib/system/libsystem_m.dylib\n0x7fff9c726000 -     0x7fff9c727fff  libsystem_blocks.dylib (65) <1244D9D5-F6AA-35BB-B307-86851C24B8E5> /usr/lib/system/libsystem_blocks.dylib\n0x7fff9c728000 -     0x7fff9c72dff3  libunwind.dylib (35.3)  /usr/lib/system/libunwind.dylib\n0x7fff9c72e000 -     0x7fff9c752fff  com.apple.MultitouchSupport.framework (304.12 - 304.12) <65CB7653-EACD-3ADB-ABB6-2E0671708301> /System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport\n0x7fff9c7e7000 -     0x7fff9c7f8ff7  libz.1.dylib (61.20.1)  /usr/lib/libz.1.dylib\n0x7fff9c8b3000 -     0x7fff9c8c1fff  com.apple.opengl (12.1.0 - 12.1.0)  /System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL\n0x7fff9c8d3000 -     0x7fff9c8dbfef  libsystem_platform.dylib (74.40.2) <29A905EF-6777-3C33-82B0-6C3A88C4BA15> /usr/lib/system/libsystem_platform.dylib\n0x7fff9c949000 -     0x7fff9cb17ff3  com.apple.QuartzCore (1.11 - 410.14) <076BDE58-8AED-3D47-84FD-548CF8E8EDB9> /System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore\n0x7fff9cd10000 -     0x7fff9cd9ddd7  com.apple.AppleJPEG (1.0 - 1) <558ACADA-C41F-3EEF-82A0-C2D7B13C5428> /System/Library/PrivateFrameworks/AppleJPEG.framework/Versions/A/AppleJPEG\n0x7fff9cda1000 -     0x7fff9cdbbfff  com.apple.Kerberos (3.0 - 1) <1B4744BF-E5AE-38E2-AA56-E22D3270F2E8> /System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos\n0x7fff9cdbc000 -     0x7fff9cdbefff  com.apple.loginsupport (1.0 - 1) <9B2F5F9B-ED38-313F-B798-D2B667BCD6B5> /System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport\n0x7fff9cde1000 -     0x7fff9cdf8ff7  libsystem_coretls.dylib (83.40.5)  /usr/lib/system/libsystem_coretls.dylib\n0x7fff9cdf9000 -     0x7fff9cea9fe7  libvMisc.dylib (563.5) <6D73C20D-D1C4-3BA5-809B-4B597C15AA86> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib\n0x7fff9e02b000 -     0x7fff9e089fff  com.apple.SystemConfiguration (1.14 - 1.14)  /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration\n0x7fff9e090000 -     0x7fff9e19eff3  com.apple.desktopservices (1.10.3 - 1.10.3) <3A6906D4-C0B8-30D1-B589-0466E5E42B69> /System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv\n0x7fff9e19f000 -     0x7fff9e206fff  com.apple.framework.CoreWiFi (11.0 - 1101.20) <993592F1-B3F1-3FAD-87BD-EA83C361BCCF> /System/Library/PrivateFrameworks/CoreWiFi.framework/Versions/A/CoreWiFi\n0x7fff9e207000 -     0x7fff9e20affb  libdyld.dylib (360.22)  /usr/lib/system/libdyld.dylib\n0x7fff9e20b000 -     0x7fff9e21affb  com.apple.LangAnalysis (1.7.0 - 1.7.0) <18D21123-A3E7-3851-974A-08E5D4540475> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis\n0x7fff9e21b000 -     0x7fff9e21cfff  libsystem_secinit.dylib (20) <32B1A8C6-DC84-3F4F-B8CE-9A52B47C3E6B> /usr/lib/system/libsystem_secinit.dylib\n0x7fff9e21d000 -     0x7fff9e43eff7  com.apple.CoreImage (11.4.0 - 366.4.19)  /System/Library/Frameworks/CoreImage.framework/Versions/A/CoreImage\n0x7fff9e49b000 -     0x7fff9e4d5fff  com.apple.QD (3.12 - 302) <0FE53180-2895-3D14-A1E7-F82DE1D106E1> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD\n0x7fff9e62c000 -     0x7fff9e62eff7  libRadiance.dylib (1450)  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib\n0x7fff9e62f000 -     0x7fff9e62ffff  com.apple.Carbon (154 - 157) <8F6ED602-5943-3E29-A793-BC331E2C183D> /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon\n0x7fff9e636000 -     0x7fff9e637fff  libDiagnosticMessagesClient.dylib (100) <4243B6B4-21E9-355B-9C5A-95A216233B96> /usr/lib/libDiagnosticMessagesClient.dylib\n0x7fff9e638000 -     0x7fff9e651fff  com.apple.CFOpenDirectory (10.11 - 194) <11F95672-55E0-3F9D-9171-5E8C56AEE948> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory\n0x7fff9e652000 -     0x7fff9e655ff7  libCoreFSCache.dylib (119.5) <2389D7DA-B8EF-3EB4-AAAF-FBEDE01CDECA> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreFSCache.dylib\n0x7fff9e6a7000 -     0x7fff9e6adfff  com.apple.IOAccelerator (205.10 - 205.10)  /System/Library/PrivateFrameworks/IOAccelerator.framework/Versions/A/IOAccelerator\n0x7fff9e8c5000 -     0x7fff9e910ff7  com.apple.CoreMediaIO (703.0 - 4791) <2FAE3CC1-145C-37AB-A836-E5D93A02BA23> /System/Library/Frameworks/CoreMediaIO.framework/Versions/A/CoreMediaIO\n0x7fff9e961000 -     0x7fff9e991ff3  com.apple.CoreAVCHD (5.8.0 - 5800.4.2) <4AAFB1C4-3708-30F9-ACFA-90564347204C> /System/Library/PrivateFrameworks/CoreAVCHD.framework/Versions/A/CoreAVCHD\n0x7fff9ec02000 -     0x7fff9ec21ff7  com.apple.framework.Apple80211 (11.0 - 1121.34.2) <90477FAE-B835-3931-80FB-FDFF02B21D9D> /System/Library/PrivateFrameworks/Apple80211.framework/Versions/A/Apple80211\n0x7fff9ec25000 -     0x7fff9edcbff7  com.apple.audio.toolbox.AudioToolbox (1.13 - 1.13) <082319FC-59F2-3D36-AC9B-94759724E302> /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox\n0x7fff9edd5000 -     0x7fff9edfefff  libc++abi.dylib (125)  /usr/lib/libc++abi.dylib\n0x7fff9ee0c000 -     0x7fff9ee14fff  com.apple.CoreServices.FSEvents (1223.10.1 - 1223.10.1) <7F5B7A23-BC1D-3FA9-A9B8-D534F1E1979A> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents\n0x7fff9ee15000 -     0x7fff9ee3aff7  libPng.dylib (1450)  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib\n0x7fff9eec6000 -     0x7fff9eed1fff  libGL.dylib (12.1) <70D51643-04AC-3400-8F11-A6FC25985289> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib\n0x7fff9eed2000 -     0x7fff9f178ff7  com.apple.CoreData (120 - 641.3)  /System/Library/Frameworks/CoreData.framework/Versions/A/CoreData\n0x7fff9f46f000 -     0x7fff9f4d5ff7  libsystem_network.dylib (583.50.1)  /usr/lib/system/libsystem_network.dylib\n0x7fff9f4d6000 -     0x7fffa00ffff7  com.apple.AppKit (6.9 - 1404.47)  /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit\n0x7fffa0100000 -     0x7fffa012dfff  libdispatch.dylib (501.40.12)  /usr/lib/system/libdispatch.dylib\n0x7fffa012e000 -     0x7fffa0214ff7  libcrypto.0.9.8.dylib (59.40.2) <2486D801-C756-3488-B519-1AA6807E8948> /usr/lib/libcrypto.0.9.8.dylib\n0x7fffa0215000 -     0x7fffa0220ff7  libChineseTokenizer.dylib (16) <79B8C67A-3061-3C78-92CD-4650719E68D4> /usr/lib/libChineseTokenizer.dylib\n0x7fffa0274000 -     0x7fffa0314fff  com.apple.Metadata (10.7.0 - 972.34)  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata\n0x7fffa0316000 -     0x7fffa0412ff7  libFontParser.dylib (158.6) <267A9AE4-4138-3112-8D73-BDFDC96568FF> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib\n0x7fffa0413000 -     0x7fffa0481ff7  com.apple.ApplicationServices.ATS (377 - 394.4) <9779E916-0788-3CAC-B1EC-F68BCB12A2B6> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS\n0x7fffa05af000 -     0x7fffa17fdfe7  com.apple.CoreGraphics (1.600.0 - 957)  /System/Library/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics\n0x7fffa17fe000 -     0x7fffa181aff3  libresolv.9.dylib (60)  /usr/lib/libresolv.9.dylib\n0x7fffa1852000 -     0x7fffa1b37ffb  com.apple.CoreServices.CarbonCore (1136.2 - 1136.2) <2DBAFC9A-6CD6-351D-B1F4-87D81AA6D640> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore\n0x7fffa1d71000 -     0x7fffa1d79fff  libcopyfile.dylib (127)  /usr/lib/system/libcopyfile.dylib\n0x7fffa1d7a000 -     0x7fffa1d81ff7  libcompiler_rt.dylib (62)  /usr/lib/system/libcompiler_rt.dylib\n0x7fffa1d8a000 -     0x7fffa1dbbfff  com.apple.GSS (4.0 - 2.0)  /System/Library/Frameworks/GSS.framework/Versions/A/GSS\n0x7fffa1dbc000 -     0x7fffa1dd1fff  com.apple.AppContainer (4.0 - 261.40.2)  /System/Library/PrivateFrameworks/AppContainer.framework/Versions/A/AppContainerExternal Modification Summary:\nCalls made by other processes targeting this process:\ntask_for_pid: 0\nthread_create: 0\nthread_set_state: 0\nCalls made by this process:\ntask_for_pid: 0\nthread_create: 0\nthread_set_state: 0\nCalls made by all processes on this machine:\ntask_for_pid: 38227\nthread_create: 0\nthread_set_state: 0VM Region Summary:\nReadOnly portion of Libraries: Total=280.6M resident=0K(0%) swapped_out_or_unallocated=280.6M(100%)\nWritable regions: Total=869.7M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=869.7M(100%)REGION TYPE                        SIZE    COUNT (non-coalesced)\n===========                     =======  =======\nActivity Tracing                  2048K        2\nDispatch continuations            8192K        2\nKernel Alloc Once                    4K        2\nMALLOC                           849.6M      128\nMALLOC guard page                   32K        7\nSTACK GUARD                       56.0M        2\nStack                             8192K        2\nVM_ALLOCATE                          8K        3\n__DATA                            22.6M      383\n__IMAGE                            528K        2\n__LINKEDIT                       101.7M      153\n__TEXT                           178.9M      358\n__UNICODE                          552K        2\nshared memory                       12K        4\n===========                     =======  =======\nTOTAL                              1.2G     1036Model: iMac17,1, BootROM IM171.0105.B08, 4 processors, Intel Core i5, 3.2 GHz, 32 GB, SMC 2.33f10\nGraphics: AMD Radeon R9 M380, AMD Radeon R9 M380, PCIe, 2048 MB\nMemory Module: BANK 0/DIMM0, 8 GB, DDR3, 1867 MHz, 0x859B, 0x435438473353313836444D2E4D313646500\nMemory Module: BANK 0/DIMM1, 8 GB, DDR3, 1867 MHz, 0x859B, 0x435438473353313836444D2E4D313646500\nMemory Module: BANK 1/DIMM0, 8 GB, DDR3, 1867 MHz, 0x859B, 0x435438473353313836444D2E4D313646500\nMemory Module: BANK 1/DIMM1, 8 GB, DDR3, 1867 MHz, 0x859B, 0x435438473353313836444D2E4D313646500\nAirPort: spairport_wireless_card_type_airport_extreme (0x14E4, 0x14A), Broadcom BCM43xx 1.0 (7.21.95.175.1a6)\nBluetooth: Version 4.4.5f3 17904, 3 services, 27 devices, 1 incoming serial ports\nNetwork Service: Wi-Fi, AirPort, en1\nSerial ATA Device: APPLE HDD ST1000DM003, 1 TB\nUSB Device: USB 3.0 Bus\nUSB Device: Bluetooth USB Host Controller\nUSB Device: FaceTime HD Camera (Built-in)\nUSB Device: iPhone\nThunderbolt Bus: iMac, Apple Inc., 28.1", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 23, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 22, 2017", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/97", "issue_status": " Closed\n", "issue_list": [{"user_name": "SuperSandro2000", "datetime": "May 12, 2019", "body": "Ask me for permission to access my micmain.js:469 MediaDevices.getUserMedia() is not supported on your browser.Click the mic iconUsing Chrome 74.0.3729.131", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 13, 2019", "body": "Hello ,Thanks for opening this issue. I've tried to reproduce the issue, the only moment I get an error is when I have already disallowed at first the access to my mic (which is normal). But once I click on the  then check , then refresh the page and the error is gone.Also, did you make sure you have a mic input?", "type": "commented", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 15, 2019", "body": "I don't have such icon on the right of the address bar and on the top left in the drop down menu with permissions is nothing either.I have a mic connected that works flawlessly with Discord.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 18, 2019", "body": "Alright, I think I've found the origin of the problem, it should be related to .Chrome disables features such as  when it comes from an unsecured origin.  is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable .What you can do is:Let me know if that works.", "type": "commented", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 19, 2019", "body": "I tried to get it running with Traefik like all other container I use and damn it was a hassle.First of all your app does not play nice with reverse proxies. I needed to change the leons server port to  or it wouldn't grab . My suggestion is to not grab it from the full URL but from a relative one.Then I had some trouble with the  file and that it always grabbed the  and redirect the calls to  to .Then it wanted to grab  over http.Traefik allows setting this Header only on 2.0 and I don't want to upgrade my reverse proxy to an alpha build. Could you change your code to better support reverse proxies?", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 20, 2019", "body": "I've made a few changes under .Please let me know your thoughts.", "type": "commented", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 20, 2019", "body": "I don't know what the smartest idea is cause I don't do web dev but the  should probably go.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 20, 2019", "body": "Alright, I'll take a look at that.Maybe you can help by contributing? I do not use Docker and Kubernetes that much today.", "type": "commented", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 21, 2019", "body": " Got it working! pr ist here I am looking into the speech stuff hopefully the next days and send PRs or open issues if I encounter new problems.Btw where does leon store its data? We would need to add a volume for that one.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 21, 2019", "body": "Awesome! And yeah as Leon said, be careful to not go for a sleepless night. I sent you an email to ask you a few questions regarding your use case.Regarding the data topic,  should answer your question.I'm gonna close this issue as it has been solved, feel free to open new ones.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "May 21, 2019", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "SuperSandro2000", "datetime": "May 21, 2019", "body": [], "type": "pull", "related_issue": "#99"}, {"user_name": "louistiti", "datetime": "May 21, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "raghavendrajain", "datetime": "Sep 30, 2019", "body": [], "type": "issue", "related_issue": "aiortc/aiortc#209"}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/260", "issue_status": " Closed\n", "issue_list": [{"user_name": "fracpete", "datetime": "Apr 16, 2020", "body": " (Naomi-3.0.M16) fails to set up virtual environment on a RPi2B as user .On a RPi2B based on , the install script  fails to find an  script in the virtual environment it created. However, there is such a script in the  directory.Full output of :Content of :Content of :", "type": "commented", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Apr 16, 2020", "body": "First off thank you for trying Naomi and I would like to apologize for this issue. We recently stumbled across it and actually have a  dealing with said issue along with some other changes and fixes. Let me verify the PR is good to go and runs properly then I will merge in the changes.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 16, 2020", "body": "Peter, so glad to see you. Thank you for checking out our project.That is a strange error that we have been trying to deal with for the last couple of days. The problem seems to be the latest version of virtualenvwrapper. If you can uninstall virtualenvwrapper, then \"pip install --user virtualenv virtualenvwrapper==4.8.4\" that should get virtualenvwrapper up and running again. Alternatively, you can source the activate script directly ($ . ~/.virtualenvs/Naomi/bin/activate) to get into the Naomi virtual environment.If you can, I would love to get some feedback on the latest version of the naomi-setup.sh script, which can be downloaded via \"git clone -b naomi-setup \" and already contains this fix. I've tested it on an up to date copy of Raspbian Buster on a Raspberry Pi, and also on a virtual machine running Ubuntu 18.04. is reporting that he is having an issue installing the current version of mock (4.0.2) on python 3.5.3, so I assume for python < 3.6 you need to use mock 3.0.5 instead, which also seems to run fine on 3.7.3. You don't happen to know any pip-fu for dealing with this sort of situation, do you? I generally try to avoid setting python requirements.txt files to specific versions.Thanks,\nAaron", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "Apr 17, 2020", "body": "I can confirm that the installation was successful. However, with all the compilation going on, it took 8+ hours on my RPi2B running off an SDCard. Is all that compilation intended?In terms of requirements: if you want reproducible builds, using specific versions for libraries is a must. I recently had a problem with opencv-python regularly where the 4.2.x version resulted in , crashing the process at random times. Fixing it to a specific version (earlier) made the problem go away...", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 18, 2020", "body": "With the installation successful, were you able to populate the profile and get a response to a simple query like \"Naomi, what time is it?\"I'll fix the libraries at the version I currently have running, and just try to keep them up to date. The problem I've had is that some people are still running Raspbian Stretch or even Jessie, and the python libraries that work with 3.7 may not work with 3.5 and vice versa. I think we have a requirement for Python 3.5 or above on the website, or at least we used to, so I should be able to focus on those versions. It sounds like the only issue  was having when running Raspbian Stretch was mock, which is a little ironic since it is only part of the unittest system.The compiling process generally takes me about 3 hours on a Raspberry Pi 3B.I am just uploading a new copy of the Naobian image based on Raspbian Buster now so it will run on a Raspberry Pi 4 (which is what I'm currently running it on).One project that I'd love to get to soon would be replacing Phonetisaurus with a G2P engine that is more standard, preferably something that could be installed directly from the repositories. Pocketsphinx and SphinxTrain are both available through apt, so that much time compiling just to get the G2P working seems a little excessive. I'm open to any suggestions. I'm hoping to eventually move beyond G2P dictionaries and train directly against the user's voice, picking up the accent and speaking style of the user.I also recently got Pocketsphinx running on my Raspberry Pi 4 which doesn't produce great results but definitely has promise, considering it's just running against a general model. I'm pretty excited to start training it and see how it does with some additional restrictions.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 18, 2020", "body": "Seems like what pip really needs is a way to select the latest version of a module based on the version of python it is being installed to, or at least a way to specify version information based on python version in the requirements file. Usually people go to great lengths to keep the API consistent between releases, so I wouldn't expect to have to worry about re-writing the calls to a module between versions, just getting the module installed.", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "Apr 26, 2020", "body": "Sorry for the late reply, workload is a bit high at the moment and only found out just now that I need to have a USB microphone for RPi (arrives hopefully end of the week). Will report back once I have a full mic/speaker setup. Will also try to get DeepSpeech going, as the project has made significant progress in terms of speeding things up ().", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 26, 2020", "body": "You need a usb microphone for the raspberry pi (the pi actually uses the microphone connector on the audio out to transmit a video signal, allowing you to use a cable popularized by the Zune player to output RCA type video. This is handy when using the sort of display that is used to add a rear-camera to a car as a Pi monitor, but a pain if you expect to be able to just plug in a headset and record audio).A lot of webcams have an integrated microphone. All the ones I have tried have worked fine as a usb microphone with the Raspberry Pi, and those tend to be a lot more common than an actual usb microphone or usb sound card. You might have one tucked in a drawer.My best usb microphone experience has been a small, usb powered conference phone () which includes both microphone and speakers and hardware audio cancellation that reduces the volume of what Naomi is saying, allowing Naomi to speak its own wake word without activating itself and me to continue to issue commands while Naomi is speaking (this requires setting the currently undocumented \"listen_while_talking\" value to True in profile.yml, or using the \"--listen-while-talking\" flag when launching). Having the speakers and mic using the same cord and not needing external power greatly simplifies the tangle of wires around my Pi.Deepspeech has made significant progress at memory efficiency. It now runs fine on my Raspberry Pi 4B (I tried running it on a Raspberry Pi 3B+ a couple of years ago, but was unable to load the whole language model into memory and without a language model, it simply returned a string of raw phonemes). Running it on the Pi with the standard english models still produces less than stellar results, but I'm excited about writing a STTTrainer for the acoustic model, and also experimenting with customizing the language model using the speechhandler intents templates and possibly reducing the vocabulary.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Aug 30, 2020", "body": "Resolved by downgrading the version of virtualenvwrapper in pythonrequired.txt", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 26, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Aug 30, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/444", "issue_status": " Closed\n", "issue_list": [{"user_name": "mfuntowicz", "datetime": "Jan 11, 2017", "body": "When running multi agents missions we sometime can see that on the second episode, the camera angle changed", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Jan 16, 2017", "body": "Turned out to be a mistake in the client code.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Jan 16, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/767", "issue_status": " Closed\n", "issue_list": [{"user_name": "ileonidmas", "datetime": "Oct 31, 2018", "body": "The agent always has continues movement. Is there a possibility to move the agent exactly one block or turn it exactly 90 degrees?", "type": "commented", "related_issue": null}, {"user_name": "AndKram", "datetime": "Oct 31, 2018", "body": "The mission XML file defines the type of commands in use and these can be Continuous or Discrete. What you want is . For example, the \"Catch the Mob\" example from MarLo: ", "type": "commented", "related_issue": null}, {"user_name": "ileonidmas", "datetime": "Oct 31, 2018", "body": "Thank you for your reply. For some reason, when the movements are discrete, it can only move. It cant attack or rotate the camera. Was it intended this way?", "type": "commented", "related_issue": null}, {"user_name": "AndKram", "datetime": "Oct 31, 2018", "body": "You can have \"deny\" lists but all discrete are: \"move\", \"jumpmove\", \"strafe\", \"jumpstrafe\", \"turn\", \"movenorth\", \"moveeast\",  \"movesouth\", \"movewest\", \"jumpnorth\", \"jumpeast\", \"jumpsouth\", \"jumpwest\", \"jump\", \"look\", \"attack\", \"use\", \"jumpuse\". What example are you looking at / modifying?", "type": "commented", "related_issue": null}, {"user_name": "ileonidmas", "datetime": "Nov 2, 2018", "body": "Thats awesome. Thank you very much. Where did you get all the command list? Can you please give me a link? We would like to know all the commands for absolute movement commands.", "type": "commented", "related_issue": null}, {"user_name": "AndKram", "datetime": "Nov 2, 2018", "body": "The XSD schema lists the commands (). We currently duplicate that in C and Python code so if you create anything that extracts the info (say a python script) that would be useful as we could use it to create the C & Python lists at build time.", "type": "commented", "related_issue": null}, {"user_name": "AndKram", "datetime": "Nov 5, 2018", "body": "Follow up in ", "type": "commented", "related_issue": null}, {"user_name": "AndKram", "datetime": "Nov 5, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/257", "issue_status": " Closed\n", "issue_list": [{"user_name": "DaveyBiggers", "datetime": "Aug 4, 2016", "body": "To help with  and  it would be extremely useful if the frames sent from the Mod included the exact position, pitch and yaw used during the render.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 4, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 4, 2016", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 4, 2016", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 4, 2016", "body": [], "type": "pull", "related_issue": "#259"}, {"user_name": "DaveyBiggers", "datetime": "Aug 5, 2016", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/121", "issue_status": " Closed\n", "issue_list": [{"user_name": "timhutton", "datetime": "Jun 29, 2016", "body": "e.g. third-person camera is super fun for tabular_q_learning.py", "type": "commented", "related_issue": null}, {"user_name": "timhutton", "datetime": "Jun 30, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "timhutton", "datetime": "Jun 30, 2016", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/Project-MONAI/MONAI/issues/4746", "issue_status": " Closed\n", "issue_list": [{"user_name": "binliunls", "datetime": "Jul 22, 2022", "body": "\nRichard's tutorial  has a great application value on deploying segmentation models on video streams. However, trivial works like splitting the video to frames need to be done before inference if videos are given instead of images.  Therefore, a video dataset accepting a video as input and frames of the video as output may be convenient to this use case and enable a fast video inference pipeline.\nA video dataset like other datasets in monai but using a video as input and frames, ordered according to the frame index in the video, as output may be what users would like to be seen.", "type": "commented", "related_issue": null}, {"user_name": "rijobro", "datetime": "Jul 26, 2022", "body": "Hi there,\nI have some datasets (video from file and video from video camera) in a . Feel free to tidy this up (e.g., the transforms need to be lifted out from the dataset) and submit this as a PR. This requires OpenCV.", "type": "commented", "related_issue": null}, {"user_name": "rijobro", "datetime": "Jul 26, 2022", "body": "I have created a PR with the dataset but don't think it should be merged until a tutorial has been created that uses it. Can be based on my tutorial that was referenced at the top of this issue.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Jul 26, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "rijobro", "datetime": "Jul 26, 2022", "body": [], "type": "pull", "related_issue": "#4767"}, {"user_name": "wyli", "datetime": "Jul 29, 2022", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Aug 8, 2022", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Aug 8, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Aug 17, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/249", "issue_status": " Closed\n", "issue_list": [{"user_name": "Felix-Franz", "datetime": "May 4, 2021", "body": "\nIf a picture has no taken timestamp the scanning timestamp will be used.\nSo old images may be displayed higher up than current.\nMostly the timestamp was not set correctly on old camera models and so mostly old pictures are the ones without the taken timestamp.\nTry to use another possible timestamp like the file created timestamp or the modified timestamp if the created timestamp does not exist.\nSince the images are mostly old ones without a taken timestamp it might make sense not to use the scan time as timestamp but a time from the past so that the images are always displayed at the bottom (e.g. 1.1.1900).Screenshots of the Windows file properties\n\n", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jun 18, 2021", "body": "Thanks . Another good suggestion.", "type": "commented", "related_issue": null}, {"user_name": "GyanP", "datetime": "Jul 20, 2021", "body": "Task done and changes pushed on branch ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Aug 8, 2021", "body": "Merged as of . Will be in next release.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jun 18, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jun 18, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jul 12, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jul 12, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "GyanP", "datetime": "Jul 22, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Aug 8, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Aug 8, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/406", "issue_status": " Closed\n", "issue_list": [{"user_name": "zxl777", "datetime": "Nov 20, 2016", "body": "macos ,Release 1.8/usr/bin/python run_mission.py\nDEBUG: Sending MissionInit to 127.0.0.1 : 10000\nDEBUG: Looking for client, received reply from 127.0.0.1: MALMOOK\nWaiting for the mission to start.............................................................................\nvideo,observations,rewards received: 18 9 0\nFrame: 320 x 240 : 3 channels\nvideo,observations,rewards received: 32 9 0\nFrame: 320 x 240 : 3 channels\nlibc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Call to write failed.\nAbort trap: 6crash log", "type": "commented", "related_issue": null}, {"user_name": "zxl777", "datetime": "Nov 24, 2016", "body": "crash on MalmoPython.so , how to fix ?", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Nov 24, 2016", "body": "I suspect this is something to do with ffmpeg. I can reproduce this manually by killing the ffmpeg process mid-mission.\nDo you get an  file in your samples folder after trying to run run_mission.py? That might contain some helpful information.Other things to try:\nDoes it work if you comment out this line: ?\nIf so, that definitely indicates a problem with ffmpeg.Do you definitely have write permission in your samples folder? And is there room on the hard drive for storing the mpeg?", "type": "commented", "related_issue": null}, {"user_name": "zxl777", "datetime": "Nov 25, 2016", "body": " Thank you!\nI have successfully solved this problem.\nI follow your prompts, I finally found python implementation of the wrong ffmpeg version.ffmpeg.out  : Wrong ffmpeg versionffmpeg.out  : Correct ffmpeg version", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Nov 25, 2016", "body": "Great! Really glad you got this sorted.\nCheers.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Nov 25, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/202", "issue_status": " Closed\n", "issue_list": [{"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": "When loading in CR3 images from my Canon 90D I'm getting a TypeError.Steps to reproduce the behavior:It should render a thumbnail and run the algorithm against it successfully. In this case it throws the TypeError and leaves a blank square on the gallery tiles where thumbnail generation failed.I'm browsing on Firefox 86 64-bit on Windows 10 1909 and the docker image was run on a Debian 10 instance under Hyper-V.Not Applicable (See Above)Not ApplicableNot Applicable", "type": "commented", "related_issue": null}, {"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": "Github doesn't like large uploads so I've put the test files . You're welcome to use them in test scripts as long as the project remains open source.", "type": "commented", "related_issue": null}, {"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": "I pulled the version of  from the image and it's v9.28, which is when CR3 support was added (v9.27 did not support it and is what's in some package managers still), so it's not an issue there.Following the code into  I'm seeing it checking the dimensions, but when I run exiftool on the file that was output by  it's showing a JPEG format error.I think the fault is around here is that the Canon CR3 format doesn't like having the embedded thumbnail extracted.  that they specifically won't release a codec for their thumbnail format. can be used to generate a jpeg but it will increase overhead quite a good deal... here's  vs  on the same test files I uploaded.The other issue is it doesn't pull the tags by default, but this should provide them in the output.This is less than ideal due to the overhead but it will provide support for modern Canon cameras since the 90D doesn't support CR2.", "type": "commented", "related_issue": null}, {"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": "Opened  with a fix.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 8, 2021", "body": "Hi , thank you very much for your contribution. This is quite interesting and I will try it soon. Are you saying that even even the latest version of dcraw won't process CR3 raw files? Generally dcraw is used to try and extract the JPEG if there is one and then if it can't, it tries alternative methods. I didn't know exiftool could convert raw to JPEG. Do you know if it's extracting from a thumbnail or actually processing the raw data?", "type": "commented", "related_issue": null}, {"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": "As of today it doesn't. It looks like this is due to some proprietary codec nonsense from Adobe. There is  by people to parse the CR3 format and this may lead to it being included in dcraw in the future, but the version currently available doesn't support it. You can try it yourself on the sample file I provided, I'd be interested to see if your output differs.Grabbing the latest source from the author's page also shows it as v9.28 and it hasn't been updated since 2018, so I'm certain this is the latest available.It's without a doubt processing it to convert to JPEG rather than extracting a thumbnail, which explains the overhead. There's probably ways to extract the thumbnail using the canon_cr3 instructions, but I think that might be too far into the weeds to dig into the proprietary format for this project.For me the patch in  will be enough to make it work, I'll be spending far more CPU and memory running the recognition algorithms against the images so I'm not as concerned about it, but I understand if it doesn't fit into your vision for the project. There might be value in pulling the MIME type or other properties for each image being processed to expand support in the future.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 25, 2021", "body": "Merged as of . Thanks so much  - very useful.", "type": "commented", "related_issue": null}, {"user_name": "TaylorBurnham", "datetime": "Mar 8, 2021", "body": [], "type": "pull", "related_issue": "#203"}, {"user_name": "damianmoore", "datetime": "Mar 8, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 8, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 23, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 25, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Mar 25, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/196", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Feb 21, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "GyanP", "datetime": "Apr 2, 2021", "body": "Task done and changes pushed on branch ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 17, 2021", "body": "Closed in favour of PR ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 21, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 1, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 1, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 22, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 17, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Apr 17, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/193", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Feb 20, 2021", "body": "A  can have multiple  if they were taken at the same time. The reason for there being more than one file could be that the camera outputs a raw file as well as a JPEG for each photo or the user has made edited versions in a program like Light Room or Darktable. If a  has multiple  then the metadata view in the  component needs to show a select box for versions as seen in this mockup. The version heading and select box should not show if there is only one  for the .If a different version is selected by the user then this needs to be saved as the  of the  model and the photo being displayed needs to be changed to the selected one. There is a new URL  that will generate the resized version of the image to be downloaded. If you save the  and then re-fetch the same URL it should give you the new image automatically.To test this you will need to copy one of the files as a differnt name and modify it so you can visually identify it as being different (e.g. using GIMP).AC:", "type": "commented", "related_issue": null}, {"user_name": "GyanP", "datetime": "Apr 2, 2021", "body": "Task done and changes pushed on branch ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": "Merged as of ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 20, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 20, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 1, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 1, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 5, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 5, 2021", "body": [], "type": "issue", "related_issue": "#211"}, {"user_name": "damianmoore", "datetime": "Apr 18, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Apr 18, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/79", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "May 3, 2019", "body": "Width and height values stored in DB against a photo are sometimes switched around. This is apparent when portrait photos from Canon cameras are uploaded as the width is shown as wider than the height. This is also visible when viewing an object recognition bounding box in the PhotoDetail component.The rotation information about rotations is currently used in thumbnail generation. This also needs to be used to calculate the dimensions when added to the DB.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 3, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 3, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 3, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/63", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Jan 20, 2019", "body": "Looking at my photos from Iceland, quite a few are tagged as Norway, Russia and Sweden. See if this is an actual bug or inaccurate GPS positions from the camera.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 25, 2019", "body": "Think this problem is a horizontal flip around Zero degrees longitude. A point showing in Norway should actually show as Iceland. These are about the same distance horizontally from London.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 20, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 20, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 20, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 25, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/30", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Jan 2, 2019", "body": "If a user uploads a raw photo, we will need a JPEG generated of it at full size so we can use it as a source to generate all other thumbnails. There might already be a JPEG with the same timestamp/number but we can't be sure that it was generated by the camera or by program at a later date.  might be able to process all the raw formats we need but it could take a bit of tweaking to get acceptable results. A notice will display when viewing the \"raw\" in detail explaining that we generated the image to be helpful. The processed resulting JPEG should be stored separately from the main photo collection as we don't want to pollute the user's photo folder.AC:", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 21, 2019", "body": "For raw processing first get the dimensions with .Then try this to extract camera-generated JPEG/PPM \"thumbnail\". It should be about the same dimensions as exiftool says (give or take a few percent).If the dimensions are not the same, exiftool cannot find any dimensions or no thumbnail can be extracted from the raw file, use the following.Try to use embedded profile if one exists.Generate with the camera's embedded white balance.If the dimensions of the last 2 commands are about the same as the  image then use the  extracted thumbnail image.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 7, 2019", "body": "Implemented as of ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 2, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 2, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 5, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 7, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/167", "issue_status": " Closed\n", "issue_list": [{"user_name": "EduFdezSoy", "datetime": "Jan 17, 2021", "body": "Error when loading the filters \nThe filters does not load (but looks like the response has them all).", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 22, 2021", "body": " if I understand correctly then you have an exposure that has a decimal point in it? Something like 1/3.7 seconds. I'll add a fix for this. Guess it's a bad assumption. Thanks for reporting.", "type": "commented", "related_issue": null}, {"user_name": "EduFdezSoy", "datetime": "Jan 22, 2021", "body": "i dont really know neither know how to check itAre those values on the db? I can take a look if so", "type": "commented", "related_issue": null}, {"user_name": "EduFdezSoy", "datetime": "Jan 24, 2021", "body": "This is what I found\n", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 24, 2021", "body": "Thanks  for investigating this. Seems like a sensible value for exposure. We'll get on with fixing this.", "type": "commented", "related_issue": null}, {"user_name": "EduFdezSoy", "datetime": "Jan 17, 2021", "body": [], "type": "issue", "related_issue": "#168"}, {"user_name": "damianmoore", "datetime": "Jan 22, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 22, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 24, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 24, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "EduFdezSoy", "datetime": "Jan 24, 2021", "body": [], "type": "pull", "related_issue": "#173"}, {"user_name": "damianmoore", "datetime": "Jan 25, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 25, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Jan 25, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/49", "issue_status": " Closed\n", "issue_list": [{"user_name": "AIIX", "datetime": "Feb 26, 2021", "body": "Gstreamer Plugins Bad seems to be missing on image and is required for hw accel playback and raspi camera via v4l2 interface", "type": "commented", "related_issue": null}, {"user_name": "j1nx", "datetime": "May 5, 2021", "body": "Gstreamer Plugins Bad is added within OVOS. Camera is not yet working, but possibly related to hardware drivers.Closing this one, in favor of a newly created Issue about it later on in time, when we get to it.", "type": "commented", "related_issue": null}, {"user_name": "AIIX", "datetime": "Feb 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "j1nx", "datetime": "May 5, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/4", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Dec 29, 2018", "body": "Detail view of photo showing camera attributes, tags, mini map, bounding boxes from object detection.", "type": "commented", "related_issue": null}, {"user_name": "Soharic", "datetime": "Jan 4, 2019", "body": "A back button to get from an enlarged photo back to the main interface would save having to use the browser controls.Once I've narrowed down my search and opened an image, perhaps also a left/right function to slide through the photos selected.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 4, 2019", "body": "Good suggestions  :)", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 22, 2021", "body": " It would be great if you could help finish off these last couple of points.Display the path at the bottom of the metadata panel:Add a link to show more metadata:One the \"More details\" link is clicked, call new GraphQL query that runs the command  on the 's  and returns all exiftool's output. Display all of exiftool's output as a table inside the  component, similar to how settings or the onboarding screens are displayed, over the photo detail screen.", "type": "commented", "related_issue": null}, {"user_name": "GyanP", "datetime": "Apr 2, 2021", "body": "Task done and changes pushed on branch  ", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 5, 2021", "body": "Implemented as of . A decision was made for simplicity reasons to display extra metadata as expandable area as it was hard to display a modal on top of a modal.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Dec 29, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Dec 31, 2018", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jan 4, 2019", "body": [], "type": "issue", "related_issue": "#41"}, {"user_name": "damianmoore", "datetime": "Jan 4, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 18, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 22, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 4, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 5, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Apr 5, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/GeneralMills/pytrends/issues/421", "issue_status": " Closed\n", "issue_list": [{"user_name": "dongskyler", "datetime": "Jul 19, 2020", "body": "First, thank you for writing such a Google trend scrapper tool. It's useful.Recently, my friend and I utilized Pytrends to scrape Google trend data of 3 keywords by month over a few years. To meet our needs, we wrote some new features on top of Pytrends.If the community is interested, I'd like to incorporate the following features into Pytrends to benefit the community:The following code also contains other features (that the community might be less interested in):The following is the code we wrote, to scrape Google trends of 3 keywords (\"Canon camera\", \"Nikon camera\", \"Sony camera\") on a monthly basis from January 2006 to February 2020.", "type": "commented", "related_issue": null}, {"user_name": "dongskyler", "datetime": "Aug 21, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photonixapp/photonix/issues/67", "issue_status": " Closed\n", "issue_list": [{"user_name": "damianmoore", "datetime": "Mar 7, 2019", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Mar 19, 2020", "body": "I just tried (rather naively I must admit) to install photonix on an RP4 of mine and failed (well, installationion went smooth I believe but running it fails).Thought photonix seemed like an awesome choice to use for my RP4 based image collection, especially since I'm going to use some ML on that image collection to experiment. :)DId not find any good contact info nor fixes for the issue I ran into so I'll just post this comment to say I believe Photonix seem very promising.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 19, 2020", "body": "Thanks for having a try . I hope to have a play with getting it running on a Raspberry Pi in the next few months but I just have to crack on with features right now.If you have an error log, please do share it here. I wouldn't expect it to do much at the moment tbh as each Docker image has to be compiled for each CPU architecture, I believe.Hopefully this project can suit your needs in the not-to-distant future. If you want to be notified when v1.0 is available please sign up to the mailing list on  .", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Mar 20, 2020", "body": "", "type": "", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Mar 20, 2020", "body": "The log in question:", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Mar 20, 2020", "body": "And this is how far I got building it. Stopper seems to be Tensorflow not supporting Arm architecture (or if it does, the references are wrong). FSEvents also seem to lack adequate platform support but did only warn about it.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Mar 20, 2020", "body": "Thanks for sending the logs and trying to build, . Seems like the build got pretty far. You might want to try removing the line in  that uses  to replace the standard tensorflow version with my own custom build. This is mainly for older x86-based systems like my home server. That way  may find an official build of tensorflow for armhf.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": " I've made some good progress so far but am not quite there. See this branch for latest developments: I managed to build Tensorflow in my related project here: Currently I'm using a new Dockerfile and an experimental feature of Docker called  which allows cross-compilation. This is the command I'm using:Installing Python dependencies on Arm is very slow because there are not many binary Wheel packages available. I've sped up my Docker build time by storing cached  packages on my own custom PyPi server.Seems like an error building matplotlib/numpy with Fortran compilter but I made sure to install .Any suggestions very much appreciated as I'm a bit stuck right now. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Nov 30, 2020", "body": "  Thank's for making an effort, appreciated! No spontanious suggestions, but I'll give it a go at some point, for sure! Since it's a lovely piece of software it would be awesome to get it up and running but I am by no means dependent on it.\nWhenever I try I'll update any findings here.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Dec 1, 2020", "body": "A small update - mainly for future me. The problem is with building Numpy. I tried using the Debian package  but it's built for a different version of Python and Tensorflow is very specific about what version of Numpy it needs anyway. Looks like I'll have to figure out what's missing to build Numpy.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Dec 1, 2020", "body": "Fixed the errors building Numpy. The problem was in my script  and  was working fine. I was accidentally wiping out all environment variables when running  in a .Next problems to solve:", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "Feb 15, 2021", "body": "I haven't really tried the Raspberry-Pi branch of Photonix, but I tried the Master branch today. While it fails with the wrong platform for Tensorflow it is possible to go beyond that by using community builds and manually install it. This way I managed to go beyond the Tensorflow parts as well (I used a version from here: )Currently I'm getting stuck at a SciPy dependency as far as I can see:", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 15, 2021", "body": "I have an image that's generally working. You can try it if you have an ARMv7 Raspberry Pi (such as my Pi2 Model B). It starts up, imports demo photos. The main downsides are:I should be able to disable certain analysis jobs when memory is tight but allow then to be switched on again through the UI. Fixing the Freetype dependency should just be a case of adding  and  apt dependencies in the builder image.Anyway this is the  that works for me on my Pi2 Model B (using  cross-compiled Docker image on DockerHub):", "type": "commented", "related_issue": null}, {"user_name": "Felix-Franz", "datetime": "Feb 21, 2021", "body": "I tested it on the Raspberry pi 4 and it worked just fine, besides the import.\nBut I'll get another error (I'm not really sure if this is because of the armhf image...)", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 21, 2021", "body": "Thanks for the update . I actually just got a Raspberry Pi 4 myself and yesterday I confirmed that the armhf/ARMv7 32-bit image works fine on it. For extra performance we should get the ARMv8 64 bit version working but the 32 bit version imports and runs the demo gallery.I think you have discovered a separate bug in Photonix though. You must have a photo that has 'Camera Model Name' in it's metadata but no 'Make'. The import code assumes that there must be a camera make if there's a camera model. I'll create a bug ticket for that. For now maybe you can try some different test images.", "type": "commented", "related_issue": null}, {"user_name": "darrepac", "datetime": "Feb 23, 2021", "body": "I have used the docker-compose for Pi.\nIndeed this time it works at least in the terminal: it is importing my 250GB photos...but I have nothing on localhost:8888, just an error", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 23, 2021", "body": "Hi . Could it be a networking or firewall problem? Are you using a browser on the Raspberry Pi or across the network? If over a network then the URL won't be localhost but the IP/hostname of the Raspberry PI. Sorry if this is obvious - just trying to eliminate any simple fixes. If possible can you run this on the Pi itself?:", "type": "commented", "related_issue": null}, {"user_name": "darrepac", "datetime": "Feb 23, 2021", "body": "I think my issue is on the performance side.... too much things running, too much photo to crawl... will try to step down", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 23, 2021", "body": " OK, sounds possible. With the  environment variable set in the  above you should get the same small number of photos as is on  which might be good to compare with. If all looks good with that then try with some batches of your own photos.", "type": "commented", "related_issue": null}, {"user_name": "darrepac", "datetime": "Feb 23, 2021", "body": "I started again with only the demo files... took quite some time to have something on the web side and got quite some errors in the terminal. Normal?photonix    |     utility.execute()\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 395, in execute\nphotonix    |     self.fetch_command(subcommand).run_from_argv(self.argv)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 244, in fetch_command\nphotonix    |     klass = load_command_class(app_name, subcommand)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 37, in load_command_class\nphotonix    |     module = import_module('%s.management.commands.%s' % (app_name, name))\nphotonix    |   File \"/usr/local/lib/python3.8/importlib/.py\", line 127, in import_module\nphotonix    |     return _bootstrap._gcd_import(name[level:], package, level)\nphotonix    |   File \"\", line 1014, in _gcd_import\nphotonix    |   File \"\", line 991, in _find_and_load\nphotonix    |   File \"\", line 975, in _find_and_load_unlocked\nphotonix    |   File \"\", line 671, in _load_unlocked\nphotonix    |   File \"\", line 783, in exec_module\nphotonix    |   File \"\", line 219, in _call_with_frames_removed\nphotonix    |   File \"/srv/photonix/photos/management/commands/classification_location_processor.py\", line 3, in \nphotonix    |     from photonix.classifiers.location import LocationModel, run_on_photo\nphotonix    |   File \"/srv/photonix/classifiers/location/.py\", line 1, in \nphotonix    |     from .model import LocationModel, run_on_photo\nphotonix    |   File \"/srv/photonix/classifiers/location/model.py\", line 6, in \nphotonix    |     import matplotlib.path as mpltPath\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/matplotlib/.py\", line 205, in \nphotonix    |     _check_versions()\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/matplotlib/.py\", line 190, in _check_versions\nphotonix    |     from . import ft2font\nphotonix    | ImportError: numpy.core.multiarray failed to import\nphotonix    | 2021-02-23 18:32:45,062 INFO exited: classification_location_processor (exit status 1; not expected)\nphotonix    | 2021-02-23 18:32:48,120 INFO spawned: 'classification_location_processor' with pid 208\nphotonix    | RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\nphotonix    | Traceback (most recent call last):\nphotonix    |   File \"/srv/photonix/manage.py\", line 22, in \nphotonix    |     execute_from_command_line(sys.argv)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 401, in execute_from_command_line\nphotonix    |     utility.execute()\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 395, in execute\nphotonix    |     self.fetch_command(subcommand).run_from_argv(self.argv)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 244, in fetch_command\nphotonix    |     klass = load_command_class(app_name, subcommand)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/core/management/.py\", line 37, in load_command_class\nphotonix    |     module = import_module('%s.management.commands.%s' % (app_name, name))\nphotonix    |   File \"/usr/local/lib/python3.8/importlib/.py\", line 127, in import_module\nphotonix    |     return _bootstrap._gcd_import(name[level:], package, level)\nphotonix    |   File \"\", line 1014, in _gcd_import\nphotonix    |   File \"\", line 991, in _find_and_load\nphotonix    |   File \"\", line 975, in _find_and_load_unlocked\nphotonix    |   File \"\", line 671, in _load_unlocked\nphotonix    |   File \"\", line 783, in exec_module\nphotonix    |   File \"\", line 219, in _call_with_frames_removed\nphotonix    |   File \"/srv/photonix/photos/management/commands/classification_location_processor.py\", line 3, in \nphotonix    |     from photonix.classifiers.location import LocationModel, run_on_photo\nphotonix    |   File \"/srv/photonix/classifiers/location/.py\", line 1, in \nphotonix    |     from .model import LocationModel, run_on_photo\nphotonix    |   File \"/srv/photonix/classifiers/location/model.py\", line 6, in \nphotonix    |     import matplotlib.path as mpltPath\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/matplotlib/.py\", line 205, in \nphotonix    |     _check_versions()\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/matplotlib/.py\", line 190, in _check_versions\nphotonix    |     from . import ft2font\nphotonix    | ImportError: numpy.core.multiarray failed to import\nphotonix    | 2021-02-23 18:33:06,459 INFO exited: classification_location_processor (exit status 1; not expected)\nphotonix    | 2021-02-23 18:33:07,463 INFO gave up: classification_location_processor entered FATAL state, too many start retries too quickly\nphotonix    | Traceback (most recent call last):\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/promise/promise.py\", line 489, in _resolve_from_executor\nphotonix    |     executor(resolve, reject)\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/promise/promise.py\", line 756, in executor\nphotonix    |     return resolve(f(*args, **kwargs))\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/graphql/execution/middleware.py\", line 75, in make_it_promise\nphotonix    |     return next(*args, **kwargs)\nphotonix    |   File \"/srv/photonix/accounts/schema.py\", line 116, in resolve_after_signup\nphotonix    |     if user.has_configured_image_analysis:\nphotonix    |   File \"/usr/local/lib/python3.8/site-packages/django/utils/functional.py\", line 225, in inner\nphotonix    |     return func(self._wrapped, *args)\nphotonix    | graphql.error.located_error.GraphQLLocatedError: 'AnonymousUser' object has no attribute 'has_configured_image_analysis'\nphotonix    |\nphotonix    | 192.168.0.90 - - [23/Feb/2021:18:36:49 +0000] \"POST /graphql HTTP/1.1\" 200 146 \"\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"\nphotonix    | 192.168.0.90 - - [23/Feb/2021:18:36:49 +0000] \"POST /graphql HTTP/1.1\" 200 185 \"\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 23, 2021", "body": "Hi  thanks for the update. That is about what I would expect. The  is known to be broken on ARM and is the last piece of the puzzle I think, just need to get  installed to work with this version of . I think the GraphQL error just shows until the user is properly logged in. Do you get photos displaying correctly and analysed (apart from location)?", "type": "commented", "related_issue": null}, {"user_name": "darrepac", "datetime": "Feb 23, 2021", "body": "photos seems to display slowly (get 2 for the moment)...", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 23, 2021", "body": "Which Raspberry Pi version are you running ?", "type": "commented", "related_issue": null}, {"user_name": "darrepac", "datetime": "Feb 23, 2021", "body": "Raspberry 3B", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Feb 26, 2021", "body": " Would be interesting to know if it's pegged at 100% CPU or if RAM is full or if disk IO is the bottleneck. I find that which SD card I use affects things drastically. You can create a swap file on disk if you are out of RAM.", "type": "commented", "related_issue": null}, {"user_name": "Felix-Franz", "datetime": "Apr 24, 2021", "body": "I get a similar error as .\nIn my case, the CPU and RAM were rather lightly used.\nPhotonix is running on a Raspberry Pi 4B (8GB) with Raspberry Pi OS (64 bit).\nBecause of the error no pictures are imported.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Apr 25, 2021", "body": "Thanks for the comment . I am at a similar point from my side. This I think is the key error:I found out that pip compiles each package it needs to in an isolated environment which means Matplotlib gets built against a different version of Numpy to that in our  file. It gets the latest Numpy version but we need a slightly older one as required by Tensorflow. I think this is the most hopeful source of information for going forwards: So far I've tried the suggestion of  and adding a   but no luck yet.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 2, 2021", "body": "This issue is now complete as of  (arm32v7 and arm64v8). Builds are also automated through GitHub Actions whenever a new version is tagged and pushed. I've also moved to a new Docker Hub organisation at  . , ,  it would be good if you could have a little test (with a small number of photos to start with).", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "May 3, 2021", "body": " - This is exciting news! I'll have a go at it as soon as I can, for sure! Great work!", "type": "commented", "related_issue": null}, {"user_name": "Felix-Franz", "datetime": "May 4, 2021", "body": " it just worked find with a smaller number of photos, so I tried a larger number.\nIt really works well, but also consumes a lot of resources for image processing \n\n", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 4, 2021", "body": "Thanks  for giving it a go and reporting back. I'm merging another feature at the moment to give control over which analysers are running which should be useful for scaling down all that CPU usage from the UI.", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "May 6, 2021", "body": "Well, I almost made it. Not as successful as  though.\nGot it to install and run but only as far as the onboarding process goes. After that, importing photos does not seem to work. Photonix reacts when putting an image in the appropriate folder but does not seem to process it (ONE image shouldn't take too much time right?).Which leads me back to some messages I briefly noted when launching Photonix (which worked \"to the end\" now, thanks to all efforts made by you  .  Some snippets from installation messages:(And then Photonix continued with the onboarding process)Please note that I have NOT tried to do any kind of troubleshooting, I just went through the photonix instalation instructions after checking that I had docker and docker-compose properly installed. So no need to go trying to find issues in Photonix based on this post - It's just me reporting back my first run of the new version. I will have to look at it myself, but must stop now for other activites. I do have a couple of notes just by looking at the logs:", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "May 6, 2021", "body": "Well, now I've experimented some more. It works mostly, so no problem with the overall functionality.\nTried to add more images than one and tried with elevation. Most images get processed but not all. Some gets wrong thumbnails and most wrong classification. Latter might be more of a tensorflow thing.Again, great work and thanks for the effort. Photonix seems awesome now that I have it running - finally! I have to dig into the classification parts more though. That's where I have my main interest tbh.", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 7, 2021", "body": "OK , thanks for testing. Good to know that it's generally working. Tensorflow failures (or any classification failures shouldn't affect thumbnails so I'm a bit confused about that.", "type": "commented", "related_issue": null}, {"user_name": "jensbrak", "datetime": "May 9, 2021", "body": " Nah, don't believe classification/tensorflow issues are related to thumbnail issues here. More likely it's due to me interrupting execution of docker at a bad timing or something, when it comes to the thumbnails. Classification works in general (the errors noted may or may not be relevant, haven't got a clue yet) but gives some hillarious results depending on what kind of pictures I feed it.TL;DR: Photonix works 'just fine' on RPi4 now. Some notable errors/warnings when installing and launching that might or might not impact functionality. Some weird thumbnail issue of oncertain origin and some classification issues likely connected to the training data.", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Jul 13, 2021", "body": "I am having the same error, photonix said during the installation that no photos were detected but they are in the /data/photos folder and visible in my container. Did you get it working in the end ?", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Jul 13, 2021", "body": "Ok, to follow up, I have about 5000 photos in the mounted directory. After about 10 minutes of completing the photonix installation my pi 4 8gb froze. I waited about 30 minutes and it came back to life and photonix was scanning. Both CPU and RAM are maxed out. So it seems that patience is key! Thanks for the great project here!", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Jul 13, 2021", "body": "Well it froze again unfortunatly. Seems that I am running out of RAM.", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Jul 13, 2021", "body": "Would there be a way to limit the number of workers so as to avoid eating up all of my RAM ?", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Jul 13, 2021", "body": "Hi , glad it's getting a bit further for you. I have some optimisation work I'm planning at the moment to make sure it doesn't run out of RAM. The analysers are set to only one worker each so can only really scale down to 'off'.For now I would recommend going to settings in the top-right of the UI and turning off most of the analysers off to start with. Jobs will still be queued for photos that are imported so you can turn them on one-at-a-time once importing has completed.Sorry it's a bit hacky on low memory machine's right now but not for much longer, hopefully.", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Jul 13, 2021", "body": "Thanks for the tips! Also, quick question. Whenever I turn on the option to watch the library for new photos it gets turned back off again, is that normal ?", "type": "commented", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "damianmoore", "datetime": "Nov 30, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#196"}, {"user_name": "damianmoore", "datetime": "May 2, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "damianmoore", "datetime": "May 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/GeneralMills/pytrends/issues/412", "issue_status": " Closed\n", "issue_list": [{"user_name": "nuertey", "datetime": "Jun 14, 2020", "body": "Anyone know how to properly and efficiently parse and print the Google Trends Categories listing, the result of executing TrendReq().categories()? It seems to be a dictionary of a list of a dictionary of a list and so on, nested several levels deep. Examples of the output are below.I can get at the topmost level alright (as indicated in the second output below) but I want to learn the correct way (Pythonic?) to parse, print and use the complete list. Either parsing it into one complete  dataframe of 'name' and 'id' columns and/or printing it similar to  would do. Perhaps with list comprehensions? Recursion maybe? A combination thereof?\nThank you\nNuertey", "type": "commented", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 15, 2020", "body": "To answer my own question, I was able to accomplish this with recursion. So I am closing this issue.", "type": "commented", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 20, 2020", "body": "I am reopening this issue as I realize that even with the way that I am successfully recursively parsing the Google Trends Categories, the order in which the elements display do not seem quite correct. Either the sub-categories (or children) are displayed ahead of the major categories, or when reversed the whole list is displayed in the wrong alphabetical order (descending). Surely someone has tried to parse and view the Google Trends Categories before and has some insights to share?So, anyone know how to correctly parse and display the Google Trends Categories list much like this: \nThank you\nNuertey", "type": "commented", "related_issue": null}, {"user_name": "tpilkati", "datetime": "Jun 21, 2020", "body": "Hi I made a version that transforms the result of TrendReq().categories() in a string exactly as in the link \nIf you need to put the subcategories in a dataframe you just have to replace the part 'text = text + ..'  with something that appends to the dataframe the way you needthere's also some code to transform it into a json or to a html", "type": "commented", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 21, 2020", "body": "Hi \nExcellent! This is precisely what I needed. Thank you a million.\nNuertey", "type": "commented", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 15, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 20, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "nuertey", "datetime": "Jun 21, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/2998", "issue_status": " Closed\n", "issue_list": [{"user_name": "Arghya999", "datetime": "Feb 20, 2019", "body": ":0.12.3: Python 3.6.8 (windows, osx, ...):Ubuntu 16.04:  python -m rasa_core.run -d models/dialogue -u models/nlu/default/current --endpoints endpoints.yml --debug --enable_api\n2019-02-20 10:58:16 INFO     root  - Rasa process starting\nTraceback (most recent call last):\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n\"\", mod_spec)\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/rasa_core/run.py\", line 244, in \n_endpoints = AvailableEndpoints.read_endpoints(cmdline_args.endpoints)\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/rasa_core/utils.py\", line 660, in read_endpoints\nendpoint_file, endpoint_type=\"nlg\")\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/rasa_core/utils.py\", line 598, in read_endpoint_config\ncontent = read_yaml_file(filename)\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/rasa_core/utils.py\", line 368, in read_yaml_file\nreturn read_yaml_string(read_file(filename))\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/rasa_core/utils.py\", line 385, in read_yaml_string\nreturn yaml_parser.load(string)\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/ruamel/yaml/main.py\", line 331, in load\nreturn constructor.get_single_data()\nFile \"/home/arghya/anaconda3/envs/myenv/lib/python3.6/site-packages/ruamel/yaml/constructor.py\", line 106, in get_single_data\nnode = self.composer.get_single_node()\nFile \"_ruamel_yaml.pyx\", line 706, in _ruamel_yaml.CParser.get_single_node\nFile \"_ruamel_yaml.pyx\", line 724, in _ruamel_yaml.CParser._compose_document\nFile \"_ruamel_yaml.pyx\", line 775, in _ruamel_yaml.CParser._compose_node\nFile \"_ruamel_yaml.pyx\", line 889, in _ruamel_yaml.CParser._compose_mapping_node\nFile \"_ruamel_yaml.pyx\", line 775, in _ruamel_yaml.CParser._compose_node\nFile \"_ruamel_yaml.pyx\", line 891, in _ruamel_yaml.CParser._compose_mapping_node\nFile \"_ruamel_yaml.pyx\", line 904, in _ruamel_yaml.CParser._parse_next_event\nruamel.yaml.scanner.ScannerError: while scanning a simple key\nin \"\", line 7, column 3\ncould not find expected ':'\nin \"\", line 8, column 3As per , the domain file is valid. (if used & relevant):actions:intents:slots:\nPERSON:\ntype: text\ncustomer_location:\ntype: text\nmodel:\ntype: text\nshop_location:\ntype: text\nworkshop_location:\ntype: text\nbattery:\ntype: text\ncamera:\ntype: text\nprocessor:\ntype: text\ndisplay:\ntype: text\noperating_system:\ntype: text\nprice:\ntype: texttemplates:\nutter_CEO_info:utter_CustomerCare_info:utter_Official_Website:utter_about_OPPO:utter_brand_info:utter_check_another_one:utter_city_name:utter_customer_location_confirm:utter_deny:utter_did_that_help:utter_goodbye:utter_greet:utter_happy:utter_mood_affirm:utter_mood_deny:utter_more_info:utter_provide_founder:utter_thanks:utter_unclear:utter_model_name_lookout:utter_model_name_confirm:Content of endpoint.yml\n......................................................\naction_endpoint:\nurl: \"\"\ntracker_store:\nstore_type: mongod\nurl: mongodb://mongo:27017\ndb: rasa\nusername:none\npassword:none#please note that  the indentation of the domain  file is  getting ruined as soon  as i am  pasting the content in  the domain  section.I have checked  the  indentation in  and getting valid response.", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Feb 20, 2019", "body": "Thanks for raising this issue,   will get back to you about it soon.", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Feb 20, 2019", "body": "Hi . It's not your  that's getting errored, it's your  (you'll notice in the Traceback that the issue is happening in ). If you put your  through yamllint, you get the same error. The issue seems to be not having spaces after your colons for  and . This fix should work:Content of endpoint.ymlP.S. For future reference, you canin your code with three ticks (```) above and below the code. This is helpful for legibility of stack traces as well as things like domain files ", "type": "commented", "related_issue": null}, {"user_name": "Arghya999", "datetime": "Feb 20, 2019", "body": "Hi  , I missed that completely.Thanks a lot for helping me out.", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Feb 20, 2019", "body": "Happy to help!", "type": "commented", "related_issue": null}, {"user_name": "harshalDharpure", "datetime": "Aug 8, 2022", "body": "(chatbotenv) C:\\Users\\HARSHAL\\mybot.sipnabot>rasa train\nTraceback (most recent call last):\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\runpy.py\", line 193, in .py\", line 91, in main\ncmdline_arguments.func(cmdline_arguments)\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\rasa\\cli\\train.py\", line 61, in train\nconfig = _get_valid_config(args.config, CONFIG_MANDATORY_KEYS)\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\rasa\\cli\\train.py\", line 170, in _get_valid_config\nmissing_keys = missing_config_keys(config, mandatory_keys)\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\rasa\\cli\\utils.py\", line 67, in missing_config_keys\nconfig_data = rasa.utils.io.read_config_file(path)\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\rasa\\utils\\io.py\", line 188, in read_config_file\ncontent = read_yaml(read_file(filename))\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\rasa\\utils\\io.py\", line 124, in read_yaml\nreturn yaml_parser.load(content) or {}\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\ruamel\\yaml\\main.py\", line 343, in load\nreturn constructor.get_single_data()\nFile \"C:\\Users\\HARSHAL\\anaconda3\\envs\\chatbotenv\\lib\\site-packages\\ruamel\\yaml\\constructor.py\", line 111, in get_single_data\nnode = self.composer.get_single_node()\nFile \"_ruamel_yaml.pyx\", line 706, in _ruamel_yaml.CParser.get_single_node\nFile \"_ruamel_yaml.pyx\", line 724, in _ruamel_yaml.CParser._compose_document\nFile \"_ruamel_yaml.pyx\", line 775, in _ruamel_yaml.CParser._compose_node\nFile \"_ruamel_yaml.pyx\", line 891, in _ruamel_yaml.CParser._compose_mapping_node\nFile \"_ruamel_yaml.pyx\", line 904, in _ruamel_yaml.CParser._parse_next_event\nruamel.yaml.parser.ParserError: while parsing a block mapping\nin \"\", line 3, column 1\ndid not find expected key\nin \"\", line 13, column 1(chatbotenv) C:\\Users\\HARSHAL\\mybot.sipnabot>", "type": "commented", "related_issue": null}, {"user_name": "harshalDharpure", "datetime": "Aug 8, 2022", "body": "I am trying to integrate a haystack with rasa in the chatbot, also I followed the haystack documentation. But the error occurred, can you please acknowledge How to solve it Link: ", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Feb 20, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "tmbo", "datetime": "Mar 21, 2019", "body": [], "type": "transferred this issue from RasaHQ/rasa_core", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/2976", "issue_status": " Closed\n", "issue_list": [{"user_name": "Arghya999", "datetime": "Feb 11, 2019", "body": ":0.12.3: Python 3.6.8 (windows, osx, ...)::Ubuntu 16.04I have a custom action where i am querying a mysql database and fill the slots based on the retrieval query. The tracker got updated with the fetched values of the respective slots.(current tracker state)2019-02-11 16:01:54 DEBUG rasa_core.processor - Received user message ‘yes’ with intent ‘{‘name’: ‘mood_affirm’, ‘confidence’: 0.7875545623995659}’ and entities ‘[]’ 2019-02-11 16:01:54 DEBUG rasa_core.processor - Logged UserUtterance - tracker now has 46 events 2019-02-11 16:01:54 DEBUG rasa_core.processor - Current slot values: PERSON: Arghya battery: 2500 mAh,Li-Polymer camera: 8GB customer_location: None display: 42.2 Ghz Kyro model: f10 operating_system: 6 in price: None processor: 26001800 pixels,25fps shop_location: None workshop_location: None 2019-02-11 16:01:54 DEBUG rasa_core.policies.memoization - Current tracker state [{‘slot_battery_0’: 1.0, ‘slot_PERSON_0’: 1.0, ‘intent_OPPO_Phones_Price’: 1.0, ‘slot_display_0’: 1.0, ‘slot_operating_system_0’: 1.0, ‘slot_processor_0’: 1.0, ‘entity_model’: 1.0, ‘prev_utter_model_name_lookout’: 1.0, ‘slot_camera_0’: 1.0, ‘slot_model_0’: 1.0}, {‘slot_battery_0’: 1.0, ‘intent_model_name_lookout’: 1.0, ‘slot_PERSON_0’: 1.0, ‘slot_display_0’: 1.0, ‘slot_operating_system_0’: 1.0, ‘slot_processor_0’: 1.0, ‘entity_model’: 1.0, ‘prev_action_listen’: 1.0, ‘slot_camera_0’: 1.0, ‘slot_model_0’: 1.0}, {‘slot_battery_0’: 1.0, ‘intent_model_name_lookout’: 1.0, ‘slot_PERSON_0’: 1.0, ‘slot_display_0’: 1.0, ‘slot_operating_system_0’: 1.0, ‘prev_action_save_model_name’: 1.0, ‘slot_processor_0’: 1.0, ‘entity_model’: 1.0, ‘slot_camera_0’: 1.0, ‘slot_model_0’: 1.0}, {‘slot_battery_0’: 1.0, ‘intent_model_name_lookout’: 1.0, ‘prev_utter_model_name_confirm’: 1.0, ‘slot_PERSON_0’: 1.0, ‘slot_display_0’: 1.0, ‘slot_operating_system_0’: 1.0, ‘slot_processor_0’: 1.0, ‘entity_model’: 1.0, ‘slot_camera_0’: 1.0, ‘slot_model_0’: 1.0}, {‘slot_battery_0’: 1.0, ‘slot_PERSON_0’: 1.0, ‘slot_display_0’: 1.0, ‘slot_operating_system_0’: 1.0, ‘intent_mood_affirm’: 1.0, ‘slot_processor_0’: 1.0, ‘slot_camera_0’: 1.0, ‘slot_model_0’: 1.0, ‘prev_action_listen’: 1.0}]How to reflect the same slot values during the conversation as a reply to the question asked by the user to the bot? Or will the slots get automatically reflected/shown with the updated values in the conversation. (if used & relevant):", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Feb 12, 2019", "body": "Please ask these kind of questions in our ", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Feb 12, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "tmbo", "datetime": "Mar 21, 2019", "body": [], "type": "transferred this issue from RasaHQ/rasa_core", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/2921", "issue_status": " Closed\n", "issue_list": [{"user_name": "Arghya999", "datetime": "Jan 21, 2019", "body": ":0.12.3: Python 3.6.8 (windows, osx, ...): Ubuntu 16.04: dialog is not following stories.md after introducing custom actions.\nUsing Jupyter notebook for writing and testing the code.I am really new to Rasa  and have been exploring it for the purpose of building a chatbot.\nHaving a difficult time when it comes to custom  actions.I was playing with utterances in the actions till now and the flow of dialog  was going  fine.\nI tried writing some custom actions for the chatbot but the flow gets struck in the first conversation itself.a path in stories.md file is as below:-#happy path model enquiry 1For the path mentioned  , I am getting :-\nYour bot is ready to talk! Type your messages here or send 'stop'\nMe :Hi I am Arghya\nBot : I am not sure what you are aiming for.expected :- utter_greetit seems that the bot is not able to predict the action as in the path.\nPlease guide me where i am going wrong and nudge me to the right direction.the nlu.md training file is as above :-##intent:city_name/home/arghya/rasa_test/data/place_finder/PERSON.txt/home/arghya/rasa_test/data/place_finder/model.txt/home/arghya/rasa_test/data/place_finder/customer_location.txtaction server code :-from  import absolute_import\nfrom  import division\nfrom  import unicode_literalsfrom rasa_core.domain import Domain\nfrom rasa_core.trackers import EventVerbosityimport logging\nlogger = logging.getLogger()import requests\nimport jsonfrom rasa_core_sdk import Action\nfrom rasa_core_sdk.events import SlotSet\nfrom rasa_core_sdk.events import UserUtteranceReverted\nfrom rasa_core_sdk.events import AllSlotsReset\nfrom rasa_core_sdk.events import Restartedclass SaveCityName(Action):\ndef name(self):\nreturn 'action_save_city_name'class SaveModelName(Action):\ndef name(self):\nreturn 'action_save_model_name'class ActionSlotReset(Action):\ndef name(self):\nreturn 'action_slot_reset'\ndef run(self, dispatcher, tracker, domain):\nreturn[AllSlotsReset()]class getServiceCentreLocation(Action):\ndef name(self):\nreturn 'action_get_service_centre_location'\ndef run(self):\nprint(\"you will receive the service location once the database is up.Working on it :) \")\npassclass getShopLocation(Action):\ndef name(self):\nreturn 'action_get_shop_location'\ndef run(self):\nprint(\"you will receive the shop location once the database is up and running.Thanks for the wait.Next Question please? :)\")\npassclass getModelPrice(Action):\ndef name(self):\nreturn 'action_get_model_price'\ndef run(self):\nprint(\"you will receive the price of the model once the database is up and running.Thanks for the wait.Next Question please? :)\")\npassclass getModelConfig(Action):\ndef name(self):\nreturn 'action_get_model_config'\ndef run(self):\nprint(\"you will receive the model configuration once the database is up and running.Thanks for the wait.Next Question please? :)\")\npass\n (if used & relevant):", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Jan 22, 2019", "body": "Please ask these kind of questions in our ", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Jan 22, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "tmbo", "datetime": "Mar 21, 2019", "body": [], "type": "transferred this issue from RasaHQ/rasa_core", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2543", "issue_status": " Closed\n", "issue_list": [{"user_name": "brainwad", "datetime": "Jul 24, 2022", "body": "When importing, there is currently the choice to move files from the import folder to the originals folder, which avoids wasting space but destroys the file structure of the import folder, which might confuse other programs (e.g. file syncing tools). Or you can copy the photos from import to originals, but this doubles the space required to store the photos. Or you can skip importing entirely and just index instead, but then they miss out on the renaming and deduplication that you get when importing.A third option, on file systems that support it (including common ones like NTFS, Ext4, APFS, but notably excluding FAT), would be to create hard links in the originals folder. This would enable the import directory's file structure to be preserved without taking up extra space. Go natively supports hard links: . For file systems that don't support hard links, the best fallback would probably be copy.It would probably be best to present this as a new, third option to users: some users might want the file duplication brought by copy (so they can edit the copies without affecting the source files in the import folder).", "type": "commented", "related_issue": null}, {"user_name": "centralhardware", "datetime": "Jul 25, 2022", "body": "by default import folder located inside docker container, and all data will be erased at every container recreate. so photoprism must prevent this case.", "type": "commented", "related_issue": null}, {"user_name": "brainwad", "datetime": "Jul 25, 2022", "body": "Yes, it would require that the import and originals directories were mounted on the same docker volume*, I think, because hard links don't work across different file systems and from the point of view of the docker container each docker volume is a unique file system.* Or neither to be on a volume, but then you would lose all your data every time the container restarts.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 28, 2022", "body": "The main use case for importing is adding files from removable media and web uploads (creating hardlinks would work against that).Note that the same original file name (in the import folder) may exist multiple times, since cameras usually name images like IMG_1234 and then start again with IMG_0001 and so on. In contrast, the file paths in the original folder must be unique, otherwise the files would be overwritten.That said, PhotoPrism saves the original file name (from the import folder) in its index, so you can always create hardlinks, softlinks, or zip archives based on that if needed.A script to do this might fit nicely in the contrib repo if someone contributes it: ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 28, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 28, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2253", "issue_status": " Closed\n", "issue_list": [{"user_name": "jbdubbs", "datetime": "Apr 14, 2022", "body": "Build 220302-0059f429-Linux-AMD64Oneplus 8 5G, T-Mobile version\n8Gb RAM, Snapdragon 865\nAndroid 11, Google Play version March 1 2022, Android Security version Feb 1 2022\nBuild 11.0.7.9.IN55CB\nFirefox Mobile 95.2.0 Build 2015851755\nChrome Mobile 97.0.4692.87Docker:Client:\nVersion:           20.10.7\nAPI version:       1.41\nGo version:        go1.13.8\nGit commit:        20.10.7-0ubuntu5~20.04.2\nBuilt:             Mon Nov  1 00:34:17 2021\nOS/Arch:           linux/amd64\nContext:           default\nExperimental:      trueServer:\nEngine:\nVersion:          20.10.7\nAPI version:      1.41 (minimum version 1.12)\nGo version:       go1.13.8\nGit commit:       20.10.7-0ubuntu5-20.04.2\nBuilt:            Fri Oct 22 00:45:53 2021\nOS/Arch:          linux/amd64\nExperimental:     false\ncontainerd:\nVersion:          1.5.5-0ubuntu3-20.04.2\nGitCommit:\nrunc:\nVersion:          1.0.1-0ubuntu2-20.04.1\nGitCommit:\ndocker-init:\nVersion:          0.19.0\nGitCommit:Open Firefox or Chrome mobile.  Scroll through lots of pictures in your \"Search\" uncategorized section.Keep scrolling fast.Scrolling gradually gets slower and slower, to the point where loading the next 50 photos takes 10+ seconds.  Firefox hard locks, Chrome \"Aw, Snap!\".", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 14, 2022", "body": "Our latest development preview comes with performance improvements: .A timeline view that is optimized for fast scrolling is planned as well: ", "type": "commented", "related_issue": null}, {"user_name": "jbdubbs", "datetime": "Apr 14, 2022", "body": "Understood.I'm assuming you would still like this bug open due to the hard crash nature of it?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": "I am aware of the technical limitations of the current architecture. Done is better than perfect, especially when you have a small team and a lot of feature requests to consider.The good news is, we've made a lot of performance and memory usage improvements in the last few weeks! It would be great if you could try this out, as it could improve your experience immediately (feedback is welcome in GitHub Discussions):All browsers and devices have some sort of memory limit, especially mobile devices. Personally never had any issues on Android, so either I'm not scrolling hard enough or if you are getting a \"hard crash\" also depends on the OS and Browser settings. Some device vendors might be more aggressive than others.General notes on using PhotoPrism:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": "I'll close this as we know the limitations and there are already a few issues targeting your use case.... hope that's ok! :)", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2320", "issue_status": " Closed\n", "issue_list": [{"user_name": "danielhoherd", "datetime": "May 14, 2022", "body": "When viewing collections of images, timezones are not considered when sorting photos. Photos from two separate cameras set to the same objective time but with a different timezone will not correctly sort chronologically. It appears that the location of the photo may be interfering with correct interpretation of the time.Photo details views show incorrect TZ and time when the TZ of the camera body does not match the photo geolocation.Steps to reproduce the behavior:I expect photos that include the TZ in their exif data to be shown with correct times and sorted correctly among other files from different TZs.It looks like there may be multiple things going on. The DB does not appear to store TZ data, although there is a  column. This column appears to be the result of some changes that take into account the location of the photo and the photo exif data. For example, the following photo shows 10:01 PM EDT.The photo was taken in EST, but clearly it is not 10pm EDT since it's daytime. The details view of that photo does indeed show that it is in America/New_York TZ.However, the camera that took that photo was set to GMT (because who wants to mess with TZ's every time you travel?) We can see in the exif data that it was taken at 10:01 GMT (+00:00):So my suspicion is that the import of photo metadata into Photoprism is ignoring fields that have more rich datetime data that includes the TZ. These fields are less ubiquitous than the TZ naive fields, but they are quite common in modern cameras, and if the TZ data is included in the exif then it should certainly be used. This is especially important for people who travel because they often forget to change the TZ on their camera, and end up shooting a bunch of photos with the clock set to the wrong timezone. If TZ data is available in the file and is used, then photoprism can gracefully deal with the photographer's mistake. If TZ data from the file is ignored, then we have chronological sort problems and incorrect metadata showing up in photo detail views.I can provide sample files if asked to, but I don't think I have anything especially rare that you couldn't create by just using exiftool to change the dates on a file or using a DSLR with its clock set to another TZ.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": "Support for SubSec timestamps was added recently. Read priority has been increased so that they are used first. Might be all that's needed! Please test ", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 18, 2022", "body": " I just tested this on  created at , and sadly the new build does not fix the problem.I looked in the database and  and  appear to be the same as they were before.  shows the geotagged location, not the TZ from the file.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 18, 2022", "body": "Your initial example showed time offsets +00:00, not time zones. Now your example shows a time zone, Z. However, reading the fields you mentioned first, leads to the DateCreated field being ignored. We can't read all fields and mix the information in creative ways.", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 18, 2022", "body": " sorry, I've been using TZ and offset interchangeably. The original example didn't include the  field, which is why the Z was not shown. The file I used in the original example (20220511-22-01-32_455c68.dng) does show the TZ for  and an offset for , just like the file in my prior comment (20211206-03-29-10_493FjF.dng) does. These two photos were taken with the same camera body and same TZ settings.", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 18, 2022", "body": "Looking more at the exiftool side of this, . After adding those contents to my  I now see my DSLR with GMT offset, and my iPhone with my local timezone offset. Here are the two files I used in examples above, and another iPhone file for comparison:So it is pretty simple to coax exiftool into providing the bare timezone from file metadata. With these pieces, it should be possible to add TZ metadata contained within the file to photoprism.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 18, 2022", "body": "DateCreated is an XMP Field, DatesTimes in Exif for Images are not UTC but local time by default. Main exception are QuickTime and MP4 videos, which default to UTC. Since when does Nikon create DNG files with XMP Metadata? Writing this to XMP sidecar files instead should work for you. Note that even though Exiftool calls -7 a time zone, it's a time offset really.", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 18, 2022", "body": "AFAIK Nikon does not create DNG files. These were run through Adobe DNG Converter, and manually geotagged. Nikon does support TimeZone as a default property though:If the feedback I'm leaving is not helpful then I can back off and let the experts do their thing. The original issue still stands though: Photoprism currently does not respect the offset of the clock in the camera that took the photo.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 18, 2022", "body": "It's helpful to learn about your needs. However, time ZONE and OFFSET have defined, distinct meanings (independent of how Exiftool labels the value +01:00, it is not a zone): Money Quote:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 18, 2022", "body": "If the timestamps have an offset, it is local time. This already is the default for Exif timestamps. If the metadata also contains the GPS coordinates, the ZONE and OFFSET should be correct and properly displayed. If the GPS coordinates are missing, then how does the Camera or other tools you use know the offset or zone?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 18, 2022", "body": "Also note that the latest release will sort by local times instead of UTC (if available). We may add settings for this in a future release.", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 18, 2022", "body": "I have created some unmodified test images that show the problem and I can share them if you'd like. I have both JPG and NEF files available.I set my D850 TZ to UTC +00:00, plugged in a GPS, and took some photos in America/Los_Angeles (-07:00). The camera was configured to set its clock by GPS time, so the clock is correct. This setting did not interfere with the camera's TZ being set to UTC.Above, we see the the image was taken at 8:17pm +00:00 (utc). However, Photoprism shows it as 8:17pm PDT, which is incorrect.This was tested in Photoprism Build 220517-b9c68f8f-Linux-AMD64 (docker image )", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "You are welcome to send samples to the samples email address specified on our contact page. This is the first time someone uses UTC in a regular camera, maybe our other Exif library requires changes too or we have to prefer GPS time to handle this special case.However, we need to be careful not to break existing functionality as local time is the default / standard. Exif does not include a time zone (which is the actual problem). These are custom Nikon Maker Notes in combination with logic the Author of Exiftool seemed to have developed for users like you:", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 19, 2022", "body": "I see several TZ and offset related tags the exif 2 tag reference  The two most relevant are:OffsetTime is included by default Nikon D850 files:And in iOS files:Even so, it appears to be not widely supported. I downloaded all the raw files for Sony, Nikon and Canon from  and not a single one of them had OffsetTime, or any TZ or Offset related information in EXIF. A lot of the Nikon and Canon files had  in their  though, but that's it. Nothing from Sony.Handling time is difficult, so it's not too surprising that there's no widely adopted standard. It's a shame though.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "As you already noted, these fields are not commonly used. It would also not make much of a difference as by convention the time in Exif is local time. Any explicit offset just confirms that by providing the specific time difference to UTC. If your pictures were taken in LA, then UTC+0 is the wrong offset. It is important to understand that UTC+0 is not the same as UTC (Z):", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "The features you are requesting seem to be...a) override any other timestamp with the GPS time if present, which is always UTCand/orb) automatically adjust the Exif local time using the matching offset for the location if the camera clock offset is present but wrong and there are GPS coordinatesIs that correct?", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 19, 2022", "body": "I'm really not sure if the implementation details you gave are exactly what I'm asking for, but if that's what it takes to solve the problem then that works for me.Really?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "Yes. As explained above +00:00 is an offset, not a zone. Z is the UTC zone. Any offset is as good as another offset and it may well change during the year if you stay at the exact same location:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "Money Quote from the URL you provided:CURRENTLY is key here. Zulu time or UTC always is the same, not just currently.Edit: For me UTC+0 was a shortcut for +00:00 as I have a hard time typing all these details on a phone. My point always was to distinguish between a ZONE and an OFFSET.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 19, 2022", "body": "I will try to explain it even better, and hope you don't find it too pedantic (back in the office now with a proper keyboard):See  and the XMP create time for example:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 1, 2022", "body": "Since the prioritization of SubSecDateTimeOriginal and SubSecCreateDate has been released, we will close this issue. You can decide if you want to submit a follow-up issue, now that you (hopefully) have a detailed understanding of the situation.Of course, feature requests should not conflict with what I wrote in the comments. In particular, we should not mix offsets and zones, as this will likely result in incorrect data in other places / for other users.", "type": "commented", "related_issue": null}, {"user_name": "danielhoherd", "datetime": "May 14, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 17, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 1, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 1, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 1, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1816", "issue_status": " Closed\n", "issue_list": [{"user_name": "stanelie", "datetime": "Dec 14, 2021", "body": "\nOn a fresh install of Photoprism, I uploaded a bunch of old pictures from my library. The exif information is not available in Photoprism, even if my old library shows it (Lychee)\nShould I send you a picture to test with?Steps to reproduce the behavior:I'd like to see the EXIF infoExample picture\nPhotoPrism® 211210-2cb90e7e-Linux-x86_64", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 14, 2021", "body": "What metadata is missing exactly?I checked the exif data of your image using exiftool and it only shows:", "type": "commented", "related_issue": null}, {"user_name": "stanelie", "datetime": "Dec 14, 2021", "body": "Here is what I see in Lychee :\n\nCapture date, camera model, focal length...Here is what I see in Photoprism :\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 14, 2021", "body": "What do you see in Exiftool?", "type": "commented", "related_issue": null}, {"user_name": "stanelie", "datetime": "Dec 14, 2021", "body": "From within the docker container, running exiftool on the example picture got the exact same info as  .Strangely, I ran exiftool on the photo on the Lychee server itself, and I got the same missing info. So, something else is going on I think. Maybe Lychee stripped the exif info after writing it to its own database (and maybe transcoding the file)??I will investigate further. Thanks for your time.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 14, 2021", "body": "Compare the SHA1 hash or other file fingerprint. Exiftool might also include the hash its output. Is it the same file?", "type": "commented", "related_issue": null}, {"user_name": "stanelie", "datetime": "Dec 15, 2021", "body": "Hum. I suspect the file was modified my Lychee upon upload.I will investigate further on my own and close this ticket for now. Thanks again!", "type": "commented", "related_issue": null}, {"user_name": "stanelie", "datetime": "Dec 14, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 14, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "stanelie", "datetime": "Dec 15, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1798", "issue_status": " Closed\n", "issue_list": [{"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": " used to be the only relevant standard for the Web (computer screens) before Apple started to push their own  profile. The print and the movie industry traditionally have other standards suitable for printers and professional use cases.As  still is the default for all Web browsers, it doesn't require additional file headers or other metadata to be displayed as intended.In a future release, PhotoPrism may render thumbnails with custom ICC Color Profiles and embed them. To make this possible, the follow-up issue  lists a number of research tasks that should be completed beforehand.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": "See also:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": "Related Publications and Libraries:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 10, 2021", "body": "For reference, these are \"actual\" color profiles PhotoPrism found in my personal photo library:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "issue", "related_issue": "#1474"}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 9, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 10, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 10, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 10, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 16, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2079", "issue_status": " Closed\n", "issue_list": [{"user_name": "graciousgrey", "datetime": "Feb 28, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Feb 28, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Feb 28, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 24, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 25, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 25, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Mar 28, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 14, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1809", "issue_status": " Closed\n", "issue_list": [{"user_name": "JaCoB1123", "datetime": "Dec 13, 2021", "body": "Hi,I noticed that the webinterface sometimes just \"dies\", although the docker container is still running.\nI had this happen a few times when looking for an image by filtering wildly. It often happened while an import was running, but I just had it occur and I'm certain that no import was running. If it died during an import, the import continues to run in the background. In all cases the webinterface doesn't respond any more (neither directly nor via my caddy reverse-proxy).I already enabled the debug mode weeks ago. Today was the first time where I had a panic in my logs. I'm not sure if it has anything todo with the actual issue though. Other than that, the only thing I can see in the logs, is that the  messages just stop at some point.Is there anything I can do to provide further information?Here's the panic I got:It mostly happened to me while scrolling through my photos and using different filters (mainly year/month/day).I'd expect the web interface to be always reachable while my docker container is running.I can provide my full logs, if these are of any help. For completeness here are the log entries right before and after the panic above: in docker:Accessed using Firefox 95 on Windows 10 x86_64.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 13, 2021", "body": "You might simply exhausting browser memory as our current views are not built to display thousands of images simultaneously. We'll provide a timeline view that can unload already loaded images for this, see roadmap. Or it's a local configuration issue in which case it can take us hours to debug it with you. We don't observe such issues and there are no other reports. As our time is very limited, we can only offer this to sponsors. Could help to go through the checklists in our Troubleshooting Guide, just in case.", "type": "commented", "related_issue": null}, {"user_name": "JaCoB1123", "datetime": "Dec 14, 2021", "body": "Thanks for the hint, I'll keep my browsers memory in view next time.\nI understand that your time is limited. I also wanted to have the problem be findable by others in case anyone has the same problem and needed a place to put the data I have collected already.", "type": "commented", "related_issue": null}, {"user_name": "JaCoB1123", "datetime": "Dec 13, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "JaCoB1123", "datetime": "Dec 13, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 14, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "photoprism", "datetime": "Dec 14, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 14, 2021", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1827", "issue_status": " Closed\n", "issue_list": [{"user_name": "hatl", "datetime": "Dec 21, 2021", "body": "Video detected as JPGThe following file is part of the originals: GX010255.MP4\nPhotoPrism adds .jpg to name: GX010255.MP4.jpg\nPlayback doesn't work.211215-93b26f19-Linux-x86_64", "type": "commented", "related_issue": null}, {"user_name": "hatl", "datetime": "Dec 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "photoprism", "datetime": "Dec 21, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 21, 2021", "body": [], "type": "converted this issue into  discussion", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2169", "issue_status": " Closed\n", "issue_list": [{"user_name": "PiotrKrzyzek", "datetime": "Mar 19, 2022", "body": "Photoprism has a wonderful set of search and filter options, though we're not able to filter photos (and others) well by the items orientation. Currently, the only orientation related filter option is 'portrait` which, if set, will give us all portrait oriented (where height is bigger than width) assets.Though there is no  filter.As referenced in my previous bug report / question,  is a BOOLEAN filter, meaning simply true or false where the default is false meaning: simply not set.As  here means  and not meaning \"NOT\", thus I propose we have either-or of the following two filter options added:The  filter would do the exact same thing as portrait (a boolean filter) except for landscape oriented (width greater than height) photos (and videos and whatnot).Whereas  could take different options such as: 'landscape', 'portrait' and could be expanded to include more advanced filters such as 'not set', 'circle', 'upside down' and so forth.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": "Note there is also a  filter, which limits results to photos/videos in landscape format with an aspect ratio of 2:1 and a minimum width of 1000 (or more):", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": "Added  and  filters, in addition to the existing  and  search filters:", "type": "commented", "related_issue": null}, {"user_name": "PiotrKrzyzek", "datetime": "Mar 19, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Mar 20, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 14, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 17, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 20, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2216", "issue_status": " Closed\n", "issue_list": [{"user_name": "gomme600", "datetime": "Apr 1, 2022", "body": "All of my photos have disappeared on the latest update.Steps to reproduce the behavior:Update the container to the latest version.Photos to appear.\n", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Apr 1, 2022", "body": "Downgrading from the latest preview build to the latest arm64 build (docker tags) has restored my photos. This seems to confirm the bug in the latest preview build.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": "It seems that the database schema migrations were not performed (correctly). Note that MySQL 8 is no longer supported, just in case.Did you see any warnings/errors when starting the container?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": "Our latest  build  includes a new command that lists the status of database migrations.To retry previously failed migrations, you can run this command next and let us know if you see any errors or warnings:If you are lucky, the problem is already solved with this...", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": "Updated the migration statements to match the queries used for indexing and started another build to test. When I run this on SQLite, at least I don't see any errors.I suspect that one of the  statements did not execute or its  condition did not match all the rows in the  table.The following commands force the updates to be executed again:", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Apr 1, 2022", "body": "Will try that out tomorrow, thanks for the help so far !", "type": "commented", "related_issue": null}, {"user_name": "cgomesu", "datetime": "Apr 2, 2022", "body": "refreshed my browser and , the photos are once again showing up. the only issue I noticed afterwards is that that sorting photos by 'newest first' now shows a few old photos at the top, instead of the most recent photos (but 'recently added' does start by showing the most recent ones).", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 2, 2022", "body": "Hopefully these commits fix the remaining issues. Indexing (no complete re-scan) should be enough.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 3, 2022", "body": "Does anyone still have problems or can we consider this done?", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Apr 3, 2022", "body": "Running the above command has fixed it (photoprism migrations run --failed).\nUnfortunately I also have old photos showing up when selecting newest first. I am running a reindex now and will report back when it's finished.\n\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 3, 2022", "body": "Thanks! Also run the 3 individual migrations mentioned above. Just started another build for testing.", "type": "commented", "related_issue": null}, {"user_name": "spyfly", "datetime": "Apr 4, 2022", "body": "I ran all the migrations mentioned above, but the  endpoint keeps timing out on the latest development preview.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 4, 2022", "body": "Noted that too  reverted the preview until this is solved.... Caching is hard!", "type": "commented", "related_issue": null}, {"user_name": "spyfly", "datetime": "Apr 4, 2022", "body": "The most recent preview still has the same issue.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 4, 2022", "body": "Our demo and other instances all work fine. What's the specific problem?", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Apr 5, 2022", "body": "Reindexing fixes old photos showing up first. Personally the endpoint returns \"{\"code\":401,\"error\":\"Please log in and try again\"}\" (which is normal because I'm not sending my session id) so it doesn't seem to be timing out.", "type": "commented", "related_issue": null}, {"user_name": "spyfly", "datetime": "Apr 5, 2022", "body": "The requests to the endpoint  seem to time out, it seems like photoprism doesn't properly handle them for some reason.In the logs I do get astatement, but the corresponding http request doesn't show up.Here's the visual result:\nAccording to top photoprism is using quite some resources, but the logs don't tell me anything about what it is doing in the background:\n\nAt some point it just gets killed, because it runs out of memory. This high cpu consumption starts after loading any page which uses the  endpoint.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": "Are you using  as referenced in our release notes at ?", "type": "commented", "related_issue": null}, {"user_name": "spyfly", "datetime": "Apr 5, 2022", "body": "My bad, was still using The issue is fixed in the new build", "type": "commented", "related_issue": null}, {"user_name": "gomme600", "datetime": "Apr 1, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 1, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 2, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 2, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 2, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 2, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 3, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1773", "issue_status": " Closed\n", "issue_list": [{"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": "Solved by doing ,, restarting and then .", "type": "commented", "related_issue": null}, {"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": "It's back again after reboot.", "type": "commented", "related_issue": null}, {"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": "Here's some output of  (there's a line with  so that might be the issue) :", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 29, 2021", "body": "Please use GitHub Discussions for  INSTALLATION PROBLEMS, SUPPORT REQUESTS, and GENERAL QUESTIONS:THANK YOU! Our Troubleshooting Guide helps you quickly find and fix common problems:Sponsors receive direct technical support via email:  ", "type": "commented", "related_issue": null}, {"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "SinTan1729", "datetime": "Nov 29, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 29, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 29, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1735", "issue_status": " Closed\n", "issue_list": [{"user_name": "rickysarraf", "datetime": "Nov 17, 2021", "body": "Consider the following:I think this is a bug because:Steps to reproduce the behavior:Folders should get indexed, especially the ones passed as a command-line argument.Latest changes from the  branch.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 17, 2021", "body": "The argument must be a RELATIVE path, not including the absolute path to your originals folders.", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Nov 17, 2021", "body": "Thank you for the quick reply. Indeed. That was the case. Sorry for the noise. But maybe it could be mentioned out in the CLI help ? Right now, it just says: On similar note, I think when passing the  option to  along with a relative path, it does a cleanup scan for the entire originals folder. This is just a gut feel because with and without , the runtime is substantially different.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": "Updated the command help to make clear the argument can be an originals subfolder:", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Nov 17, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 22, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 1, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 2, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 4, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 11, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Dec 11, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1537", "issue_status": " Closed\n", "issue_list": [{"user_name": "Zazou49", "datetime": "Sep 19, 2021", "body": "Hey,I just discovered Photoprism, sounds exciting ! I configured via Docker and tested to import few hundred photos / videos. Works flawless !I have just a question that I can't resolve myself, is it possible to \"hide\" information on card view ? For example, I don't want to show Camera information and display less information from GPS (Just city, not region)Thank you", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 19, 2021", "body": "While it's technically possible, it would reduce rendering performance, increase code complexity and require additional settings incl documentation.", "type": "commented", "related_issue": null}, {"user_name": "Zazou49", "datetime": "Sep 19, 2021", "body": "Hi Thank you for your answer, I understand your position. I'll switch to thumbnail view, it's almost perfect for me. I also sax that a \"Google Photos\" view was founded, I hope it will be developed in a future :)", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 20, 2021", "body": "We do our best to deliver it as soon as possible. Still, there are 263 open issues and users want support, bugfixes as well as performance improvements along the way. For now, we're extremely happy Facial Recognition is done! ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 20, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2222", "issue_status": " Closed\n", "issue_list": [{"user_name": "sunbeam60", "datetime": "Apr 5, 2022", "body": "The current parameters cause ffmpeg to fail on an Synology 718+ w/Intel J3455 - I will open a bug for this.I  what is happening is that the current invocation of ffmpeg attempts to create a hardware device called qsv (-init_hw_device) which it attempts to set to just \"hw\" ... but as far as I understand -init_hw_device, the short-hand device name you are creating has to be set to an actual device (quoting from ):So I think the reason ffmpeg spits out  is that hw is not an actual device.-filter_hw_device is then trying to pass a device to all filters in ffmpeg's filter-chain ... I presume this was  to be the 'qsv' device (which is incorrectly set), not the generic 'hw' device which doesn't actually exist. From :So I simply cannot understand how anyone has been able to get HW decoding/encoding to work using these parameters, whatever their configuration. As far as I can decode ffmpeg's docs, this won't work on  system.If I remove the init_hw_device and -filter_hw_device gubbins and simply invoke...... which  PhotoPrism's invocation but  a named hardware device being passed to all filters (or at least without the attempt to create a named hardware device and pass it to all filters, ffmpeg works.If I additionally remove the -qsv_device parameter, ffmpeg continues happily and at same speed as with the -qsv_device parameter. It's only when I switch to  that the speed  drops due to libx264 now being used instead of the QSV hardware (3 fps instead of 20 fps).", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 5, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 5, 2022", "body": "I think all that is required is to remove the following parameters  which are erroneous.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": "Removed  and started a new preview build for testing... works this way on my Core i7-10700T.Thanks for your advice and becoming a sponsor! ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": "Note that this change also removes the  config option. It turned out to be specific to the  encoder commonly used on the Raspberry Pi 4. Now has a fixed value as for custom parameters of the other encoders.", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 5, 2022", "body": "Ran a test now and it's not good news :(... and, indeed, when I try to play the file in the UI, the sidecar file is now obviously present and plays.So, I was extremely cocksure that I'd found the issue, but it seems I've been proven wrong, or at least come up against another issue. Both are run inside the docker container, but something is preventing the VAAPI device being created when invoked from PhotoPrism but  when invoked from bash, inside the container.I'll need to think this through and family calls so might return to it tonight and redo the steps above just to make sure I've not done something silly. As it stands, I've teetered over the limit of my docker + linux knowledge. Perhaps something is brought in when bashing into the container that doesn't exist outside this...", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": "Permission issues? Running the command as root in the terminal while PhotoPrism is running as non-root user?", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 5, 2022", "body": "Yes, it could be. I’m certainly root when logging into the container. Not sure what that implies for PhotoPrism if so.", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 6, 2022", "body": "Trying to re-run the container with:.. and unfortunately get the same result.But my plex container runs with a restricted user, non-root/admin, and employes ffmpeg transcoding just fine.Any ffmpeg experts around?", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 6, 2022", "body": "Couple of issues/PRs in Jellyfin that could relate to this:\n\n... it could be a Synology specific issue, although why it works when I bash into the shell and run the same command line is still beyond me.Will try to set permissions on the render devices (by bashing into the container, running chmod before making a video serve request on the UI), but it will have to be after work - lunch break is over :)", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 6, 2022", "body": "Ok, it's probably a Synology issue and probably related to the jellyfin issues I posted in the comment above.I had forgotten that root and admin is not the same on a Synology. When I change the PhotoPrism container to run as UID=0 (root), the ffmpeg invocation using QSV works fine and indeed the video transcodes MUCH quicker.This is why the call succeeds when bashing into the container - one arrives as root when running .So the answer is something along the lines of what jellyfin is doing in . If you look at the comments in :It's exactly the same issue as I've described.So: iterate over the drivers and ensure the current user has access to them. This is a solution specific to Synology - or at least all devices where the renderD128 doesn't have RW permissions by default. up to you whether you want to add a specific fix for Synology like what Jellyfin is doing.Until then, Synology users who wish to transcode with QSV need to run as root.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 6, 2022", "body": "We can't make any more changes for the next release. However, I took the opportunity to refactor video transcoding along the way when implementing this today:There used to be an automatic fallback to the software encoder, which should now work again if hardware transcoding failed. Logging has been improved too. Happy indexing/testing! :)", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 6, 2022", "body": "Understood - I'll open a specific improvement issue for the Synology fix so you can prioritise this one separately.Automatic fallback is awesome ", "type": "commented", "related_issue": null}, {"user_name": "sunbeam60", "datetime": "Apr 6, 2022", "body": "Synology improvement proposal here: ", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "I am seeing issues /w QSV, it is complaining about incorrect parameters. This is an example, but it happened a bunch of times on my server.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "Would be great if you can share more details, such as your Linux kernel version / operating system, CPU type and model, and anything else that could help to reproduce this. It may also help to change the user PhotoPrism is running as in case this is a permissions issue.", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "CPU: Intel(R) Core(TM) i5-9400\nKernel: 5.16.14-051614-generic\nI can run ffmpeg as software and it runs no problem, i use my main user 1000. It's not a permission issue. Well at least a file permission problem....Plex transcodes fines, but I know they use a customized ffmpeg. I will try to identify what the issue is, but no luck so far.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "You could try one of our other Docker images based on a different Debian version as it could depend on that as well.", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "No change /w bullseye or jammy.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "Just to be sure: you installed the drivers and shred ALL required devices as shown in our example docker-compose.yml?", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "This is my full .env file for photoprism:My photoprism in my docker-compose.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "What logs do you see when starting the container for the first time? You should see information about the drivers getting installed. Note init only runs once on the first start the variable was defined. Otherwise use docker-compose up --force-recreateAlso note that while I've added magic to make the installation work as UID 1000, you should generally use the PHOTOPRISM_UID env variable for switching to the UID after init as installing drivers requires the container to start as root. Starting as UID 1000 and expecting distribution packages and device access to work 100% is experimental to say the least.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "I recommend trying as root, just to exclude the possibility of a permission or privilege issue after all, since these are the single most common cause for problems. Especially for transcoding and device access.", "type": "commented", "related_issue": null}, {"user_name": "rezzalex", "datetime": "May 29, 2022", "body": "Hello everyone,I can't succeed to make hardware acceleration works on my NAS Asustor 5202T, the CPU is an Intel® Celeron™ CPU @ 2.00GHz\nand this NAS is supposed to have hardware transcoding functions.my log when finishing a transcode is\nmy docker compose file contains this :I tried also previously with\n withtout any successhere are my logs when first starting the container :", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "May 29, 2022", "body": "Hardware transcoding is one of the special features that causes a very high support and maintenance overhead for our team. To account for that, it's currently only available to sponsors.", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "Changed to root...runnng convert to get more low level details now...", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "This took longer than expected.... I think running it as root worked... so I assume don't use user in docker-compose lol.", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "May 29, 2022", "body": "One last follow up, using PHOTOPRISM_UID causes it to break again so Ill just stay root.", "type": "commented", "related_issue": null}, {"user_name": "blue-kaleidoscope", "datetime": "Jun 16, 2022", "body": "Hi, I'm also having issues with using QSV transcoding on my QNAP-TS451+ which has an Intel Celeron J1900. According to Intel it supports QSV.\nUsing QNAP's (catastrophic) QPhoto I can transcode iPhone videos quite fast. So I assume QSV is working there. The only thing to watch out: You have to use QPhoto as a root user.I believe Photoprism is by far a better solution but I could use your help with hardware transcoding.When executing ffmpeg as root user inside the container as follows I get an error message:An excerpt of my  looks as follows:When powering up photoprism I see the following messages:Am I doing everything correctly? It looks like I am running it as root. Looking forward to your support.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 15, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 18, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 7, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1785", "issue_status": " Closed\n", "issue_list": [{"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": "When setting up a remote sync WebDAV server (Settings → Sync) with both \"Download remote files\" enabled and also \"Upload local files\" enabled, Photoprism will first download remote files as expected, but then will upload those same files back to the WebDAV server, creating duplicates.Additionally, there is an issue with the upload path: The sync server is configured to download files only from WebDAV subfolder A/… The spurious uploads then happen on path A/A/… (duplicated A). Example: Photoprism downloads file  and then reuploads it as .Set up as described above.Only files which were not downloaded from WebDAV sync server should be uploaded to a WebDAV sync server.\nN/ARunning PhotoPrism® 211010-83b4f783-Linux-x86_64Due to  I had to remove the sync target several times during the download phase. Maybe that has had an influence afterwards?", "type": "commented", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": "In my setup, the WebDAV server is a Nextcloud instance where I intend to keep the master copy of all my pictures, perform backups, etc… All my \"picture sources\" (mobile phones, dslr cameras, etc…) are already configured to push their pics to Nextcloud, and I expected to use the \"Upload local files\" option of the Photoprism sync target to handle Photoprism as an additional \"picture source\" for whatever gets directly uploaded/imported into Photoprism.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": "While we've tested two-way sync, it's a hard problem in software engineering, and there undoubtedly are cases when duplicates are created. Also, files cannot be deleted or moved remotely, which guarantees duplicates in the long run anyway. The exact result may additionally depend on when you enabled the options and if you had connection issues in between (which seems to be the case).The proper solution to your problem seems mounting your Nextcloud files with  which is :Consistency, availability, and partition tolerance are other hard problems in computer science we can't simply solve as a reaction to your issues (wish we could, but no... there's tradeoffs everywhere):", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": "As a general rule, you should enable \"preserve filenames\" when using two-way sync:", "type": "commented", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": "Preserve filenames was enabled when this bug happened.I will consider the davfs recommendation, it may help in my specific scenario.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": "How can it create duplicates if names are the same local and remote?", "type": "commented", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": "See the note \"Additionally, there is an issue with the upload path\" in my initial entry.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": "I see, so it's not a duplication issue, but the path simply isn't configured / resolved correctly?", "type": "commented", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": "The main issue is that the upload should have never happened in the first place. The secondary issue is that the upload, instead of overwriting every file with a (hopefully) identical copy of itself, resulted in copies of the files being created at a separate location.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": "Obviously it worked as designed when we developed & tested WebDAV sync with Nextcloud. Edit: Maybe just a slash in the remote path name is missing and the URL parser doesn't fix it automatically?Since Nextcloud is based on PHP, even one-way sync isn't really great in terms of scalability and performance. Compared to other options, it's particularly bad for cloning your entire library.I'm still not sure if something broke, if it's a more complex timing issue, or if the feature just needs to be better documented. To fully test it, we need hours, which we don't currently have.That being said, two-way sync is generally not a good idea, so it may be best not to offer it at all. If you need it, there are more advanced tools where maintainers have done nothing but catch all edge cases and optimize performance over the last few years.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": "The checkboxes have been replaced with radio buttons so that only upload OR download can be selected at the same time:", "type": "commented", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "issue", "related_issue": "#1781"}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "daniel-callejas-sevilla", "datetime": "Dec 1, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 1, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 27, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 17, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1559", "issue_status": " Closed\n", "issue_list": [{"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "I get a bunch of pictures/videos under . After examining them, I mark them as . After a while, like say 15 minutes, they show up again under .The logs do capture what I've described above. Is this a bug somewhere ?Installation Details:", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "And then the same also end up showing under \nMay I ask what is the purpose of  ? I didn't find any documentation of it. Also, how do I deal with the items under .I may also have uncovered a bug in PhotoPrism. An item, that is marked , gets listed under .", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Seems more like an issue with the files. When photos are in review, they have their quality set to -1, same as for photos with indexing issues e.g. because the format is not properly supported and no thumbs could be created. May happen for new RAW formats Darktable and RawTherapee can't properly handle yet.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "You're welcome to send samples for testing as guessing what the issue might be takes too long (there are just too many possibilities what can go wrong with files): ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Also please use the stable version, the last tag is 20210925 - there is no version from 27/09/2021, also no preview.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Before debugging further, it would be best to check your error / warning logs e.g. ", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "May I ask what issues would those be ? I mean if I click on those files, the video is played proper.Thumbnail creation has had another erratic behavior lately. See below image for example. Thumbnails end up showing in the main view on PhotoPrism (Under ). This has been annoying as I have had to explicitly validate that they are thumbnails (looking at their file size) and delete them.\n.", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "Under , I get many such errors on first index run. On consecutive index run, those errors are not seen afaik.And by , I meant that I'm using the stable version from the 25th of September. Yes. Just that I build the images locally.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Exactly, videos would show under hidden when ffmpeg can't create a thumb as the assumption is that the files are broken.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "You deleted video JPEG previews manually? That would explain the issue.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Are you using a case-insensitive file system?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 27, 2021", "body": "Errors look like the same files might exist multiple times, maybe with different capitalization... this shouldn't happen usually, never seen this. Any manual database changes?", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "No. I'm on .I've triggered a full-index now. Please allow me to have it complete and then I'll share my findings on this issue.", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 27, 2021", "body": "No. Everything is as close to the docker-compose recipe you've prepared. I only rebuild locally. The only exception about database is that I had to manually  the  version of  because there was an issue with previous version uncovered this past week.The instance I have running is:", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 30, 2021", "body": "Today morning, magically, all those annoying thumbnails were gone.After taking a closer look at the logs, I noticed that I have a scheduled job to run  and  once a day. That seems to have done the magic.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": "This commit will search for orphan rows in the  table and remove them after indexing, in case there are any inconsistencies from earlier issues.", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Oct 1, 2021", "body": "Tested with the latest changes from  branch. They all look good now. Thank you 🙏🏽", "type": "commented", "related_issue": null}, {"user_name": "rickysarraf", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 30, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 2, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Oct 18, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1519", "issue_status": " Closed\n", "issue_list": [{"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "Trying to run photoprism/photoprism:preview docker image and getting an error about table not existing. I've tried running migrate, I've tried deleting all the tables from the db. All the other tables seem to get created, but not the subjects_dev6 tableThere's no other error messages. I can't figure out how to enable any extra logging that might be able to help debugging.", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "from a clean database", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "okay, latest latest preview image gives me dev8 instead of dev6", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "Why do you run migrate manually? Not needed. Should fix this anyway....", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "testing/trying/etcphotoprism start did the same thing. its something todo with the automigrate", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "Can you manually try to delete this table first?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "Don't need it anymore.", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "What specific database version is this? It's working in our test environments. Try changing the subject_bio column to see if it helps.", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "looks like mysql 8.0.26I'll try to made the code change this weekend and update accordingly", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "Try mariadb as in our example docker-compose.yml", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "Yea I'll try that too. I think I can probably swap over my server to mariadb without issue, i don't think my mysql server is used by much. does say works with mysql 8 so its probably worth running tests with it.This is all hypothetical though, I won't be able to play with it till after work", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": "It should. Docs are for our stable release, haven't tested facial recognition on MySQL yet. So thanks for reporting this!", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 10, 2021", "body": "It allows it to run and create tables. I havn't tested anything yet though.", "type": "commented", "related_issue": null}, {"user_name": "halkeye", "datetime": "Sep 24, 2021", "body": "Github doesn't let me know about commit messages. But I saw in the latest update this has been fixed, so I think this is safe to close.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 24, 2021", "body": "Yes, tested several times with MySQL 8 today.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 10, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 10, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 18, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 24, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1481", "issue_status": " Closed\n", "issue_list": [{"user_name": "dominikholler", "datetime": "Aug 18, 2021", "body": "On an instance with slightly more than 240k photos, the  GET /api/v1/photos slows down sometimes.\nThis make the web UI annoying to use.210523-b1856b9d-Linux-x86_64longer extract from the log: ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 19, 2021", "body": "Does it improve when you use  instead of ?Taking a look at database query optimization after completing the tasks we are working on right now, see roadmap. From my experience, MySQL often does stupid things when queries get more complex. Doesn't mean our implementation can't be improved, there should be an issue for this already.It's hard to optimize a database for a large number of different use cases. Sorting in particular can make a major difference. It may help to increase database caches for now, or use an SSD (if not the case yet).", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 19, 2021", "body": "I added this to the scope of ", "type": "commented", "related_issue": null}, {"user_name": "dominikholler", "datetime": "Aug 19, 2021", "body": " sounds great. It reflects my intention.", "type": "commented", "related_issue": null}, {"user_name": "dominikholler", "datetime": "Aug 19, 2021", "body": "Yes, in a small check [1] the request improved from an average of 2.7784 seconds to 0.57389 seconds.[1]  I use HDD for the db. (I just have SSD for the photoprism thumbnail cache.)\nI added  without noticing an improvement. Also the performance seems to be limited by cpu, because mysql process is eating a lot cpu and nearlt no waiting on io.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 19, 2021", "body": [], "type": "issue", "related_issue": "#1438"}, {"user_name": "graciousgrey", "datetime": "Aug 19, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 19, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1413", "issue_status": " Closed\n", "issue_list": [{"user_name": "timdonovanuk", "datetime": "Jul 5, 2021", "body": "My photoprism database is really starting to crawl along - I've been able to temporarily slap a plaster on it by enabling the recommended settings documented here, which to some degree simply just caches some of the first queries run when viewing the page, but doesn't help with subsequent queries (e.g. when offset moves by 60).This is actually better than it was, before making the recommended changes I was seeing queries take 800ms+.I noticed when scrolling through in mosaic view that a colossal amount of data is being queried for:photosooof!I am not a database expert by any means, but as soon as I start removing JOINS, query times goes from 800ms to about 5ms. I don't see in mosaic view that camera, lense info or location is needed, which seems cause most of the slow execution time.It could of course be a hardware issue io issue, but my mariadb sits on an nvme drive. FWIW I have around 73k photos indexed, although a lot are trash/hidden I guess, as there are 110,593 rows in the files table.Thanks for reading! :)", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 5, 2021", "body": "You're right in that this can be optimized. The REST API as it is right now doesn't care how the result view looks like, so it always returns the same data. Joins may also be optimized. If you scroll a lot to find pictures instead of using the search, there is a high chance you're actually waiting for the timeline view. It's on our roadmap already.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 19, 2021", "body": "I will close this ticket in favor of this: ", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 19, 2021", "body": [], "type": "issue", "related_issue": "#1438"}, {"user_name": "graciousgrey", "datetime": "Jul 19, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1501", "issue_status": " Closed\n", "issue_list": [{"user_name": "domharrington", "datetime": "Aug 28, 2021", "body": "After a recent index of another batch of photos, I've started getting this warning in the Library page:Going to Library -> Hidden I see lots of files without thumbnails:When I click on the top left one, it does actually play a video (which I think is a motion picture). The same static picture is also present in my library, but it's not marked as a motion picture. If I go and find that picture in the library, it has a filename of \"Photos from 2021/PXL_20210715_220515448.MP.jpg\".Navigating to the filesystem and doing a search for this, I find the following files:Opening the .MP file in VLC confirms that it is indeed the file that's hidden. It seems as though it somehow got unlinked or detached from the photo that's searchable. Is there anything I can do here to reattach these hidden images? Any idea how it may have gotten into this state?Side note: I've never heard of an .MP file before - this was taken using a Google Pixel 2 XL and exported from Google Photos using Takeout. VLC confirms that it is an MP4 file though! Any help would be greatly appreciated.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 28, 2021", "body": "Might be because MP is not a standard extension for videos (from what I know)? Does it work when using mp4 as extension?", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "Should I try and rename one of them, then trigger an index of the album folder? Will that cause it to get picked up, or is there something else i should do?", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "Seems like it was maybe introduced as a file extension in Google Camera v7.5 with Android v11But it's definitely a bit strange that this is the actual file extension, and not something like Someone else asking about this over on Reddit as well Edit: apparently there's a special format that Google are using by embedding an mp4 directly in to the jpeg: , seems like maybe the .mp file is a fallback ", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "Adding an .mp4 extension onto the end of the .MP file  and triggering a reindex seems to have done the trick! My hidden count has gone down by 1, i'm now going to try and do it for the rest of them.I'm surprised no one else has encountered this... would it be possible to add support for this to happen automatically?", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "If anyone else has this issue, you can rename all of the files with the following command to add .mp4 to the end:This appears to be reindexing properly and resolving the hidden files!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": "We'll add it to the next release.", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "Lots of them still appear to be stuck in the hidden view, and now it shows this when I open it:The ajax request still shows the .MP file:How can I clear out these photos from the db that no longer have files on disk any more?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": "Try the photoprism purge command in a terminal.", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "  thank you! That did the trick for ~500 of them (out of 900). I'll try and figure out what's going on with the remaining 400 tomorrow. I'm going to sign up to Patreon tomorrow to donate to your project as well. Thanks again.", "type": "commented", "related_issue": null}, {"user_name": "domharrington", "datetime": "Aug 29, 2021", "body": "Running purge again cleared out another 400, leaving only 5 remaining. Hopefully I can figure these out separately.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1455", "issue_status": " Closed\n", "issue_list": [{"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "The time/timezone in this image taken on my Canon EOS 1300D is detected incorrectly.\nThe image was taken at 19:55 local time (Timezone Europe/Berlin). However it is detected as taken at 21:55 local/19:55 UTC.\nI would expect it to be detected as taken at 19:55 local time.I'm running PhotoPrism® 210523-b1856b9d-Linux-x86_64 in docker.\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 1, 2021", "body": "Could you post the output of , please? Note we're on vacation right now.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 1, 2021", "body": "The time in photos is usually local time, unless they contain an explicit time zone. If they don't contain a location and it was getting estimated, you may want to disable estimates in Settings.", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "Output from exiftoolDisabling estimation and doing a full reindex on the folder did not change the local or UTC time in the image.\nThe image does not contain a location as the camera has no GPS module.", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "The folder also contains some images from my phone which do have GPS location in the exif data.\nIn case this might be the reason for some estimation within the same folder, this is the exiftool output from one of the images from my phone (taken at 20:24 local time):", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "After adding a GPS-location to the image using exiftool, the time is set correctly after a reindex\n", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "After taking a look at the code in photoprism/internal/meta/exif.go (I'm not familiar with go, so this is just a wild guess), it might be that the timezone is being looked up from GPS-data only (which is missing here) and not from the TimeZone attribute within the exif data.\nSo adding the timezone from the TimeZone attribute if the GPS-data is missing might be a possible solution.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 1, 2021", "body": "Can you try our latest development preview, and also check if your photo has any sidecar files or embedded XMP data?", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 1, 2021", "body": "Using the latest preview docker image (PhotoPrism® 210717-3ce4fddb-Linux-x86_64) did not change the data after reindexing.Sidecar content for this image:I used  to check for XMP data and it only has one entry: Rating = 0", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": "Is this solved, or is there anything specific we can / must do?There's no timezone in the metadata posted above:At best, images without GPS coordinates may contain a time offset, but that's not a time zone. For the difference, see .By default, photo metadata stores the  in local time, while MP4 / QuickTime videos use UTC.Please send an example file for testing to  in case you think this is a bug, and you want to us to proceed with debugging. I'll close this for now.", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Aug 30, 2021", "body": "If the CreateDate is Interpreted as Local time, then there seems to be a Bug, because in then ui the CreateDate is shown as UTC time.I'll Email you more information and the example file later today.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 3, 2021", "body": "I just tested with your file locally with the preview image and it behaves as expected.I am just wondering why it is not working for you. Could you try on the preview image deleting the file with the wrong information and importing it again?", "type": "commented", "related_issue": null}, {"user_name": "tobire", "datetime": "Sep 9, 2021", "body": "I just set up a fresh photoprism environment using the same docker-compose file I use on my production environment and the imported file now has the correct timestamp...\nI will try to find out why it isn't working in my main installation and will update the issue when I have any more information on a possible cause.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 10, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 29, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1407", "issue_status": " Closed\n", "issue_list": [{"user_name": "marecabo", "datetime": "Jul 3, 2021", "body": "First, thanks for this awesome photo organizing app!Some pictures (panoramas taken with a Samsung Galaxy S8 straight out of camera) unfortunately do not get indexed due to the following errors:Would it help you to have a look at some affected files?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": "Log output suggests it may be related to ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": "Can you verify your files are fully standards compliant, and not \"broken\" in any way? See comments in the golang issue referenced above.", "type": "commented", "related_issue": null}, {"user_name": "marecabo", "datetime": "Jul 3, 2021", "body": "With  and  from this , it returnsfor both images in the end. :/Is there a way to be more tolerant regarding such images, as any desktop image viewer or browser can show them without complaints?", "type": "commented", "related_issue": null}, {"user_name": "marecabo", "datetime": "Jul 3, 2021", "body": "As a workaround:  is able to resave these corrupted images with .They then get indexed successfully by PhotoPrism and pictures appear to not have any visual differences.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": "We didn't implement the JPEG standard ourselves in Go, Google did it. Ask them. See linked issue.", "type": "commented", "related_issue": null}, {"user_name": "GordonFreemanK", "datetime": "Jan 4, 2022", "body": "In case someone else finds this thread by googling it like me. I had the exact same issue with panoramas and the Galaxy S8, it seems the S8 camera app saves invalid panorama files. I didn't try magick, but I got my images fixed with an open source application, . Just drag and drop a picture on the app, then menu Tools > Export JPEG and check the  option.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 3, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "jackbrown1993", "datetime": "Nov 11, 2021", "body": [], "type": "issue", "related_issue": "#1594"}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1400", "issue_status": " Closed\n", "issue_list": [{"user_name": "PawelTroka", "datetime": "Jul 2, 2021", "body": "in official docs it is written that it is a storage PATH for cache, database and sidecar fileshowever I am interested in what folders are under this directory, so I can mount cache on a fast external medium-sized SSD drive, and configuration on a small fast internal SSD drive", "type": "commented", "related_issue": null}, {"user_name": "PawelTroka", "datetime": "Jul 2, 2021", "body": "ok, from  and I think it is those 3:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 2, 2021", "body": "Yes.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 2, 2021", "body": "You can also configure a different cache path by the way, it doesn't need to be in storage. See config options.", "type": "commented", "related_issue": null}, {"user_name": "PawelTroka", "datetime": "Jul 2, 2021", "body": "thanks!", "type": "commented", "related_issue": null}, {"user_name": "PawelTroka", "datetime": "Jul 2, 2021", "body": "btw which one will grow the biggest? assuming I have 1 TB of originals, how much can I expect each of them to grow?", "type": "commented", "related_issue": null}, {"user_name": "PawelTroka", "datetime": "Jul 2, 2021", "body": "ok, from:\n\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 2, 2021", "body": "You can reduce the JPEG quality and (pre-rendered) size to save storage, depending on your use case and expectations. Disk space is extremely cheap nowadays, so I personally recommend going for quality and performance even if that means 30-40% overhead. Do the math how much it costs extra, compared to your camera equipment and server hardware.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 5, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "photoprism", "datetime": "Jul 5, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1388", "issue_status": " Closed\n", "issue_list": [{"user_name": "zaharcelac", "datetime": "Jun 24, 2021", "body": "When I import video files (mp4) from my phone they got assigned wrong time. Photos made at the same moment are treated properly.:A photo and a video were shot at 14:51 EDT. Imported photo has correct 14:51 EDT timestamp. Imported video got 18:51 EDT.\nIf I check source video file with  tool, it shows 18:51 UTC.It looks like import ignores timezone metadata from video.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 24, 2021", "body": "Cab you provide us with example files for testing?Time zones are not really standardized in metadata. We calculate it based on the GPS data if any.", "type": "commented", "related_issue": null}, {"user_name": "zaharcelac", "datetime": "Jun 24, 2021", "body": "Here is the full output of video metadata. These discrepancies happen with both my Samsung phones.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jun 24, 2021", "body": "I can confirm the same issue with videos I took with my OnePlus device. I think I noticed this issue also with videos I took with an older Canon camera.\nI'd be happy to provide you with a video & photo which I took with the same device on - where should I sent those to?", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jun 25, 2021", "body": "   Testfiles can be send to  :)@zahaecelac we will have a closer look next week", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 30, 2021", "body": "Thanks for providing those samples!While the file names show about the same create time (15:33 vs 15:40), the  fields have a few hours difference:When there is no time zone, this typically means it's the local time since there is no convention (which is a shortcoming of the standard). This makes sense as not all photos / videos have GPS data. If it would be UTC by default, you wouldn't know the local time later.The JPEG additionally contains a , which should always be UTC. The file with the \"wrong\" time is the video however, so this doesn't help.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 5, 2021", "body": "I'll go ahead and close this as we've received no more feedback.", "type": "commented", "related_issue": null}, {"user_name": "zaharcelac", "datetime": "Jul 5, 2021", "body": "Hi! Should we expect a fix for videos where dates contain explicit timezone?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 5, 2021", "body": "It's something that should be fixed in your phone or camera as its use of the created date field seems inconsistent, see above. Probably best we can do is release the batch edit feature so that you can manually fix it - or have the money to manually maintain a list of problematic devices to fix their metadata. You may also look into existing Exif tools to update your video metadata and then reindex.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 6, 2021", "body": " I've checked with iPhone X, Samsung S7, Samsung A5, and OnePlus 7T - all with the same timezone issue. So it's not an obscured bug.What about using exiftool's  flag?See  for the documentation.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 6, 2021", "body": "Interesting, does this add a timezone automatically? Need to be careful, can't change the behavior with every release or depending on the Exiftool version.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 6, 2021", "body": "From what I know, it's an Android specialty and may also depends on the OS version.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 6, 2021", "body": "I'm not an exiftool expert, but I've checked also with a video taken with iPhone X iOS 14.6:Same with Samsung A5 Android 8.0.I came across some old forum threads about this topic:\n\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 6, 2021", "body": "Found this on  So basically as expected. May then be fixed for you, and broken for others. Need a camera model lookup table to get it right for everyone. Worst we can do is changing the behavior with every release.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 6, 2021", "body": "If it's an issue of \"fix for some, broken to others\" - shouldn't PhotoPrism aim to have it fixed for the majority of users? I'm not saying that  case is the same as for most users, but the quote you pasted states that UTC is the Quicktime standard in this case.BTW, what about cross checking the timezone using GPS tags when available?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 6, 2021", "body": "If you knew how often we changed things back and forth already Impossible for us to know at development time who the majority is. We'll work something out, but after having the time to think about it and perform testing.Thank you very much for looking this up in the Exiftool docs / forum! ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": "Added the Exiftool flag as requested, and started a new preview build:Let us know if this works for you!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 12, 2021", "body": "Had to tweak Exiftool parameters to make it work, and started a new preview build for testing:The new Docker image should be available within the next hour, unless the build fails and needs to be restarted (check link above).", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 13, 2021", "body": "As it turns out, the  parameter converts  to local time using the server's time zone - which doesn't help as it will still be treated as local time by PhotoPrism instead of UTC.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 13, 2021", "body": "Was just about to try the preview build and test this.So if I took a video in timezone  and another video in timezone  and my server's timezone is set to  -  will treat all videos as if they were taken in timezone  as the local time?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 13, 2021", "body": "With this fix, MP4 and Quicktime video create dates will be explicitly stored as UTC if their metadata doesn't contain a specific time zone. Other video metadata needs to include an explicit time offset or will be assumed to be in local time. I'm starting a new preview build which should be available within the next one or two hours. Check our build server for this:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 13, 2021", "body": "Development preview and  have been updated. Feedback welcome!", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 16, 2021", "body": "Hey , sorry for the late reply - I struggled setting up a preview image alongside a latest image, I ended up using SQLite for that. Plus, I wanted to gather some relevant an irrelevant videos to test this.Anyway - this seems to be working great!\nI did noticed an issue of a mp4 not being recognized as a video but with a \"live\" tag (HTML class  instead of ), and so this fix wasn't applied to it - and so the timezone wasn't detected.\nWhat are \"live\" files as opposed to \"video\" files? Is it an issue of file size and/or video length?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": "Excellent, thanks for testing! Another user tested for us as well, so we thought this is working and I closed the issue.Live photos are somewhat special in that there are \"official\" Apple LivePhotos and short videos we just classify as \"live\" so that you can hover with the mouse to watch them. The last case shouldn't result in other metadata as it's handled directly in the Exiftool JSON parser before the classification as \"live\" happens.Can you provide us with live photos to debug the issue? Either attached to this issue, or send them to  if private.", "type": "commented", "related_issue": null}, {"user_name": "dror3go", "datetime": "Jul 16, 2021", "body": "My bad - the timezone seems to be correct. I guess I was too fast to jump into conclusion.\nThanks again!", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jun 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 30, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 5, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 8, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 12, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 12, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 13, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 13, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 27, 2021", "body": [], "type": "issue", "related_issue": "#1447"}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "dror3go", "datetime": "Oct 25, 2021", "body": [], "type": "issue", "related_issue": "#1668"}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1362", "issue_status": " Closed\n", "issue_list": [{"user_name": "citadella", "datetime": "Jun 7, 2021", "body": "I was receiving the following errors in my logs when scanning these Fuji RAW files:time=\"2021-06-07T00:34:44+02:00\" level=error msg=\"location: record not found (find cell s2:14cab798d68c)\" time=\"2021-06-07T00:34:45+02:00\" level=error msg=\"location: Error 1366: Incorrect string value: '\\xC5\\x9Fikta...' for column photoprism.places.place_city at row 1 (create place s2:14cab7b3667c)\" time=\"2021-06-07T00:34:45+02:00\" level=error msg=\"location: Error 1366: Incorrect string value: '\\xC4\\x9F\\xC4\\xB1' for column photoprism.cells.cell_name at row 1 (create cell s2:14cab7b3667c)\"time=\"2021-06-07T01:54:24+02:00\" level=error msg=\"thumbs: thumbnail not cached\" time=\"2021-06-07T01:54:24+02:00\" level=error msg=\"thumbs: thumbnail not cached\"After chasing this down a bit it seemed it may have been a permissions issue, but Michael suggested turning off Darktable and it seems RawTherapee does indeed recognize these files:2021-06-07 12:00:57 DEBU camera: cache hit for fujifilm-x100s\n2021-06-07 12:00:57 INFO media: 11 thumbnails created for FujiRAWlg-02073 [1.361047004s]\n2021-06-07 12:00:56 DEBU metadata: a05be2c98993488a46e5fc1e045So it seems to be an issue with Darktable. Darktable was converting the files into .jpgs and I could download the .jpgs from within PP, but they would not display as thumbnails anywhere.", "type": "commented", "related_issue": null}, {"user_name": "citadella", "datetime": "Jun 7, 2021", "body": "Have confirmed it's working within RawTherapee, so it appears to be an issue exclusively with Darktable:2021-06-07 12:51:51 DEBU index: updated yaml file FujiRAWlg-00877.yml\n2021-06-07 12:51:51 INFO converting Unsorted Recovered Pics/Fuji RAW LARGE/FujiRAWlg-00880.RAF to jpg\n2021-06-07 12:51:51 DEBU lens: cache hit for zz\n2021-06-07 12:51:51 DEBU camera: cache hit for fujifilm-x100s\n2021-06-07 12:51:51 DEBU index: image classification took 148.85892ms\n2021-06-07 12:51:51 DEBU index: FujiRAWlg-00878.RAF.jpg was modified (new size 3921779, old size 6234504, new timestamp 1623063110, old timestamp 1622999034)\n2021-06-07 12:51:51 INFO index: updated main raw file “Unsorted Recovered Pics/Fuji RAW LARGE/FujiRAWlg-00878.RAF”\n2021-06-07 12:51:51 DEBU photo: using label Fujirawlg to create title for pquai2j2f0xirhgk\n2021-06-07 12:51:51 INFO index: updated related jpg file FujiRAWlg-00876.RAF.jpg\n2021-06-07 12:51:51 DEBU media: no new thumbnails created for FujiRAWlg-00876 [52.859µs]\n2021-06-07 12:51:51 DEBU index: updated yaml file FujiRAWlg-00876.yml", "type": "commented", "related_issue": null}, {"user_name": "mtoupsUNO", "datetime": "Jul 31, 2021", "body": "I have mixed feelings about this. I have tried both darktable and rawtherapee with my Fujifilm RAF files and they both have problems.Darktable only recently added support for my camera, so previously I needed to disable it. Rawtherapee had support first, but it is buggy (see ) so it is generating bad jpgs.I would prefer to have the ability to choose between them, using the \"Disable Darktable\" and \"Disable Rawtherapee\" checkboxes in Advanced Settings. It looks like  would always disable darktable for RAF even if I decided to disable Rawtherapee also.It may be sensible to default to Rawtherapee instead of Darktable, but please allow the user to override this default using Advanced Settings. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 31, 2021", "body": "Does it work with the version of Darktable that comes with our latest Docker image?", "type": "commented", "related_issue": null}, {"user_name": "mtoupsUNO", "datetime": "Aug 2, 2021", "body": "No it does not, when using docker image  which includes darktable 3.4.1.My Fujifilm camera is supported in darktable starting with version 3.6.0 (released about 1 month ago). I verified support for these RAF files with the standalone darktable package.I thought I could test out the new darktable pretty easily by upgrading it within the photoprism docker image. But as it stands, this line If I could override this with a config option (without recompiling photoprism, building a new docker image, etc), that would make it much easier for me to test. For now, I was able to work around this by going back to  which predates Using the older photoprism, I updated darktable to 3.6.0 and then verified that photoprism  work correctly with the new darktable on my Fujifilm RAF files. This confirms that darktable is definitely preferable to RawTherapee in my case. This is why I ask for you to let me choose which converter to use (which was true prior to ). Thanks!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 2, 2021", "body": "It should work out of the box without additional configuration, so just removing this or asking the user to configure it first won't do. We'll find a solution for our stable release. Note you can change the CLI command names (not the parameters though) to implement more complex logic eg in a  wrapper script.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": "We've added comma-separated file extension blacklists for both converters so that you can configure what to use for which RAW format:Does this work for you?", "type": "commented", "related_issue": null}, {"user_name": "citadella", "datetime": "Oct 1, 2021", "body": "Aye, should do!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 8, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 8, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 8, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 8, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 16, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 4, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 21, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 24, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 29, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 11, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/327", "issue_status": " Closed\n", "issue_list": [{"user_name": "aytacabay", "datetime": "Oct 27, 2020", "body": "I have a question ?How can I solve this problem?I am making software as a hobby. Sorry if this is logically wrong.\nHow do you think I should do this ?", "type": "commented", "related_issue": null}, {"user_name": "aytacabay", "datetime": "Oct 28, 2020", "body": "import React, { useRef, useCallback, useState, useEffect } from 'react';\nimport { View, Text, TouchableOpacity, Image, ScrollView } from 'react-native';import { RNCamera } from 'react-native-camera';\nimport ViewShot from 'react-native-view-shot';\nimport Draggable from 'react-native-draggable';export default function CameraFuncScreenShot() {const [base64ScreenShot, setbase64ScreenShot] = useState(null)\nconst [cameraIsOpen, setCameraIsOpen] = useState(true);let viewRef = useRef();let takeAPicture = async () => {\nif (cameraIsOpen) {\nawait viewRef.current.capture().then(base64 => {\nsetbase64ScreenShot(base64)\n})\n}\nawait setCameraIsOpen(false)\n}\nuseEffect(() => { }, [cameraIsOpen])return (\n<ViewShot\nref={viewRef}\noptions={{ format: 'jpg', quality: 0.9, result: 'base64' }}\nstyle={{ width: '100%', height: '100%', flexDirection: 'column', justifyContent: 'flex-start', alignItems: 'center' }}\n>\n<Draggable\nimageSource={require('../assets/imgIcons/1.png')}\nrenderSize={80}\nx={200}\ny={300}\n/>\n{\ncameraIsOpen\n?\n<RNCamera\noptions={{ format: 'jpg', quality: 0.9, result: 'base64' }}\nstyle={{ width: '100%', height: '100%', flexDirection: 'column', justifyContent: 'flex-end', alignItems: 'center', backgroundColor: 'red', }}\ntype={RNCamera.Constants.Type.back}\nflashMode={RNCamera.Constants.FlashMode.on}\nzoom={0}\nmaxZoom={0}\nandroidCameraPermissionOptions={{\ntitle: 'Permission to use camera',\nmessage: 'We need your permission to use your camera',\nbuttonPositive: 'Ok',\nbuttonNegative: 'Cancel',\n}}\nandroidRecordAudioPermissionOptions={{\ntitle: 'Permission to use audio recording',\nmessage: 'We need your permission to use your audio',\nbuttonPositive: 'Ok',\nbuttonNegative: 'Cancel',\n}}\n>\n{\n({ camera, status, recordAudioPermissionStatus }) => {\nif (status !== 'READY') {\nreturn (\n<View\nstyle={{\nwidth: '100%',\nheight: '100%',\nbackgroundColor: 'lightgreen',\njustifyContent: 'center',\nalignItems: 'center',\n}}\n>\nWaiting\n\n)\n} else {)\n}", "type": "commented", "related_issue": null}, {"user_name": "serbanstef", "datetime": "Nov 26, 2020", "body": "same with react-native-maps. any progress?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Nov 27, 2020", "body": "The main readme have a support table to explain what is technically possible and what is not.\nThere is likely no chance we can fix something in this library if some frame don't allow to get pixels from.\nFor camera I would recommend to look at possible ways offer by camera lib themselves to get an image.For reactnativemap a workaround is also mentioned in the main readme", "type": "commented", "related_issue": null}, {"user_name": "mobeendev", "datetime": "Mar 3, 2021", "body": " did you found any solution or other library.\nI am also trying to do something like this, but when a screen-shot is taken it only shows the camera image and the nested/overlayed images is not displayed.", "type": "commented", "related_issue": null}, {"user_name": "aytacabay", "datetime": "Mar 3, 2021", "body": "unfortunately no.", "type": "commented", "related_issue": null}, {"user_name": "impu1", "datetime": "Aug 11, 2022", "body": "In case anyone in the future stumbles upon something like this - this might help.\nSo I had the same issue where I had an overlaying component (position: 'absolute') that did not get included in the screenshot. I basically wrapped the component in a view with style={{ flex:1 }} and it started working.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Nov 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/425", "issue_status": " Closed\n", "issue_list": [{"user_name": "dfrl-dev", "datetime": "Jul 11, 2022", "body": "Am I misunderstanting, or is it impossible to capture a custom/complex view?I've implmented a native component for a piece of camera hardware, and I'd like to capture a thumbnail, but can only seem to get blank white images.Is the interoperability table the hard and fast list of supported views?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jul 15, 2022", "body": "it may or may not working depending on if the underlying technology allows to be captured with the current approach we have.\nrespectively:Then, there are two possibilities to solve a view that wouldn't be captured:Thanks", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jul 15, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/377", "issue_status": " Closed\n", "issue_list": [{"user_name": "alenmestrov", "datetime": "Sep 7, 2021", "body": "Hi,Is it possible to use react-native-view-shot with expo-camera in iOS?For example, like this:\n<ViewShot style={{ flex: 1, backgroundColor: \"transparent\" }}\nonLayout={(res) => { this.setVariables(res.nativeEvent.layout); }}\nref={ref => { this.imageContainer = ref; }}>\n<Camera style={{ flex: 1}}\nratio={this.state.ratio}\npictureSize={this.state.pictureSize}\ntype={this.state.cameraType}\nref={ref => { this.state.camera = ref; }}\nonCameraReady={ () => { if(this.state.pictureSize == null) { this.setupPictureSize(); }}}>\n\nThank you very much in advance.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 28, 2022", "body": "It's probably not supported at the moment and it would be wiser to have the \"expo-camera\" itself providing a way to \"capture\" on the camera component itself, because snapshotting the View that is produced by the camera won't be the best quality & can easily be broken if the rendering implementation changes.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 28, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/313", "issue_status": " Closed\n", "issue_list": [{"user_name": "michaelVictoriaDev", "datetime": "Aug 4, 2020", "body": "I have an error sayingthe captureRef code :\nmy code looks like this", "type": "commented", "related_issue": null}, {"user_name": "ViraliVasa", "datetime": "Jan 13, 2022", "body": " Any solution?", "type": "commented", "related_issue": null}, {"user_name": "mellertson", "datetime": "Apr 25, 2022", "body": "Did you find a solution to this?  I am getting the same exact error.", "type": "commented", "related_issue": null}, {"user_name": "mellertson", "datetime": "Apr 25, 2022", "body": "I got it working.  Here is what worked for me.The bit that got it working for me was to wrap ViewShot inside a SafeAreaView and then inside a ScrollView.", "type": "commented", "related_issue": null}, {"user_name": "Carry3", "datetime": "Aug 15, 2022", "body": "because of 'snapshotContentContainer: true' so you should use ref in scrollView maybe waht I say is wrong but when  I delete 'snapshotContentContainer: true' it works", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 28, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/395", "issue_status": " Closed\n", "issue_list": [{"user_name": "CanRau", "datetime": "Feb 27, 2022", "body": "I've seen  but still can't get it working. In another screen I pasted  which works just fine, so I've no idea what I'm doing wrong.\nSo far only tested on Samsung Note 8 running Android 9Successfully taking snapshot without failure", "type": "commented", "related_issue": null}, {"user_name": "CanRau", "datetime": "Mar 30, 2022", "body": "Haha, after fixing it myself the last time (forgot about this issues) I had to re-start my React Native project, installing all dependencies etc, I again failed to get it working until I realized that I had set  &  in the  options without realizing that they were (way) bigger then the actual view  I'm not sure how I fixed it the last time, but wanted to share that this might help.", "type": "commented", "related_issue": null}, {"user_name": "CanRau", "datetime": "Mar 30, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/370", "issue_status": " Closed\n", "issue_list": [{"user_name": "fdagostino", "datetime": "Aug 2, 2021", "body": "When capturing a View with two children, a gl-react Surface rendering a photo taken with the camera and a Image PNG mask on top of it, and saving it to the media library, the saved image contains only the image rendered in the Surface and is not including the PNG mask/overlay.Please, let me know if you need further info.react-native@0.63.2  invalid\n└── react-native-view-shot@3.1.2 Android - ExpoWhen capturing a View it should capture all the visible children.When capturing a View it is capturing only one child.You can find a repro here: ", "type": "commented", "related_issue": null}, {"user_name": "GitKat", "datetime": "Sep 5, 2021", "body": "Ok I ve been working on it recently. and found a way to achieve this.\nFor some reasons when you Capture directly with camera opened it will always gets on top.\nI solve this issue first taking the picture from camera\nThen sets the Image uri to rasterize other elements  on top of uri came from the camera", "type": "commented", "related_issue": null}, {"user_name": "GitKat", "datetime": "Sep 5, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "fdagostino", "datetime": "Sep 5, 2021", "body": "Hey , can you provide me with a code example?", "type": "commented", "related_issue": null}, {"user_name": "GitKat", "datetime": "Sep 6, 2021", "body": "Ok. my code is not so professional I'm just learning...\nfirst: Im using RNCamera to open camera,\nI dont use RNCamera to take pictures. Im using CaptureRef/CaptureScreen (upto you what you use )inside captureHandle function , this is where I  get the capture from RNCamera, and then setting the URI to a hook and passing\nit to image component and take an auto screenshot() after 600 msHere in second step,  im using ImageBackground Component to set my image as background.now that takeScreenShot() function capture the screen with text on it, im using date for my use case...Now the last step if you want further more actions its upto you. im just uploading at this point.", "type": "commented", "related_issue": null}, {"user_name": "GitKat", "datetime": "Sep 11, 2021", "body": "u can close this issue now !", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 28, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/281", "issue_status": " Closed\n", "issue_list": [{"user_name": "KaeganClark", "datetime": "Jan 3, 2020", "body": "Bug report\nI am using captureScreen like so:It worked the first few times I used it and now I keep getting this error:Version & PlatformAndroidExpected behavior\nTake a screenShot.Actual behavior\nError and fail to screenShot.Help is greatly appreciated!", "type": "commented", "related_issue": null}, {"user_name": "abdulbasit1248", "datetime": "Jan 5, 2020", "body": "For android the stable package is 2.5.0 please install this one.Then\nin MainApplication.Java you have to import like this\n", "type": "commented", "related_issue": null}, {"user_name": "KaeganClark", "datetime": "Jan 6, 2020", "body": "\nSteps I followed:Same error.", "type": "commented", "related_issue": null}, {"user_name": "KaeganClark", "datetime": "Jan 13, 2020", "body": " \nI have found that if the react native camera is authorized then the captureScreen() fails but if it is unauthorized the screenshot works. Ideas?", "type": "commented", "related_issue": null}, {"user_name": "KaeganClark", "datetime": "Jan 14, 2020", "body": "I solved this by removing this for loop  from viewshot.java.", "type": "commented", "related_issue": null}, {"user_name": "KaeganClark", "datetime": "Jan 14, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/411", "issue_status": " Closed\n", "issue_list": [{"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "This is failing the build and started happen after the new release of the package.", "type": "commented", "related_issue": null}, {"user_name": "UNIDY2002", "datetime": "May 25, 2022", "body": "Hi, I've also encountered a similar error:", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Looks like this is only supported in updated Java / React-Native versions. What version are you guys using?", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": " react native version 0.63.4", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Looks like a problem with Java, you need at least Java 8 for lambdas to work. I think RN changed Java to Java 11 starting 0.65, but I can't find anywhere before that what's the default Java version.", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": " is there any other way to solve this with my current react version. As this started happen only from today.", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Not without upgrading Java or changing the code to not use lambdas.", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Can you share part of your build.gradle files to see whether or not you are setting the java/jdk version to 7?", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "// Top-level build file where you can add configuration options common to all sub-projects/modules.buildscript {\next {\nbuildToolsVersion = \"30.0.3\"\nminSdkVersion = 21\ncompileSdkVersion = 30\ntargetSdkVersion = 30\nrenderscriptVersion = 21\nndkVersion = \"20.1.5948944\"\ngooglePlayServicesAuthVersion = \"16.0.1\"\nfirebaseMessagingVersion=\"21.1.0\"\n}\nrepositories {\ngoogle()\njcenter()\n}\ndependencies {\nclasspath(\"com.android.tools.build:gradle:4.1.0\")\nclasspath 'com.google.gms:google-services:4.3.5'\nclasspath \"com.bugsnag:bugsnag-android-gradle-plugin:5.+\"\n//classpath(\"com.android.tools.build:gradle:3.5.3\")\n// NOTE: Do not place your application dependencies here; they belong\n// in the individual module build.gradle files\n}\n}allprojects {\nrepositories {\nmavenCentral()\nmavenLocal()\ngoogle()\nmaven {\n// All of React Native (JS, Obj-C sources, Android binaries) is installed from npm\nurl(\"$rootDir/../node_modules/react-native/android\")\n}\nmaven {\n// Android JSC is installed from npm\nurl(\"$rootDir/../node_modules/jsc-android/dist\")\n}}", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "What's your Java version? Looks like it's not in the configuration file. Try  and  to see what it points to.From RN 0.63, the environment setup clearly states that Java 8 is needed, so I'm not sure why you would be using anything older than that: ", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "I have installed jdk11 while doing the setup. ", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Please also share the  file. The RN docs clearly define Java 1.8 () for RN 0.63. Any chance java 1.6 is defined there?", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "There's something your build stack that's telling the compiler to use Java 7, but I can't really find where that would be defined.", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "\ncompileSdkVersion 30\nbuildToolsVersion '30.0.3'\ncompileOptions {\nsourceCompatibility JavaVersion.VERSION_1_8\ntargetCompatibility JavaVersion.VERSION_1_8\n}", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Maybe android studio is pointing to java 7? From your error above, the compiler is using java 7 instead of 8.", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "If i remove this package, everything seems to be working fine.", "type": "commented", "related_issue": null}, {"user_name": "cristianoccazinsp", "datetime": "May 25, 2022", "body": "Yes, but you're still compiling with Java 7, so any new package you add may complain as everything requires at least Java 8.The easiest option is to send another PR to replace the lambda with a regular  call, but the same issue will happen with future libraries. It would be good to find the root cause of why you are using Java 7 instead of 8.", "type": "commented", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": "Much Thanks, will try it ", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 25, 2022", "body": [], "type": "pull", "related_issue": "#387"}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 25, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "EashSundar", "datetime": "May 26, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/258", "issue_status": " Closed\n", "issue_list": [{"user_name": "daheeahn", "datetime": "Sep 20, 2019", "body": "Hi! My component contains both the view and the camera screen (RNCamera). However, when I do 'captureScreen' using view-shot, only the camera's screen is captured and my view is not included in the result. How can I capture the entire screen?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Sep 20, 2019", "body": "I'm not sure to understand what you meant by \"only the camera's screen is captured\".\nI think it captures everything except the \"system UIs\" like the status bar, and i'm not sure we can technically do it in Android/iOS.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jan 19, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/250", "issue_status": " Closed\n", "issue_list": [{"user_name": "viralS-tuanlv", "datetime": "Aug 30, 2019", "body": "How can i take a shot with rncamera on ios, i try on android work perfect, but ios just a black screen when i take a shot. Have any PR to fix it? thanks", "type": "commented", "related_issue": null}, {"user_name": "Taym95", "datetime": "Aug 30, 2019", "body": "I'm just curious, why you would like to take a snapshot of a camera and not returning picture from rncamera directly?", "type": "commented", "related_issue": null}, {"user_name": "viralS-tuanlv", "datetime": "Aug 30, 2019", "body": "Because i am using rncamera for recoginze face, and if i am using take picture of rncame ra it very slow and heavy size, on android, i used to view shot and very fast and good, so i want to use take screenshot instead take picture of rncamera", "type": "commented", "related_issue": null}, {"user_name": "wcandillon", "datetime": "Oct 18, 2019", "body": " I'm having a similar issue/use case. Is the solution to pipe the camera input into a GLView? Something similar to  ?", "type": "commented", "related_issue": null}, {"user_name": "minuitagency", "datetime": "Nov 16, 2019", "body": "Hi, did anyone found a solution for this ?Thanks,Théo", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jan 19, 2020", "body": "I think if react-native-view-shot can't capture the pixels (and renders black instead) which happens inconsistently between iOS and Android, there is a low chance we can solve it from this library and I would look more at what the initial library (react-native-camera) offers in term of capturing the camera content.It seems there is a method  which is probably what you need to use for now.If the usecase to use react-native-view-shot was because of compositing/rendering some effects on top of it, I would suggest to look either at react-native-webgl (if you can write it in GL) or simply have an intermediary  and snapshot that, or even writing custom native code to do what you need.", "type": "commented", "related_issue": null}, {"user_name": "Taym95", "datetime": "Sep 11, 2019", "body": [], "type": "issue", "related_issue": "#254"}, {"user_name": "gre", "datetime": "Jan 19, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/245", "issue_status": " Closed\n", "issue_list": [{"user_name": "yigerendehaotianqi", "datetime": "Aug 18, 2019", "body": " iOS? Android?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 18, 2019", "body": "I don't think you need this library to take a camera picture, directly use   function: (we probably should remove that mention from our README)thanks", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 18, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/235", "issue_status": " Closed\n", "issue_list": [{"user_name": "jordangarvey", "datetime": "Jul 23, 2019", "body": "I’m trying to render a basic View with a border radius inside the ViewShot component (), but the saved image in the camera roll has a white background. Is it currently possible to save an image with transparency?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": "I don't think this issue is on  side. probably the cameraroll / lib you are using is adding the white color and does not support transparency?Proof that it's not a problem on this lib side:this example implement screenshotting a rounded rectangle and putting it back into an  on top of a background color. It properly works with defaults (which uses PNG)", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/227", "issue_status": " Closed\n", "issue_list": [{"user_name": "klawdyo", "datetime": "Jul 9, 2019", "body": "Warning: componentWillReceiveProps is deprecated and will be removed in the next major version. Use static getDerivedStateFromProps instead.view-shot: 2.6.0\nreact: 16.8.6\nreact-native: 0.60.0 Androidget print screen  then save in camera roll", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": "this is now fixed", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/225", "issue_status": " Closed\n", "issue_list": [{"user_name": "skuttenkuler", "datetime": "Jun 27, 2019", "body": "import ViewShot from \"react-native-view-shot\";\ncomponentDidMount () {\nthis.refs.viewShot.capture().then(uri => {\nconsole.log(\"success\")\nCameraRoll.saveToCameraRoll(viewShot, ['photo']);this.state = {\nimageURI : 'file:///sdcard/screenshot.jpg',\n}<ViewShot ref=\"viewShot\" options={{ format: \"jpg\", quality: 0.9 }}>\nAndroidTo take View Snap shot then save snapshot to Android Camera Roll", "type": "commented", "related_issue": null}, {"user_name": "sergiosrax", "datetime": "Jul 4, 2019", "body": "Is happening for me too", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": "please check with latest release (3.0.2) and prefer the use of React.Ref (via React.createRef or useRef)", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/214", "issue_status": " Closed\n", "issue_list": [{"user_name": "gaminilesh", "datetime": "May 15, 2019", "body": "Hey I'm having this issue. Please help!", "type": "commented", "related_issue": null}, {"user_name": "Taym95", "datetime": "May 15, 2019", "body": "The issue is in import, use it like this:If it works close this issue, please ;)", "type": "commented", "related_issue": null}, {"user_name": "gaminilesh", "datetime": "May 15, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "davychhouk", "datetime": "Jun 3, 2019", "body": " I encountered the same problem which was because of improper linking in iOS project.\nYou have to manually link this package since  does not really link the package for you in the iOS project.I was able to get away from this issue following this manual linking: If it works, please consider closing the issue.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/193", "issue_status": " Closed\n", "issue_list": [{"user_name": "nkaczor", "datetime": "Jan 13, 2019", "body": "I am trying to use react-native-camera along with react-native-view-shot\nI have following code:And I can see this Text component in live, but unfortunately it isn't contained in view shot result image. This also concerns other components as well.\nI've found this  which describes my problem, but without any working solutionreact-native@0.57.8\n└── react-native-view-shot@2.5.0 AndroidElements which are drawn on top of react native camera preview are visible in snapshotElements which are drawn on top of react native camera preview aren't visible in snapshot", "type": "commented", "related_issue": null}, {"user_name": "OleksandrKucherenko", "datetime": "Jan 15, 2019", "body": "Camera is a difficult thing... its a direct buffer to GPU video memory. Capturing of the screen with GPU video buffer is not so trivial.Camera, OpenGL - those things require special code for making possible capturing and not always its possible.", "type": "commented", "related_issue": null}, {"user_name": "OleksandrKucherenko", "datetime": "Jan 29, 2019", "body": "hint: deep tech details: ", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jan 29, 2019", "body": "Yes, I don't think it will be easy to offer a cross-library dependencies unless we have a way to interop between react native libraries but it's pretty tricky. I think what you are looking for is a way to get a frame from the Camera natively\nif doing effect over it, then you can use in second step the camera snapshot and draw something on top and do a second shot.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jan 29, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/181", "issue_status": " Closed\n", "issue_list": [{"user_name": "BipinAle", "datetime": "Dec 5, 2018", "body": "How to get image from uri?captureRef(this.refs.viewShot, {\nformat: \"jpg\",\nquality: 0.8,\nwidth: size, height: size,\nresult:\"tmpfile\"\n})\n.then(\nuri => console.warn(\"Image saved to\", uri),<== this uri is pointing towards android cache file and when I open there is nothing.\nerror => console.warn(\"Oops, snapshot failed\", error)\n);androidI want to the change uri to base64.There is no data in cache folder.", "type": "commented", "related_issue": null}, {"user_name": "ZMChoo", "datetime": "Dec 19, 2018", "body": "Hi  , I also faced the same issue here, the cache file is empty, how would you fixed it?", "type": "commented", "related_issue": null}, {"user_name": "BipinAle", "datetime": "Dec 20, 2018", "body": "@mun5865 cache file is never accessible. In my case, i converted that uri in to base64 and got the actual image.", "type": "commented", "related_issue": null}, {"user_name": "Piyush132000", "datetime": "Dec 9, 2020", "body": "hlo friends, i am here with solution of this problem , For solving this problem you have to go on your App permission on your real mobile and allow for camera storage then you can easily save your ViewShot on Your Mobile.go to App Permisssion in your App.info\nallow Camera accesss storage", "type": "commented", "related_issue": null}, {"user_name": "BipinAle", "datetime": "Dec 5, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/163", "issue_status": " Closed\n", "issue_list": [{"user_name": "alz10", "datetime": "Jul 20, 2018", "body": "how can i change the file destination when i capture() it? the  puts it in a folder where it is hidden and cannot be seen in my phone file manager.For example i wanted to save it in  folder..", "type": "commented", "related_issue": null}, {"user_name": "alz10", "datetime": "Jul 20, 2018", "body": "solved: Using ", "type": "commented", "related_issue": null}, {"user_name": "omorhefere", "datetime": "Jul 31, 2018", "body": "How did you avoid this error: \"Error: open failed: EACCES (Permission denied)\"?", "type": "commented", "related_issue": null}, {"user_name": "HeinXtet", "datetime": "Sep 14, 2020", "body": "before save to file request storge permission of both", "type": "commented", "related_issue": null}, {"user_name": "Piyush132000", "datetime": "Dec 6, 2020", "body": "hii bro i am also facing same problem if you find answer of this problem please help me", "type": "commented", "related_issue": null}, {"user_name": "Piyush132000", "datetime": "Dec 9, 2020", "body": "hlo friends, i am here with solution of this problem , For solving this problem you have to go on your App permission on your real mobile and allow for camera storage then you  can easily save your ViewShot on Your Mobile.go to App Permisssion  in your App.info\nallow Camera accesss storage", "type": "commented", "related_issue": null}, {"user_name": "alz10", "datetime": "Jul 20, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/159", "issue_status": " Closed\n", "issue_list": [{"user_name": "personapp", "datetime": "Jul 6, 2018", "body": "readFile() method of both \"react-native-fs\" and \"rn-fetch-blob\" works well for static asset files or the output of \"react-native-camera\" library. But the readFile() method can not read the file that captured via \"react-native-view-shot\" as tmpfile.react-native@0.52.2\nreact-native-view-shot@2.3.0\nreact-native-fetch-blob@0.10.8\nrn-fetch-blob@0.10.11\nreact-native-fs@2.10.14Am I missing something ?\nThanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Jul 6, 2018", "body": "how do you check if it worked? is console.warn logging something?", "type": "commented", "related_issue": null}, {"user_name": "personapp", "datetime": "Jul 6, 2018", "body": "This is not the full code. I reduced it to highlight the issue.console.warn logs that RNFetchBlob failed to read file. This only happens when the filePath comes from react-native-view-shot", "type": "commented", "related_issue": null}, {"user_name": "personapp", "datetime": "Jul 6, 2018", "body": "I also tried to read file via ImageStore.getBase64ForTag(). It throwed \"ERCTERRORDOMAIN0\" error.\nI should also note that, Image component can load the captured image file.", "type": "commented", "related_issue": null}, {"user_name": "personapp", "datetime": "Jul 10, 2018", "body": "Any solution please.", "type": "commented", "related_issue": null}, {"user_name": "jgreen210", "datetime": "Sep 6, 2018", "body": ", are you seeing this for android, iOS or both?", "type": "commented", "related_issue": null}, {"user_name": "jgreen210", "datetime": "Sep 7, 2018", "body": "If this problem you are having is just for android, there's some chance  might help.  Although, I suspect you have some other problem. That's since we didn't get any failures due to the issue fixed by this PR in our tests (which compare pngs saved by this library with checked-in reference images) and since you're getting errors for base64 images too, and there's no buffering involved in that case.", "type": "commented", "related_issue": null}, {"user_name": "serhiipalash", "datetime": "Nov 13, 2018", "body": "Hi  !\nI think this bug is still active. has this bug in Expo 31 compare to Expo 30 (iOS tested). In Expo 31  was updated to use  v2.5.0, and now  result \"tmpfile\" uri is not compossible with  or  any more.\nThat is because \"tmpfile\" uri path is in  and  and  can only work with paths that are in public system scope . When you try to save taken view snapshot to user photos or temporary dir, you will see an error \"Couldn't read ...\".As I remember in Expo 30 version of  was something around v1.1.0.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 11, 2019", "body": "I think this is fixed. I was properly able to get the data from file and render it back in works on iOS and Android.Please report back if you are still having an issue (writing an example to reproduce the bug is the best)", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 11, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/148", "issue_status": " Closed\n", "issue_list": [{"user_name": "hungdt-ibl", "datetime": "May 17, 2018", "body": "  Android?Build release android complete", "type": "commented", "related_issue": null}, {"user_name": "OleksandrKucherenko", "datetime": "Oct 16, 2018", "body": "", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 10, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/91", "issue_status": " Closed\n", "issue_list": [{"user_name": "herel", "datetime": "Aug 28, 2017", "body": "** render device **\n\nAny suggestion?", "type": "commented", "related_issue": null}, {"user_name": "herel", "datetime": "Aug 29, 2017", "body": "Fixed  :Dchange in RNViewShotModule.javaFile externalCacheDir = context.getExternalCacheDir();\nFile internalCacheDir = context.getCacheDir();toFile externalCacheDir = context.getExternalCacheDir();\nFile internalCacheDir = context.getExternalCacheDir();Switch to an accessible directory or\nHe seems to be running out of memory", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 29, 2017", "body": "Hi, thanks for the issue,that createTempFile code was borrowed from react-native's it looks like the current logic to pick the cache folder is:so i'm not sure it's about running out of space problem because it should always pick the bigger space.. however maybe there are some weird other issue?would you mind trying to investigate more, like what is the exception exactly (if any?)also try to log value of  and ", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Sep 22, 2017", "body": "not sure what we can do better than current approach (which should chose the best available space)", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Sep 22, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/85", "issue_status": " Closed\n", "issue_list": [{"user_name": "Friendly-Robot", "datetime": "Aug 22, 2017", "body": "I'm currently using a derivative of React-Native-Camera that doesn't have a capture functionality. So in order to \"capture\" the image of the camera, I'm attempting to utilize React-Native-View-Shot. However, despite successfully returning the promise with the uri, the saved view shot renders a black image. Is this behavior normal for camera views?I've tried taking the view shot from  and from the  to no avail. Does anyone have another suggestion which I may attempt?Unfortunately, using a different camera library which does support capture is not an option. :(", "type": "commented", "related_issue": null}, {"user_name": "Friendly-Robot", "datetime": "Aug 23, 2017", "body": "I ended up switching between the non-capturing camera view and React-Native-Camera using state in order to get the shot.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 23, 2017", "body": "Yeah I think camera as well as videoviews and glkviews is going to produce a black result (and not sure if this can be solved).", "type": "commented", "related_issue": null}, {"user_name": "Friendly-Robot", "datetime": "Aug 23, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "n1ru4l", "datetime": "Jan 8, 2019", "body": [], "type": "issue", "related_issue": "react-native-camera/react-native-camera#2035"}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/79", "issue_status": " Closed\n", "issue_list": [{"user_name": "fxfactorial", "datetime": "Aug 13, 2017", "body": "Hi,I am trying to take a snap shot of an image that is created by react-native-camera along with an image inside of that image, to create a watermark like effect. What I'm getting though is a blank image, I feel like the code is correct and that the zIndexing might be part of the part?", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 25, 2017", "body": "Hi  , are you still experiencing this issue, have you find a workaround? I have not tested snapshotting zIndexing a lot yet, maybe we should test with a simpler example see if things would work (like snapshotting a container with 2 absolute rectangle overlapped zIndex)", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Aug 25, 2017", "body": "Also I don't think  as a background is longer supported in React Native, they seems to have introduced an ImageBackground tho (  )", "type": "commented", "related_issue": null}, {"user_name": "fxfactorial", "datetime": "Aug 25, 2017", "body": " Yes, still have this issue. Correct, Image this way is deprecated but this is on 0.45 when it was okay.", "type": "commented", "related_issue": null}, {"user_name": "jonasmorthorst", "datetime": "Feb 2, 2018", "body": "Hi  - are you still facing this issue?", "type": "commented", "related_issue": null}, {"user_name": "fxfactorial", "datetime": "Feb 2, 2018", "body": "", "type": "", "related_issue": null}, {"user_name": "gre", "datetime": "Mar 29, 2018", "body": "please try with  , probably will fix it", "type": "commented", "related_issue": null}, {"user_name": "mobeendev", "datetime": "Mar 3, 2021", "body": " I put an image on top of camera and took screenshot, but image is not displaying when screen shot is taken!\ncan you suggest what should I do.", "type": "commented", "related_issue": null}, {"user_name": "gauravroyzz", "datetime": "Aug 5, 2021", "body": "any update on this? i have a mapView with 2 views absolute positioned , I cant see them on the screenshot", "type": "commented", "related_issue": null}, {"user_name": "fxfactorial", "datetime": "Aug 13, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "gre", "datetime": "Mar 29, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/58", "issue_status": " Closed\n", "issue_list": [{"user_name": "valentinancy", "datetime": "Apr 26, 2017", "body": "I was tried the example code and i got a uri like this:\nfile:///data/user/0/com.last/cache/ReactNative-snapshot-image1446502249.png just like:\n\"/storage/emulated/0/Pictures/ReactNative-snapshot-image799473449.png\"so i could use the path for the CameraRoll library from react native like:\nCameraRoll.saveToCameraRoll(path,'photo')Thanks and please help hehe", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Apr 26, 2017", "body": "have you tried using the  option ? ", "type": "commented", "related_issue": null}, {"user_name": "valentinancy", "datetime": "Apr 26, 2017", "body": "Thanks! sorry i miss reading your documentation!", "type": "commented", "related_issue": null}, {"user_name": "bhawnaparasher", "datetime": "Apr 23, 2019", "body": "How did you solve this?\nI am facing the same issue, please help!", "type": "commented", "related_issue": null}, {"user_name": "valentinancy", "datetime": "Apr 26, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/gre/react-native-view-shot/issues/61", "issue_status": " Closed\n", "issue_list": [{"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "i mean if you save image into path: DCIM/Camera ,\nthe image file could find in [file manager],but not showed in system photo application", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Apr 30, 2017", "body": "have you tried to use  ?according to some answers on  to save into gallery should be android's  which is exposed by the lib in dirs.PictureDir with this code: ", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "i am  very gratefull to recieve your reply,  i have allready tried use dirs.PicturesDir , and DcimDir ，if you use origin-android-system , image in those Dir is abled to be finded in gallery 。 but if you  use customization-android-system  ,such as xiaomi,oppo,huawei,lenovo ,it is can't , it lead to if user  use other app such as wechat or facebook ....,they can't find the image they have captured.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Apr 30, 2017", "body": "I'm not sure to know the answer :'(\nmaybe the customization-android-system use another path?", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "i checked those picture,it is in the same path。but not showed，it is seems customization-android-system have traced the Image url  when take a photo or take a screenshot or save a picture 。and only show picture is traced", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "\nhere is a sample , when i use wechat, wechat could display the image i have save from browser or screenshot i have taked just now。but, not showed if i use  react-native-view-shot,  and they are in the same path.", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Apr 30, 2017", "body": "maybe they rely on the filename? (like the DCIM number) or maybe they rely on a given extension?\n(have you tried JPG vs PNG ?)", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": ",yeah,i have tried, i tried different filename and quality, even the same file name but still not show。i  search the reason from net,seems when save a image,it is neet to notice system ,to let system know gallery is updated.code like this\n", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "Apr 30, 2017", "body": "do you want to try that and provide a PR if this works?", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "i don't know the real reason,i am not a android developer,i just search from the net. it is a difficult work for me ,but i think i will spend time to figure it out  ...", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "Apr 30, 2017", "body": "now it 3:00 clock beijing time ,i need to go sleep.i will try it tomorrow . i think this is the solution ,it is in stackoverflow,the link you give me", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "May 7, 2017", "body": "hey,dear  , i have tried and suceed by add this,work perfectly\n", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 7, 2017", "body": "oh! cool :) would you like to create a PR ?", "type": "commented", "related_issue": null}, {"user_name": "aqnaruto", "datetime": "May 7, 2017", "body": "thank you ,but ,what is pr?  public project??pull request? i am allready create a pull request", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 7, 2017", "body": "sorry, a Pull Request, on Github", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 23, 2017", "body": "see PR in short: a better solution is to use CameraRoll.saveToCameraRoll and we will deprecate use of path in this library", "type": "commented", "related_issue": null}, {"user_name": "gre", "datetime": "May 23, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/2277", "issue_status": " Closed\n", "issue_list": [{"user_name": "mbrine555", "datetime": "May 1, 2018", "body": "When using the dependency parser, it seems like there can be a lot of ambiguity when trying to assign negation, unlike with something like Stanford's parser.For example:returnsThis output seems to imply that the negation could refer to either , which is not the case. A Stanford output for the same sentence looks something like:Is there any way to clear up this ambiguity currently?", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "May 1, 2018", "body": " Which Stanford dependency scheme is that?", "type": "commented", "related_issue": null}, {"user_name": "mbrine555", "datetime": "May 1, 2018", "body": " Sorry, I should've been more clear. It's the Universal Dependency scheme used in Stanford's CoreNLP 3.9.1.", "type": "commented", "related_issue": null}, {"user_name": "dxiao2003", "datetime": "Jun 15, 2018", "body": "If you run the visualizer you can see that the \"not\" refers specifically to the first \"is\" token.  Not exactly the same parse as Stanford but at least it's not ambiguous as to which \"is\" it's referring to.", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Dec 14, 2018", "body": "Merging this with . We've now added a master thread for incorrect predictions and related reports – see the issue for more details.", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "Jan 13, 2019", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "May 1, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ines", "datetime": "Jul 6, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ines", "datetime": "Aug 15, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "ines", "datetime": "Dec 14, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lock", "datetime": "Jan 13, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/77", "issue_status": " Closed\n", "issue_list": [{"user_name": "sffranke", "datetime": "Dec 13, 2019", "body": "Hi all,\nthe docker image crashes after trying to set up a wakeword and refuses tu start up again.\nSo I tried the virtual environment version. It always crashes when restarting after saving changes.\n./run-venv.sh: line 28:  2303 Segmentation fault      python3 app.py \"$@\"Any hints how to run this amazing software under raspbian buster?\nTIA\nSteffen", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Dec 13, 2019", "body": "I'm getting a lot of bug reports lately about the latest Rhasspy on Rpi, so I'm guessing there's something out of whack with Python 3.7 and one of the dependencies. I'll take a look ASAP and push a new version soon with (hopefully) some fixes.", "type": "commented", "related_issue": null}, {"user_name": "sffranke", "datetime": "Dec 13, 2019", "body": "At the moment I just restart it after each save 'n crash. I am very impressed of your work, thanks so much for sharing!", "type": "commented", "related_issue": null}, {"user_name": "KiboOst", "datetime": "Dec 13, 2019", "body": "Hi,I had exact same behavior/error.Here is my config:\nConfig:But curiously I don't have it anymore ! The only change I can remember of is settings a custom snowboy hotword. Not sure if realated of course.", "type": "commented", "related_issue": null}, {"user_name": "frkos", "datetime": "Dec 23, 2019", "body": "Just switched from ALSA to pyAudio and had the same issue...\nI've tried to change microphone device in settings and found that when I select  the issue disappears (I'm using PS3eye camera as a mic)So in profile I have:Please try to do the same, maybe it will help you too\nI'm in the middle of testing but the result is promising", "type": "commented", "related_issue": null}, {"user_name": "NullEnt1ty", "datetime": "Jan 26, 2020", "body": "In my case this was fixed by switching from  to . I'm using the  which seems to act odd with PyAudio.This is the content of my  file if someone wants to debug:", "type": "commented", "related_issue": null}, {"user_name": "moritzschaefer", "datetime": "Mar 22, 2020", "body": "I still face this exact same issue (Pi 3B). I pulled the most recent git repository and each time I save+restart from within the webapp, I get the segfault.", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Dec 13, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Jan 15, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/2", "issue_status": " Closed\n", "issue_list": [{"user_name": "dmshimself", "datetime": "Dec 31, 2018", "body": "Hi - I installed a fresh copy of hass.io on a Pi3 with a ps3 USB camera plus audio USB device plugged in and managed to get to the web interface just fine.  As a test, I selected that (2: USB Camera-B4.09.24.1: Audio (hw:1,0). and press Hold to Record and held the mouse down, but when I let go, the Release to stop remained red.  I left it for a while but nothing came back.  In the add on log I get the following.  Any thoughts appreciated and I'm happy to run any other initial tests recommended.  The main hassio system log showed nothing unusual that I could see, just saying the add was started. version 1.13", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Feb 18, 2019", "body": "Everything in the log looks OK to me. Can you try updating to the latest version and, if that doesn't work, try switching the microphone system from PyAudio to ARecord in the Settings page? Thanks.", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Mar 21, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/okfn-brasil/serenata-de-amor/issues/69", "issue_status": " Closed\n", "issue_list": [{"user_name": "Irio", "datetime": "Sep 16, 2016", "body": "No legalese allowed in this document. It is expected to be written just in Portuguese, since the data journalists are all fluent in the language.Check \"Texto Atualizado\" at  for the original text.", "type": "commented", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Sep 21, 2016", "body": "Artigo 2 - Atende as seguintes despesas:", "type": "commented", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Sep 21, 2016", "body": "Artigo 1 - Pode utilizar mais R$ 1353,04 reais, o deputado que for:", "type": "commented", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Sep 21, 2016", "body": "Artigo 3 - A cota pode ser usada da seguinte forma:Artigo 4 - Reembolso é efetuado mediante requerimento padrão, assinado pelo parlamentar, que assume inteira responsabilidade pela despesa, atestando que:4, 5 e 6: Estão no item 2.Sobre o item de uso da cota para ursos, palestras, seminários, simpósios, congressos ou eventos do mesmo gênero:", "type": "commented", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Sep 21, 2016", "body": "Artigo 5. Sobre transporte aéreo e de serviços e produtos postais podem ser feitos desde que sejam comprovados com Requisição de Passagem Aérea (RPA) e Requisição de Serviços Postais (RSP)Artigo 6. As empresas de transporte aéreo credenciadas devem informar, quando solicitado, informações detalhadas dos bilhetes emitidosArtigo 7. Se o bilhete emitido é contra as normas, é descontado automaticamente em folha de pagamentoArtigo 8. Despesas com telefone só são reembolsadas quando comprovadas que são de responsabilidade do deputado.Artigo 9. Não é admitido aluguel de imóvel que pertença ao próprio deputado ou qualquer entidade que ele tenha participaçãoArtigo 10. Artigo 11. A cota é calculada proporcionalmente ao período de efetivo exercício no mês.Artigo 12., desde que não haja convocação de suplenteArtigo 13. Artigo 14. A cota não pode ser adiantada, transferida de um beneficiário para outro.Artigo 15. ", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 3, 2016", "body": "Great work! Let's publish it? Where? We can convert it do  and add to the repo, or as a linked Gist, whatever…", "type": "commented", "related_issue": null}, {"user_name": "Irio", "datetime": "Sep 16, 2016", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "cuducos", "datetime": "Oct 19, 2016", "body": [], "type": "issue", "related_issue": "#91"}, {"user_name": "cuducos", "datetime": "Oct 20, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Nov 4, 2016", "body": [], "type": "", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 5, 2016", "body": [], "type": "", "related_issue": null}, {"user_name": "anaschwendler", "datetime": "Nov 5, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 8, 2016", "body": [], "type": "", "related_issue": null}, {"user_name": "wisner23", "datetime": "Nov 13, 2016", "body": [], "type": "", "related_issue": null}, {"user_name": "Irio", "datetime": "Feb 27, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "cuducos", "datetime": "Feb 28, 2018", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/7744", "issue_status": " Closed\n", "issue_list": [{"user_name": "mitchellspryn", "datetime": "Apr 11, 2021", "body": "When attempting to install spacy via pip (both CUDA and non-CUDA) on ubuntu 18.04, there are lots of compilation errors:", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "Apr 12, 2021", "body": "Hi, you probably just need to upgrade pip and related tools before installing spacy:After upgrading pip, it should install binary wheels on x86_64, so you probably won't need to compile anything. And even if you do, the newer pip will be able to compile the package correctly.We would also strongly recommend using a virtual env. You can see general install instructions here, click \"virtual env\" to add the additional instructions for that:", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "Apr 12, 2021", "body": "Let me move this to installation section of the new discussion board...", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "Apr 12, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "explosion", "datetime": "Apr 12, 2021", "body": [], "type": "locked and limited conversation to collaborators", "related_issue": null}]},
{"issue_url": "https://github.com/okfn-brasil/serenata-de-amor/issues/95", "issue_status": " Closed\n", "issue_list": [{"user_name": "pmargreff", "datetime": "Oct 26, 2016", "body": "Hi guys, how about data visualization, do you already chat about? It could be a good way to show to people who haven't any technical skills or are from different field how the project would save a lot of money. I started develop some charts using D3. I'm curious to know about the kind of features you think is relevant to show (?). At this time I'm trying the money by person, by state and by subquota, what do u think about it?", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Oct 27, 2016", "body": "That's a great topic — thanks !I'm aware  was working on something visual, and  might be interested in this material for communication purposes. this is a topic to be discussed in the  in the near future — when we consolidate a structure for a website focused on communication this repo here tends to narrow its focus to data science, while the other one will embrace many communication related topics I guess.Filipe and Pedro — what do you think?Pablo would you mind sharing something you already have? Maybe some screenshots would be nice to give more substance to what can be done from it.", "type": "commented", "related_issue": null}, {"user_name": "filipelinhares", "datetime": "Oct 27, 2016", "body": "Hey !I'm studying data vis and starting to dive in D3 and other tools. As  said, when we consolidate a structure for the new website we can use visualizations to improves the experience and interaction with the data.Data visualization is an awesome field to explore in this project .", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Oct 28, 2016", "body": "We think on three basic kind of visualizations to startup.: Compare total values by states monthly, we do that dynamically using .: A heatmap, to show companies who have received most piece of money, the heatmap show the value, company name, cnpj and ranking position on mouse hover event. We use  because we have found only D3 heatmap using days and months to index the values (calendar heatmaps), maybe in near future we can adapt  to accept a simple matrix and use that.: A  to each congress person. The radar contain 18 axes with subquota categories and the money spent in the each category. Maybe in a second moment we can join 2 or more persons in the same radar to compare (). We have tried use , but the problem is, it was build in D3 V3 and it isn't compatible with D3 V4, would be awesome if someone help or refactor this model to version 4, because this Radar looks better in compare with all others.After finish an reasonable model and documenting the basic (maybe on the next week) I can open the repo to anyone get it and add new visualizations.", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Oct 29, 2016", "body": "Another thing, maybe show the values by person in this , where each color is a sub-quota group and inside we can set the companies and correspondent value in the proportion. What do you think about that?", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Oct 31, 2016", "body": " Those pieces of dataviz are awesome! I think we should fit it somewhere, sure thing.  do you think the new website has an section for that?  any thoughts on that?", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Nov 3, 2016", "body": "Hey,  a (temporary) first version, you can check the  with a basic documentation to start up the project. Feel free to suggestions or ideas.", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 3, 2016", "body": "Wow! That's awesome. Thanks for that, !I definitively believe this could help people understand the importance of this quota.  can take it into account while planning our website and  while planning our communication.Just one minor detail (I'm not criticizing, just trying to make data more meaningful for people): I think the view by state has a lot of bias: , and the allowance also differs from state to state (some pages there are returning  — we can check that later). How difficult is it to ponder the total by state according to:Once more, many thanks, mate!", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Nov 3, 2016", "body": " I understand, it could point to the wrong way if u don't have all details, but I don't think it's hard to fix.For the first suggestion is possible  from this state, it will show the mean, and show the number of congress person and total value in the tooltip.I'm only a little confuse with the second one, you say the possible total  or the ? And the suggestion is about generate a number equalizing this two metrics in the same one or divide in two different views/charts?About the  - it's probably because it isn't a server, in the really is something like by backup computer and it isn't properly prepare to maintain a website stable. I have another problem with the size of  from the third view, I thinking how compact or make this file smaller (1.3 MB on this moment), but I haven't any good idea yet.", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 3, 2016", "body": "About the second one, I suggest (it's merely a suggestion, I haven't put a lot of thought on it) dividing:On a second stage dataviz could show who (within this given state) pushes the mean up or down…About the  it was com camera.gov.br (not on your server). I was trying to link the max allowance by state for you ; )", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Nov 3, 2016", "body": "Yes, I get it and make some sense, but I don't know if will have some impact if the people don't have an idea from the value itself. Maybe a line or something marking the max limit could representing almost the same.I really like about the suggestion to see the outliers. I'll think in something and exec when I have some time.I updated to mean value, and it really equalize much better, but some weird things happen' like in the lasts months of year (2014/2015) we can see the value bit the max. I'll try to find why and when it's happen to try explain this point.Another observation, I was checking the last behavior and the  value isn't consistent for all occurrences, you can .", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 3, 2016", "body": "Good. I couldn't spend some time on that today, I'm sorry about it.We're debating this on , but we haven't reached a decision yet. Hold on a while longer ; )", "type": "commented", "related_issue": null}, {"user_name": "ronybarbosa", "datetime": "Nov 4, 2016", "body": "Why don't you use tools like kibana for data visualization ?", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Nov 6, 2016", "body": " About the net_value bit the roof, I did found any reason, but I send a request to like you suggest on .@ronydj Hello, I never use that, the only thing I know: the people use that with Elastic Search (and it doesn't mean nothing to me). If you know, you don't care about teach, you have free time, contact me on: pmargreff at gmail dot com. I added two new charts on site, monthly value and average by party(really like this one).", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 6, 2016", "body": " Looking forward to check what they're gonna say ; )There's a small  (Montly instead of Montly in one of the titles). But overall it's very good ; )", "type": "commented", "related_issue": null}, {"user_name": "kassimorra", "datetime": "Nov 10, 2016", "body": "Hi Guys,\nFriend of mine told my about this great project. I got interested on this subject.Is there anywhere that can I see what you want to show ?I read this topic but didn't found the storytelling or the analysis that need to be done.Kassim", "type": "commented", "related_issue": null}, {"user_name": "pmargreff", "datetime": "Nov 10, 2016", "body": "I get a answer about the net_value bit the max value, the complete answer was:Translate:And the  say that. I believe it could impact on some other metrics too.", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Nov 11, 2016", "body": "Great,  — many thanks for sharing their response.Welcome . Sorry about not getting back to you sooner.We were discussing that these days and probably  will be in touch — he's focused on communication and probably you both could better discuss what would be interesting in terms of .", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Mar 24, 2017", "body": "Closing this as dataviz is more relevant at  repo now.", "type": "commented", "related_issue": null}, {"user_name": "cuducos", "datetime": "Oct 31, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "cuducos", "datetime": "Mar 24, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Irio", "datetime": "Feb 27, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "thaisvergani", "datetime": "Feb 2, 2019", "body": [], "type": "issue", "related_issue": "#454"}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/1826", "issue_status": " Closed\n", "issue_list": [{"user_name": "sanjeeku", "datetime": "Jan 11, 2018", "body": "I just installed Spacy 2.0.5 in Python 3.6.4 (that Anaconda).\nI also installed the default model ('en')\nSpacy is giving seg fault when I try to load my text file (it is about 2MB in size).Here's the code the reproduces it:\nPython 3.6.4 |Anaconda, Inc.| (default, Dec 21 2017, 21:42:08)\n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.I tried with another text file (slightly larger though) with the same result.Is there any other information I can provide to troubleshoot this seg fault?", "type": "commented", "related_issue": null}, {"user_name": "sanjeeku", "datetime": "Jan 11, 2018", "body": "An update:  I tried on a new/clean aws instance where I installed Spacy differently (using conda forge). I still got the same seg fault.Here's the environment info:\n(py3) ubuntu@ip-172-31-16-211:~$ python -m spacy info --markdown", "type": "commented", "related_issue": null}, {"user_name": "sanjeeku", "datetime": "Jan 11, 2018", "body": "Further update:\nI had an old conda env with spacy 1.9.0 installed.\nBoth text files were parsed perfectly.\nSo the SegFault issue is only with Spacy 2.0 or later (I have tested with 2.0.4 and 2.0.5)", "type": "commented", "related_issue": null}, {"user_name": "godelstheory", "datetime": "Jan 30, 2018", "body": "I am experiencing a similar issue, though it occurs when using the English language model  method. The problem occurs < 1% of the time in a corpus of 250K documents, but I have yet to determine its root cause. An example paragraph is shown below.Similarly, the problem occurs in 2.0.5, but is not present in 1.9.0. I have reproduced this across multiple machines.", "type": "commented", "related_issue": null}, {"user_name": "sanjeeku", "datetime": "Feb 5, 2018", "body": "Confirming that this bug continues to exists in Spacy v2.0.7\n - I am happy to privately send you the text files on which it is bombing. Please let me know where to send.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Feb 17, 2018", "body": " Thanks, could you mail to  ?", "type": "commented", "related_issue": null}, {"user_name": "sanjeeku", "datetime": "Mar 5, 2018", "body": " -- Just emailed the text file.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 29, 2018", "body": " Thanks for the text, finally got to this.Your issue is simply that the text is too long. This is rather frustrating --- I wish we used less temporary memory per word than the neural networks currently do. However, I don't see a way around this without significantly impacting performance.I've added an error message and added an option on the  class to note the problem. In your case, the solution is very simple: just process each newline individually. Your problem is different. I think the problem occurs from parsing the text twice. This shouldn't cause a segfault, but as a workaround, you can avoid doing that for now? You can verify that the double-parsing is the problem by changing the first line to .", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "Jan 7, 2019", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Jan 12, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 29, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 29, 2018", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "idc9", "datetime": "Apr 9, 2018", "body": [], "type": "issue", "related_issue": "chartbeat-labs/textacy#154"}, {"user_name": "ines", "datetime": "Dec 8, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lock", "datetime": "Jan 7, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/2470", "issue_status": " Closed\n", "issue_list": [{"user_name": "prashant334", "datetime": "Jun 21, 2018", "body": " \nbelow is a code that I have used as u mentioned in my previous question.\nafter execution getting error likeTraceback (most recent call last):\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 118, in \nmain('en')\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 86, in main\ntrain_ner(nlp, train_data, output_directory)\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 32, in train_ner\nfor batch in minibatch(get_gold_parses(nlp.make_doc, train_data), size=3):\nFile \"/home/prashant/.local/lib/python3.6/site-packages/spacy/util.py\", line 393, in minibatch\nbatch = list(cytoolz.take(int(batch_size), items))\nFile \"/home/prashant/PycharmProjects/sales_ag/AG_NER.py\", line 19, in get_gold_parses\ngold = GoldParse(doc, entities=entity_offsets)\nFile \"gold.pyx\", line 418, in spacy.gold.GoldParse.\nKeyError: 0", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 22, 2018", "body": " ANY UPDATE HOW TO RESOLVE THIS?", "type": "commented", "related_issue": null}, {"user_name": "SandeepNaidu", "datetime": "Jun 22, 2018", "body": "Prashant,\ntrain_data is empty? Did you not paste that code? Can you paste some sample?", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 22, 2018", "body": "I intentionally keep it empty because they are too big.  But here I am attaching\n", "type": "commented", "related_issue": null}, {"user_name": "SandeepNaidu", "datetime": "Jun 22, 2018", "body": "Try changing this line\ngold = GoldParse(doc, entities=entity_offsets)\nto\ngold = GoldParse(doc, entities=entity_offsets['entities'])Later after fixing this,\nAlso the label strings you are adding to the ner component are different from the sample text you have given. Make sure they match.", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 25, 2018", "body": " yes labels I have changed. Please review below code.", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 25, 2018", "body": "\ngold.pyx key error is not now showing. But ner results are zero. Here I am attaching my test data.", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 25, 2018", "body": "   What could be the reason for gold.pyx error?", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Jun 25, 2018", "body": "As  mentioned above, the labels in your code are not consistent. In some places, you're using , and in others, it says . If your results are bad, this can also have other reasons: for example, the task might just be difficult to learn, especially with just a handful of examples.Crime locations and victims aren't very well-defined, independent categories. They're mostly defined by the  and  within the text. For example, if \"California\" refers to the US state, it's always a  entity. But is it a crime location? That really depends. Sometimes you can find that information in the same sentence. Sometimes not. That makes those labels very difficult to predict from machine learning point of view. A better strategy for training a model could be to focus on improving the predictions of , ,  etc. and then use the dependency parse (or a custom parser) to extract whether they refer to a crime location or a victim.By the way, for general usage questions like this that are specific to your code, you might want to  instead. This will reach more people and you'll often get quicker replies. On the issue tracker, we mostly try to focus on bug reports, performance issues and feature requests. It's also always helpful if you can provide a small code snippet that illustrates the problem, instead of dumping an entire script. This makes it easier for us and others to reproduce the problem and help.", "type": "commented", "related_issue": null}, {"user_name": "prashant334", "datetime": "Jun 25, 2018", "body": " I have used CRIME_LOCATION only in nlp.meta['name']. Another question is how to assign 2 entities for meta.", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "Jul 25, 2018", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Jun 21, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "ines", "datetime": "Jun 25, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ines", "datetime": "Jun 25, 2018", "body": [], "type": "issue", "related_issue": "#2479"}, {"user_name": "lock", "datetime": "Jul 25, 2018", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/711", "issue_status": " Closed\n", "issue_list": [{"user_name": "dexterbeng", "datetime": "Jan 1, 2018", "body": "First of all, sorry for I am a beginner, and thanks to those people who read this topic!I have a small request is that once my PIR sensor detects an object, it will trigger my Picamera to streamed real-time to preview the video feed on the mirror display for around 5 seconds.How to accomplish it?", "type": "commented", "related_issue": null}, {"user_name": "sdetweil", "datetime": "Jan 4, 2018", "body": "to accomplish this, you will have to write a plugin to extend the mirror functionality.i have extended the autosleep function to use the camera of my webcam for motion detection.\n(see my smart-mirror/motion pull request to look at the code. mostly in the motion.js detection side)but I use an external program for the motion detection..  see    it can also stream the video..  you would have to write the code that opens the window and streams the data to it.. (and close the window etc)..", "type": "commented", "related_issue": null}, {"user_name": "dexterbeng", "datetime": "Jan 21, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/702", "issue_status": " Closed\n", "issue_list": [{"user_name": "dexterbeng", "datetime": "Dec 18, 2017", "body": "First of all, I added up a pi camera with my mirror, and I want to display the capturing video in my mirror display, how do I set up with my repository?The expectation result as shown below:\n", "type": "commented", "related_issue": null}, {"user_name": "dexterbeng", "datetime": "Jan 1, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/690", "issue_status": " Closed\n", "issue_list": [{"user_name": "transmit-ter", "datetime": "Oct 19, 2017", "body": "Hi there!\nMy Smart mirror does not respond to microphone\nThere are no errors in all sources when i run\npi@raspberrypi:~/smart-mirror $ npm startI did follow step by step process on docs.smart-mirror.io\nI tried rec test.wav with PS eye like post , the bars moved but It did not work on smart mirrors.pi@raspberrypi:~/smart-mirror $ aplay -l\n**** List of PLAYBACK Hardware Devices ****\ncard 0: ALSA [bcm2835 ALSA], device 0: bcm2835 ALSA [bcm2835 ALSA]\nSubdevices: 8/8\nSubdevice #0: subdevice #0\nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \nSubdevice : subdevice \ncard 0: ALSA [bcm2835 ALSA], device 1: bcm2835 ALSA [bcm2835 IEC958/HDMI]\nSubdevices: 1/1\nSubdevice #0: subdevice #0pi@raspberrypi:~/smart-mirror $ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: CameraB409241 [USB Camera-B4.09.24.1], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0pi@raspberrypi:~/smart-mirror $ nano ~/.asoundrc\npcm.!default {\ntype asym\nplayback.pcm {\ntype plug\nslave.pcm \"hw:0,0\"\n}\ncapture.pcm {\ntype plug\n# This is your input device (it may be different from what is seen here)\nslave.pcm \"hw:1,0\"\n}\n}My Environment\nEnvironment name and version: Raspberry Pi 3, node.js v6.11.4\nOperating System and version: Raspbian", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Oct 19, 2017", "body": "I  I know how to fix this :) From the smart mirror folder run:Then go into the remote config app and ensure that you have the correct input device selected and save.Give that a shot and let me know if you run into any issues.", "type": "commented", "related_issue": null}, {"user_name": "transmit-ter", "datetime": "Oct 20, 2017", "body": "well, i tried\ngit pull\ngit checkout dev\nnpm install sonus@next\nand config my input device (USB Camera-B4.09.24.1)\n\n, but it did respond once after i said \"what can i say\". After that nothing happened.", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Dec 12, 2017", "body": "i have the same problem did u find solution?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Dec 13, 2017", "body": "You may also want to take a look at ", "type": "commented", "related_issue": null}, {"user_name": "transmit-ter", "datetime": "Dec 26, 2017", "body": "Yeah thanks a lot for your help, it worked", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 24, 2017", "body": [], "type": "added", "related_issue": null}, {"user_name": "transmit-ter", "datetime": "Jan 1, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/654", "issue_status": " Closed\n", "issue_list": [{"user_name": "poetic420", "datetime": "Jun 12, 2017", "body": "i closed the magic mirror window..\ni try to open it again and cant..pi@raspberrypi:~ $ npm start\nnpm ERR! Linux 4.9.24-v7+\nnpm ERR! argv \"/usr/bin/nodejs\" \"/usr/bin/npm\" \"start\"\nnpm ERR! node v6.11.0\nnpm ERR! npm  v3.10.10\nnpm ERR! path /home/pi/package.json\nnpm ERR! code ENOENT\nnpm ERR! errno -2\nnpm ERR! syscall opennpm ERR! enoent ENOENT: no such file or directory, open '/home/pi/package.json'\nnpm ERR! enoent ENOENT: no such file or directory, open '/home/pi/package.json'\nnpm ERR! enoent This is most likely not a problem with npm itself\nnpm ERR! enoent and is related to npm not being able to find a file.\nnpm ERR! enoentnpm ERR! Please include the following file with any support request:\nnpm ERR!     /home/pi/npm-debug.log", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 12, 2017", "body": "I ran the comman in directiry\nstill a problempi@raspberrypi:~/smart-mirror $ npm startRemote listening on \nERROR s:641\nreturn binding.open(pathModule._makeLong(path), stringToFlags(flags), mode);\n^Error: ENOENT: no such file or directory, open '/home/pi/smart-mirror/keyfile.json'\nat Error (native)\nat Object.fs.openSync (fs.js:641:18)\nat Object.fs.readFileSync (fs.js:509:33)\nat Object. (/home/pi/smart-mirror/sonus.js:17:29)\nat Module._compile (module.js:570:32)\nat Object.Module._extensions..js (module.js:579:10)\nat Module.load (module.js:487:32)\nat tryModuleLoad (module.js:446:12)\nat Function.Module._load (module.js:438:3)\nat Module.runMain (module.js:604:10)", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jun 12, 2017", "body": "Looks like you're missing your Google Cloud Speech keyfile: ", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 13, 2017", "body": "hey thanks for replay!\nthis is my first project and im new to all of this so sorry if i have stupid questions..i have an issue with the Train module keyword (snow boy)\ni downloaded the public module, but i cant seem to understand where to save it\"\" Once trained, download the model and save it to the root of the .\"\"HOW do i save it in root of the smart mirror dirctory????\ncan you guys explain where do i need to save it?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jun 13, 2017", "body": "the root of your smart mirror is . Here is an overview of some basic naviation and file manupulation in linux:\nYou can also use the Pi's GUI file system. Just download your model and then use the file system to move it (click and drag, nothing complex).", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Jun 15, 2017", "body": " is this now resolved?If not please update us and let us know how we can help. If it is please comment what solved the issue and then close the issue. thank you.Also, we're available on  to help assist you in real time.", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 19, 2017", "body": "well, i tried everything you said...\nI run my smart mirror and no mic recognition or any reaction to \"smart mirror\" command..\nwhen run \"npm start\" i get the following error:pi@raspberrypi:~/smart-mirror $ npm startRemote listening on \nERROR le.js:471\nthrow err;\n^Error: Cannot find module '/home/pi/smart-mirror/node_modules/grpc/src/node/extension_binary/grpc_node.node'\nat Function.Module._resolveFilename (module.js:469:15)\nat Function.Module._load (module.js:417:25)\nat Module.require (module.js:497:17)\nat require (internal/module.js:20:19)\nat Object. (/home/pi/smart-mirror/node_modules/grpc/src/node/src/grpc_extension.js:38:15)\nat Module._compile (module.js:570:32)\nat Object.Module._extensions..js (module.js:579:10)\nat Module.load (module.js:487:32)\nat tryModuleLoad (module.js:446:12)\nat Function.Module._load (module.js:438:3)", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 19, 2017", "body": "-also:$ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: Camera [Vimicro USB2.0 UVC Camera], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0\"And get only errors:\npi@raspberrypi:~ $ $ arecord -l\nbash: $: command not found\npi@raspberrypi:~ $ **** List of CAPTURE Hardware Devices ****\nbash: #HOME SECOND FLOOR WALL 2.py: command not found\npi@raspberrypi:~ $ card 1: Camera [Vimicro USB2.0 UVC Camera], device 0: USB Audio [USB Audio]\nbash: card: command not found\npi@raspberrypi:~ $   Subdevices: 1/1\nbash: Subdevices:: command not found\npi@raspberrypi:~ $   Subdevice #0: subdevice #0Current behavior: smart mirror opens but no reaction to commands at allthanks for your reply!", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 20, 2017", "body": "all works!!!\nafter new ras jessy installed and a little help from the guys at discord chat (they are the best!)\nthank you everyone for the help!\nthe smart-mirror is amazing!", "type": "commented", "related_issue": null}, {"user_name": "poetic420", "datetime": "Jun 20, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/627", "issue_status": " Closed\n", "issue_list": [{"user_name": "jh13626", "datetime": "Mar 30, 2017", "body": "Voice recognition is only recognized when the voice recognition is clicked on the remote page when performing Speech Recognition.Next voice recognition must be pressed again when performing the following voice recognition :\nIs there a way to allow continuous recognition of voice recognition?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Mar 30, 2017", "body": "ok i need to update this to be more clear... voice recognition on the remote has nothing to do with voice recognition on the smart-mirror... please refer to the docs on how to configure voice recognition.first you'll have to configure your capture device then you'll have to configure the voice recognition...\nthere is no continuous voice recognition as that would kill your API requests quota and would cost you a lot of money not to mention send all nearby voice conversations to google...Also, we're available on  to help assist you in real time.", "type": "commented", "related_issue": null}, {"user_name": "jh13626", "datetime": "Apr 2, 2017", "body": "\nThank you. Let me ask you a little more.\nDoes remote page speech recognition work without sound configuration and voice configuration?\nI think I have completed all the configurations for speech recognition. However, if I do not press the Speak button on the remote control page, the Smart Mirror will not respond. Continuous speech recognition, as I say, is speech recognition without touching the Speak button with the mouse.And let me know if you have the server code for the discord chat.\nThank you.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Apr 2, 2017", "body": "What do you mean server code for discord chat?Yes the remote does not require sound or voice configuration it is done completely different than the smart mirror...When you configure the mirror properly you will say the hotword as in \"smart-mirror\" and then there will be a white bar at the bottom of the screen and then you will say the command... so it goes like this...\"Smart-mirror\" white bar appears \"show me how to tie a bow tie\" and then YouTube loads with the video...", "type": "commented", "related_issue": null}, {"user_name": "jh13626", "datetime": "Apr 15, 2017", "body": "\nThanks to you, speech recognition is well solved. But I have additional questions. Is it possible to run other applications on the Smart Mirror program? For example, can I run a motion recognition program or run a camera application to operate with voice?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 15, 2017", "body": " Absolutely. Check out the existing motion plugin (or really any plugin for that matter).\nDev docs: ", "type": "commented", "related_issue": null}, {"user_name": "jh13626", "datetime": "May 5, 2017", "body": "\nHi There is one more thing to ask.\nI want to implement the following subway service.()\nHowever, I do not know what path to write the code because the folder structure is different.\nFirst, I created a subway folder in the plugins folder. Help me...", "type": "commented", "related_issue": null}, {"user_name": "aishaamila", "datetime": "Apr 30, 2018", "body": "I cannot use the voice recognition. When i click on voice button on my phone, It says that unsupported on ios/safari but i am using android. Is there anything i need to change ?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Mar 30, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Mar 30, 2017", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/536", "issue_status": " Closed\n", "issue_list": [{"user_name": "evancohen", "datetime": "Jan 12, 2017", "body": "As an alternative motion input source, we should track camera movement (so people don't need additional hardware on top of what they've already got)As a part of the config, you should be able to specify if you are using a timer, IR motion sensors, or camera.\nNo idea what the performance implications would be for monitoring your webcam like this.We're currently limited to a timer and motion sensor (that requires additional libraries). It'd be nice to explore options to give us the same functionality but don't require that additional hardware.I've seen some other people do interesting things like this (looking at you )", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Jan 12, 2017", "body": "booo.... privacy concerns.... boooo..... bathroom creeper alert.... lol", "type": "commented", "related_issue": null}, {"user_name": "rabidaudio", "datetime": "Jan 13, 2017", "body": "I've got a little proof-of-concept going on . It's currently just a short python script that emulates 's console messages. It should be pretty trivial to convert it to javascript, but at the time I wasn't aware there were OpenCV bindings for javascript.I'm using a webcam because I have a spare and also because it has a microphone, but I imagine most people would want to use the Raspberry Pi camera module. I suspect OpenCV will treat these the same, but I haven't played with the Pi's camera module before.Running the script chews up a bunch of CPU cycles, but it does run, at least on a Pi 3 model B. Currently I'm opening a video stream and processing every frame, but it should be possible to take a picture periodically (e.g. every second) and then sleep. I'll play with this tonight and see how it goes.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Jan 13, 2017", "body": "you'll probably need to rebase your branch from the upstream dev... but if you can port the python to js and incorporate it into motion.js I can set up a drop down box in the configUI to use either pir or webcam", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jan 13, 2017", "body": " scales down the image to reduce comparison times (and as you mentioned, we wouldn't necessarily have to compare every frame. You could do < 10 FPS and it'd still be effective.I doubt we'd need the full power of OpenCV to get this to work. That said, if it's performant enough we could do some pretty cool things with it :)", "type": "commented", "related_issue": null}, {"user_name": "sdetweil", "datetime": "Feb 22, 2017", "body": "I just submitted a pull request that supports this, uses  for the detection,\nand a small enhancement to the autosleep plugin.", "type": "commented", "related_issue": null}, {"user_name": "sdetweil", "datetime": "May 3, 2020", "body": "add 0.26", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jan 12, 2017", "body": [], "type": "added", "related_issue": null}, {"user_name": "sdetweil", "datetime": "Feb 22, 2017", "body": [], "type": "pull", "related_issue": "#591"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#598"}, {"user_name": "sdetweil", "datetime": "Sep 26, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sdetweil", "datetime": "Apr 29, 2020", "body": [], "type": "removed", "related_issue": null}, {"user_name": "sdetweil", "datetime": "May 3, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/418", "issue_status": " Closed\n", "issue_list": [{"user_name": "HumzahF", "datetime": "Oct 25, 2016", "body": "Hi everybody,\nFor some reason the app is not picking up my microphone. I set up the keyfile and everything in the config file.speech : {\nprojectId: 'magicmirror-147422',\nkeyFilename: './home/pi/smart-mirror/MagicMirror-9d1fd3963297.json',\nkeyword : \"Smart Mirror\",\nmodel : \"smart_mirror.pmdl\", // The name of your model\nsensitivity : 0.5, // Keyword getting too many false positives or not detecting? Change this.\ncontinuous: false // After a keyword is detected keep listening until speech is not heard\n}I also tried opening the app in dev mode to see if I have any errors but\nnothing happens when I say smart mirror, no errors and no activity comes on the console.\nI also ran 'npm run microphone-debug' and my microphone is not there in the dropdown menu.  I also ran 'arecord -l' and here is my result:$ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: Camera [Logitech Webcam], device 0: USB Audio [USB Audio]\nSubdevices: 1/1\nSubdevice #0: subdevice #0and this is in my /.asoundr:\npcm.!default {\ntype asym\nplayback.pcm {\ntype plug\n# This is your output device (In this case AUX out on the Pi)\nslave.pcm \"hw:0,0\"\n}\ncapture.pcm {\ntype plug\n# This is your input device (it may be different from what is seen here)\nslave.pcm \"hw:1,0\"\n}\n}this is all that I have doneIm on a Raspberry Pi 3any help would be appreciated", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 7, 2016", "body": "I'm pretty sure that we sorted you out on Gitter, but for anyone else with this problem:\nCheck out the .", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 7, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/421", "issue_status": " Closed\n", "issue_list": [{"user_name": "filimikr", "datetime": "Nov 4, 2016", "body": "Hey,I have an issue with my usb (creative) camera for the microphone.\nRaspberry recognizes the device and in the sound config file(asoundrc)  I set it as default recording device.\nMicrophone-debug recognizes the device as well, and I can record and play my recording.\nAlthough when I run the Smart-Mirror the microphone is not working. I think that my config.js file is properly edited.Any help would be appreciated. Thank you!(Raspberry 3/raspbian upgraded/smart-mirror updated)", "type": "commented", "related_issue": null}, {"user_name": "kurtdb", "datetime": "Nov 4, 2016", "body": "If you start the mirror using , do you see any logging concerning the speech?", "type": "commented", "related_issue": null}, {"user_name": "filimikr", "datetime": "Nov 4, 2016", "body": "No, i have only this folowing error (and one more for the rss), but nothing for speech.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 4, 2016", "body": "You might want to give the troubleshooting steps a shot and just run the keyword spotter and speech recognition to see what is up: ", "type": "commented", "related_issue": null}, {"user_name": "filimikr", "datetime": "Nov 6, 2016", "body": "Actually I re-installed everything from the scratch(raspbian-smart-mirror etc), and now things are better. The app recognizes the usbCamera normally, and after I trained my own model, the keyword seems like it's ok(it shows this white line across at the bottom.(sonus shows me that it recognizes the keyword as well)The problem is that after the white effect , it does nothing. It doesn't listen to any command, like \"What can i say or sth, and The white effect stays there until I close the appps. I ran it in dev mode and no errors appeared.\n:/ thanks I imagine that It's something with the APIs or the json key, but everything seems correct , So I really don't know where the problem is.. Errors I see after the npm start dev Propably the error was for the Google Speech keys, but I Fixed it. It was working properly for some minutes and then I received the error:Any thoughts about that?\nthanks", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 7, 2016", "body": "You might want to review the steps that you used to get your key. If it doesn't have valid permissions then hotword detection will work but streaming will fail: ", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 7, 2016", "body": "also the project name in the  file must match the project name in  file", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 14, 2016", "body": "Sounds like you haven't enabled billing. Can you double check that?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": " is this now resolved?If not please update us and let us know how we can help. If it is please comment what solved the issue and then close the issue. thank you.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 2, 2016", "body": "haven't heard a response in 10 days. I'm closing this issue...  if you're still having issues please comment on this issue and we can reopen..", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 2, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 2, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/326", "issue_status": " Closed\n", "issue_list": [{"user_name": "justbill2020", "datetime": "Jun 30, 2016", "body": "Merging Motion detection and Auto-Sleep / Auto-Wake Functionality.  I'll start working on this today. should have something for it later on today or tomorrow.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Aug 12, 2016", "body": "Currently looking into implementing johnny-five.io for motion detection...", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Aug 19, 2016", "body": "Have you made any progress? Not sure what camera you are using, but I've come across this to work with my PS Eye:  re:", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Aug 19, 2016", "body": "@glcohen I have not further pursued this at this time as i've spent a lot of time troubleshooting on gitter getting the mirror up and running and focusing on the documentation. However, the motion detection we were looking at implementing was using a PIR rather than the camera... as a camera in a bathroom is kinda creepy in my humble opinion... My suspicion would be a bathroom is the likely place people would be putting it.", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Aug 19, 2016", "body": "Fair point; however I wouldn't rule the option out. It's likely possible to distort the image in such a way that the camera can pick up motion but see nothing valuable. You can also set the config to save no images.It's a simpler alternative for those who have a single unit camera/mic like the PS Eye. I'll do some thorough experimenting and report back.Of course people could always choose to be creepy, we can't stop that!", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Aug 19, 2016", "body": "to each their own I guess... adding that functionality isn't a bad thing... if you want to fork the project and work on that to see what you can come up with that would be cool....", "type": "commented", "related_issue": null}, {"user_name": "sdetweil", "datetime": "Sep 30, 2016", "body": "it should be configurable, and device extendable... you want to use PIR, then do this, you want to use camera, then do that (new cam type, extend this). In my case the mirror will be out in a hallway, and I am using webcam for voice, so have cam for motion detection.", "type": "commented", "related_issue": null}, {"user_name": "GobleSt", "datetime": "Oct 10, 2016", "body": "I agree with  .  Mine is next to the front door.  I think we should have both options for what different people want to do.  The webcam option can continue in issue  I suppose.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 7, 2016", "body": "the issue with using the webcam is limited resources on the PI... as stated in . since most are using a Rasp Pi adding webcam support for motion detection might have to wait until they release the raspberry pi 8... which should have the performance needed for this whenever they get around to that version of coarse  lol", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": "removed suggestion status, moved this into feathub and added the in progress and help wanted labels...added to Please refer to feathub for all future comments on this suggestion. use this issue for additional development support or details...", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": " i started working on this again using johnny-five... the main issue is that it requires root permissions to use the raspi-io node module and access the GPIO pins on the pi... i don't like that... also it seems like the IPC renderer doesn't actually trigger the commands as it should... all of this is frustrating as hell... however the  script I added to the  works perfect... if you want to take a look here's the branch link... ", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": "holy crap nevermind... i figured it out!!!!!", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": "completed johnny-five functionality in PR  tested this extensively and should be ready to go...", "type": "commented", "related_issue": null}, {"user_name": "GobleSt", "datetime": "Dec 6, 2016", "body": "did a git pull today and see that the auto time out works great!.... the wake up not so much.  How do I disable it until I am ready to do a complete rebuild?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": "blank out the wake_cmd and sleep_cmd...", "type": "commented", "related_issue": null}, {"user_name": "GobleSt", "datetime": "Dec 6, 2016", "body": "...in the config.js .  Thanks!", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 7, 2016", "body": [], "type": "modified the milestone:", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": [], "type": "issue", "related_issue": "#64"}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 6, 2016", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 15, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Dec 15, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/307", "issue_status": " Closed\n", "issue_list": [{"user_name": "geman220", "datetime": "Jun 25, 2016", "body": "As far as I can tell PulseAudio is crashing causing audio playback to fail on the mirror.  Opening Pauvcontrol and muting then unmuting the output sometimes will enable playback for a short period (5-10 seconds).  You can keep muting / unmuting to resume playback for short bursts of time.  The same behavior seems to happen just by turning the volume up/down.  It's almost as if changing any setting \"wakes up\" pulse.  Because of this I am fairly certain the issue exists within PulseAudio.  Unfortunately I am unable to get my microphone to work with the mirror if pulseaudio is not running.Running  I noticed the following pop up whenever the audio drops out:After more fiddling, I am not sure what combination of things has made the biggest impact but here is where I'm at.I am seeing far less of the errors from pulseaudio in regard to   Sometimes I can play a whole YouTube or SoundCloud, it's hit or miss.  In general I am getting longer play times before a crash.  Once it does crash it will not recover until I close the mirror and re-launch.    I am using the original 2.5A wall wart but I may buy a new one just to be sure.I disabled and removed bluetooth from my pi.  I'm sure it wasn't using much power, but it was using SOME power so it's gone.  I've made more completely arbitrary changes in my daemon.conf, (specifically increasing  and   and I'm now up to 15 minutes (and counting) of continual playback in Soundcloud.  My advice is to play around with those values until you get good playback.  Like I said I've also made more changes but I'll have to check them all to give a more detailed explanation.  Looks like the issue is 100% related to Pulseaudio, but more specifically it's resource usage.  If it's possible, my suggestion would be to move away from Pulseaudio.Wanted to update the changes I made to the best of my memory.I believe that's all the changes I made, but it was a long process of trial and error so I can't guarantee those were the only changes I made, but hopefully it helps get someone at least 90% of the way there.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jun 26, 2016", "body": "Thank you  for this super well documented issue! Seems to share the same root cause as . When streaming recorded audio and simultaneously playing, PulseAudio (more specifically the ALSA sync) hangs on a rewind.I'll be looking at two potential solutions here: fixing the PulseAudio issue, and removing PulseAudio as a dependency for the smart mirror.", "type": "commented", "related_issue": null}, {"user_name": "geman220", "datetime": "Jun 26, 2016", "body": "No problem, happy to help however I can.  Hopefully if someone else has this issue they can at least try this workaround.  I appreciate your help!", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 2, 2016", "body": "After some investigation I have found that it is actually fairly straightforward to get this working without PulseAudio (which should solve this issue).First, you'll want to determine your playback and recording devices:Here the playback device is card 0, device 0, or hw:0,0 (hw:0,1 is HDMI audio out).Then you'll want to determine the recording device:Here the recording device is card 1, device 0, or hw1:0. I've modified this configuration file to work for every microphone I could test with, it should be general enough to work for everyone.And finally you'll want to use these to fill in your   file:Reboot!\n allows several applications to record from the same device simultaneously. This is essentially what what PulseAudio was doing for us before, only it's a huge dependency and was crashing. Luckily  comes as a part of ALSA on recent distributions of the Raspbian so this is an easy fix.tl;dr Audio on Linux is ", "type": "commented", "related_issue": null}, {"user_name": "7h30n3", "datetime": "Jul 3, 2016", "body": "Still doesn't work for me.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 3, 2016", "body": " not a lot to go on there... Can you please include the make and model of your webcam, the output of , and any terminal errors you get after activating the keyword?", "type": "commented", "related_issue": null}, {"user_name": "Keopss", "datetime": "Jul 7, 2016", "body": "My keyword is not recognizer :( i tried with annyang.service() in console and microphone start to work. But any answer with keyword", "type": "commented", "related_issue": null}, {"user_name": "GobleSt", "datetime": "Jul 8, 2016", "body": "After following the \"remove pulseaudio\" instructions, no voice is detected at all for me.\nThe first thing I noticed is the my aplay -l looked slightly off:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`\nOnly 7/8...weird but w/e.\nPrior to reboot the arecord -l looked similar with 0/1 as opposed to 1/1.  After reboot it shows 1/1.\n\n\n\nFirst thing I attempted was:\n\n\n\nNo change.I checked npm start dev and saw no recognition of voice.  3 errors related to traffic which I haven't implemented but nothing else.I checked audio via browser and OMG it is horrendous.  I will try to troubleshoot this evening because I am running out of time this A.M.....", "type": "commented", "related_issue": null}, {"user_name": "7h30n3", "datetime": "Jul 9, 2016", "body": " I finally found an easy solution for voice recognition (Hotword Detection always worked) and I'm still using PulseAudio. I just removed the \"Volume Control (ALSA)\" from the taskbar as described here:Maybe you can verify that.", "type": "commented", "related_issue": null}, {"user_name": "GobleSt", "datetime": "Jul 9, 2016", "body": "I reinstalled pulseaudio etc and removed the volume control as per above....commands are responsive again but playback (soundcloud, youtube) still has issues.  I haven't been able to work on this because working longer hours lately....will try to devote sometime soon.", "type": "commented", "related_issue": null}, {"user_name": "Keopss", "datetime": "Jul 12, 2016", "body": "I have done this steps:1º - \n2º - Configured \n3º - Setup Audio Sound Output 4º If i follow steps of \"Update 3\" i get sound some seconds and later silence a any sound is playedKeyword works perfectly. but no sound :(", "type": "commented", "related_issue": null}, {"user_name": "franklinam1", "datetime": "Jul 22, 2016", "body": "Hey all, so I had this same problem and my setup is as follows:Symptom - USB MIC does not work, tried all troubleshooting steps in guide and in bugs but no luckI found a post on the RP site that had similar symptoms so I changed the following and it seemed to fix all of my issues\"where there is a file called aliases.conf in /lib/modprobe.d which contains the line options snd-usb-audio index=-2 and overrides the etc/modprobe.d/ files, so you need to change that one.Comment out with a # the line “options snd-usb-audio index=-2”\"In /usr/share/alsa/alsa.conf I un-commented “load card-specific configuration files (on request)”I'm not a programmer or dev at all, but your mirror inspired me to code. Heres a link and credit to the person who resolved this issue for me. Thanks all!", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Sep 22, 2016", "body": "Closing this out because this is no longer an issue :)", "type": "commented", "related_issue": null}, {"user_name": "geman220", "datetime": "Jun 26, 2016", "body": [], "type": "issue", "related_issue": "#284"}, {"user_name": "evancohen", "datetime": "Jun 28, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 2, 2016", "body": [], "type": "issue", "related_issue": "alexa-pi/AlexaPiDEPRECATED#82"}, {"user_name": "evancohen", "datetime": "Jul 5, 2016", "body": [], "type": "issue", "related_issue": "#333"}, {"user_name": "evancohen", "datetime": "Jul 7, 2016", "body": [], "type": "modified the milestone:", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 7, 2016", "body": [], "type": "issue", "related_issue": "#335"}, {"user_name": "evancohen", "datetime": "Aug 3, 2016", "body": [], "type": "issue", "related_issue": "#368"}, {"user_name": "evancohen", "datetime": "Aug 31, 2016", "body": [], "type": "issue", "related_issue": "#361"}, {"user_name": "evancohen", "datetime": "Sep 22, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Sep 22, 2016", "body": [], "type": "assigned", "related_issue": null}, {"user_name": null, "datetime": "Sep 22, 2016", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "evancohen", "datetime": "Sep 29, 2016", "body": [], "type": "issue", "related_issue": "#403"}, {"user_name": "decentralgabe", "datetime": "Oct 8, 2016", "body": [], "type": "issue", "related_issue": "#386"}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": null, "datetime": "Jan 25, 2017", "body": [], "type": null, "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/266", "issue_status": " Closed\n", "issue_list": [{"user_name": "sata73", "datetime": "May 31, 2016", "body": "It would be great to show different calendar or even RSS feeds per user via voice command, eg. \"Show Evans calendar\" or log in as a user via voice command to show specific content and log out to show default content.\nIt could probably be done by adding users to modules in the config and define if it is for this or for that user or for all.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 31, 2016", "body": "I think the best approach here would be to use the webcam and facial recognition here, as suggested in ", "type": "commented", "related_issue": null}, {"user_name": "sata73", "datetime": "Jun 1, 2016", "body": "Thanks for your fast reply!\nI thought about voice control, because not everyone is fine with having a cam spying into the room. And since voice control is already enabled, it might be worth a thought, using the available technology.\nThe command home shows the default layout. Probably different layouts could be defined which can be called by custom commands like Evans home? Is that possible?", "type": "commented", "related_issue": null}, {"user_name": "jallen1227", "datetime": "Sep 5, 2016", "body": "Agreed on the privacy relative to cameras, what about switching based upon use of Pi's NFC module.  That way, a phone or tablet could be used to switch between users.  Certainly not as convenient as voice but opens up options and validation / security of content shown.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": "Migrated to Issue will be closed and tracked on feathub moving forward. Please refer to feathub for all future comments on this suggestion.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 31, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/170", "issue_status": " Closed\n", "issue_list": [{"user_name": "mojosoeun", "datetime": "Mar 20, 2016", "body": "Hello.\nFrom 9am to 4pm KST, voice recognition didn't work. But after that time it began working again. I'm trying to figure out why but I can't solve it.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Mar 21, 2016", "body": "This is a known issue. My current theory is that because of the high volume of mirrors we are collectively using up Electron's Google speech key.I am currently investigating solutions (and am open to suggestions)", "type": "commented", "related_issue": null}, {"user_name": "shekit", "datetime": "Apr 12, 2016", "body": "Any updated theory on this? I'm facing a very random issue where my code was working earlier in the day and suddenly stopped working, no voice detection.. nada..Here's another issue I opened on annyang that details all my attempts ", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 18, 2016", "body": "You are correct in you assumption that the issue is with key utilization (there have been may discussions about this on the gitter chat, which I suggest you check out).I'm looking at alternatives (BlueMix, Microsoft, etc) as well as investigating offline  to reduce quota usage.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 20, 2016", "body": "Another update: I've got keyword spotting functioning in the  branch. There are a number of issues that exist with this implementation, namely poor performance and some comparability issues with certain microphone setups.", "type": "commented", "related_issue": null}, {"user_name": "shrimp69", "datetime": "Apr 26, 2016", "body": "I just set it up today and it seems I already made too many requests. I have my own Google API keys but in a matter of 2-3 minutes I made over 500 requests and now it seems to be down for me.How do I add this branch to my existing git folder? ( on the raspberry )", "type": "commented", "related_issue": null}, {"user_name": "skydazz", "datetime": "Apr 26, 2016", "body": "I am still getting \"Google speach recognizer is down :(\" when I plug in any sort of microphone in to it. I have my Own Speech Keys. May I suggest that its a driver issue. Is it set to only be compatible with a list of mics? (PS: It was \"Say \"What Can I Say\" to see a list of commands\" when I unplug the microphone.)", "type": "commented", "related_issue": null}, {"user_name": "skydazz", "datetime": "Apr 26, 2016", "body": "Also I get the \"[1444:0426/103700:ERROR:logging.h(813)] Failed to call method: org.freedesktop.NetworkManager.GetDevices: object_path= /org/freedesktop/NetworkManager: org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.NetworkManager was not provided by any .service files\" Message in the Terminal When I start it.", "type": "commented", "related_issue": null}, {"user_name": "skydazz", "datetime": "Apr 27, 2016", "body": "I have started from scratch. I downgraded from Jessie to wheezy and I am following the documentation exactly as it is printed (except config.js). I am using a USB camera as a mic as it states in the documentation also. Will post results soon", "type": "commented", "related_issue": null}, {"user_name": "skydazz", "datetime": "Apr 27, 2016", "body": "Ok, my issue now, still with sound, is getting the usb camera mic as the mic being used, I can use any USB sound device for anything. I have tried turtle beach px22 controller, USB sound card, and a USB camera", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 28, 2016", "body": " have you tried following the directions in the troubleshooting section of the documentation? You may also want to look at  (which was an old thread on the issue that may help you find an alternative solution)", "type": "commented", "related_issue": null}, {"user_name": "Sachin1968", "datetime": "Apr 29, 2016", "body": " Were you able to resolve the issue you had with \" Failed to call method: org.freedesktop.NetworkManager.GetDevices: object_path= /org/freedesktop/NetworkManager: org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.NetworkManager was not provided by any .service files\"I have the same issue and can't figure out how to resolve it. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 29, 2016", "body": " that error is unrelated to this thread and is harmless - you can be safely ignore it.\nYou can find more info on .", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 2, 2016", "body": "So, another update for you all :)\nKeyword spotting officially works in the in the  branch. Unfortunately the Pi is not quite powerful enough to process everything in real time. Because of that I've added a clap detector to that same branch, all you have to do is clap (a configurable number of times) and the mirror will start listening to you.\nYou'll have to install  (it's a dependency for clap detection)You will also have to run  after switching to this branch because of the new dependencies. Make sure you update your  file to reflect the new properties in !Since this is all very new stuff I haven't had the chance to test it extensively. I already anticipate there being issues with the clap detection microphone configuration, luckily this is totally something that you can set up. In your config you can use the clap  object to change the following settings for clap detection:As always, if you have any questions you can post them here or ask on gitter.\nSince commands are intermittent in the  branch I've added a shim to Annyang to \"simulate\" a request. This can be done in the dev console with the following:", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 13, 2016", "body": " we'll get you sorted out in gitter :)", "type": "commented", "related_issue": null}, {"user_name": "joerod", "datetime": "May 13, 2016", "body": "I had the same problem with overusing my 50 speech API calls in about 10 minutes of use so I'm happy to test the new \"clap\" feature.", "type": "commented", "related_issue": null}, {"user_name": "Keopss", "datetime": "May 18, 2016", "body": "Hi  ! i don´t know what happen with my rabs :(I have installed smart-mirror-master and works fine.Then i install sox and smart-smirror with keyword then edit config.js and add overrides options but no clap and speech detection.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 18, 2016", "body": "The geniuses over at  have created an offline keyword spotter that should work. In order to find out I need your help to train the keyword \"smart mirror\". Just follow these steps:I'll continue to keep this thread updated with my progress and should hopefully have a working prototype this weekend!", "type": "commented", "related_issue": null}, {"user_name": "Keopss", "datetime": "May 23, 2016", "body": "Hi! how gone? fine?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 23, 2016", "body": "I have a working implementation of keyword spotting, I'm currently trying to fix an issue on the Pi 3 that causes recognition to fail because of a native PulseAudio issue.", "type": "commented", "related_issue": null}, {"user_name": "trenkert", "datetime": "May 23, 2016", "body": "Hello, I've succesfully installed smart mirror and I am very impressed!However, I also get \"Google Speech Recognizer is down\"...How long will it take for you to implement the new solution into the main branch?Just an idea:\nCould you use jasper with pocketsphinx () to train a number of keywords than then either activate Google STT or Amazon Alexa? Or is Kitt.ai definitely better for keyword recognition?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 24, 2016", "body": "I tried Jasper... it's quite resource intensive and is painful to use as a dependency (building projects on top of it is great, building integration into an existing project not so much).I also tried PulseAudio (same native recognition engine that Jasper uses) and it wasn't quick enough to recognize keywords without significant lag.Snowboy (from the folks at kitt.ai) is super lightweight and very fast. Sure it requires a wrapper for their Python library, but that's not too difficult.I actually have a working prototype with snowboy on the  branch (using the OSX binaries). The only problem on the Pi now is with PulseAudeo, which is having issues with the Pi 3. Once I sort that out (and I think I have a fix) we'll be good to go.It's been a long journey, with lots of painful dead ends, but I'm feeling really close!tl;dr Snowboy is great, I should have something working really soon.", "type": "commented", "related_issue": null}, {"user_name": "trenkert", "datetime": "May 24, 2016", "body": "cool! What's the problem with pulseaudio?", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 24, 2016", "body": " it's an issue with Bluetooth that causes PulseAudio to crap out. Even after disabling it within the config the issue persists (which makes me think there may be another root cause). I'm worried that the real cause is a conflict between dependencies of the mirror and keyword spotter (but I haven't confirmed this yet, and I don't  it's the cause).", "type": "commented", "related_issue": null}, {"user_name": "chenguoguo", "datetime": "May 25, 2016", "body": "Sorry for coming into this late. Snowboy is a C++ library and doesn't have much dependency. It will work as long as you can feed it linear PCM data sampled at 16K, with bits per sample 16, and number of channels 1. PyAudio is only used for demo purpose. If it turns out that PyAudio is the problem, we can turn to other alternatives for audio capturing.In the  we are trying to add examples of using Snowboy in different programing languages. So far the examples are using PortAudio or PyAudio, but if you look at the code (e.g., the C++ demo code ), you can see that switching the audio capturing tool should be easy., let me know if it turns out that PyAudio is the problem. We can look into other alternatives for audio capturing.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 25, 2016", "body": "Hey  thanks for dropping in! Awesome to see you all so committed to your (super awesome) project. I managed to write a pretty hacky IPC between Node and your pre-packaged Snowboy binaries/Python wrapper. It's  not the ideal way to use Snowboy with Node, but I just wanted to see if I could get something that would work.I don't think it would be too challenging to  so it could be easily consumed via Node. I'll take a look at it this weekend if I get the chance For everyone else: I managed to coax PulseAudio into cooperating on my Pi, and everything seems to work super well! You can test it out by doing the following:As always let me know if you have any issues over on .", "type": "commented", "related_issue": null}, {"user_name": "chenguoguo", "datetime": "May 25, 2016", "body": "That's great !  is also helping us working on the NodeJS module, see the issue . He'll likely get something soon.", "type": "commented", "related_issue": null}, {"user_name": "trenkert", "datetime": "May 26, 2016", "body": " I've experienced a similar issue. I would guess it has to do with pulseaudio-bluetooth. It works for me when I start pulseaudio manually once again after login.", "type": "commented", "related_issue": null}, {"user_name": "ojrivera381", "datetime": "Jul 18, 2016", "body": " Thanks. I rebuilt it all seemed to be fine on my lab monitor in my office however when I moved it to its perm location speech stopped working. Also how do i exit it and get to the main desktop with menus. right now if I alt+f4 is closes the window but I can't see any menus to go through pi settings or launch terminal etc.. Thanks again.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 18, 2016", "body": " is the Pi still connected to the same WiFi network? Have you exceeded your 50 query/day quota? The menu is missing because you have  installed.\nYou can probubly also press the windows key on your keyboard, (which opens the Raspbian equivalent of the start menu). You can also get to the terminal via the recycling bin on the desktop (hacky, I know).If those two things look good, I would follow the instructions for troubleshooting in the docs.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Mar 23, 2016", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "evancohen", "datetime": "Mar 24, 2016", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "evancohen", "datetime": "Mar 24, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "evancohen", "datetime": "Apr 22, 2016", "body": [], "type": "issue", "related_issue": "#228"}, {"user_name": "evancohen", "datetime": "Apr 29, 2016", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 1, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 25, 2016", "body": [], "type": "", "related_issue": null}, {"user_name": "evancohen", "datetime": "May 27, 2016", "body": [], "type": "pull", "related_issue": "#261"}, {"user_name": "ikucukkaya", "datetime": "Jun 10, 2016", "body": [], "type": "issue", "related_issue": "#282"}, {"user_name": "evancohen", "datetime": "Jun 11, 2016", "body": [], "type": "issue", "related_issue": "#287"}, {"user_name": "evancohen", "datetime": "Jun 11, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jun 11, 2016", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 22, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": "Jan 25, 2017", "body": [], "type": null, "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/59", "issue_status": " Closed\n", "issue_list": [{"user_name": "Shubham0209", "datetime": "Feb 6, 2016", "body": "Wouldn't it be nice that instead of showing a single comment all the time, mirror may show different comments and greet us based on the detected face.Can you suggest me any method to so.Moreover i would also like to \"HOW TO USE PUSHBULLET API\". Basically being a newbie i don't know how and where to add these api code.please can you tell me about this in detail.THANKS!!!", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Feb 7, 2016", "body": "I've experimented with face detection in the past, but I haven't pushed it out publicly because there are a number of issues with it. As for PushBullet I haven't completed this integration yet. I'm experimenting with Android devices this week and will probably have PushBullet done in the next couple weeks.", "type": "commented", "related_issue": null}, {"user_name": "andrewda", "datetime": "Feb 7, 2016", "body": " You could possibly make a branch with the face detection code implemented. I would love to take a look and see what I can do about contributing!", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Feb 7, 2016", "body": "I'll clean up what I have and do that :)", "type": "commented", "related_issue": null}, {"user_name": "kurtdb", "datetime": "Feb 8, 2016", "body": "What framework did you use? I've been reading up on this and it seems like OpenCV is the framework to use for this?", "type": "commented", "related_issue": null}, {"user_name": "Shubham0209", "datetime": "Feb 9, 2016", "body": " how can we integrate webcam to take our picture on command like \"click my picture\"?By doing this we can integrate the feature of selfie mirror.", "type": "commented", "related_issue": null}, {"user_name": "therealgambo", "datetime": "Mar 5, 2016", "body": ", Do you know how ironic that sounds? I can understand the want and need from a technical point of view, but do you really need your picture taken only for it to be displayed in a screen behind a mirror. It kind of defeats the whole purpose of a mirror.", "type": "commented", "related_issue": null}, {"user_name": "Shubham0209", "datetime": "Mar 5, 2016", "body": "@missionrulz actually it was only step 1.What i was actually thinking was to maybe post those pictures on fb etc or send them to the phone. so the basic idea was to click the picture and share it. I know it sounds weird.lol", "type": "commented", "related_issue": null}, {"user_name": "tomasvr", "datetime": "Mar 6, 2016", "body": "Thats a pretty neat idea! The point is that you would be able to see the full picture you're about to take of yourself through a big mirror. Only placing the camera at the right position seems kind of tricky because it should be at eye height, where the screen is.", "type": "commented", "related_issue": null}, {"user_name": "Shubham0209", "datetime": "Mar 6, 2016", "body": "yeah  you feel me right. lol", "type": "commented", "related_issue": null}, {"user_name": "paviro", "datetime": "Mar 11, 2016", "body": "Not all of it is published yet but I started adding facial recognition to another mirror project you could probably use the code as it is right now to get something up and running:\n", "type": "commented", "related_issue": null}, {"user_name": "kurtdb", "datetime": "Jul 4, 2016", "body": " how fast is the facial recognition? I don't think the rpi (even v3) is fast enough to do this in a user-friendly way. (e.g. look at the video at )A user won't wait for 5 seconds to see his/her information, they want it now. I've been looking into alternatives, but it seems that the way to go here might be either a cloud hosted service or an expensive camera that can do the facial recognition for you. (although I didn't find any camera's that had an API from which you could stream the information)", "type": "commented", "related_issue": null}, {"user_name": "paviro", "datetime": "Jul 4, 2016", "body": "", "type": "", "related_issue": null}, {"user_name": "cybertza", "datetime": "Jul 22, 2016", "body": "there is a really great idea on the selfie mirror is the 1 picture a day stream, if you captured 1 picture a day of yourself for ever, it would be rather cool, and with an overlay you may be able to calibrate it so that your face is always in the same place, but yea, just interesting.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Jul 22, 2016", "body": " I like the idea! Want to take a shot at it and send a PR?", "type": "commented", "related_issue": null}, {"user_name": "paviro", "datetime": "Jul 22, 2016", "body": "", "type": "", "related_issue": null}, {"user_name": "7h30n3", "datetime": "Aug 28, 2016", "body": "What's the current status of the face recognition feature?Maybe we could use the face detection (Just \"face detected\" and \"no face detected\", no certain faces) as a substitute for the hotword detection.", "type": "commented", "related_issue": null}, {"user_name": "paviro", "datetime": "Aug 28, 2016", "body": "", "type": "", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": "Migrated to Issue will be closed and tracked on feathub moving forward. Please refer to feathub for all future comments on this suggestion.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Feb 7, 2016", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "evancohen", "datetime": "Feb 9, 2016", "body": [], "type": "issue", "related_issue": "#63"}, {"user_name": "evancohen", "datetime": "Apr 6, 2016", "body": [], "type": "issue", "related_issue": "#215"}, {"user_name": "evancohen", "datetime": "May 31, 2016", "body": [], "type": "issue", "related_issue": "#266"}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sdetweil", "datetime": "May 10, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/204", "issue_status": " Closed\n", "issue_list": [{"user_name": "ryansaam", "datetime": "Nov 4, 2021", "body": "\nUnable to run project\nI followed the \"2.2 With Bundler\" install steps \nTo have the project run\nReact.js, Node v16.13.0Error:Error:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 4, 2021", "body": " is delivered as ES2020 module, but amazingly even latest  creates an app setup to use  old  7.0 which is not compatible with ES2020\n(ES2020 support was introduced in Babel 7.8 which was released in January 2020, I don't know why FB uses such really old versions in )You can either update your environment or update  to latest one from  (2.5) as I've just posted an update that includes polyfils for ES2018Note that  2.5 is not yet released on NPM (likely next week), but you can install it using\n", "type": "commented", "related_issue": null}, {"user_name": "ryansaam", "datetime": "Nov 4, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 4, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/158", "issue_status": " Closed\n", "issue_list": [{"user_name": "Krammig", "datetime": "Sep 1, 2021", "body": "Would love to try the demo but all that is displaying is the rotating progress circles and a message saying Starting Detection.\nAppreciate if you could let me know if there is something I need to do at my end.Chrome\nFirefox\ni5 16GB", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Sep 1, 2021", "body": "I just tried with Chrome and Firefox and it works fine here.Btw, do you have a GPU or integrated graphics?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Sep 2, 2021", "body": "any updates?", "type": "commented", "related_issue": null}, {"user_name": "Krammig", "datetime": "Sep 4, 2021", "body": "Sorry just saw your reply.\nDedicated GPU\nI had tried with both Chrome and Firefox.\nWill check the inspector in the coming hour and let you know.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Sep 8, 2021", "body": "closing due to idle time, will reopen if information is provided.", "type": "commented", "related_issue": null}, {"user_name": "Krammig", "datetime": "Sep 1, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Sep 1, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Krammig", "datetime": "Sep 4, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Krammig", "datetime": "Sep 4, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Sep 8, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/243", "issue_status": " Closed\n", "issue_list": [{"user_name": "avestaHarsh", "datetime": "Jan 21, 2022", "body": "I am using the face comparison feature in my electron app. On the laptop (with default camera) it's working fine but when I try to run this app in the system(PC) (which does not have a default camera), also I have tried setting external webcam in the system, the app is not starting even not getting any errors/warnings.Human library version: \nTensorFlow/JS version : \nNodeJS and version : \nOS : \nElectron version : \nRAM : \nProcessor : Below is my configuration for Human.Note: I have also checked with the updated latest version of the Human Library ( npm package), TensorFlow/JS ( npm package), and checked in i5 processor CPU but still not working.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 21, 2022", "body": "there have to be some errors/warnings, did you open browser inspector window (it exists even in electron) and see what's written there?", "type": "commented", "related_issue": null}, {"user_name": "avestaHarsh", "datetime": "Jan 21, 2022", "body": "The app does not start itself. so the browser inspector window will not be prompt.Here Is the terminal screen.\nAll I came to know after spending a day is, the app is crashed while I am creating an instance of Human class.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 21, 2022", "body": "So the EXE you've built works on your notebook, but completely fails to start on your desktop system?What you've just posted is build log (and PLEASE, post such items as text, not as image screenshots) but that should not be relevant since EXE is already built?Enable  logging (see  and post what happensWithout actual error, nothing I can do.", "type": "commented", "related_issue": null}, {"user_name": "avestaHarsh", "datetime": "Jan 21, 2022", "body": "Yes, whatever I have developed/built is working on my laptop but not working on a desktop.I got this error in the console.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 21, 2022", "body": "this error is an issue with your electronjs logging - it cannot open a log file and it aborts with unhandled error.", "type": "commented", "related_issue": null}, {"user_name": "avestaHarsh", "datetime": "Jan 24, 2022", "body": "To overcome the above error I have tried with close electronjs logging, and the error was gone but still unable to start the app.The issue with below line, something is wrong with the default config value.\nIf I am commenting above line then the app starts smoothely.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 24, 2022", "body": "I'd love to help, but I need an actual error to be able to do anythingCheck  documentation on how to display browser console log and post entire log here\nI don't use  at the moment, but there are plenty of hints in this thread:\n", "type": "commented", "related_issue": null}, {"user_name": "avestaHarsh", "datetime": "Jan 24, 2022", "body": "At the time of , something goes wrong at below linethat's why I have debugged today in the library (more specifically in Human class, namely human.ts (under src folder) / human.esm.js(under dist folder)) and tried to console logs but nothing consoled anything.I am curious to know, Does Human class looking for a default camera by default and my desktop does not have a default one and my laptop does have so it works well?Currently, I have set  , ,   for face comparison, do I make any mistake here? Do I need to change the backend for the electron app?(Also I have tried with add an additional webcam to the desktop before compiling my app).", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 24, 2022", "body": "Human does not look for input at all at the time of creating an instance. It only looks at input once you call \nEven then, it doesn't care about WebCam - it uses HMTLElement you pass - Image, Video, Canvas, etc.Re: WebGL - I don't see why that would matter in Electron.But you can simplify startup by saying  so it just creates a class instance without triggering any work at the startup time.Again, don't try to  whats going on - why don't you enable proper logging and see whats actually going on?\nHuman logs everything to console and its very informative - any error will be immediately visible", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 31, 2022", "body": "closing the issue as no actual information or logs were provided so far (its been over 10 days).\nonce logs are available, issue can be reopened.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 21, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 21, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 31, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/205", "issue_status": " Closed\n", "issue_list": [{"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Safari throws the following exception:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": "Strange that  is undefined on error in Safari, but ok, I'll patch TFJS to check for that condition. However, that happens inside error handler - there is no working around the fact that GL context got destroyed.Can you copy & paste browser log instead of screenshots? I need them to submit patch to TFJS as that is where error happens (I could create a workaround in , but its better to fix at the source)?Anyhow,  backend doesnt exist in browser, so if you select it Human will automatically use  insteadSimilar for  unless  is enabled in experimental flags in browser ()Can you try couple of things?", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Of course!", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": "actually, can you paste the log using  ?\nit should be more verbose since exception handler there is disabled by default, so it should show stack trace", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Here you go:", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Enabling  doesn't make a difference (Human will still use the  backend). ", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Sorry, forgot to mention: It does work fine in the latest Chrome and Firefox.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": " runs several checks to see if browser supports  correctly before allowing its usage ( is still experimental), check output of  to see details.I cannot believe that Safari still hides WebGL v2 behind a flag and uses v1 by default as v2 was added in Safari 12 which is over 3 years ago!Yup, I though that might work. Chrome and Firefox have WebGL v2 enabled by defaultAnyhow, I'll create a workaround for WebGL v1 (which is basically to disable some internal features if GL v1 is detected so performance will be slightly slower)", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Yeah, well... I've stopped being surprised by this kind of BS from Safari a long time ago. Honestly I was surprised Safari is supported as well as it is  The only thing worse than Safari right now (not counting IE) is Mobile Safari. I'll be testing there as well once you've published a fix.Btw, I've also tested with v2.3.2 and no difference - in case that helps.", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": "Btw, I appreciate the fast and helpful answer! I've only started looking into this project but so far I'm beyond impressed! Is there any way to donate or support you in some other way?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": "Didn't think it would, but wanted to check - human v2.5.1 is first release that uses my custom built  and wanted to make sure that is not the cause in any way by default uses  backend which my custom variation of  with some tweaks, but it requires  2.0 (main difference is in startup time - its about 2.5x faster, but that makes it about 0.1x slower in inference which in my book is a good tradeoff) basically disables  and forces fallback to default New code is on github, still using same 2.5.1 tag (i'll wait few days to see if there are more issues with 2.5.1 before publishing 2.5.2)Also added a trivial fix to TFJS itself: thanks!no donations needed\nsupport? i welcome any suggestions and contributions!", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": "quick update: my patch for  is approved & merged to master branch which triggered new build of  and  - new ci pipeline reduces this process from months waiting for a release to hours!", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 9, 2021", "body": "Awesome. Thanks for the update!", "type": "commented", "related_issue": null}, {"user_name": "MaKleSoft", "datetime": "Nov 8, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 8, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/278", "issue_status": " Closed\n", "issue_list": [{"user_name": "Nekronik", "datetime": "Jul 5, 2022", "body": "We have also received reports of different errors, that appear to be internal, where we don't know how to fix them nor what they mean.\nAny help would be very appreciated.Sadly I don't have more code context for this one.\nWe are also getting this message sometimes \nWe have not identified what produces them, nor found a common pattern.\nNo errors", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 5, 2022", "body": "there is a lot going on here, can you provide info what is your platform - where are you running human? which browser and on what hardware?most of the errors point to either browser/hardware incompatibility or extremely underpowered device.for example:All 3 of those sound like they have same root cause - .The only one that's left is:", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Jul 5, 2022", "body": "For the related-together errors, human always runs on:I have not yet found a way to determine which devices are not going to be able to run human. Do you have any ideas on that regard?For the ", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 6, 2022", "body": "I haven't tested using less popular browsers such as Samsung Internet or Mi Browser and don't know their capabilities - and unfortunately, browser capabilities vary drastically, so it could be that some don't support WebGL v2.Regarding which devices are not going to be able to run human - well, it depends. Key factors are:Regarding shape error, it happens here: `but  are results of a model execution, so only way this results in error is if model did not execute correctly - again most likely due to low memory issues. in any case, i'll add some additional checks in that part of the code in the future as well.", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Jul 6, 2022", "body": "I have checked which devices produced the errors, and for all the errors the devices where indeed old (2012 - 2018), but the error regarding the shape was appearing on newer devices:So the theory about that this error was due to a low memory is probably not correct.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 6, 2022", "body": "Yes, that reshape error was suspicious from start, but only way that happens is if something within model went wrong - I'll have to dig deeper into that.If you have any insights on what was being processed at the time of the error?Anyhow, I'll add some error handling code to capture error as well as attempt to treat it as non-critical. Likely target end of next week as I'm traveling at the moment.", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Jul 6, 2022", "body": "The source was a video stream of the front-facing camera with 1080p resolution.\nThis is how we are currently consuming human:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 6, 2022", "body": "Nothing strange in either input or in config.\nI'll need to dig into this when I'm back.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 7, 2022", "body": "i figured out a  issue, but that is already fixed a while back, you just need to update  on affected clients.old code waswhich caused a random bug when model was updated as there is no guarantee that order of resulting tensor will be exact - so following code was sometimes trying to analyze wrong result.code fix identified each tensor based on its expected shape instead of relying on hardcoded order of variables, so it cannot be anything else:let me know if there is anything else remaining here that should be looked at?", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Jul 7, 2022", "body": "I have indeed not received any report for the reshape error on version , sadly I can't update past  since the next released version introduced the IndexedDB.\nOnce we can update we will double-check it. I will open a new issue if I spot it again.\nFeel free to close the issue or leave it open if you have some work related to it pending.\nThanks :)", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 7, 2022", "body": "i'll close this issue and keep the webview one one - target for new release is end of next week.", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Jul 5, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 5, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jul 7, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/273", "issue_status": " Closed\n", "issue_list": [{"user_name": "a1782680475", "datetime": "Jun 9, 2022", "body": "When using the front camera, the content of the camera is opposite to the reality. How can I mirror it like a mobile camera?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2022", "body": "you can either set a CSS transform on the output canvas yourself like this:\nor you can use build-in methods in  by simply enabling\n\n(note that  must also be set to true - which is the default, just don't disable it)", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 10, 2022", "body": "First of all, thank you for your answer.\nI can do this by using scaleX (-1), but some face detection boxes will be flipped so that they cannot be read. See the following for specific effects.\n\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 10, 2022", "body": "Boxes seem fine, what's reflected is text - and you don't have to use built-in methods, you can draw it yourself.And what about second method I've mentioned?", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 10, 2022", "body": "\nThe second method seems to have some serious problems... I'm not sure if there is something wrong with my code. I before used face-api.js,At that time, I could make some changes by modifying the source code, but the human is obviously more complex. I hope my demand can be supported from the project, because obviously, if the left and right are reversed, it is difficult for users to center their faces in the picture. I hope to be able to do the same as a self timer camera.Thank you.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 10, 2022", "body": "Second method should not produce results you're seeing, I'll take a look, but may be a slow response as I'm leaving for a trip tomorrow.", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 10, 2022", "body": "Thank you very much. Do you need my code?", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 10, 2022", "body": "It's all according to your time. I'm not in a hurry. I can wait until you come back from your trip.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 10, 2022", "body": "Yes, having your code would help.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 10, 2022", "body": "i just a quick look before my flight and i don't think there is an issue, its just a question of which canvas you're drawing.if you have 'config.filter.flip = trueinputVideo` as input, it will be correctly flipped before processing. but then the question is what is displayed on screen? if you display original video (or copy it to output canvas), it will not be flipped so detected results will look exactly opopsite of image.instead, you can draw processed image (so after it has been flipped) - it is part of the result output.for example:i've just updated  and you can see how its handled there - you can simply enable/disable flip and it works fine.", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 13, 2022", "body": "Think u.\nAccording to your method, the problem has been solved.", "type": "commented", "related_issue": null}, {"user_name": "a1782680475", "datetime": "Jun 9, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/154", "issue_status": " Closed\n", "issue_list": [{"user_name": "Utopiah", "datetime": "Aug 12, 2021", "body": " Can't get the node webcam demo to run Return some prediction from the connected working webcam node/CLI", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 12, 2021", "body": "can you try to explicitly install  and verify there are no errors during installationmy best guess is that there is something wrong with tfjs-node to tensorflow.so node binary bindings and that is RPI specific (I cannot reproduce on non-RPI platform)you might also want to try with  v14 as it uses binary bindings API v7\nand what is the output of  on your RPI? It's likely  architecture which means 32-bit kernel. not sure if latest tensorflow supports 32bit kernels, so you might need to use older tensorflow (which means older version of  since it has tensorflow.so pre-packaged).the stack trace you've posted points to error during kernel op registration which happens first time any backend-specific kernel op is called (which in this case happens to be )", "type": "commented", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 12, 2021", "body": "Thanks. I rolled back to node 14 but initially got a segfault (no error message) trying to run . I then ran  which gave me no error and  (so I assume TFjs working). I then tried again and got :Details on RPiAlso FWIW I did  resulted inwhich is about correct. Ironically enough with(switched back with nvm) so I assume it might be related to the webcam part on the RPi rather than tfjs-node.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 12, 2021", "body": "Ah,  14 and 16 binary bindings (NAPI v7 vs v8) are not compatible, so any modules that rely on binary bindings must be reinstalled (as bindings are established during installation). In your case, that would apply to both  and  as they rely on external binaries.This last stack trace shows that  did not provide a valid image, so later  complained about valid image type.But yes, I'd agree that it looks like  is now working (it definitely wasn't in the first stack-trace) and the remaining issue is  not binding to  correctly.I know this is a pain, but can you try with both Node 16 and 14, but each time make sure that both  and  are freshly installed?", "type": "commented", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 13, 2021", "body": "Tried but same result. What I did was move ~/human to ~/human_node16 then switch to node14 using nvm.I then pulled human again from git then ran npm install there. Again node worked but not node-webcam :", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 13, 2021", "body": "you've mentioned that  alone can extract image screenshow from your webcam\nany chance you can try test project with  alone?\nexamples are at i really don't know how to troubleshoot that library and that's why i included the note in the example that this example  is unsupported", "type": "commented", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 13, 2021", "body": "Just tried cloning their repo then  and npm with their example and both worked.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 13, 2021", "body": "i had a hard time reproducing on my current hardware, but then remembered that i could setup  for emulation...anyhow, updated  is uploaded, it should work now:issue was in trying to promisify output of \nit used to work, but somewhere it broke - at the end, i just used callbacks instead", "type": "commented", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 14, 2021", "body": "Indeed, just pulled and tried, works now!Thanks a lot, closing the issue.", "type": "commented", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 12, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 12, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Utopiah", "datetime": "Aug 14, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/275", "issue_status": " Closed\n", "issue_list": [{"user_name": "maxcodefaster", "datetime": "Jun 20, 2022", "body": "\nHumanGl throws error: Requested texture size [4739x4739] greater than WebGL maximum on this browser / GPU [4096x4096].\nModels I have activated are face tracking and emotion detection.\n2048 or 4096 seems to be reasonable limits. At least as of 2020 it looks like .Human v2.8\nSamsung SM-G991B\nChrome Mobile Webview 102.0.5005\nAndroid 12", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 20, 2022", "body": "can you describe what is your workflow? what is the actual input device and/or resolution?\nnote that if  is not manually disabled,  will automatically resize any input larger than QHD (looking at width > 3840), but you can also change  and  to any value - can you try that?", "type": "commented", "related_issue": null}, {"user_name": "maxcodefaster", "datetime": "Jun 21, 2022", "body": "Thank you for your work and quick reply.The error was thrown by a Samsung SM-G991B, Chrome Mobile Webview 102.0.5005 with Android 12. This device seems to have a 4k front camera, so that is where perhaps the error is coming from.I disabled  because I was worried about performance degradation, but it was probably unnecessary. The workflow we have is:\nCreate video stream, and every 500ms run human detection on that stream to get the emotions. ()So I see two options:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 21, 2022", "body": "Both options are valid:If only thing you're interested is emotion, you might want to experiment with  value in  (i didn't expose it as configurable value, but i probably should) - its basically how tight is crop around the face going to be before passing it to other models such as emotion - and it does have quite an impact.", "type": "commented", "related_issue": null}, {"user_name": "maxcodefaster", "datetime": "Jun 21, 2022", "body": "Wow especially the last part  sounds very interesting ;)  If you could implement a config that would be awesome! Its better than forking.I just deleted  and will await the results. Do you think it would be wise to set  or  to lets say a resolution of 480p to further improve performance?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 21, 2022", "body": "i totally forgot, it is exposed in config, but as hidden optional parameter: , default value is 1.4setting low resolution only really improves performance when you're running in web worker as transferring pixel data between threads is not cheap. otherwise, i'd suggest to run with 720p minimum. reason is that after face detection, you still want cropped face box to be ideally 384px (its resized up or down to that resolution). now, if your face takes 90% of frame, then running at 480p is ok. but if face is smaller, then having larger input resolution helps.btw, i've just noticed that you've disabled face mesh model (because you don't need it). but...face detector is really quick, but resulting boxes are a) not really precise, b) there are some false positivesface mesh when runs also stabilizes face boxes by removing false positives and rewrite face box coordinates with precise ones. so depending on your use case, you may want to try with mesh enabled even if you don't care about mesh detection", "type": "commented", "related_issue": null}, {"user_name": "maxcodefaster", "datetime": "Jun 22, 2022", "body": "Found the config you are referring to here: My config with scale factor set to 1.1 and mesh not disabled looks now like this:Let's see how those results will perform. Thank you very much! Do you know of any other performance tweaks for my specific usecase?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 22, 2022", "body": "Depending how static the input is, you may want to play with caching setting.Btw, I'm closing this issue as resolved as original item is no longer a problem - but feel free to post on this thread if you have any further questions.", "type": "commented", "related_issue": null}, {"user_name": "maxcodefaster", "datetime": "Jun 23, 2022", "body": "Ok thank you so far!To further explain my usecase: I am taking an MediaInput stream from mobile devices front camera and analyze every 500ms the emotions. On low level to medium level devices this causes a lot of performance issues, as we also run video playback and media recorder. So I am thinking on handling the emotion detection on server site. Or do you know of any suitable optimizations to keep human running on the client?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 24, 2022", "body": "In that case, I'd go for a web worker implementation - its not faster than running  in the main thread (its actually a touch slower as there is extra step or copying pixel data to worker thread which takes couple of ms), but it completely frees up main thread to do anything else without locking up every 500ms when detection runs.", "type": "commented", "related_issue": null}, {"user_name": "ButzYung", "datetime": "Jun 24, 2022", "body": "A bit off topic, but recently I discovered that it's actually faster to pass  instead of pixels data as () in web worker. The reason is that  stays in GPU the whole time, while for  it needs to be extracted from the main thread and put it back to GPU in web worker, and these processes take CPU time. The difference can be quite significant, in some cases up to 25% of fps increase. This is tested on  models running on web worker, but I suppose the improvemnet applies to  as well.", "type": "commented", "related_issue": null}, {"user_name": "maxcodefaster", "datetime": "Jun 20, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 20, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 22, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/116", "issue_status": " Closed\n", "issue_list": [{"user_name": "lghasemzadeh", "datetime": "May 3, 2021", "body": "Hello Vladimir,I want to read streams from a camera not webcam?\nHow can I do it?\nI just simply changes all getVideoTracks()[0] to getVideoTracks()[1] in the index.js and face3d.js, but I got the 'Exception Error' on the screen.\nActually after allowing the camera to capture, the camera's lights get on (like what happen with webcam) and it seems camera is ready but no stream!Thx", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 3, 2021", "body": "It was connected without changing anything. I just changed it to the original getVideoTracks()[0] and run again and it worked with my external camera.\nHere I want to know what if I have several cameras and webcams connected to my laptop, what is the priority or how can I select a specific one to get streams from?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 3, 2021", "body": "before call to  there is a call to  which returns all streams that fit given constraints - and if you want to select a specific camera, you select contraints that match that camera.check out specs at ", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 25, 2021", "body": "Actually there are two cameras (s and ca), both of them are recognized by the algorithm and they appear in the permission pop-up. One of them works but the other one (ca) doesn't.\nI face different situation when I change usb ports. For the ca camera sometimes I receive error of 'CAMERA ERROR: STARTING VIDEO FAILED' and sometimes even I select it trough pop-up permission but the webcam get active (I mean I select camera ca but the webcam starts to give stream). what is the reason? How can I fix it? both cameras are very similar in characteristics and same brand.\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 26, 2021", "body": "How are you accessing second camera?Also, can you open browser inspector, there should be a more detailed error noted there.\n(human library logs to browser console in more details)", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 26, 2021", "body": "I do nothing, as soon as I connect the S camera via usb to the laptop it will be recognized by the algorithm and will be in the list of devices it the pop-up permission. I select it and the algorithm starts getting streams from the camera. but it is not true for the other camera (ca), I don't know why?\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 26, 2021", "body": "it's not  that picks up multiple cameras, it's the browser.\ndemo simply asks browser 'give me camera that fits constraints' and since default constraints is basically anything, browser gives it full list.\nyou picking camera in the browser pulldown list doesn't mean you're selecting it, demo will still try to use default (first one) if there is no more specific constraints.\nso you  second camera and that means nothing, it will still try to access first.what you should do is list all cameras programmatically and then choose which one to use.for example, this will list all cameras:and then in  modify camera constraints to explicitly choose which one to use:i might be able to do it with a dynamic selector, but i don't have multiple webcams available, so cannot test anything.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 26, 2021", "body": "i've added some additional messages to function  in , new code is on  main branch:it should log something like this to browser console on startup:", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 3, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 3, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 24, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/152", "issue_status": " Closed\n", "issue_list": [{"user_name": "websocketing", "datetime": "Aug 10, 2021", "body": "I'm glad to see this project. I wonder if this project supports anthropometry? I need to measure the human parts of the generated manikin. I hope I can get your reply and information.", "type": "commented", "related_issue": null}, {"user_name": "websocketing", "datetime": "Aug 10, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 10, 2021", "body": "no, it doesn't support  - but that is a really interesting topic - i'll do some research on it\nand if you have a working example, let me know", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 10, 2021", "body": "from the top of my head...given that we don't know distance or reference scale to start with, we need to start with either:if focal length is well-known, its a decent startif relying on a refence object, there is issue with 3d depth:\nis reference object closer or further away from measured?\ncould partially be solved with 3d reconstruction models, but that is pretty low precision\nonly \"real\" way to do it would be using spectroscopic cameras so 3d reconstruction is precisewithout that, we simply don't know if we're looking at a doll in a model house or a giant on a hilland in both cases, we still have issues withso all-in-all, i don't think anthropometry would be any good without having a well-defined camera parameters to start with\nwhich fortunately in some cases can be extracted from the image exif properties for images, but it's missing for video feedsbut not all cameras with same params have same level of distorsion - that depends on the actual glass elements\nso most distorsion correction software works with a built-in lens distorsion database and still only corrects barrel distorsions\nwhile perspective distorsions are hit-or-miss (just look at photoshop or lightroom)all-in-all, i could add some for-fun functions that do different measurements, but only if\na) user provides camera parameters in the config\nb) there are no significant perspective distorsions (meaning camera angle is straight-on)let me know your thoughts", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 13, 2021", "body": "closing as it cannot be implemented without user providing a lot of information manually", "type": "commented", "related_issue": null}, {"user_name": "websocketing", "datetime": "Aug 10, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 10, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 12, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Aug 13, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/115", "issue_status": " Closed\n", "issue_list": [{"user_name": "lghasemzadeh", "datetime": "Apr 25, 2021", "body": "\nHi Vladimir,I followed the structure you wrote me before, but I still get error and can not install.\nBut I was able to install it on my friend's laptop (I think there is sth wrong with my pc), now I wanted to run it as I was previously doing but it is not working anymore.\nI just want to see the demo as before. In vs code, activating the 'Go live' and selecting the demo. page opens, camera activated but no stream/video, no output.I updated and install it again but did not work.\nAny idea?thx", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 25, 2021", "body": " is not a VSCode feature, it's a feature of some plugin installed in VSCode that creates a live web server - and I have no idea how that plugin works since it could be any number of them.Can you use dev web server that ships with  instead? It's fully documented.And if there are issues with browser, would be good to get a log from browser inspector, not just a screenshot.My best guess is that a \"live server\" that a plugin created is not a fully functional one or it references a wrong root path so page fails on loading models, but since there is no log, I really can't tell.At a minimum \"go live\" must be selected on the project root and then manually navigate to demo in your browser - it's the only way path to models can be reached as they are below project root, not inside demo folder.But none of this has to do with library issues...", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 27, 2021", "body": "closing as no user feedback and issue is not library related.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 27, 2021", "body": "I just restart the system and then it worked.\nI think there were several files processing in the queue and the browser cache was engaged, that is why it was not working and after restarting the problem was resolved.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 25, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 27, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 27, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/125", "issue_status": " Closed\n", "issue_list": [{"user_name": "lghasemzadeh", "datetime": "Jun 9, 2021", "body": "Hello Vladimir,There are several good head pose models available out there but when I test them in different conditions their performance drops.\nI checked this  to learn more details about head pose estimation (FACE: FACING CENTER/LEFT/RIGHT ...) of Human but I didn't find what I am looking for.\nHow the head pose task works? What are the inputs? what is the reference parameters to calculate it? Do you use MediaPipe facemesh to calculate the 3D face and position?\n do you use a pre-trained model for head pose estimation? if yes would you please share the link.Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2021", "body": "entire gesture analysis is done in  and is based on calculations performed as a last step of  method and takes  as input.specifically, for face gesture, its method  that looks at z-coordinate of the points of edges of the eyes:\n(i choose edges of the eyes instead of edges of the face as edge face is more likely to get occluded with higher angles)", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jun 9, 2021", "body": "Thank you\nok so you use the eyes outer corners and nose tip (or some where there) to calculate the distance and then the head pose.\nThen what about the pitch yaw and roll of head? where they come from?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2021", "body": "pitch/yaw/roll also come from face mesh point coordinates, but math is very different (and more complex)\nsee  method , code is annotated.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Feb 7, 2022", "body": "Hello ,Hope you are fine.\nI am struggling with head pose extracted from mediapipe facemesh. when I changed the camera position the head pose estimation gives wrong estimations, which I expected it actually.\nFor example when I change the camera position from the center at the front to center at the bottom (there is not a huge difference in camera position), and change the thresholds for up, down, right and left but it doesn't work well correctly.\nDo you have any idea?Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Feb 7, 2022", "body": "are you having difficulty implementing your own algorithm or having an issue with algorithm in human?\nmy algorithm for calculating face rotation angles should be agnostic to camera position.\n(some other things may depend on camera position such as eye gaze estimation, but not face itself).take a look at:\n", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Feb 7, 2022", "body": "I first checked the Mediapipe then tried the Human and I faced same problem. see the different for same  but different camera positions in images below.\nFirst image: the camera is exactly at the , and my head is center as well, the estimation is correct.\nSecond image: the camera position is at the , and my head is again center exactly the position in previous image, but the estimation is wrong and shows 'Head Up'. Of course from point of view of the camera my head is up but I need world actual coordinate not the head pose w.r.t. the camera' coordinate.\nI changed the thresholds but it still doesn't give correct estimation. can  or  work for Human?\n\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Feb 7, 2022", "body": "i don't see an issue - there is no way that anyone (actual person or AI algorithm) can determine if person is looking up or camera is placed on a low mount unless there is some reference point.And if you have a reference point, then it's easy to calculate angle and simply subtract it from a detected one. Of if you already know angle of the camera, just subtract it from detected angle.Trying to modify thresholds or looking how OpenCV does it is IMO non-necessary and overly complicated.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jun 9, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jun 9, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/102", "issue_status": " Closed\n", "issue_list": [{"user_name": "Nekronik", "datetime": "Apr 13, 2021", "body": "After using the library I got some questions:. Is the  method return signature correct? I'd expect it to not contain a .. Is it possible to only draw the face mesh and not the box?. Is it possible to customize drawing styles? Are they global or per draw method request?Also, since I had to write better types for the gestures and it is one of your goals, this is what I got. Maybe this can help you.The  is not complete since I did not know all the  possible values.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": " returns  which i use in automated tests, otherwise  return values should be ignored.\nNote that  internally is just a  call with different preprocessing and using embedded test images.Sure, by setting Yes, you can customize it via  objectEntire  class is documented in Currently they are global. But its a good idea to have a local override, I'll add that today and update here when done.Why do you need such strong typing for gestures?\nI didn't create fixed typings for it because gestures are considered expandable - idea is that user can add additional gesture detections (although that code to provide user-level functions is not yet published)", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Apr 13, 2021", "body": "Thanks, I was not aware of the typedoc site.To ensure I don't misspell any of the provided gestures.I was no aware of it. Depending on the implementation of this feature the gestures could be strongly typed for the pre-defined ones and just a matter of providing a generic with the type of the user-defined gestures.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": "how about a compromise - strongly type gesture.part, but leave gesture itself as generic.", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Apr 13, 2021", "body": "This would work, up to you, you know the library roadmap better than me :)\nI will keep it strongly typed on my end.But this would be an improvement anyway since right now I have to dobecause  has asignature, instead of", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": "btw, i've just implemented global/local draw options. is global (will bump version to 1.5 as this renames existing ``human.draw.drawOptionshuman.draw.*` method accepts optional parameter `drawOptions` which is used to override global options for that draw method only.and  are now defined as strongly typed interface :)new version will be published later today.i think that was the last part of this issue, so i'll close it for now.", "type": "commented", "related_issue": null}, {"user_name": "Nekronik", "datetime": "Apr 13, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/99", "issue_status": " Closed\n", "issue_list": [{"user_name": "MuriloRS", "datetime": "Apr 13, 2021", "body": "\nNo face is found in this image:\n\nfile://src/models/facemesh.jsonfile://src/models/blazeface-back.json`, // can be 'front' or 'back'.\n// 'front' is optimized for large faces\n// such as front-facing camera and\n// 'back' is optimized for distanct faces.\ninputSize: 256, // fixed value: 128 for front and 256 for 'back'\nrotation: false, // use best-guess rotated face image or just box with rotation as-is\nmaxFaces: 1, // maximum number of faces detected in the input\n// should be set to the minimum number for performance\nskipFrames: 11, // how many frames to go without re-running the face bounding box detector\n// only used for video inputs\n// e.g., if model is running st 25 FPS, we can re-use existing bounding\n// box for updated face analysis as the head probably hasn't moved much\n// in short time (10 * 1/25 = 0.25 sec)\nminConfidence: 0.3, // threshold for discarding a prediction\niouThreshold: 0.2, // threshold for deciding whether boxes overlap too much in\n// non-maximum suppression (0.1 means drop if overlap 10%)\nscoreThreshold: 0.5 // threshold for deciding when to remove boxes based on score\n// in non-maximum suppression,\n// this is applied on detection objects only and before minConfidence\n},face: {\nenabled: true,\ndetector: { modelPath: 'file://src/models/blazeface-front.json', enabled: true, rotation: true, return: true },\nmesh: { modelPath: 'file://src/models/facemesh.json', enabled: true },\niris: { modelPath: 'file://src/models/iris.json', enabled: false },\ndescription: { modelPath: 'file://src/models/faceres.json', enabled: true },\nemotion: { modelPath: 'file://src/models/emotion.json', enabled: false },\nage: { modelPath: 'file://src/models/age.json', enabled: false },\ngender: { modelPath: 'file://src/models/gender.json', enabled: false },\nembedding: { modelPath: 'file://src/models/mobileface.json', enabled: true },\n},\n// body: { modelPath: 'file://models/blazepose.json', enabled: true },\nbody: { modelPath: 'file://src/models/posenet.json', enabled: false },\nhand: {\nenabled: false,\ndetector: { modelPath: 'file://src/models/handdetect.json' },\nskeleton: { modelPath: 'file://src/models/handskeleton.json' },\n},\nobject: { modelPath: 'file://src/models/nanodet.json', enabled: false },\n};`The result:Why is not finding an face in that image?**Environment", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": "using  model it doesn't detect a person as that model is trained for center-aligned faces,\nthis photo is too far to the edge. and in general  model is good only for webcam input, not for photos.use  model (which is default for a good reason) and it works fine:", "type": "commented", "related_issue": null}, {"user_name": "MuriloRS", "datetime": "Apr 13, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/118", "issue_status": " Closed\n", "issue_list": [{"user_name": "okeoke85", "datetime": "May 11, 2021", "body": "Actually this is a question,How can i get frames or images from uploaded video to send detection with the second of frame in healty and performanced way, it is not directly related to human api if you went through same paths and if you can help me, it would be a peach. Thanks in advance.lets say i have 5 min. video and i need to get an image every five seconds, desired result will be likeInput : video url, i dont have any video on machine as file, i just have url,Results = [\n{ sourceVideo : videoUrl, time:takenTime, detections:[] },\n{ sourceVideo : videoUrl, time:takenTime, detections:[] }\n]", "type": "commented", "related_issue": null}, {"user_name": "mayankagarwals", "datetime": "May 11, 2021", "body": "I am looking to do something similar. Except using a camera streamFor your problem, you can probably use ffmpeg to read frames from a video file (found an example ) and for each of the images, run the detector. Keep agressive caching enabled as that will give better results (how is described here : )I am also trying to find a tool to read frames from camera stream and its surprisingly hard to find one that fits the bill.", "type": "commented", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 11, 2021", "body": "you can use this, i already did with video streaming; i have video url as input and i want to do it server side with node.js, you can use this function on client;", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 11, 2021", "body": "your question is about triggering detection of a video from url at fixed intervals, here's a quick example:your question is about how to get and decode video stream in a nodejs environment (and additionally, how to read webcam from nodejs environment), i've just answered that in ", "type": "commented", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 11, 2021", "body": "But i'm on node.js side, will this work?const video = document.getElementById('video');I mean i dont have a htmlVideoelement i just have the url of  the video like this;", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 11, 2021", "body": "Ah, didn't know that. And no, definitely not going to work.\nNodeJS has no either image or video decoding functionality. Image is simple enough to do on CPU using Node's v8 engine, but decoding video in JavaScript is waaaay too slow, so you really do need an optimized external decoder, most commonly .\nAnd to use it from nodejs, I'd look at .(you might go to a lower level and do actual bindings from NodeJS to  or whatever decoding library, but that gets really messy really fast, I'd stay on  level)If your video (from URL or from file, doesn't matter) is fixed-length, then it's easy to generate screenshots every x seconds and save them as list of files:And once you have list of images, it's easy to process them using .\n(I'd still use  as target for that to avoid actual disk writes)If your video is a stream, then it's a bit trickier, but should still be doable with a  followed by  loop", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 11, 2021", "body": "FYI, A bit off-topic, but maybe it helps to clarify few things...Modern browser components:Out of all that, NodeJS shares  language engine with Chrome, the rest is browser-specific.\nEven simple  function doesn't exist in NodeJS and requires a 3rd party library as it's implemented in Browser's network layer.But NodeJS does add some functionality on top of V8 in form of built-in libraries as well, such as  for filesystem access or  used to create web server which doesn't exist in Browser.And for desktop apps, there is  which is basically NodeJS V8 + Blink engine, so a lot more functionality.So when looking what is doable in NodeJS, best to look what is done in V8 inside Browser.\nOr use  which is a complete headless Chrome browser that can be triggered and controlled from NodeJS.Modern browsers are by far the most complex software today other than OS itself.", "type": "commented", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 11, 2021", "body": "Such valuable infos, thank you Vladamir.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 15, 2021", "body": "i'm closing this issue as it was a general question. please open a new issue if there are specific implementation problems.", "type": "commented", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 15, 2021", "body": "Sure, thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 17, 2021", "body": "i just did a quick prototype using  to decode input video (demo is using a file, but same applies to stream or webcam, just change input params) and pass data to  for processing. everything is done via pipes, so there are no ugly temp files.main trick was to use motion jpeg as output format which is then easily parsed for frame start/end markers which gives jpeg per each frame.", "type": "commented", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 17, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "okeoke85", "datetime": "May 11, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 11, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 15, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/105", "issue_status": " Closed\n", "issue_list": [{"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": "Hello,Regarding the talk we had I attached 4 photos. It think there is sth wrong because I don't get even a single frame correct gaze direction.\n\n\n\nand I have another question:\nwhere can I find the FACE related prints on the left up corner, first line? I just want to simply change the 'FACE: FACING CAMERA' to sth els, e.g. FACE: FACING CENTER. just changing a word. I check index.js, node.js files but did not find where you print those words.Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 18, 2021", "body": "Those are good examples - and yes, my simplified math is just too simple - comparing area sizes of irises is insufficient when you're facing camera, but looking away from cameraI'll update here when I have a new solution readyinside  it just calls built-in helper functions indside  to draw\nand the one you're looking for is \nbut again, they just print what they get from   object - words are defined in so you can either:for example, really silly but simple way of converting entire result to string, replacing strings and returning back as result\n(better way would be to walk the object and replace values as needed)", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": "These are what I have in the Human folder.\nI don't have demo and src folders.\n\n\nis it ok to just download the missing folders and putting them into my Human folder?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 18, 2021", "body": "this does not look like  official npm package or git clone.\nthis looks like someone manually copied  folder and placed  and  folders inside  and that's what you're using.you're missing all of the sources and documentation.where does this copy come from and how was it installed?", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": "A friend of mine prepared it and gave it to me. I just got the file and extracted it.\nhow should I fix it or can I continue with it?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 18, 2021", "body": "you can continue using it, but then you wont be able to update easily as i rollout any fixes or changes (your friend would have to prepare it again)documentation you can see online here on github, so that's not an issue\nbut you're missing actual sources, you only see the demo (things in '/dist' are compiled and minimized, so not readable) - that's why you cannot find things when you're searching for stringsyou could also download everything from here on github, but then you also wouldn't be able to update automatically\nso it would be best to either use  to install NPM package or  to clone repositoryanyhow, for purpose of this issue, to rename  to  you don't need any of that, you can still do that by doing either:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 18, 2021", "body": "ok, i've just made some changes to :i've tried also 'looking up' and 'looking down', but human eye just doesn't have height compared to iris height to be able to run math precise enough - it's ok for close zooms of eye, but otherwise not precise enough so i won't enable it.new code is already on  and will be published on  later this week (waiting for some fixes from tfjs team to be able to bundle new tfjs 3.4.0)", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 19, 2021", "body": "Hello Vladimir,you changed the text but, than you, but I still need to find where you print those stuffs at the left up corner.\nthe only thing I found from the folder that I have is the below screenshot.\nI don't know maybe js is completely different from what I know (python). I want to see the lines of code that you print those strings, Especially the first line (FACE: .....)here by commenting the line 165, all the texts at left-up corner will disappear. but I want to access each line of that corner text separately.\nİt is very straight forward when I go through the github folders -> src -> gesture -> gesture.ts -> line 42 to 65. I don't have the src file and I can not find the source of last explanation you made (simple object -> string -> replace -> object conversion - walk the object and replace values as needed).\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 19, 2021", "body": "Yes, it can be done and I've tried it - the problem is reliability of results:There is no source, that is just how it can be done on your side\nAnd those are two different methods, not two steps in one method:simple object -> string -> replace -> object conversion:walk the object and replace values as needed):or the same with a map and regex function:if you do that before  is called, it will replace values as you want and then draw will draw your new results.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 19, 2021", "body": "ok, i've changed it to include up/down:as suspected, precision is not perfect, but why not", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 19, 2021", "body": "let me know if there are any remaining questions regarding gaze detection or this issue can be closed?", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 20, 2021", "body": "Currently, I don't have but I will have as soon as I get able to install the library (Human). As you said it is better to have the right package but I have difficulty to install it. I clone the repository and do the npm i but I get this error. I tried some solutions but it didn't resolve the error.\nHope you can help :)", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 20, 2021", "body": "to install human, you don't need to clone it and then install it's dependencies\ninstead, just run  to install  package from  and that's itthen to update  at any time, just run  to update to latest minor version or  to update to latest major versionor alternatively copy  from git main branch using \nin that case, to update to latest changes, you'd use but again, no need to run  inside human in either caseinstalling dependencies inside  is only needed if you plan to make changes to the library as it installs \nand in that case, best procedure would be to have a separate tree for :and such local fork can be used in your actual project by installing it from local path", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 21, 2021", "body": "i'm closing this issue as related to gaze detection.\nif there are any other questions, let me know.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 27, 2021", "body": "Hello,\nI am able now to play with prints at the left corner, and changing them the way I want,\nI need to find two more things:Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 28, 2021", "body": "best is to take a look at how drawing is done in  and copy those functions to your code and use them instead of built-in one. for example, in , inside function , it calls  - remove that and replace with call to your function. and you can use  as template to build that function. same for gestures.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 24, 2021", "body": "Hello Vladimir,I have tested the gaze direction in several different situations and the performance is not robust, as soon as I changed the position of the camera or light condition the accuracy drops. Actually I expected it because gaze direction function is a rule-based (mathematics based) and the ratios will change with both position of camera and the person.\nI thought it will be a good idea to integrate a robust DL based gaze estimation model into Human. There is an open source python pre-trained model that I am using for my study.\nI am supposed to change the algorithm to JS and then integrate it to Human, or is there any way to skip this conversion part?What is your idea? any solution to make the gaze direction finding part work more accurate?The Iris distance is working reverse, When I get more close to the webcam it shows higher distance and we I get more far from the camera it shows lesser distance.Thx", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 24, 2021", "body": "Can you share the model so I can take a look?Iris was calculating iris size, not actual distance. I've corrected that, latest code is on git.Values returned now are distance from camera in cm corrected for a typical webcam field of view of 88 degrees\n(for example, when i'm sitting in front of notebook, iris distance will be ~30cm).Note that there is no way to determine camera field of view programatically, so for more correct measurements user should adjust this value accordingly.Btw, this iris distance was an actual issue - can you in the future open a separate issue for such items so it can be tracked and closed correctly?", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "May 25, 2021", "body": "I just shared the link via email.\nSure I will open separate issue for iris distance.\nThanks", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "May 25, 2021", "body": "No need to open separate issue for Iris distance anymore since it's already fixed in main branch as of few days ago\nJust please do so for any new issues you find in the futureAnyhow, I've just created a new discussion item and tried to answer most of the questions there: ", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Feb 7, 2022", "body": "Hi Vladimir,I checked the iris detection of mediapipe and I faced problem regarding the directions correctly and while searching for similar issues I found . It can give the iris position inside the eyes correctly when it is at right and left but for up and down it gives wrong estimation since the eyelid landmarks move as iris landmarks move and by the result the position of iris inside the sclera (eye) remains constant. see the video in the link I shared.\nI think you have this problem for Human as well. Do you have any idea?Thx", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Feb 7, 2022", "body": "exactly the same problem and i don't see an easy way out. but its also a low priority for me given everything else.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 21, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/57", "issue_status": " Closed\n", "issue_list": [{"user_name": "simon-lanf", "datetime": "Jan 7, 2021", "body": "I know that right now I can know if the user's eyes are facing the camera by knowing they are both at the same distance from the camera. The problem is that if someone is in the far left for example, this person would have to turn it's head in an angle from the camera's point of view and now this person is not facing the camera.I wonder if there is a way to detect these cases. We could triangulate where the person is looking from the 3d coordinates, or try to guess if the person is looking by adding parameters X and Z in the equation, for example if you are in the far left, you should be facing slightly the right.Let me know if you have any idea.edit: I'd like to add that I'm not looking for exact gaze or where the iris points. just if the head if turned to the camera.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 8, 2021", "body": "one way that comes to mind is to compare the size of the iris for left and right eye - if a person is looking towards camera, they should be within small margin. and that should hold regardless if person is in the center of the frame or at the edge and looking at an angle.for example:but...iris is a small object, so it could be unreliable in borderline cases where face is small or iris size cannot be detected accurately.another way would be to look at z-axis of points on left and right edge of the face.\nlike it's currently done in :but instead of currently fixed 10% threshold, project both points on a semi-circle with radius being distance from the camera and adjust z value before calculating difference. and use iris-based calculation for distance (included in the default result set) to calculate radius of the semi-circle .it's a bit more approximations, but since points are much more spread out i'd guest it could end up being more reliable.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 11, 2021", "body": "i've added gesture detection for iris-based  and published a new version, so i'm closing this issue.", "type": "commented", "related_issue": null}, {"user_name": "simon-lanf", "datetime": "Jan 7, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 8, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 11, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/55", "issue_status": " Closed\n", "issue_list": [{"user_name": "ost12666", "datetime": "Jan 5, 2021", "body": " says emotions are in the result root but it is actually inside the face object.Also I am not sure human.defaults work, at least sometimes it returned undefined", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 5, 2021", "body": "also there is a type 'discust' should be 'disgust'", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 5, 2021", "body": "and 'surpise' should be 'surprise'not sure if those are doc errors or also errors in the code", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 5, 2021", "body": "I guess code const annotations = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surpise', 'neutral'];", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 5, 2021", "body": "you're correct. and the code was ok (in , but docs were not).\nfixed via ", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 6, 2021", "body": "Code is still wrong in surprise:const annotations = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surpise', 'neutral'];While doc is corrected://  'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 6, 2021", "body": "it was missing a git push - sorry :)", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 6, 2021", "body": "The library is awesome! thanksI really like the gestures, very useful", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 6, 2021", "body": "Thanks!Feel free to suggest additional gestures, current ones are just few samples.", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 6, 2021", "body": "I am now trying to implement head nods detection using head down and head up gestures so its more time based than static gestures. I implemented raise hand by looking at all gestures that has finder, hand or finger name in it. I am also using facing camera and face neutral for detecting user is looking at the camera.", "type": "commented", "related_issue": null}, {"user_name": "ost12666", "datetime": "Jan 5, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 5, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 5, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/42", "issue_status": " Closed\n", "issue_list": [{"user_name": "lenonMax", "datetime": "Dec 8, 2020", "body": "\nWhen I open the mesh and iris models on my laptop(i5,2.3GHz), the first time I open the web page, there will be 4-5 seconds of lag. During this time, animation and other things cannot be rendered.\nps: After the execution of human.detect(videoElement,config), the web page will freeze, and the video will not freeze if it is only opened.\n1.Enable the mesh and iris models in file of config.\n2.Preload the JSON and bin files.\n3.Open a new page and call the camera to enable face detection\n**Environmentthanks a lot.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 8, 2020", "body": "stall is due to initial model warmup.issue was that some models cannot be pre-warmed-up using empty image (e.g., face returns no results for empty image, so models that rely on detected face cannot be warmed up).added embedded sample image for purpose of warmup.fixed via: ", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Dec 8, 2020", "body": "Oh,I see.So cool!Thanks.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Dec 9, 2020", "body": "After my test, I found that the sample photos must contain detectable faces in order to trigger the face detection, otherwise it will not be effective. Could I pass in any picture?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 9, 2020", "body": "built-in  function uses embedded photo that cant be changed.\nbut warmup is just an early call to detect, nothing else.so you can skip  and do explict call to  using your sample picture, only thing is you want to disable caching of frame results and use picture only for warmup.something like", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Dec 8, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 8, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 8, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/41", "issue_status": " Closed\n", "issue_list": [{"user_name": "djthegr8", "datetime": "Nov 27, 2020", "body": "\nCamera access not supported in the browser, tried in Chrome and Edge.\n\nOpen demo\n\nWorks as required\n", "type": "commented", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 27, 2020", "body": "Never mind, this seems a solved problem ", "type": "commented", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 27, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/63", "issue_status": " Closed\n", "issue_list": [{"user_name": "lghasemzadeh", "datetime": "Jan 27, 2021", "body": "Hello,\nI have checked the facemesh demo of Mediapipe () and face landmark detection (). The second one really works better and more robust even when I change my head pose extremely and quickly.Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 27, 2021", "body": "it's a tradeoff between precision and performance and it's fully tunable in configuration.\nhow it works is that library tries to re-use previously detected face bounding box without running it again and just runs the mesh and iris detector instead. so if the face moves suddenly, face will disappear and bounding box gets invalidated so it re-runs on the next frame. again, behavior is fully tunable.also disabled by default for performance reasons is face angle calculation so if head tilts more than ~20 degrees, it will get invalidated. also can be enabled via configuration.yes, it does, you just have to set configuration parameter  to a value higher than 1. again, it's about performance vs precision.check out configuration documentation for list of all configurable parameters: Pre-trained network, trained on imdb dataset.pre-trained network, not just simple math.\nmath is used to calculate gestures such as .list of all models used are in credits page: \nlist of pre-defined math-based gestures that are included (others can be added by user) is in ", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 27, 2021", "body": "Thank you very much for the detailed answer.\nI still have lots of questions about mediapipe also. I don't want to bother you, so is there any forum for asking these questions? for example a forum that I can ask the library's developer directly my questions or sth like that? (not the tensorflow github forum)", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 27, 2021", "body": "for mediapipe, authors state that general questions should be posted via ", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": "closing the issue as the original question is answered.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 28, 2021", "body": "Hello Vladimir,I am sorry for asking alot, I am amateur but very curious about this library.What do you mean by performance? precision is also a concept of performance. you mean if I make the model to act more accurate, the performance will decrease? but what is performance here? is it the speed of algorithm?mediapipe (the new version) also does the face detection/bounding box/mesh/iris jobs as well but why it doesn't have lag, shaking and disappearing even when I rotate my head more that 30 or 40 degrees? in its demo I don't tune anything and it works very well.For the age prediction have you done any fine tuning over the pre-trained network or any other changes?For emotion detection, why you didn't use simple math (calculating difference in distances between eyebrow etc.) what will be its deficiency or problem?If simple math won't work for emotion detection correctly, would it work for mouth opening and gaze direction?!!Thx\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": "yes, if you make model to act more accurately it will decrease performanse. this can be due to many things.\nfor example (and there are other examples as well):new version of mediapipe is written in C++ and compiled to WASM. so entire pre and post processing that i've mentioned above is much faster than in JavaScript.also, JavaScript is single threaded, so any calculation will block processing - thus more lag. mediapipe can perform asynchronous interpolations between model runs. meaning, it runs the model prediction, but also calculates averages between each run so it can draw much smoother frames.i have added async processing in  library as well, but due to overal nature of JavaScript, it doesn't help much.in reality, mediapipe is a superior way of doing it, but it's proprietary - you can only do what they allow you and not much else, not room for user modifications/improvements/tunings.also, mediapipe models are pre-compiled to tflite format and cannot be easily re-used elsewhere.no additional fine tuning.it's possible, but emotions are complex - it's easy to differentiate between angry vs happy, but not so easy to mathematically describe surprise vs digust. i just found using model to be easier and more reliable.it does work for anything you can clearly describe mathematically. so things like 'mouth open' or 'looking left' or 'leaning down' are easy.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 28, 2021", "body": "The other reason of lower performance can be merging several models together in human, is it correct? since mediapipe has separated all functions (e.g. hand tracking and facemesh are separated and work solely, maybe that is why it has better accuracy and performance?)Do have plan to publish the python version of your work?The demo that I checked was for facemesh and iris tracking which is provided for js! not C++ I think\n\n\nIf I'm not mistaken the older version has Iris and mesh for C++,Since I just focus on eyes and head, is it better to use mediapipe not Human? since I need both high accuracy and performance at the same time.I previously wrote a script which was using dlib and tensorflow facial landmarks, from those landmarks I calculated for head pose and gaze direction (a rule based/mathematical method, not using any DL) but since the landmarks were not accurate, my model had low accuracy. since this facemesh is very precise about coordinate of eyes/Iris landmarks, I think it would solve my problem.\nI saw this solution from this video in linkedin which is similar to your package: In above you mentioned I can easily mathematical calculate for looking left or down, are they just examples and is it true for other directions such as up and right?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": "All models in  are separate and each can be enabled/disabled individually in configuration.\nThere are some dependencies such as:No.MediaPipe JS solution uses JS just to load WASM module which was written in C++. Actual processing is done in WASM, not JS.\nJust like their Python solution is not actually written in Python, it just exposes a loadable Python library.Your choice. With  you get full configurability - you can tweak it as much as you want to get equal or even higher precision than MediaPipe. On the other hand, using MediaPipe out-of-the-box is pretty good.Any such rules are simple (and yes, up and down are already included). Complex rules that cannot be done reliably with just math and where pre-trained models are better would be ones that vary from face to face - for example, your facial expression for surprise might not be mathematically same as for me.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 28, 2021", "body": "ok, thank youIs this library (Human) available for both academic and industry use? I know it is open-source but is it for both purpose freely?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": "Re: licenseLibrary is released under MIT license.\nSome models are originally under Apache 2.0 license.Both are permissive - so in short, yes.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 29, 2021", "body": "I really appreciate your detailed answer.In the Human library is there any black box module? can I use/change/modify all the functions and modules?", "type": "commented", "related_issue": null}, {"user_name": "ButzYung", "datetime": "Jan 29, 2021", "body": "In terms of functionality, /TFJS solution is obviously better than the current MediaPipe JS soltuion. The current MediaPipe JS solution is quite buggy. It doesn't work on web worker. It is not well documented and sometimes you have to do trial-n-error just to make things work. Its output is bugged. For example, MediaPipe JS facemesh output doesn't give the z coordinates for some unknown reasons.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 29, 2021", "body": " other than the fact that included models are pre-trained, there is nothing you cannot change/modify.\nand if you want to add additional modules or swap out existing ones, that is also pretty easy as design is modular.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": "Hello Vladimir,Hope you are well.Thank you", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 18, 2021", "body": "Yes, thank you! :-)Btw, can you open a new issue for this? Both questions are valid, but unrelated to this issue that is already closed.I just tried using webcam and it works for me.\nCan you send some sample images where its misdetected along with details of your configuration?Note that mathematically  is defined as difference between area size of left and right iris and if they are within 25%, then  will return .This threshold can be changed from 25% to anything (see , but it's not exposed as user-configurable value as I don't want to have too complex of configuration file (it's already pretty complex), so I just use some value that looked ok.Face can be up/center/down and left/center/right at the same time, so gestures returns two separate results.\nBut gestures return object is just that - an object - and If you want to combine them, it's just a question of printing them together instead separately.", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Apr 18, 2021", "body": "Ok, I will open a new issue", "type": "commented", "related_issue": null}, {"user_name": "lghasemzadeh", "datetime": "Jan 27, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 27, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 28, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Jan 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/40", "issue_status": " Closed\n", "issue_list": [{"user_name": "djthegr8", "datetime": "Nov 26, 2020", "body": "\ni am getting very low fps for WASM in my hand-tracking, so just one model that too without landmarks. This would give me 3FPS with WASM SIMD Threaded, which is very very less\n\nSimply check my \n\n10FPS, at least?The source code is ", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 26, 2020", "body": "when filing a performance issue, try to provide as much data as possible:anyhow, i did some digging and seems there is a quite a bug in  backend which causes this model to loose hand tracking so it has to be redone almost every frame - so all the caching optimizations are not used. this needs to be fixed in tfjs.see  for details.", "type": "commented", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 26, 2020", "body": "", "type": "", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 26, 2020", "body": "chrome  - can you be precise?\nthere are prod, beta, dev and canary channels and i cannot guess which one you use. and i don't know if your chrome was updated immediately or not (e.g., chrome 87 came out few days ago and had several breaking changes).resolution - no, it should be reported in the issue.\nplus the link you sent is just a link to readme and inside it link to demo goes back to my page. so i don't know where your demo is.webgl - anytime there is a performance issue, there should be something to compare it with. reference model, different backend, etc. just saying \"it's slow\" doesn't help me to investigate.anyhow, seems there is a bug in wasm backend as i wrote before, i'll wait for a feedback from tfjs team.", "type": "commented", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 26, 2020", "body": "", "type": "", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 26, 2020", "body": "i checked your demo and execution on my notebook with i7-8750 is 9+ fps, that is well within expectations.\nlooking at stats, image processing is ~10ms and model inference is ~100ms, so on my system resolution could be 4x and it would only drop to 8fps.i'm afraid that until there is more optimized wasm backend published by tfjs team, there is nothing i can do here.", "type": "commented", "related_issue": null}, {"user_name": "djthegr8", "datetime": "Nov 26, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 26, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 24, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/48", "issue_status": " Closed\n", "issue_list": [{"user_name": "vladmandic", "datetime": "Dec 15, 2020", "body": "I tried to use the MediaPipe API in my project, but unfortunately it doesn't seem to support web worker (a must in my case, since there are some intensive 3D animations, and there is little room in the mani UI thread for other CPU-intensive task). So at the end I tried your   library instead, but I encountered some issues.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 15, 2020", "body": "I've converted this conversation to an issue as it's better fitted and I can track enhancements.Sure, that is ok.\nAlthough I'm curious what are benefits you're seeing with loading via importScript instead of importing ESM module?Ahhh, the magic of missing items in workers hits again - I'll try to find an alternative.\n is used only to load embedded JPEG data that is used for warmup as all browsers have a built-in decoder.\nI could embed ImageData instead, but as that is uncompressed it would increase library size.\nOr I could use a 3rd party JPEG parser, but I try to limit additional dependencies.\nWill figure out something.PoseNet model has only some optimizations that should not impact it's accuracy - did you try changing default parameters in config?That should not be a problem. Only reason why I avoided it because it's too big to embed. I'll run some tests tomorrow.", "type": "commented", "related_issue": null}, {"user_name": "ButzYung", "datetime": "Dec 15, 2020", "body": "I want to have the option to switch between human and the conventional TFJS (loaded via importScripts), as it is not possible to use both import and importScripts in worker at the same time. Maybe I can load TFJS via import as well, but module support in web worker is still fairly new (Chrome 80+, no Firefox), so browser support is a concern.Yeah my config is customized. I have disabled all face-related models, leaving only body and hand. For body I only need to detect one person so I set maxDetections to 1. But that doesn't seem to be the cause of the problem. Even if I leave the body config untouched, the accuracy is still the same. In fact if I don't lower scoreThreshold to something below 0.5, most of the time the body is not detected at all. Even if it is detected this way, the scores of some body parts are low and the arms are \"jumping\" here and there (my app is focusing on uppper body detction, not the full body).", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 15, 2020", "body": "True. And it's even worse for mobile platform - Chrome still doesn't support modules there.\nFor my apps I prefer to use imports as usual to avoid unnecessary complications with importScript, but then create a bundle at the end and load that bundle instead. If you look at , that is exactly what it does on each source file change.Strange - I'll investigate.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": " i've spend too much time trying to work with a broken tfjs 2.8.0 release, just downgraded back to tfjs 2.7.0 and re-implemented  so it should work with web workers.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": " regarding your comment on  - note that body score is just average of scores for each keypoint. so if you're looking at just upper body and lower body is hidden, then average score is going to be low although score for upper body parts is high. if looking at upper body only, set  to low value such as  and check for each  in your app instead.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": ": i've implemented special case for single body detection (i didn't have that special case for body, only for hand), but i really don't like how it behaves - keypoints are accurate, but it cannot determine left vs right so every few frames points from left hand get switched to right hand and vice versa.Difference is performance and not precision - single pose just uses  to determine most likely keypoint out of each possible ones. multi pose actually traverses the tree to find most likely neighbor.Try it out, but most likely I'd remove this.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": " Work on Body MobileNet modelI've switched default in  to  instead of  and I like it much more.\nI have to keep it small and performant, but you can try other variations. E.g., MobileNet with with 8 strides is so much slower than with 16 strides - which makes sense since it analyzes 4 times bigger matrix (each area that is analyzed is image size divided by stride vertically and horizontally).You can try different MobileNet models like this:\n\n\n\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": " Work on Body ResNet modelOk, this was a bit messier than I wanted since ResNet and MobileNet models return results in different order (?!), but finally  is compatible with both.You can enable ResNet models like this:\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 16, 2020", "body": " I've figured out what is the difference in MediaPipe new compiled demo.Anyhow, model returns point cloud for each keypoint (e.g. there are multiple possible points for ). JS code finds one with highest score and that's it - but difference between them can be 0.0001% and switching back and forth, so result is \"jumpy\".Compiled one finds average between all points with high scores (as they are all ) - thus the result is much smoother output and confidence score is still high.It would be doable to do in JS as well, but it's not high on my list right now.Anyhow, that's all from me for now on this thread - major work and 6 updates.", "type": "commented", "related_issue": null}, {"user_name": "ButzYung", "datetime": "Dec 17, 2020", "body": "Been testing the new version of body detection. Unfortunately it's still not that accurate and \"jumpy\". Changing models don't seem to help much. ResNet is better, but not much. What's even stranger is that when maxDetections is left as default or bigger than 1, sometimes it would detect more than 1 body (the max I noticed was 4), even though I am the only one sitting in front of the camera LOL. When maxDection is 1, behavior is somewhat different like you mentioned as it will mess up left and right hands sometimes (something I already noticed when using the TFJS version in the past, though it was less serious). But besides the hands problem, maxDections=1 still looks better than >1 (which is even more jumpy) in general.I notice that the body detection of your live demo looks better than how I use it in my app in general. I have tried various config combo and can't really figure out the reason. Maybe it's because I am running my app on  but not a native browser? Web worker (BTW your demo doesn't seem to work with web worker option ON)?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 17, 2020", "body": "Ok, I'll leave it in.Strange, I find it much better up to a point of really good except for some jitter due to lack of smoothing (see my notes on keypoint cloud).\nBtw, I did notice that body models in general are very sensitive to lighting conditions and tend to be quite jumpy in darker areas.I'll take a look. It was working, but I probably broke it unintentionally.Electron vs browser shouldn't matter.\nWhich backend are you using? I believe there is a rounding issue in WASM backend at the moment which can cause some precision errors (I have several issues on that open with TFJS team), better try WebGL if that is an option.", "type": "commented", "related_issue": null}, {"user_name": "ButzYung", "datetime": "Dec 17, 2020", "body": "WebGL. Checking the console but couldn't see anything wrong in the config or any config difference between your demo and my app. The only \"difference\" is the reports on tf flags, in which your demo seems to show more details, but for properties that exist on both sides, they return the same value.On a side note, I can have both  (without body, just hand detection) and TFJS PoseNet (loaded in the conventional way) running at the same time. Yeah it's clumsy, but at least it works lol", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 17, 2020", "body": "Can't explain the difference since I haven't seen your app.Regarding  demo and web workers - I just tried it in Chrome & Edge and it works. Even better if you enable  as then UI refresh is completely detached from processing.Which browser? Firefox is missing several features and I've decided not to support it for web workers currently. It would be possible, but a major pain - I'd rather wait for Firefox team to finally implement things like offscreenCanvas.Update: Ahhh,  that hosts live demo resolves relative paths differently than my local environment, so  was not even loading (error 404).", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 24, 2020", "body": "i'm closing this issue as there is a lot of things worked on here, but feel free to open a new one to track further work.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 4, 2021", "body": "FYI, I've managed to successfully convert and implement MediaPipe's  model as alternative to .\nPoseNet is still the default, but can be switched via configuration options. If interested, check out model notes as it's performance is quite difference.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 15, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 15, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 17, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Dec 24, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/84", "issue_status": " Closed\n", "issue_list": [{"user_name": "lenonMax", "datetime": "Mar 8, 2021", "body": "\nPart of face even no real face can be recognize as humanbeings.\n\n1.Open  face detector only.\n2.Fist to the camera.\n3.Function of human.detect returns value which has face and the face.length>0.\nThe fucntion reutrns value no face if there is no face in the detection area.**EnvironmentLooking forward to ur reply.\nTks.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 8, 2021", "body": "how it works is that detector prepares list of candidates for face boxes and then mesh actually discards invalid entries and also stabilizes facial bounding boxes\nif mesh is disabled, output of detector only is not reliablecan you retry with mesh enabled,\nbut instead of just looking at , check the values fori'll update docs to reflect thisbtw, version  is a bit old, there have been several improvements since.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "Thank you for reply.\nThe resaon I set the mesh disabled is in order to save some flow.\nI 'll try it now.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "So,I found my fist scored 0.8330078125 and sometimes my face scores below that.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "And I found the type of human in node_modules@vladmandic\\human\\dist change to typescript.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "That's the  value? Because  (when mesh is enabled) should be far lower for the fist :)Yes, I've switched the project source to TS. But the stuff in /dist is always compiled JS.Let me know if you have any other questions...", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "That's just result.face.confidence then disable the mesh. And I also got the faceConfidence and boxConfidence which is high score when I use my fist to the camera.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": " is just a pointer to  if mesh is disabled or to  if mesh is enabled.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "I found a way around this problem by double-checking the returned data, but it doesn't work on Firefox and will report an error.\nthe error is :", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "my code,photo is the base64 of current photo.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "Yes. but none face is still get high score.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "Can you post values for  AND ?That's a separate issue and it's due to fact that image is not ready at the time when you call .\nBasically, you assign  and then call  immediately after, but browser may take some time to actually process the src you just assignedWhat you need is to wait for image to actually get loaded before calling  - something like:", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "or if you have image data in base64 already, you can skip image DOM element completely, something like:", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "Oh.I see. That's my fault.I didn't consider that.lol. Thank you so much.\nThe value of faceConfidence maybe around 0.7.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "And does it drop after a while?I just noticed that you have  in your config meaning actual face detection is skipped for some frames and instead it returns interpolated values (for performance reasons)When testing, it's best to force full detection each frame by setting ", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "I hope to recognize faces more accurately even if the mesh is disabled.The reason why I disable it is that it spends more time to load. The user experience is not particularly good", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "And the browser will caton before runing the scripts.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "Sorry i didn't notice it.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "if you only need face detection and nothing else, you can try alternative model\ndefault model is BlazeFace, but that only works when combined with FaceMesh\nbut there is also FaceBoxes which is a standalone detector - exactly for the reason that BlazeFace is not that good when running as standalone modelI'm still looking for a single good algorithm for face detection that can replace both.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "I will do as u say.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "Regarding load time - is it actual  or  that is not good for user experience?\nYou can completely skip initialization time if you set  (instead of default  as  takes time to initialize GL shaders, but WASM has no such thing). Once initialized,  is faster than , but for just face detection that wouldn't matter.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "To summarize,", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 9, 2021", "body": "I will do it. It's so cool to discuss with you. I like the repo which is wonderful.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 9, 2021", "body": "I just published an update:", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 10, 2021", "body": "Thanks a lot.I will try it now.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 10, 2021", "body": "You can skip testing faceboxes, I've decided to remove it and focus on improving primary model - blazeface. Everything else stands.Also note that if you have a face in the frame with high score (e.g. 0.9) and you cover the face with the hand, but corner of the face is still visible, you may get one false positive with low score (e.g. 0.1) for the hand, but also another positive hit with mid score for partial face (e.g. 0.5).that is actually desired behavior, so when reporting false positives you need to check actual visualization what is it reported on.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 10, 2021", "body": "Okay.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 10, 2021", "body": "\nMy fist was recognized as a face.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 10, 2021", "body": "With mesh disabled?\nWhat's the boxConfidence?How about with mesh enabled? And what's the faceConfidence then?", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 10, 2021", "body": "It works well when mesh is enable But I disabled the mesh and the value of confidence is 0.5xxx.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 10, 2021", "body": "Glad it works with mesh.And is the performance with mesh when using WASM backend acceptable?And even without mesh, the score drops, so setting minimum score to something like 0.75 would solve both cases.Anyhow, I'll take another look to tweak scoring of model without mesh as it does seem a bit high.", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 10, 2021", "body": "It is faster than before when i used WASM and with mesh enabled.I guess u have solved my problem already.\nThanks！(:", "type": "commented", "related_issue": null}, {"user_name": "MuriloRS", "datetime": "Apr 13, 2021", "body": "Where can I find this faceboxes model?", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Apr 13, 2021", "body": "I wrote a month ago:If you really want to check it out, you can clone the repository and go back to version 0.9.x ( model was removed with 1.0 release). alternative, old versions are archived on , so you can just install a 0.9.x version of  using .", "type": "commented", "related_issue": null}, {"user_name": "lenonMax", "datetime": "Mar 8, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 8, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Mar 10, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/vladmandic/human/issues/38", "issue_status": " Closed\n", "issue_list": [{"user_name": "Jimmysh", "datetime": "Nov 23, 2020", "body": "When multiple people's face gesture. The information can't be matched to real one.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 23, 2020", "body": "can you elaborate - why not? each face returns separate face embedding feature vector, so it's just a question of having a loop to go through all detected faces.i've updated embedding notes: ", "type": "commented", "related_issue": null}, {"user_name": "Jimmysh", "datetime": "Nov 24, 2020", "body": "result.gesture.face data can't match with face.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 24, 2020", "body": "ahh, my bad, i was talking about embedding.\nyes, gestures currently mashes everything together - and it's not only about face.i need to redesign that part.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 24, 2020", "body": "Implemented via Gesture is now returning a different format, it's an array of objects which also includes index of a person that gesture belongs to.Documentation and demo have also been updated.Example output:Please confirm before this case can be closed.", "type": "commented", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 25, 2020", "body": "closing as resolved.", "type": "commented", "related_issue": null}, {"user_name": "Jimmysh", "datetime": "Nov 23, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 23, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 24, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 24, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 24, 2020", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "vladmandic", "datetime": "Nov 25, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/312", "issue_status": " Closed\n", "issue_list": [{"user_name": "Sivajstme", "datetime": "Apr 18, 2018", "body": "can we add a google cardboard view camera like shown in the below image to the google vr view using three.js\n", "type": "commented", "related_issue": null}, {"user_name": "Sivajstme", "datetime": "Apr 18, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/272", "issue_status": " Closed\n", "issue_list": [{"user_name": "Emanouel41", "datetime": "Nov 8, 2017", "body": "I know this one has been posted before but none of it helped me and none of it seemed to solve my issues so i would love to have your help.So here is the case:I am trying to run a vr video throught html file\n\nthe file is the classic congo video but i changed it and added another injected file that was shot with a 360 camera.Since i do not have my own domain i try to run it with Xamp (not sure if CORES properly added but i am having a hard time fixing it).*\nSorry its a png but i am not sure how to post the code here since parts of it are being \"deleted\" for some reasonError:\n", "type": "commented", "related_issue": null}, {"user_name": "Emanouel41", "datetime": "Nov 9, 2017", "body": "never mind i solved it my self", "type": "commented", "related_issue": null}, {"user_name": "bmobio", "datetime": "Sep 24, 2018", "body": "Hello,\nI'm facing the exactly the same problem. Can you tell me how you fixed it ? Thanks.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Nov 20, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/233", "issue_status": " Closed\n", "issue_list": [{"user_name": "BrendanCarlin", "datetime": "Jul 20, 2017", "body": "I tried adjusting the color properties of hotspots in hotspot-renderer.js, but my project still displays with the default hex values.  I saw on a closed 2016 issue that hotspots were not customizable, but I wanted to check in and confirm that this was still the case.Any assistance is appreciated.  Thanks", "type": "commented", "related_issue": null}, {"user_name": "aerialglasgow", "datetime": "Jul 20, 2017", "body": "Hi BrendanI was able to alter the hotspot colors here// Constants for the focus/blur animation.\nvar NORMAL_SCALE = new THREE.Vector3(1, 1, 1); // AERIAL\nvar FOCUS_SCALE = new THREE.Vector3(1.5, 1.5, 1.5);\nvar FOCUS_DURATION = 1000;// Constants for the active/inactive animation.\nvar INACTIVE_COLOR = new THREE.Color(0x00ABCE); // AERIAL ORIG (1, 1, 1)\nvar HOTSPOT_COLOR_NORMAL = 0x00ABCE;\nvar HOTSPOT_COLOR_ARROW = 0x54A719;\nvar HOTSPOT_COLOR_OUTER = 0xffffff;\nvar ACTIVE_COLOR = new THREE.Color(0.8, 0, 0); // AERIAL ORIG (0.8, 0, 0);\nvar ACTIVE_DURATION = 100;\nvar FOCUSON_ACTIVE_DURATION = 1000;\nvar FOCUSOFF_ACTIVE_DURATION = 200;// AERIAL Amendments\nvar HOTSPOT_INNER_RADIUS = 32;\nvar HOTSPOT_INNER_RADIUS_FACTOR = 0.6;\nvar STANDARD_DISTANCE_FROM_CAMERA = 4;// Constants for opacity.\nvar MAX_INNER_OPACITY = 0.8;\nvar MAX_OUTER_OPACITY = 0.6;\nvar FADE_START_ANGLE_DEG = 35;\nvar FADE_END_ANGLE_DEG = 60;You should be able to search the code library for these entries. Hope that helps.", "type": "commented", "related_issue": null}, {"user_name": "BrendanCarlin", "datetime": "Jul 20, 2017", "body": "Thanks, Aerial.  That's where I'm making my adjustments, but I'm not seeing it update in my project.  I tested again this morning and it still didn't work.  Additionally, I adjusted the color properties on  and  within  but that also didn't work.After adjusting the source code on hotspot-render.js, do I need to recompile the API using build?  I assumed having the entire project directory on the server would be sufficient to reflect those updates when changes were made within the src folder.", "type": "commented", "related_issue": null}, {"user_name": "aerialglasgow", "datetime": "Jul 20, 2017", "body": "Ah yeah, I think you would have to recompile the API, your changes were probably overwritten", "type": "commented", "related_issue": null}, {"user_name": "BrendanCarlin", "datetime": "Jul 21, 2017", "body": "UPDATE:  It's working now.  I'm not sure why it wasn't before, but I'm good.  Thanks again.", "type": "commented", "related_issue": null}, {"user_name": "BrendanCarlin", "datetime": "Jul 21, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "BrendanCarlin", "datetime": "Jul 21, 2017", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "BrendanCarlin", "datetime": "Jul 24, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/167", "issue_status": " Closed\n", "issue_list": [{"user_name": "georgedumontier", "datetime": "Jun 3, 2017", "body": "Default yaw works great on the first scene I load, but when the user clicks a hot spot and loads a new scene via vrView.setContent(), the default yaw seems to load relative to where the user the was looking when they pressed the hot spot.So if the user is looking really far left and loads a new vr View. The new image's default yaw will load too far to the left. I'd like the new image to load in the same direction no matter where the user is looking when they press the hot spot.Anyone else having this issue? Sorry if this is unclear", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 4, 2017", "body": "When a new scene is set up it does not take into account the current rotation of the camera.You can subtract the current camera yaw ( y rotation ) here:...", "type": "commented", "related_issue": null}, {"user_name": "georgedumontier", "datetime": "Jun 4, 2017", "body": "Thanks for the response. Unfortunately I don't think it's working for me.I changed line 202 in world-renderer.js to\nMaybe it's an order of operations thing? Do I need some parenthesis?", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 4, 2017", "body": "are you building it? ", "type": "commented", "related_issue": null}, {"user_name": "georgedumontier", "datetime": "Jun 4, 2017", "body": "Nope, sorry I'm kind of a newbie. I don't usually use npm or any package manager.Can I not just edit the js file in my src folder?", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 4, 2017", "body": "For a quick fix you can edit embed.js in /build\nsearch for ", "type": "commented", "related_issue": null}, {"user_name": "georgedumontier", "datetime": "Jun 4, 2017", "body": "You sir, are a saint.Thank you very much. Works perfectly.", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 4, 2017", "body": "you're welcome thanks :)", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 4, 2017", "body": "For mobile this needs to be changed.  For now use this to cancel it on mobile", "type": "commented", "related_issue": null}, {"user_name": "gol4er2219", "datetime": "Jun 27, 2017", "body": "Were you able to find a fix for mobile? I would really like it to be consistent between desktop and mobile.\nThanks!", "type": "commented", "related_issue": null}, {"user_name": "georgedumontier", "datetime": "Jun 27, 2017", "body": "No fix on mobile yet. I don't think it takes into account the gyroscope. But  is the mastermind, he might have some ideas.", "type": "commented", "related_issue": null}, {"user_name": "gol4er2219", "datetime": "Jun 28, 2017", "body": "OK thank you! I'll keep looking into it and see if I find anything I can use.", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jun 29, 2017", "body": " lol thanks.  I'll see what i can do...", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jul 1, 2017", "body": "Try this out:", "type": "commented", "related_issue": null}, {"user_name": "gol4er2219", "datetime": "Jul 1, 2017", "body": "You're so awesome  !! It looks like those changes worked! Thank you  as well. Thank you both.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Jul 11, 2017", "body": "Tommytee added this fix to master @ ", "type": "commented", "related_issue": null}, {"user_name": "georgedumontier", "datetime": "Jun 4, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "tommytee", "datetime": "Jul 1, 2017", "body": [], "type": "pull", "related_issue": "#208"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/145", "issue_status": " Closed\n", "issue_list": [{"user_name": "transpirman", "datetime": "Apr 20, 2017", "body": "Hi,I tried the hotspot example in VR mode (Google Chrome on S7 Edge)When I gaze at any hotspot, the white circle grows but I do not see any recticle.I would also like to activate a short timer so gazing at hotspot would trigger a click after a few second (like a \"fuse button\" as described here: ).\nIs there a way to do that ?Do someone have code examples of how to capture these events like Hotspot onFocus, etc...Thanks !", "type": "commented", "related_issue": null}, {"user_name": "weddingdj", "datetime": "Apr 20, 2017", "body": "I had the same issue using Google Chrome 57 on a Nexus 5. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "willian-sisinfo", "datetime": "Apr 25, 2017", "body": "Same thing using latest version on iOS. The docs are not clear how we should proceed when VR mode is true. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 26, 2017", "body": " its easy, you have function HotspotRenderer.prototype.focus_ ,   there you can put some setTimeout and add logic if it focus more than 1 sec or whatever do this  if someone focus and than blur just remove it. Should work.", "type": "commented", "related_issue": null}, {"user_name": "transpirman", "datetime": "Apr 26, 2017", "body": "Thank you for the code.Actually I started building on that already, but encountered a problem for which I found a workaround. I will try to post code asap.The problem : when hotspots are created, the 'update' function in renderer emits a focus event, and no blur event.\nThe workaround: initialize a FocusEnabled status to false, and delay a true assignment", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 26, 2017", "body": " it will help to you this.hotspots[id].show check it before settimeout in focus", "type": "commented", "related_issue": null}, {"user_name": "transpirman", "datetime": "Apr 26, 2017", "body": " Oh thank you for the tip ! I did not think of it.What I did so far in hotspot_renderer:\n\nvar HoverTimer; // \nvar HoverReadyTimer; // \nvar HoverReady = false; //  = function(pitch, yaw, radius, distance, id, image, is_stereo) {\nclearTimeout(HoverReadyTimer); HoverReady = false;\nHoverReadyTimer = setTimeout(function() { HoverReady = true; }, 500);\t// ready for real blur event\n...\n = function(camera)\nif (isIntersected && !this.selectedHotspots[id] && ) {\nthis.emit('focus', id); this.focus_(id);\n}\n = function(id) {\nvar hotspot = this.hotspots[id];// Tween scale of hotspot.\nthis.tween = new TWEEN.Tween(hotspot.scale).to(FOCUS_SCALE, FOCUS_DURATION)\n.easing(TWEEN.Easing.Quadratic.InOut)\n.start();\n// color change\nvar inner = hotspot.getObjectByName('inner');\nthis.tween = new TWEEN.Tween(inner.material.color).to(ACTIVE_COLOR, ACTIVE_DURATION)\n.start();// Virtual Click (todo: real fuse button)\nif (HoverReady == true) { var that = this; HoverTimer = setTimeout(function() { that.emit('click', id); that.up_(id); }, 2000); }\n};I will try to create a proper 'pull request' once I get how to fully use GitHub :-)Now, any idea about the reticle not showing up ?", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 26, 2017", "body": " i found the reason, just need change this in package.json and rebuild", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "May 8, 2017", "body": "for the reticle, uncomment ", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "May 9, 2017", "body": "The false calling of focus that happens when a hotspot is added can be stopped by moving this line  two lines down so it is under \"this.effect.render...\"This fix is also needed: Then, adding to the code from fix2015: ( hotspot-renderer.js )These changes (and the reticle enabled) are in the branch named \"gaze\" here. (will do a pr soon)", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 29, 2017", "body": [], "type": "issue", "related_issue": "#153"}, {"user_name": "tommytee", "datetime": "May 19, 2017", "body": [], "type": "issue", "related_issue": "#135"}, {"user_name": "tommytee", "datetime": "Jul 1, 2017", "body": [], "type": "pull", "related_issue": "#210"}, {"user_name": "tommytee", "datetime": "Jul 11, 2017", "body": [], "type": "pull", "related_issue": "#223"}, {"user_name": "lincolnfrog", "datetime": "Jul 12, 2017", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/8", "issue_status": " Closed\n", "issue_list": [{"user_name": "PoC22", "datetime": "Apr 23, 2016", "body": "May be just me - but when I use the  variable it has different results depending on whether I'm viewing on mobile (iPhone 6 Plus, Safari) or desktop (iMac 2015, Safari) - both latest versions as of writing.I can also confirm this is the case with the latest version of Google Chrome on iPhone. - initial view as expected when using the  variable, e.g. =90 - does what it says on the tin.\n - ignores  completely.Images used taken with  camera.Cheers.", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Dec 1, 2016", "body": "This should be fixed, please try again.", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Dec 1, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/131", "issue_status": " Closed\n", "issue_list": [{"user_name": "fix2015", "datetime": "Mar 15, 2017", "body": "Hi guys, i found bug, when i try change rotation it not change, it work on with thiscamera.parent.\nhere what i do\n\nbut if i do that\n\nbut in this way center will be change and than i can't find correct camera.rotation(xyz)", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "Mar 27, 2017", "body": "The camera rotation is controlled by device orientation (gyroscope, accelerometer) or a mouse (on desktop) using the webvr-polyfill or a webvr capable browser.The camera parent ( cameraDummy ) is what you probably want to rotate.If you really want to control the camera directly (and correctly know where it is positioned when using the webvr-polyfill on the desktop) you will need to update camera.quaternion ( or camera.rotation ), vrDisplay.phi_ and vrDisplay.theta_.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Jun 15, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/2", "issue_status": " Closed\n", "issue_list": [{"user_name": "wahengchang", "datetime": "Apr 14, 2016", "body": "Is it possible to read 360 (not 720) images taken by cardboard APPIt look like only both mono and stereo Spherical panorama(not cubic format) images are supported for new , is 360 (horizontal  only panorama) planed to support ?", "type": "commented", "related_issue": null}, {"user_name": "gbentley", "datetime": "May 1, 2016", "body": "I'd also like to see this integrated as a query string parameter, so that standard Cardboard Camera photos can be displayed correctly.", "type": "commented", "related_issue": null}, {"user_name": "marklundin", "datetime": "Jun 11, 2016", "body": "What's the format of the Cardboard Camera photos? Are they equirectangular?", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Jun 12, 2016", "body": "Yes. Also see this Cardboard Camera => over-under converter, which we recently released: ", "type": "commented", "related_issue": null}, {"user_name": "marklundin", "datetime": "Jun 12, 2016", "body": "Awesome!", "type": "commented", "related_issue": null}, {"user_name": "gbentley", "datetime": "Jun 12, 2016", "body": "There is an issue with the convertor though - the resulting image on all my\nconversions ends with a black space on the right of the top image,\ncompressing the top image, and making the stereoscopy a little useless.I was delighted when it was released, but this issue hasn't been fixed (and\nvery few channels to provide feedback on it).On Mon, 13 Jun 2016 5:03 am Boris Smus  wrote:", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Jun 13, 2016", "body": "Please attach an example of the cardboard camera input and converter output\nto this thread.On Sun, Jun 12, 2016 at 12:26 PM Geoff Bentley \nwrote:", "type": "commented", "related_issue": null}, {"user_name": "marklundin", "datetime": "Jun 13, 2016", "body": "Any change this could be opensourced ? The OdsConverter looks like it would be really handy", "type": "commented", "related_issue": null}, {"user_name": "gbentley", "datetime": "Jun 13, 2016", "body": "Images below.  What's really strange is that the converter gives back a PNG (check the file format), but calls it a JPG.\n There is a JS-based convertor you can use at  - it doesn't blur it, but it works.  Also see  . All discussed at Here's the result from the JS-based convertor:\n", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Dec 1, 2016", "body": "Ok, if there are issues with the cardboard camera converter, please file a new issue.", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Dec 1, 2016", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "zdrawku", "datetime": "Feb 24, 2017", "body": [], "type": "issue", "related_issue": "#110"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/120", "issue_status": " Closed\n", "issue_list": [{"user_name": "bobvanluijt", "datetime": "Feb 7, 2017", "body": "Any advice on how to convert a regular image into an equirectangular image? I've created a 3d image that I would like to convert into an equirectangular image for vrview.", "type": "commented", "related_issue": null}, {"user_name": "yvan-sraka", "datetime": "Feb 25, 2017", "body": "Hi, what is the format of your image?\nPerhaps the the  could do the trick !", "type": "commented", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "Jun 26, 2017", "body": "These \"how to\" questions are better posted on stack overflow.", "type": "commented", "related_issue": null}, {"user_name": "bobvanluijt", "datetime": "Feb 7, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "Jun 26, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/839", "issue_status": " Closed\n", "issue_list": [{"user_name": "stolarek", "datetime": "Sep 24, 2021", "body": "Hello everybody,\nI use the nice library in my system with multiple cameras. It works just fine. The system takes pictures once a day via the connected cameras. Now to my problem: I want to display the live stream of individual cameras on my settings page and the user should be able to switch between the images of the cameras and align the cameras. I use currently for every webcam one \"WebcamStreamer\" class with port 8080 + n and switch the livestream with them. After a few minutes of inactivity, the cameras should be switched off. I use WebcamStreamer stop(). Is it possibele to restart an existing Streamer or schould a new object be created?\nI tried different combinations but at some point I can't see the livestream.\nWhat is the best way? Is it possible to switch different cameras with just one \"WebcamStreamer\"?\nI would be happy if there are suggestions here.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "Hi  ,so sorry to tell you that restart webcam streamer, webcamera API must reallocate stream reading postion again. but this is not implemented, please see comments:/**", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/71", "issue_status": " Closed\n", "issue_list": [{"user_name": "libbybaldwin", "datetime": "Nov 2, 2016", "body": "When following the link to the converter tool and instructions on the page below, I can't convert a file to use in an embedded VR View. The text states \"To use these images with VR view, download the image, and then use our conversion tool to create a stereo 360 image that meets our image specifications.\"I have tried both dragging and manually opening my downloaded Cardboard Camera app-captured file to the converter page. The page/app appears to run for about 15 minutes (converts locally or in cloud?), hijacks my Chrome/Ubuntu, then stops and nothing happens. I do not get directed to the converted file or an error message.The image I used works perfectly in the Cardboard app with a Cardboard viewer. Captured and viewed on Nexus 5X, Android 7.0. Downloaded to desktop computer and used in converter page. After failed conversion, found error in browser: \"Uncaught TypeError: canvas.toBlob is not a function\"", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Nov 2, 2016", "body": "Thanks Libby, which browser are you using? Could you paste your user agent from ?", "type": "commented", "related_issue": null}, {"user_name": "libbybaldwin", "datetime": "Nov 2, 2016", "body": "Hi Boris.Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Nov 3, 2016", "body": "Well, looks like your browser doesn't have canvas.toBlob, but I think Chrome has had this for a while.Just to clarify, this is Chrome on Linux, right? Is it Chromium? Are you sure that you're up to date? M48 is quite old.", "type": "commented", "related_issue": null}, {"user_name": "libbybaldwin", "datetime": "Nov 3, 2016", "body": "Yes, Google Chrome. It is supposed to inform me to update or just auto update, but I noticed it doesn't or hasn't. Update bars are white. Ubuntu 14.04 - yes I am due to update OS, Chrome and also hardware in fact (need new laptop). Didn't think it was all SO old as to fail at something like this.", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Nov 3, 2016", "body": "Yeah, sorry. We rely on that function call to make image conversion more\nperformant. Try upgrading and let me know if it works?On Thu, Nov 3, 2016 at 11:30 AM Libby Baldwin \nwrote:", "type": "commented", "related_issue": null}, {"user_name": "libbybaldwin", "datetime": "Nov 3, 2016", "body": "Yes, I will try upgrading, starting with Chrome only.Under Image specifications the page says \"Mono images should be 2:1 aspect ratio (e.g. 4096 x 2048)\".\nUnder Real World Capture, Cardboard Camera App section it implies (as I read it) using default image from Cardboard Camera App (anyways no options or settings besides audio on/off) as input to converter.\nTwo of my .jpg images from the app are 9308x1641 and 9174x1604. What am I missing? Should I scale the images to nearest power of two, or should the converter accept any size? (sorry if this is OT, I can move to another issue, perhaps relating to documentation)", "type": "commented", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "Nov 3, 2016", "body": "Once you run the CC images through the converter they should meet the VR\nView spec.On Thu, Nov 3, 2016 at 11:46 AM, Libby Baldwin \nwrote:", "type": "commented", "related_issue": null}, {"user_name": "libbybaldwin", "datetime": "Nov 3, 2016", "body": "Strictly concerning this Issue: Upgraded Google Chrome  48 and I am able to convert. Fix is to upgrade browser. I am running version 54 now.In my case, I was running the 32bit version (for unknown reasons) of Google Chrome version 48, which is no longer supported. Updates stopped coming earlier this year with no warning from browser itself. As of today I am running Ubuntu 14.04 on 64 bit hw. I followed instructions to update Google Chrome AND force [arch=amd64] for this and future Google Chrome updates. Scroll down to the answer from  from Mar 6 (2016) to see update instructions: Thank you! I have many holes to fill in my knowledge of VR, 360, OU, etc terminology, which explains my wonky question. Working on it.. :)", "type": "commented", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "Nov 3, 2016", "body": "Glad you got this fixed!", "type": "commented", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "Nov 3, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/39", "issue_status": " Closed\n", "issue_list": [{"user_name": "03difoha", "datetime": "Aug 11, 2016", "body": "I have tried to convert the example 'coral' image and a panoramic image I took on the cardboard camera app on the cardboard camera converter site and everytime it gives me the error", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Aug 11, 2016", "body": "Please attach the image you're trying to convert.On Thu, Aug 11, 2016 at 7:45 AM 03difoha  wrote:", "type": "commented", "related_issue": null}, {"user_name": "03difoha", "datetime": "Aug 11, 2016", "body": "", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Aug 11, 2016", "body": "Can't reproduce. Mine converted without a hitch.", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Dec 1, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/140", "issue_status": " Closed\n", "issue_list": [{"user_name": "theunreal", "datetime": "Mar 31, 2017", "body": "Currently, the click event returns an  which is always  when the user didn't click on an hotpost.If the click event would return the  and  of the clicked point, it could be really helpful.", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Mar 31, 2017", "body": "You can write function that will return worldRenderer.camera.rotation and than do that -\nYaw: worldRenderer.camera.rotation.y * 180 / Math.PI,\nPitch: worldRenderer.camera.rotation.x * 180 / Math.PI,", "type": "commented", "related_issue": null}, {"user_name": "theunreal", "datetime": "Apr 1, 2017", "body": "Hey  , How I can actually access the worldRenderer?", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 3, 2017", "body": "try merge this commit  , and you will have methold  vrView.getPosition(); that return to you YAW and PITCH", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 3, 2017", "body": "", "type": "commented", "related_issue": null}, {"user_name": "theunreal", "datetime": "Apr 7, 2017", "body": "Looking awesome  !\nBut I can't figure out how to import your unminified version in my Angular 2 project and test it.\nPreviously I Just included the following js file in my index.html and declared the VRView object:\nI tried to include your vrview.js but I got  error in the iframe", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 7, 2017", "body": "you need clone project and write command\nnpm run watch - its for  unminified version", "type": "commented", "related_issue": null}, {"user_name": "willian-sisinfo", "datetime": "Apr 25, 2017", "body": "Hi . This seems awesome... just spent the weekend calculating and transforming the panorama pixels into coordinates and now I see your branch.I've tried what you told (cloning and running npm), but I'm not being able to see the hotspot I created... Is there any change how they are created or displayed?Thank you.", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 26, 2017", "body": " is shouldn't hide your hotspot, maybe its something else, can you show your script maybe i will find pb", "type": "commented", "related_issue": null}, {"user_name": "willian-sisinfo", "datetime": "Apr 26, 2017", "body": "Sure. Actually is pretty much the aquarium example provided by google, only difference is JSON structure.`tp = {\nstartPreview: function (tourId) {\n$.ajax({\nurl: '/dashboard/tour-virtual/preview',\ntype: 'POST',\ndata: {\n'tour' : tourId\n},\nbeforeSend: function () {\n$.LoadingOverlay('show');\n},\n}).success(function (data) {\n$('#modalTour').modal('show');//            vrView.on('getposition', function(e) {\n//                   console.info(e);\n//            });}`", "type": "commented", "related_issue": null}, {"user_name": "fix2015", "datetime": "Apr 26, 2017", "body": " i fixed, now try this  or just clone my repo  , and there you will see hotspot and on /examples/hotspots/index.html you will find, how get ", "type": "commented", "related_issue": null}, {"user_name": "willian-sisinfo", "datetime": "Apr 26, 2017", "body": " That's great! I'll test it later on. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "thriskel", "datetime": "May 22, 2017", "body": "apparently it is getting the center of the view pitch and yaw, but how do I make it so it gives the pitch and yaw where the click was located?", "type": "commented", "related_issue": null}, {"user_name": "nathanmartz", "datetime": "May 5, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/837", "issue_status": " Closed\n", "issue_list": [{"user_name": "GivouDev", "datetime": "Sep 6, 2021", "body": "Hello Guys,the API doesnt detect any Webcams on my Linux Machine.The Code i use is not that important, because...its not doing really something but here:\n`package org.givou.ai;import com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;import javax.swing.*;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;public class Main {\nstatic JFrame ui;}\n`I have installed all Libraries and slf4j-simple extra, because i got this StaticLoggerBinder error.\nNow when i try to fire up my project it shows me this:\n[main] INFO com.github.sarxos.webcam.Webcam - WebcamDefaultDriver capture driver will be used\n[main] WARN com.github.sarxos.webcam.Webcam - No webcam has been detected!And Webcam.getDiscoveryService().getWebcams() is also showing 0 webcams.\nhwinfo --usb finds my webcam aswell, and i can access it with mplayer.\nThanks to anybody who tries to help me :)", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "hi ,\nthere indeed some cases camera can not be detect. The root cause is that default built-in camera driver depends on bridj which is a JNA solution from java to native code. Unfornaturely, bridj community stopped update for a long time so that can not work for new OS.\nWhat you can do is change other driver.Alex", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/834", "issue_status": " Closed\n", "issue_list": [{"user_name": "cameronboddie", "datetime": "Aug 13, 2021", "body": "Hello I have a demo where I need to use the webcam to capture a QR. Simultaneously, I will be on a zoom call with my PM. Is there a way to make this work? It works while not on Zoom but doesn't work even if my camera is off. I haven't been able to properly troubleshoot the issue.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Aug 13, 2021", "body": " underlay camera hardware is an exclusive resource, once you are in Zoom meeting, the camera is taken by Zoom. You can not open it again from another process. at JVM high level, it is hard to debug.", "type": "commented", "related_issue": null}, {"user_name": "cameronboddie", "datetime": "Aug 15, 2021", "body": "Thank you for the quick response. I appreciate it, I'll look into some work arounds for the demo.", "type": "commented", "related_issue": null}, {"user_name": "cameronboddie", "datetime": "Aug 15, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/833", "issue_status": " Closed\n", "issue_list": [{"user_name": "brianmichalk1", "datetime": "Aug 9, 2021", "body": "Sometimes my USB glitches while in a getImage() call, which hangs my program.  I've put this into a separate thread and use a semaphore timeout to indicate when getImage() takes too long.  Killing the thread sometimes works but doesn't seem to be the correct approach.  Should I use Webcam.shutdown?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Aug 13, 2021", "body": "Hi  ,Java is working at high level, for webcam-capture, it is a lighweight of wrapping of camera accessing via JNA. so device glitch is not handled well well though we introduced WebcamDiscoveryListener.\nWhen you open camera without reset new Driver explicitly, default is:\n\nit is thread unsafe and blocking getImage().My suggestion is:\nyou can attach a WebcamDiscoveryListener to watch camera devices though it is not fully guaranteed.\nmanage the getImage() thread gracefully with timeout and WebcamDiscoveryListener", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/832", "issue_status": " Closed\n", "issue_list": [{"user_name": "chottu33", "datetime": "Jul 12, 2021", "body": "Hellowhile the recording is on, webcam is not getting connected again, if the camera is disconnected and connected back.\nObservation: Discovery Service of the camera is not running\nOS: Ubuntu 20, Centos 8\njars version - webcam-capture-0.3.13-20200330.202351-7.jar, bridj-0.7-20140918-2.jar\njava version: 1.8.0", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Jul 24, 2021", "body": "could u please provider hardwaew info?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/829", "issue_status": " Closed\n", "issue_list": [{"user_name": "kashis7870", "datetime": "Jun 12, 2021", "body": "Please help me to fix this problem, i am getting a black screen at the given location and also getting few errors.\nPlease find attached.", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": "please check your camera, probably VGA is not proper for your hardware", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Sep 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/828", "issue_status": " Closed\n", "issue_list": [{"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "I tried to run this codeAnd  got this error:Please help me to fix this error.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": "The SLF4J logging framework JAR is missing in your classpath (with other required dependencies as well, e.g. BridJ). See  wiki page for more details on how you can fix it. Please also check the  section on the main page for the ZIP. In short - you need to have Webcam Capture JAR in the classpath, but also SLF4J and BridJ (or JARs for other capture framework if you are using something non-default, e.g. MJPEG).", "type": "commented", "related_issue": null}, {"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "Thanks for replying", "type": "commented", "related_issue": null}, {"user_name": "Maruti-Nandan", "datetime": "Jun 12, 2021", "body": "I added that but now the picture that is captured is totally black.\nSo, How should I fix that?\n", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/830", "issue_status": " Closed\n", "issue_list": [{"user_name": "shichiye", "datetime": "Jun 15, 2021", "body": "At present, I know that the camera can be obtained by a given name, but I cannot confirm which camera it is. I want to know whether the corresponding USB camera can be obtained by the USB camera similar to the hardware id. I don’t seem to find the corresponding api.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2021", "body": "There is no such API in the Webcam Capture project. What we have here is a very basic abstraction over the UVC device. There is no API to get the USB information. Even if we go into the lower level and dive deep into the native C++ code, there is no such thing as well. The operating system abstracts the device and completely decouples it from the USB transport.One could in theory use some 3rd party library like  to scan the ports and correlate UVC data from Webcam Capture with the hardware device class present in the USB port, but I never tried this and cannot even tell if such thing is possible. This is just a brief idea that came into my mind.", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Jun 16, 2021", "body": "I'm working on publishing as open source a native driver capable of providing actual device ids and very good performance. For the moment it will work on Windows only. Maybe tomorrow it's ready, I will reply back.We use the driver in a production app and works really well.", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Jun 17, 2021", "body": "Hi,I just published the driver here: Please let me know if you try it successfully or you have any problem.To get the device id you will need to cast  to ", "type": "commented", "related_issue": null}, {"user_name": "shichiye", "datetime": "Jun 18, 2021", "body": "  Thank you both for your answers.\nI will try it, and then I will reply to you ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Could be possible to know the id device of the USB with the relative istance ?USB\\VID_0C45&PID_6366&MI_00\\6&183AF011&0&0000", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": "Yes, you can with  and casting  to ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Could you help me with an example in pure Java... I tried to follow the tutorial but after loading the dll no output of which webcam is connected to the computer. Without loading dll everything is working well. i tried to contact you on twitter also.\n", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "Also dll are loaded in the project Netbeans as in photo.\n\nI also tried to put in same place where opencv read it's own dll. OpenCv could work well without any issue but this driver won't work....", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": " This library tries to load its DLLs from the natives folder relative to where the program is executed. Adding to netbeans or as jar won't do anything.Please make sure to run your program in a folder where natives is located. Also I recommend setting up a slf4j backend such as logback for viewing logging messages.", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "It works. Was a matter of libraries... I'm not using maven project... Standalone application developing here.\nThanks for the support", "type": "commented", "related_issue": null}, {"user_name": "eduramiba", "datetime": "Dec 10, 2021", "body": " Great! ", "type": "commented", "related_issue": null}, {"user_name": "pacioc193", "datetime": "Dec 10, 2021", "body": "I think the best ways to solve this issue in the future... release a zip file with all the dependency ahah", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "shichiye", "datetime": "May 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/827", "issue_status": " Closed\n", "issue_list": [{"user_name": "mayurb90", "datetime": "Jun 7, 2021", "body": "Hi,\nI am looking for a way to get HardwareId for the webcam using Saroxs library. I can get the camera name but looking for VIDPID of it.\nAny help would be much appreciated .Thanks", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": "Hi ,Sorry to disappoint you. Unfortunately, this is not possible. The Webcam Capture API get the webcam name and the webcam ID (nothing fancy, 0 for the 1st device, 1 for the 2nd device, 2 for the 3rd device, etc) from the operating system. These are the only parameters the underlying driver can access from Java. On Linux there is no ID at all, only the camera name, and not always. Sometimes it's only the device file from the  directory.", "type": "commented", "related_issue": null}, {"user_name": "mayurb90", "datetime": "Jun 12, 2021", "body": "Hi ,Thanks for the information but do you have any suggestion on what could be done in the scenario where system is connected with multiple webcams but we just want to use one for the feed. Also, when you said webcam name from the OS, do we know how OS gets this information, does it have something to do with driver?Thanks", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jun 12, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/821", "issue_status": " Closed\n", "issue_list": [{"user_name": "LuisHenriqueFA14", "datetime": "May 26, 2021", "body": "I don't use java a lot, so i don't know almost nothing about java (what that i need to compile or what is a .class file), but i'm trying to make a camera app to me, but i can't use it...I was having some troubles with package, i solved it, and when i was done, it was an error like:./com/github/sarxos/webcam/Webcam.java:20: error: package org.slf4j does not exist\nimport org.slf4j.Logger;So i started my search to solve it. I found a lot of issues about that, and i found something about put the  dir into the classpath (idk where are the classpath lol)So, i need some help with the libs, like, where to put it ?\nI need to put the  dir to my root path ?\nI need to put the  dir to my root path or the   ?\n(Please, be so specific bc i'm dumb)\nThank you, and sorry about my english ", "type": "commented", "related_issue": null}, {"user_name": "jrbgarcia29", "datetime": "May 28, 2021", "body": "you need to build your project by running \"mvn install\" in your project base dir so that dependencies will be downloaded and missing jars will be available to your local repository", "type": "commented", "related_issue": null}, {"user_name": "LuisHenriqueFA14", "datetime": "May 28, 2021", "body": "When i run  it returns an error talking .\nHow can i install it ?\nIt has another way to build my project ?\nThank you again!", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "May 31, 2021", "body": "This is not issue, please find java maven guileline by yourself", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "May 31, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/818", "issue_status": " Closed\n", "issue_list": [{"user_name": "hqwl159", "datetime": "May 6, 2021", "body": "When the device number is greater than 15, the device information cannot be obtained.In Centos7\nI can read the camera info on the path of \"/dev\"\nI use the webcam-capture codeThe output  is \"1\".But I can read  all the  camera device info when the device name ranges from \"video0\" to \"video15\".So I guess the code cannot get device information after “video15”.Is my code error ?", "type": "commented", "related_issue": null}, {"user_name": "flx5", "datetime": "May 7, 2021", "body": "That is an issue with the default driver OpenIMAJ. It only scans the device files 0 to 15. .\nYou might want to try the JavaCV driver. In that implementation every device in /dev/video* should be recognized.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": "Thank you most sincerely!!!\nThe webcam-capture can work without camera driver.  Does the webcam-capture or Linux OS  use the  OpenIMAJ as default driver?", "type": "commented", "related_issue": null}, {"user_name": "flx5", "datetime": "May 7, 2021", "body": "If you don't specify which driver should be used with setDriver, webcam-capture uses the default builtin driver which is OpenIMAJ regardless of the operation system.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": "Thanks a lot!!!\nI find  it in README.", "type": "commented", "related_issue": null}, {"user_name": "hqwl159", "datetime": "May 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/807", "issue_status": " Closed\n", "issue_list": [{"user_name": "zy5651768", "datetime": "Feb 23, 2021", "body": "I use default driver ,fps is low on ubuntu (  3-5 fps ) ,maybe change driver will good?", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 23, 2021", "body": "Hi,\nBecause pure Java preview panel and decoding is not high performance as native c, if your camera is 1080p, base one your PC performance, 3-5 fps make sense.Regarding other driver, maybe you can try JNI based driver like opencv or vlcj which are litter faster but not significant. and also you can set System properties in your launch arguments:OpenGL or XRender is enabled to get low latency, something like\nexport _JAVA_OPTIONS=-Dsun.java2d.opengl=True\nexport _JAVA_OPTIONS=-Dsun.java2d.xrender=Truebase on your hardware, maybe higher fps can be there.\nAlex", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 27, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/801", "issue_status": " Closed\n", "issue_list": [{"user_name": "schdai", "datetime": "Jan 5, 2021", "body": "Hi, I'm using video capture card to get image from some devices. There are more than one input, including  S-Video, Composite and etc.  Can webCam select input channel？ How？TKS。", "type": "commented", "related_issue": null}, {"user_name": "alexmao86", "datetime": "Feb 2, 2021", "body": "use WebcamCompositeDriver", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": "The drivers from WebcamCapture project supports only UVC devices and so will not be able to detect cameras connected via S-Video or Composite Cinch, but in general, if you implement (create) custom driver for every input you desire, then you can bundle these drivers together using the  as  suggested.Just for your information. I never found a Java project to stream video from S-Video or Composite Cinch. I guess you will need some kind of TV tuner for this. Not sure, I never worked with analog video data.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/794", "issue_status": " Closed\n", "issue_list": [{"user_name": "hatemibr", "datetime": "Nov 22, 2020", "body": "Hello there,I have been trying to find solution in all over the internet. I deeply investigated every ticket about this error but i have lost my day doing that. I am lost I NEED your help:I ' am using the last stable version of webcam (webcam-capture-0.3.12) - and I have successfully added the jar files to my program and i just tried to add this codebut I am getting a very strange Exception :any ideas what's going on here ?\nI am using Windows 10 64bit", "type": "commented", "related_issue": null}, {"user_name": "hatemibr", "datetime": "Nov 22, 2020", "body": " can I get your attention on this bug please ?", "type": "commented", "related_issue": null}, {"user_name": "Moimus", "datetime": "Nov 23, 2020", "body": "+1 I have nearly the same issue. I'm using 64 bit ubuntu on a raspberry pi 3b+ with Java 8 (64 bit too) though. While everything works fine on win 10 it just won't run on ubuntu", "type": "commented", "related_issue": null}, {"user_name": "hatemibr", "datetime": "Nov 23, 2020", "body": "You said it works fine on win10 64bit?can you please provide the steps you followed in order to let it works on win10 ?\nStep by step", "type": "commented", "related_issue": null}, {"user_name": "DanielMartensson", "datetime": "Dec 25, 2020", "body": "I have the same issue with Raspberry Pi 4B. I can't use the camera. I have open an issue.", "type": "commented", "related_issue": null}, {"user_name": "hatemibr", "datetime": "Mar 3, 2021", "body": "This issue has been fixed by moving  library to C:\\Windows\\System32.This library can be found into the documents which the author developed. (src\\com\\github\\sarxos\\webcam\\ds\\buildin\\lib\\win64)", "type": "commented", "related_issue": null}, {"user_name": "hatemibr", "datetime": "Mar 3, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/790", "issue_status": " Closed\n", "issue_list": [{"user_name": "ktrocki", "datetime": "Oct 22, 2020", "body": "Hi  ,\nI am trying to capture information about which ports my external cameras are connected to. I have a physical 6 camera array that I would like to match to my 6 camera live streamed GUI array. All 6 cameras are identical so the following code produces identical names other than the last index.  I believe they are discovered in an arbitrary order as the display order changes run to run. Is there a way to get more information about the webcam (COM Port, Physical Port etc?) so that I can match physical camera location to display location? Thanks!Output:\nUSB2.0 UVC PC Camera 0\nUSB2.0 UVC PC Camera 1\nUSB2.0 UVC PC Camera 2\nUSB2.0 UVC PC Camera 3\nUSB2.0 UVC PC Camera 4\nUSB2.0 UVC PC Camera 5", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": "This is not possible. The abstraction layer presented to us by operating system prevents us from checking to which port the UVC device is connected to.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Feb 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/771", "issue_status": " Closed\n", "issue_list": [{"user_name": "geekeritcom", "datetime": "Apr 24, 2020", "body": "When I use the application to open the camera for the first time, it always throws an exception, but it is normal to open the program for the second time and the camera afterwards.It is very strange that this exception always appears when I run the program for the first time after restarting the server, and it is always good when running afterwards, but I cannot ask the client to always start the program twice. After restarting the server, I tried The following method does not workLooking forward to your answers urgently, thanks~", "type": "commented", "related_issue": null}, {"user_name": "geekeritcom", "datetime": "Apr 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/768", "issue_status": " Closed\n", "issue_list": [{"user_name": "gwynncausing", "datetime": "Apr 14, 2020", "body": "Recently when I used Sarxos Camera with 'slf4j-api' but there was an error like thisthen it was stated here  to change it into slf4j-simple and then the next error is this'java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory'I am currently doing a personal project in java swing with a camera API", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2021", "body": "See ", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/764", "issue_status": " Closed\n", "issue_list": [{"user_name": "geekeritcom", "datetime": "Mar 23, 2020", "body": "How to perform QR code analysis while displaying camera pictures，Now I can only display pictures or QR code, but they cannot work at the same time", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Mar 23, 2020", "body": "Hi ,This is not true. You can display both image from webcam, and QR code. The problem is how you implement this :)You can for example open camera in async mode so it does not block when you call , pass it to , and use the very same  instance in a separate  to read QR code from image. Both image display and analysis will be done here in parallel.Another example would be have a custom  which reads the code from image before it's returned from , but this solution will most likely drop effective FPS since QR code analysis time will be added to the  execution time. QR code analysis and image display will be done here one after the other (and for every image)Please take a look at this basic example. Here image from webcam id displayed in parallel to the QR code analysis:This example is the embodiment of the the first idea I described above.", "type": "commented", "related_issue": null}, {"user_name": "geekeritcom", "datetime": "Mar 24, 2020", "body": "Thank you very much for responding so fast. Yesterday I also found a solution by referring to the image of the network camera. I saw your reply code today. Thank you very much. In any case, thank you for your reply and such a great tool.", "type": "commented", "related_issue": null}, {"user_name": "geekeritcom", "datetime": "Mar 26, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/744", "issue_status": " Closed\n", "issue_list": [{"user_name": "oper2000", "datetime": "Oct 15, 2019", "body": "Works on previous Mojave MacOS", "type": "commented", "related_issue": null}, {"user_name": "oper2000", "datetime": "Oct 15, 2019", "body": "duplicate ", "type": "commented", "related_issue": null}, {"user_name": "oper2000", "datetime": "Oct 15, 2019", "body": "duplicate ", "type": "commented", "related_issue": null}, {"user_name": "oper2000", "datetime": "Oct 15, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/742", "issue_status": " Closed\n", "issue_list": [{"user_name": "raphaelbn", "datetime": "Oct 4, 2019", "body": "Hi,I'm facing a problem with sarxos api. I'm using windows 10 and java 8, but the camera is black, like is not working, not capturing image.When I use windows 8 or linux, the application works fine.There is something to do?My code is below:`\npublic class App {}\n`", "type": "commented", "related_issue": null}, {"user_name": "noorball", "datetime": "Oct 7, 2019", "body": "This code works just fine for me (Windows 10, Java 8).\nYou can try to check permissions in Camera privacy settings in default Windows settings and in Vendor settings app (e.g. Lenovo Vantage etc).", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 8, 2019", "body": "It's so weird..\nI am using desktop pc and I've already checked and setup the Camera privacy in Windows.\nFor me is showing a black panel.\nLooks like the application can communicate with the camera, but could not show the image.\nI don't know if it's a driver problem or hardware problem.\nMy camera is C922 Pro Stream Webcam.", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 8, 2019", "body": "In others application from Windows the image is showing.", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 8, 2019", "body": "The debug console:`\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/C:/Users/raphael.nascimento/Downloads/logback-classic-1.0.9.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/C:/Users/raphael.nascimento/.m2/repository/org/slf4j/slf4j-log4j12/1.7.26/slf4j-log4j12-1.7.26.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See  for an explanation.\nSLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]\n09:46:07.748 [main] INFO  com.github.sarxos.webcam.Webcam - WebcamDefaultDriver capture driver will be used\n09:46:07.755 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:08.275 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:08.275 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:08.279 [main] DEBUG com.github.sarxos.webcam.Webcam - Setting new resolution 640x480\n09:46:08.362 [main] DEBUG com.github.sarxos.webcam.WebcamPanel - Starting panel rendering and trying to open attached webcam\n09:46:08.369 [SwingWorker-pool-2-thread-1] DEBUG com.github.sarxos.webcam.WebcamLock - Lock Webcam C922 Pro Stream Webcam 0\n09:46:08.372 [atomic-processor-1] INFO  c.g.s.webcam.ds.cgt.WebcamOpenTask - Opening webcam C922 Pro Stream Webcam 0\n09:46:08.372 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Opening webcam device C922 Pro Stream Webcam 0\n09:46:08.373 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device 0 starting session, size java.awt.Dimension[width=640,height=480]\n09:46:08.692 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device session started\n09:46:08.694 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Clear memory buffer\n09:46:11.280 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:35.854 [atomic-processor-1] DEBUG c.g.s.w.d.b.WebcamDefaultDevice - Webcam device com.github.sarxos.webcam.ds.buildin.WebcamDefaultDevice@7548be95 is now open\n09:46:35.855 [SwingWorker-pool-2-thread-1] DEBUG com.github.sarxos.webcam.Webcam - Webcam is now open C922 Pro Stream Webcam 0\n09:46:35.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:35.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:38.864 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:41.326 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:41.332 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:41.333 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:44.334 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:46.751 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:46.758 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:46.758 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:49.759 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:52.168 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:52.174 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:52.175 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:46:55.175 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:46:57.605 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:46:57.612 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:46:57.612 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1\n09:47:00.613 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Searching devices\n09:47:03.028 [frames-refresher-[0]] ERROR c.g.s.w.d.b.WebcamDefaultDevice - Timeout when requesting image!\n09:47:03.041 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device C922 Pro Stream Webcam 0\n09:47:03.041 [webcam-discovery-service] DEBUG c.g.s.w.d.b.WebcamDefaultDriver - Found device Logi Capture 1`", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 8, 2019", "body": "I passed the parameter -client when run by command line e its works fine.\nLike this:\n\"java -jar -client class.jar\"Why I need to pass this parameter?", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 9, 2019", "body": "Problem solved..\nI was using Spring Tools Suite, its a IDE based on Eclipse, but the camera doesn't work in that.\nNow I am using Eclipse IDE and the camera is working fine.", "type": "commented", "related_issue": null}, {"user_name": "raphaelbn", "datetime": "Oct 9, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/741", "issue_status": " Closed\n", "issue_list": [{"user_name": "kumarmayank503", "datetime": "Sep 24, 2019", "body": "Hi Sarxos,I have used your api to display webcam and scan barcode .Though when i deployed it on server machine or if i am trying to open my local computer url from someone else machine,when I click camera icon in their machine ,it is opening webcam panel and camera in my machine.I am using jar -webcam-capture0.3.12.jar and bridj 0.7.0.jar.\nPlease help me why it is happening and what is the solution.Below is my code which I am using.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2021", "body": "The code above is using cameras from the computer where the code is running. It is impossible to open video feed from the other computers using code attached above.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/739", "issue_status": " Closed\n", "issue_list": [{"user_name": "kumarmayank503", "datetime": "Sep 16, 2019", "body": "Hi Sarxos,I am using your above code ,did some modification according to our requirement but issue is that when result is coming null and at that time if I am closing the webcam panel then still camera keep getting on and going to infintie loop.Below is my code:package com.fourcs.clm.iwarranty.eclaims.controllers;import java.awt.Dimension;\nimport java.awt.FlowLayout;\nimport java.awt.image.BufferedImage;import javax.swing.JFrame;import com.fourcs.clm.iwarranty.eclaims.domain.ClaimHelper;\nimport com.fourcs.clm.sh.core.domain.Product;\nimport com.github.sarxos.webcam.Webcam;\nimport com.github.sarxos.webcam.WebcamPanel;\nimport com.github.sarxos.webcam.WebcamResolution;\nimport com.google.zxing.BinaryBitmap;\nimport com.google.zxing.LuminanceSource;\nimport com.google.zxing.MultiFormatReader;\nimport com.google.zxing.NotFoundException;\nimport com.google.zxing.Result;\nimport com.google.zxing.client.j2se.BufferedImageLuminanceSource;\nimport com.google.zxing.common.HybridBinarizer;public class WebcamQrCodeTesting extends JFrame  {}Can you advice if result is null at that time if I am closing webcam panel then how should I know that webcam panel is closed or open so that I can put that condition and close the camera on that particular condition.Please advice as it is going on infinite loop.", "type": "commented", "related_issue": null}, {"user_name": "kumarmayank503", "datetime": "Sep 16, 2019", "body": "It is working after adding this line..setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);But this line start giving another issue that is closing the server forcefully.So after running this functionality on my project I can not run any other functionality.Soi i dont want to use this line.", "type": "commented", "related_issue": null}, {"user_name": "kumarmayank503", "datetime": "Sep 16, 2019", "body": "Its start working fine after adding a listener and having a  flag as cancelled inside that like below:this.addWindowListener(new WindowAdapter() {\npublic void windowClosing(WindowEvent evt) {\ncancelled = true;\nsetVisible(false);\n}\n});", "type": "commented", "related_issue": null}, {"user_name": "kumarmayank503", "datetime": "Sep 16, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/714", "issue_status": " Closed\n", "issue_list": [{"user_name": "SoftPathTechnologies", "datetime": "May 9, 2019", "body": "Hi Sarxos, i am working on my Final Year Project, Surveillance System Based on Motion Detection and i am Using WebCam Capture in Java. Can you tell me what image processing algorithm you used in its implementation please? So I can discuss that in my literature review.Thanks as i anticipate your favourable response.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "May 10, 2019", "body": "Hi @lilblazekonceptzThe motion detection algorithm I used is very simple. The individual parts may be described somewhere in a literature, but to be honest I never checked and implemented it based solely on some very pragmatic assumptions which are described later.The fundamental steps are the following:This is done to reduce noise. We use a filter which performs a box blur on an image. Box blur is based on the moving window in which we calculate average values for every channel in RGB space. The blur filter I'm using is derived from JHLabs filters library by Jerry Huxtable. I cannot describe in details how it works, but you can check the code to understand more, but believe me - there is no magic there.In general this is implementation of This is done so we can work on luminance data. It is much easier to do conversion from RGB to luminance because if we stay within RGB we will need to perform motion detection for every channel separately and then average the output, which is generally more complex.The conversion is done by changing RGB data from every pixel to NTSC luminance (aka brightness). The formulation used for this is based on old NTSC spec (from 1953). It's deprecated, but we don't care, because this is not used to display anything on screen and we only need it for calculation. If we would need to display resultant luma on screen, then I would use newer spec.The newest formula used to calculate luma, defined in sRGB spec, is :And the 1953 formula used to convert RGB to luma is:This is done in code  (the variable names in code are different, but please don't be mislead by this).The values 77, 151 and 28 are equivalents of 0.299, 0.587 and 0.114 which are coefficients resulting from CIE color matching functions and standard chromaticities of RGB channels. RGB values used in algorithm are in range from 0 to 255 and luma coefficients are in range of 0 to 1, so:Luma is later converted back to RGB in such a way that resultant pixel is gray. Each channel value (R, G, and B) has the same value which is equal to luma. There is also alpha channel used in formulation, but this does not matter in our case because we use JPG data which has no alpha channel present.This is done by by comparing all pixels with the previous values (from previous image) to calculate how many of these pixels are different. Pixel intensity difference must be greater than a given threshold (configurable value). If this is true, increase number of different pixels by one. At the end calculate percentage value of modified area by using this simple formulation:Where:After we have percentage value of modified area we can then compare this value with a configured value. If percentage area is higher than a given value, then motion detection is fired and motion detector is engaged. If percentage area is lower than a configured value, then there is no motion and motion detector is disengaged.When you take a look into the code:You may see that there are other things happening. For example a center of gravity is calculated, a threshold points are stored, but these have no connection to the motion detection algorithm itself. It's just so you can get these values from the object and visualize on your image (e.g. mark different pixels with a different colours, mark motion centre of gravity, etc).The code may look a little bit crappy so please do not use it as a reference on how one should implement such algorithms. It was implemented mainly to serve a very basic purpose and was much shorter, but later was modified by a community to add no-engage zones, support for pluggable algorithms, etc, and today I would implement it differently.Take care and I wish you a best luck with your Final Year Project! Computer engineering is awesome :)", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "May 10, 2019", "body": "Hi @lilblazekonceptz,One more thought. The default driver is derived from  so it would be very nice if you reference it in your literature:", "type": "commented", "related_issue": null}, {"user_name": "SoftPathTechnologies", "datetime": "May 11, 2019", "body": "", "type": "", "related_issue": null}, {"user_name": "shinobisoft", "datetime": "Mar 14, 2020", "body": " Sorry to reopen this, but is the method mentioned above also suitable for light detection? I have an old Android phone that I use as a security camera but it doesn't have or support night vision. I've written a DVR for this camera but I'd like to figure a way to detect day light changes to enable or disable recording at certain times of day. If the above method isn't suitable do you have any ideas how to implement what I'm looking to do?Thanks for your time!", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "May 12, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/713", "issue_status": " Closed\n", "issue_list": [{"user_name": "msgilligan", "datetime": "May 3, 2019", "body": "I'm having trouble getting camera input on macOS Mojave due to a requirement for .I'm not sure what combination of changes I need to make so that the OS knows to prompt the user for permission. It may require code-signing. If anyone has experience with this, please let me know. If not, I'll share information here when I figure it out.If changes to  are required we should make them. If not, we should at least document what needs to be done to make it work.", "type": "commented", "related_issue": null}, {"user_name": "msgilligan", "datetime": "May 3, 2019", "body": "So, it looks like all you have to do to get it working is add the following to your :", "type": "commented", "related_issue": null}, {"user_name": "arman-sydikov", "datetime": "Jul 28, 2019", "body": "\nI have put those lines into , but asking for camera permission dialog does not pop up.", "type": "commented", "related_issue": null}, {"user_name": "msgilligan", "datetime": "May 3, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "msgilligan", "datetime": "May 7, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/660", "issue_status": " Closed\n", "issue_list": [{"user_name": "sachinrke", "datetime": "Aug 31, 2018", "body": "hello sir,\nfirst of all, i would like to congratulate you for this wonderful project (Webcam-capture API). you have done grate work. now my issue is that i want to zoom in/out, panning and increase/decrease brightness in IP camera using this (Webcam-Capture API). please help me out to achieve these functionality for the camera...\nthank you in advance....", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Sep 9, 2018", "body": "Hi , this unfortunately is not possible with existing Webcam Capture API. If your IP camera expose some kind of REST API which can be used to zoom/pan/tilt then you can try using e.g.  to communicate with your IP camera.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Sep 9, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "sarxos", "datetime": "Sep 9, 2018", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/smellslikeml/ActionAI/issues/56", "issue_status": " Open\n", "issue_list": [{"user_name": "shamine5", "datetime": "Jul 6, 2022", "body": "I would like to do activity recognition on a live cctv camera. In the Read.md section it is mentioned that the deployment file has reference codes to do it, where can I find this file.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4659", "issue_status": " Open\n", "issue_list": [{"user_name": "Gloriabhsfer", "datetime": "Aug 9, 2022", "body": "Hi all:\nI have follow the steps form the manual for UE4 and AirSim for Linux to install and build the project. However, I encounter with question while I am trying to open the blocks Uproject. My system is Ubuntu 20.04 and Unreal_Engine 2.47.2. I try convert the project when I am open the projectAny ideas?Best,\nGloria", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 9, 2022", "body": " have you updated your UE version?", "type": "commented", "related_issue": null}, {"user_name": "Fergus-MW", "datetime": "Sep 14, 2022", "body": "Hi Guys, I've got the exact same issue. Was a solution found? I've tried reinstalling Airsim and Unreal to no avail.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 12, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4648", "issue_status": " Open\n", "issue_list": [{"user_name": "EvelynM7", "datetime": "Jul 29, 2022", "body": "Originally, this problem showed up in computer vision mode. I was trying to modify camera parameters in the settings.json, but when that was not working due to cv mode, I found the client.simSetFocalLength function. However, this gave the following error:  I thought this might be an issue with cv mode, so I also tried it with multirotor settings and with simGetFocalLength. Both of these are giving similar argument errors in the debug and terminal as well as in the client.py.I tried looking up solutions to this problem and the only reason I could find was one person saying that it might be due to client and server versions, but I checked that both of mine are up to date. (There seems to be hard coded 1s for the versions in client.py?) Any suggestions or solutions would be greatly appreciated!\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 12, 2022", "body": ", which python client version are you using?", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Aug 16, 2022", "body": ", I'm using Python 3.10.6 64 bit in Visual Studio Code.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 16, 2022", "body": ", what number do you get running this:\nimport airsim\nairsim.", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Aug 16, 2022", "body": "I'm now getting .", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 16, 2022", "body": ", sorry it is ", "type": "commented", "related_issue": null}, {"user_name": "EvelynM7", "datetime": "Aug 16, 2022", "body": ", It says '1.7.0'.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 16, 2022", "body": "Sorry, now I see you are running from source. Are you running the simulation from a binary? This is typically a version mismatch between client and server.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 12, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4614", "issue_status": " Open\n", "issue_list": [{"user_name": "dskuma", "datetime": "Jul 8, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jul 11, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4610", "issue_status": " Open\n", "issue_list": [{"user_name": "DonMaEn", "datetime": "Jul 7, 2022", "body": "When using the orthographic camera, holes appear in the depth map images saved to disk when using the client.startRecording() function. These holes only appear on the ground mesh, not on buildings or trees. I have tested this using the pre-compiled AirSimNH scene and the LandscapeMountains scene. In the below images you can see in yellow the hole, and you can see how the trees are not affected by this bug. The RGB image is from the exact same area as the depthmap image, and you can see there is no visible hole in the ground mesh.Also, see the following screen shot of a point cloud I created from the depth maps and RGB images that shows where the holes appear in the AirSimNH scene:\nN/ABecause I am using centos stream 8 I have not been able to compile Airsim from source, and therefore could not test on the master branch.I tried using the latest version 1.8 pre-compiled binaries, however when running my collection script airsim crashes. I'm guessing this is because I am still using version 1.7 of the airsim python API. 1.8 is not yet available on pypi so I cannot test this theory.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jul 8, 2022", "body": "Thanks for the report ! The holes always appear in the same places or this is a stochastic behavior?", "type": "commented", "related_issue": null}, {"user_name": "DonMaEn", "datetime": "Jul 8, 2022", "body": ", good question. The holes do not always appear in the same place. The following image is a point cloud I created using the same method, but of a smaller area. You can still see the holes, but they are in different locations.\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jul 8, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4589", "issue_status": " Open\n", "issue_list": [{"user_name": "JiancongWang", "datetime": "Jun 23, 2022", "body": "I am running 5 virtual cameras with fov 90 degree in +-xyz (omit the -y for now since it points to the ground) direction to form a cube map and I want to stich them together to form an epirectangular image. But I found that the exposure of the images are drastically different with each other even when the image are collected by python script simultaneouly (so there should not be any effect of light direction change). This is from the landscapeMountain scene.  I suppose this is due to auto exposure. So I want to ask if there is a way to set manual exposure level here?Here is how I added the virtual camera in the settings.json.\n\"Vehicles\": {\n\"PhysXCar\": {\n\"VehicleType\": \"ComputerVision\",\n\"DefaultVehicleState\": \"\",\n\"AutoCreate\": true,\n\"PawnPath\": \"\",\n\"EnableCollisionPassthrogh\": false,\n\"EnableCollisions\": true,\n\"RC\": {\n\"RemoteControlID\": -1\n},\n\"Cameras\": {\n\"MyCamera0\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": -90\n},\n\"MyCamera1\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0\n},\n\"MyCamera2\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 90\n},\n\"MyCamera3\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 180\n},\n\"MyCamera4\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 90, \"Roll\": 0, \"Yaw\": 0\n}\n}\n}\n}The collection script (modded from fov_change.py so the rest of it is the same):requests = [airsim.ImageRequest(\"MyCamera0\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera1\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera2\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera3\", airsim.ImageType.Scene),\nairsim.ImageRequest(\"MyCamera4\", airsim.ImageType.Scene)\n]\nclient.simSetCameraFov(\"MyCamera0\", 90)\nclient.simSetCameraFov(\"MyCamera1\", 90)\nclient.simSetCameraFov(\"MyCamera2\", 90)\nclient.simSetCameraFov(\"MyCamera3\", 90)\nclient.simSetCameraFov(\"MyCamera4\", 90)responses = client.simGetImages(requests)\nsave_images(responses, \"new_fov_90\")This is running UR 4.27.2 with the latest airsim from github.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 24, 2022", "body": "Hi ! This may be due to missing image postprocessing. Can you try applying the solution given in  thread?\nI like that image. How did you generate it?", "type": "commented", "related_issue": null}, {"user_name": "JiancongWang", "datetime": "Jun 24, 2022", "body": "I think that post processing is not the problem here. In real camera terms, I want to make sure the camera ISO/aperature/exposure time are all the same for the image so that their exposure is as closed as possible. I think now in airsim the image is auto adjusted to fit a certain target. I want to turn that off and instead manually fixed the picture intensity.\nI tried histogram match all the image to the middle one. It certainly looks better but the seam between stiching is still very bad.The stiching is done by the py360 convert package. So once you have the 5 fov90 views by the 5 cameras in the settings.json, you can treat it as the cube map faces and convert that to equirectangular using this.\n", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 24, 2022", "body": "Nice package, thanks! changed the default unreal camera to the cinematographic one. I think what you are searching for is  method added by that PR", "type": "commented", "related_issue": null}, {"user_name": "JiancongWang", "datetime": "Jun 27, 2022", "body": "Thank you for sending me this! I will give it a try.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 24, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4578", "issue_status": " Open\n", "issue_list": [{"user_name": "lukas-utopiacompression", "datetime": "Jun 16, 2022", "body": "When multiple vehicles (see settings.json below) specify a camera with the same name (e.g. \"img0\") ROS1's tf frame_ids become ambiguous i.e. only one of the drone has a transformation from itself to  or . However, all drones should have their own distinct transformation from their respective img0 frame to their NED frame.Related PR  fixes the same issue but for ROS2.\nRelated PR  fixes this partially.Screenshot of rqt_tf_tree, when camera frame_ids are ambiguous.\nScreenshot of rqt_tf_tree, when camera frame_ids are distinct (e.g. by adding the vehicle_name to the frame_id):\nn/a", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 20, 2022", "body": "Hi ! Thanks for submitting the PR. I'd like to know why you didn't upgrade to ROS2 when you ran into this issue.", "type": "commented", "related_issue": null}, {"user_name": "lukas-utopiacompression", "datetime": "Jun 20, 2022", "body": "Good morning ,Happy to help!We haven't transitioned for two reasons:", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 20, 2022", "body": "Thanks for the reply, . Keep in mind that you can use the Ros1-Ros2 Bridge to connect your project with ROS2 nodes.", "type": "commented", "related_issue": null}, {"user_name": "lukas-utopiacompression", "datetime": "Jun 16, 2022", "body": [], "type": "pull", "related_issue": "#4579"}, {"user_name": "jonyMarino", "datetime": "Jun 20, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4576", "issue_status": " Open\n", "issue_list": [{"user_name": "HarrySoteriou", "datetime": "Jun 16, 2022", "body": "I have searched through all issues both open and closed that highlight low FPS of simGetImages such as , ,  and .How close are you to resolving the msgpack bottleneck and what are the most up to date tricks that can significantly improve FPS?I think I saw something about compiling with specific binaries but I had a hard time compiling AirSim before so I don't want to risk it if it's not worth it. If at least 30fps are not attainable then AirSim is not a viable RL simulator.I am training an RL script that takes as input low resolution RGB images (256x256x3) and then takes action. I started with 1-2FPS and started trying everything I could to increase my FPS because training 1 million timesteps took a whooping 5d7h and I can't run experiments that take so long.I use the following function to make image calls but I have to ensure I am not getting an empty string binary or else training stops (hence the while loop and the .shape==0 check)Specs: Windows 10.0.19044\nUE: 4.27.2\nAirSim: 1.6\nGraphics Card: NVIDIA GeForce GTX 1080 Ti\nRAM: 4GB\nPython: 3.9.10\nmsgpack-python 0.5.6\nmsgpack-rpc-python 0.4.1I have tried the following:Steps 4 and 5 cause instabilities (when evaluating a trained example that used asynchronous steps or increased wallclockspeed, it's behaviour was worse than random. An example trained for 1million time steps without using 4 and 5 converged to the expected behaviour). With everything I tried I am running training with 5-7 FPS.", "type": "commented", "related_issue": null}, {"user_name": "misakabribri", "datetime": "Jul 27, 2022", "body": "Here are testing results on my machine:Devices: Intel i7-9700K+NVIDIA GeForce 3070\nImage size: 1270*720\nAverage rendering time: 40msI also need an approach to increase the FPS to 30.", "type": "commented", "related_issue": null}, {"user_name": "mgrova", "datetime": "Sep 20, 2022", "body": "Hi ,\nI am also trying to get a valid FPS for visual odometry tests (30 fps would be perfect), but I can't get more than 15 fps approx.Have you made any progress in this time?Best regards and thanks in advance!", "type": "commented", "related_issue": null}, {"user_name": "misakabribri", "datetime": "Sep 21, 2022", "body": "Hi \nMaybe you can package your UE4 project as a released app and then try again. By my observation, if the UE4 project's window is not active, it will also cause low fps. Also, I use multi threads in C++ code to reduce total time of the image capture loop.", "type": "commented", "related_issue": null}, {"user_name": "mgrova", "datetime": "Sep 21, 2022", "body": "Hello !I'm working with the binary of my own enviroment with \"NoDisplay\" as \"ViewMode\" in settings.json file. In addition, I use the C++ wrapper and ROS to grab the images. I'm going to try to generate the binaries in release mode, thanks!Using multithreathing have you achieved 30 fps when grabbing the images in C++ in real-time?", "type": "commented", "related_issue": null}, {"user_name": "misakabribri", "datetime": "Sep 21, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "mgrova", "datetime": "Sep 22, 2022", "body": "Im taking the images (one depth and one scene) at 640x480. Simulating only these two cameras and using the ROS1 wrapper, I don't get more than 8-10 FPS in my workstarion with RTX 2080Ti.Do you have any repository or code with which you can get that frequency when obtaining the images?Sorry for the inconvenience and thank you very much.", "type": "commented", "related_issue": null}, {"user_name": "misakabribri", "datetime": "Sep 23, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 22, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4573", "issue_status": " Open\n", "issue_list": [{"user_name": "JiancongWang", "datetime": "Jun 15, 2022", "body": "Hi guys,\nI am trying to add custom cameras to a car when using computer vision mode. The settings.json I use is as follows{\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"ComputerVision\",\n\"Vehicles\": {\n\"PhysXCar\": {\n\"VehicleType\": \"PhysXCar\",\n\"DefaultVehicleState\": \"\",\n\"AutoCreate\": true,\n\"PawnPath\": \"\",\n\"EnableCollisionPassthrogh\": false,\n\"EnableCollisions\": true,\n\"RC\": {\n\"RemoteControlID\": -1\n},\n\"Cameras\": {\n\"MyCamera0\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": -90\n},\n\"MyCamera1\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 0\n},\n\"MyCamera2\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 90\n},\n\"MyCamera3\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 0, \"Roll\": 0, \"Yaw\": 180\n},\n\"MyCamera4\": {\n\"X\": 2, \"Y\": 0, \"Z\": -1,\n\"Pitch\": 90, \"Roll\": 0, \"Yaw\": 0\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"ImageType\": 0, \"CameraName\": \"MyCamera4\", \"Visible\": true},\n{\"WindowID\": 1, \"ImageType\": 0, \"CameraName\": \"MyCamera1\", \"Visible\": true},\n{\"WindowID\": 2, \"ImageType\": 0, \"CameraName\": \"MyCamera2\", \"Visible\": true}\n],\n\"Recording\": {\n\"RecordOnMove\": false,\n\"RecordInterval\": 0.05,\n\"Cameras\": [\n{ \"CameraName\": \"MyCamera0\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true },\n{ \"CameraName\": \"MyCamera1\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true },\n{ \"CameraName\": \"MyCamera2\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"Compress\": true }\n]\n}\n}This runs fine when I set the sim mode to \"Car\". But when I switch the sim mode  to \"ComputerVision\", it gives me an error of \"there were no compatible vehicles created for current simmode\". So I am writing to ask how to resolve this issue?I am running this on Ubuntu 20.04, unreal4 4.27.2.I tried to add the \"PawnPath\" and add a drone to the vehicle setting and still no luck.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Jun 16, 2022", "body": "For Computer vision simmode, the vehicle type must also be ComputerVision", "type": "commented", "related_issue": null}, {"user_name": "JiancongWang", "datetime": "Jun 16, 2022", "body": "Thanks for the tip! In that case can I just replace the vehicle type of \"PhysXCar\" to \"ComputerVision\"?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 20, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4554", "issue_status": " Open\n", "issue_list": [{"user_name": "skarful", "datetime": "May 31, 2022", "body": "I am simulating a drone with a lidar and a camera sensor. I wish to obtain images every 100ms and hence have set the parameter in the airsim_node launch file as below:\nHowever, I see a lot of variance in the time between receiving two images. In the screenshot below, I have printed the time (in milliseconds) between receiving two images. The range goes from around 21ms to 99ms.Is there a way I can get a fixed frame rate? () I tried changing the  to 0.2 (a slower rate) but the variance in obtaining the image still exists. This time it ranges from around 140 to 190msThe settings.json file is attached below.I tried looking up the code and see that image callbacks are handled using an  in . Could this be an issue? Is there an alternative I can use to maintain a fixed frame rate? Any help will be appreciated, ThanksFrom airsim_ros_wrapper.cpp:", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 13, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4543", "issue_status": " Open\n", "issue_list": [{"user_name": "TorsteinOtterlei", "datetime": "May 25, 2022", "body": "How do you make the snow texture that is shown when pressing F4 appear when taking pictures using AirSim (scene camera) ?Is there any way to programmatically enable this snow texture with code ?When in ComputerVision mode, pressing F4 gives you an overlay on the map that puts snow on all the textures. However, this snow texture does not show in the live camera-view or when taking pictures using AirSim. The issue is shown in the image below (downward facing camera).I have tried all the simSetWeatherParameter options, but these only show a \"filter\" over the camera with showfall etc.Airsim version (pip package): 1.6.0\nOS: Windows 11 Home 21H2 1000.22000.675.0\nPython version: 3.6.13\nUnreal version: 4.27.2settings.json below", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 22, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4525", "issue_status": " Open\n", "issue_list": [{"user_name": "godhj93", "datetime": "May 16, 2022", "body": "I want to get depth images in realtime. I have tried to solve it but failed. I tried following methods:But the problem is that It autimatically normalize the depth value from 0~100m*, Its value is almost 0 or 255 because It sees sky(depth is infinity) so it becomes useless value.So my question is that Is there any method to change maximum value for normalization like 100m*.\nI'm happy to get any solution to get depth value in realtime.I want to get depth images in realtime.OS: Ubuntu20.04\nPython: 3.9\nUnreal: 4.25.4I described it above.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4522", "issue_status": " Open\n", "issue_list": [{"user_name": "nabil-mokhtar", "datetime": "May 13, 2022", "body": "I'm using airsim for simulating drone to acheive 3d mapping using RTABMAP algorithm , so using \nwhich create ROS node publishes this topics:my problem is depth topic hz is   = each frame takes  seconds , copmared to rgb topic hz which is ", "type": "commented", "related_issue": null}, {"user_name": "SabryTarek", "datetime": "May 14, 2022", "body": "I have the same issue with Depth-Image\nDepth-Image FPS: ", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4494", "issue_status": " Open\n", "issue_list": [{"user_name": "JuanshuB2", "datetime": "Apr 23, 2022", "body": "Is it possible to get the LiDAR debug points as an 2D image directly for AirSim? Like the one above but only the green points. whitout the objects?I want the 2D image from the LiDAR pointcloud not to do it manually.The image could be get from the 3D point cloud transformation to 2D, but if AirSim already give us that projection, it can be useful to be able to delete the background in the image. I have figured out about subtracting an image with the green points and another without them, but as the points are visible in the world, I have no access to a image as the second one.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 2, 2022", "body": " that is not currently possible. Why do you need the 2D image of the points reached by the lidar?", "type": "commented", "related_issue": null}, {"user_name": "JuanshuB2", "datetime": "May 4, 2022", "body": "Thanks for your reply . I need the 2D image because we want to do the overlapping camera-lidar. Now we are transforming the 3D pointcloud to 2D image but the overlap sometimes is not easy, so a ground truth like that would be great. However, I guess we can work with that image, even if we can not have both. Maybe some kind of image processing can be done to delete the green points.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "May 2, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4488", "issue_status": " Open\n", "issue_list": [{"user_name": "BBBBBBob", "datetime": "Apr 19, 2022", "body": "In the OpticalFlow setting, the optical flow cannot be returned when the camera moves in a certain direction.  Here is a video example :\n\nThe problem could be that the velocity is mapped between the -1 and 1, but the Unreal Engine Material only returns the black color when the velocity is non-positive, even though  is enabled.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 29, 2022", "body": "  optical flow is not intended for visualization. Please try with OpticalFlowViz (which corresponds to ImageType 9 instead of 8 )", "type": "commented", "related_issue": null}, {"user_name": "BBBBBBob", "datetime": "Apr 30, 2022", "body": "  Hi, thanks for your reply! Indeed, I would like to retrieve the optical flow velocity from the image, so what is the right way to get the velocity information?", "type": "commented", "related_issue": null}, {"user_name": "COATZ", "datetime": "Jul 5, 2022", "body": "Hi , were you able to find a solution to get the optical flow from AirSim?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 29, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4473", "issue_status": " Open\n", "issue_list": [{"user_name": "StardustLu", "datetime": "Apr 9, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "OPyshkin", "datetime": "Apr 15, 2022", "body": "Have you rectified the input images?", "type": "commented", "related_issue": null}, {"user_name": "StardustLu", "datetime": "Apr 21, 2022", "body": "Thanks for your reply :)\nI did nothing with the RGBD image\nI just edit  and record them, then I calculate the correspondence by RGBD image and extrinsic parameters in ", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Apr 12, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4435", "issue_status": " Open\n", "issue_list": [{"user_name": "SaundersJE97", "datetime": "Mar 25, 2022", "body": "I am using a reinforcement learning algorithm for motion planning and frequently when the drone collides with an object the drone teleports to the wrong start location.  I have noticed this happens more often when the drone is closer to the obstacle.  The behaviour has not been consistent, sometimes the drone will not take any movement commands, alternatively the drone will gain a huge speed boost and will consequently be thrusted in the forward direction.Please see the following YouTube video for what I have been experiencing: I am using the following client command to fly the drone through the environment.  Where action is a variable in the range of [-1, 1].The teleportation code is as following.(Please see code above)(No errors experienced from python on UE)", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 28, 2022", "body": " This is a known limitation for doing RL quickly with our flight controller. You are teleporting the drone but the state of the controller remains the same. We would need to have an API method for saving the state of the controller and another one for loading it.\nAnother way that was asked is to increase the clock frequency to go fast again to that location.\nYou can spawn a new drone on that point and set its kinematic state, but you are not going to have the same controller state that when it collided.\nFor now the only solution is to re run the simulation till that moment.", "type": "commented", "related_issue": null}, {"user_name": "SaundersJE97", "datetime": "Mar 28, 2022", "body": "Thank you for your reply, this clears it up.  Ideally I would benefit from a vehicle specific 'reset(vehicle_name)' api call.  Is there a function within the SimpleFlight Controller that would reset the controller state?\nFor example would resetting the firmware under 'SimpleFlightApi.hpp' would provide this?Thanks,\nJack", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 29, 2022", "body": "@SaundersJE9, In that case, you can call  before \nI think you would not need to cancel the last task.", "type": "commented", "related_issue": null}, {"user_name": "SaundersJE97", "datetime": "Mar 30, 2022", "body": " That works for a single drone, however I was considering doing multi-agent reinforcement learning and, although still useful, using  resets all drones within the environment.  I understand it is currently a feature request to add , I would be happy to investigate how to add this to the api since I will need it in my project.Taking a look at AirLib and more specifically RpcLibServerBase.cpp, the command either  or 'getVehicleApi(\"\")->reset()` is called.  I tried modifying this to add a specific vehicle reset function within the api:From my understanding this only resets the 'simulation' drone and not the AirLib instance of the drone.  I am missing the controller reset function.  I had some issues with this and had to run it on the game thread.  Unfortunately, this still didn't fix the issue I outlined in the beginning of this thread.Looking into  the object VehicleApiBase inherits from UpdatableObject which when reset is called will revert back to it's initalized state.  I added  to the reset(vehicle_name) api call but I still get the rebounding effect when hitting.  My current thought is I need to reset the controller.  However, within the SimpleFlightApi.hpp the firmware is reset when calling .So if the controller is also being reset, I am not sure what could be causing it.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 30, 2022", "body": " I would need to dig into this. Another idea is to use  but we don't have a  so I don't know how your RAM usage will end. Can you give it a try?", "type": "commented", "related_issue": null}, {"user_name": "SaundersJE97", "datetime": "Mar 31, 2022", "body": "I hadn't considered that function, I'll give it a try and report back.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 28, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4428", "issue_status": " Open\n", "issue_list": [{"user_name": "mgodmere", "datetime": "Mar 22, 2022", "body": "The Airsim ROS2 package produces jerky drone movement every time an image is captured while using any movement command such as the  subscriber.  I verified this by slowing the capture rate by setting the roslaunch  to 1Hz.  When flying with a constant velocity the drone stops moving for a split second at 1Hz.  When using the default rate of 20Hz the drone struggles to move at all since its movement is interrupted by frequent image captures.  This jerky movement gets worse the more cameras or higher resolution images are used.  I'm guessing this is due to a threading or client issue with the package although the settings make it seem like they run in separate callback threads.  The standard airsim client obviously doesn't produce this behavior.  I will also mention that the ROS2 package runs just fine with image capture in realtime when using an RC controller.This is a show stopper for using the new ROS2 interface and I want to avoid rewriting/forking it.  I was hoping to run several vehicles with multiple cameras, but this seems like a currently unscalable proposition.  Am I using it incorrectly?  Can someone verify that this is a current issue?  It seem strange to me that this is a bug/issue when moving and capturing is one of the core use cases of airsim.", "type": "commented", "related_issue": null}, {"user_name": "nikola-j", "datetime": "Jun 6, 2022", "body": "I have the same issue, did you maybe figure out a way to fix this?", "type": "commented", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jun 24, 2022", "body": " I did a quick test with your PR, even when the vehicle stand still, similar behavior of this issue still existed (It seems like the camera processing also affects the timestamp of other topics). You can refer to ", "type": "commented", "related_issue": null}, {"user_name": "nikola-j", "datetime": "Jul 14, 2022", "body": "Hi , do you mean similar behavior to the current issue (jerky movement) or to your issue (timestamps)? Was there a difference in timestamps without my changes? Did you build ros2 airsim in Debug or Release?", "type": "commented", "related_issue": null}, {"user_name": "hoangvietdo", "datetime": "Jul 14, 2022", "body": "Hi , what I mean is that both of these two issues share the same problem which involves running a vehicle and capturing images. This setup causes both time-delay in the timestamp as well as the problem in this issue.I actually don't know about Debug or Release, I just simply copy and paste your , build, run the simulation and check the timestamp.The time difference when using a stereo camera has been detected and discussed for a long time in ros1, still, no PR has been proposed to solve it. I have not checked the timestamp before your PR in ros2.", "type": "commented", "related_issue": null}, {"user_name": "nikola-j", "datetime": "Jul 20, 2022", "body": "Yes, but this PR isn't meant to fix the issue with the timestamps being out of sync, but to make the ros2 implementation usable. Without this fix, the drone won't move properly, and the timestamps would be even more out of sync.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 23, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "nikola-j", "datetime": "Jun 7, 2022", "body": [], "type": "pull", "related_issue": "#4559"}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4414", "issue_status": " Open\n", "issue_list": [{"user_name": "COATZ", "datetime": "Mar 15, 2022", "body": "Hi,Thanks a lot for adding the OpticalFlow feature to AirSim but I am having some issue getting it.What are the correct options of the  to get the 2-channels ground truth  from my front drone camera (not the OpticalFlowVis) using the Python API.My camera parameters are width = 100, height = 100 and I am currently using:\n.\nBut the output is a single channel image instead of the two expected:\n  (and not 20000 as expected)I tried with the option \"pixel_as_float=False\" and got a 3-channels output of . But the raw ground truth optical flow should be float so I am guessing that is not the right option...Thank you!", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Mar 22, 2022", "body": "Since simGetImages just returns the buffer of a SceneCaptureComponent2D instance, I would not expect to get output with multiple entries for each channel. Looking at how OpticalFlow's post process material is defined , the 2 channels (vx & vy) are mapped to the emissive color of the material, meaning that these channels will be encoded in the pixel color of the output of simGetImages. I don't know exactly how Unreal's material editor maps a float2 to a color value, so you may need to play around with the output, but it looks like you need to parse each pixel's color for the values of both channels. If you find out how this conversion works, please post that in this issue.", "type": "commented", "related_issue": null}, {"user_name": "BBBBBBob", "datetime": "Apr 1, 2022", "body": "Hi, I have met the same problem, have you found a solution to fix it?", "type": "commented", "related_issue": null}, {"user_name": "BBBBBBob", "datetime": "Apr 11, 2022", "body": "In the material,  x-velocity is encoded in the red channel and y-velocity in the green channel. If we observe the optical flow image, there are only red and green colors in the OpticalFlow setting. However, Unreal Engine only returns the black color to the material when the speed is non-positive. It is correct that the optical flow image is dark when the vehicle is at a standstill, but it does not return any optical flow when the camera moves to the right side or upwards.  Here is an example: \nThis problem only exists in the OpticalFlow setting.", "type": "commented", "related_issue": null}, {"user_name": "wangpinzhi", "datetime": "Aug 30, 2022", "body": "Hello, have you found a solution? I met the same problem.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Mar 22, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4412", "issue_status": " Open\n", "issue_list": [{"user_name": "zimmy87", "datetime": "Mar 15, 2022", "body": "Currently, UDetectionComponent::calcBoundingFromViewInfo() calculates the location and size of the 2D box around detected objects using a 2D projection of the output of . As can be seen in , due to the size and shape of colliders as well as the possibility of an actor containing invisible components, AActor::GetActorBounds may or may not reflect the bounds of the visible meshes attached to an actor. Since AActor::GetActorBounds also returns a rectangular box, it's possible a 2D projection extends beyond the visible meshes depending upon how the target actor is rotated relative to the camera. One possible solution to this would be to iterate through an actor's components and use the output of  for each visible component as the basis for the 2D box. However, this solution may result in a performance impact due to the additional iteration over each detected actor's components. More research should be done to determine if this is feasible or if there is a better solution.This would address concerns raised in  that the detection APIs do not accurately represent the bounds of detected objects on a 2D display.", "type": "commented", "related_issue": null}, {"user_name": "bozcani", "datetime": "Jun 9, 2022", "body": "Is there any progress on this issue? I am not a expert of Unreal programming expert but what do you think about using segmentation information inside the Airsim to refine object bounding boxes?Currently, we can record segmentation images and refine bounding boxes in an offline manner. Can we make this refinement inside Airsim as a built-in feature?", "type": "commented", "related_issue": null}, {"user_name": "dellhuyongcai", "datetime": "Jul 19, 2022", "body": "Is there any progress on this issue?I just know the box is axis aligned bounding box,has anyone else tried the original bounding box ? it seems that ue4 does not have original bounding box function.", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Mar 15, 2022", "body": [], "type": "issue", "related_issue": "#4367"}, {"user_name": "zimmy87", "datetime": "Mar 15, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4353", "issue_status": " Open\n", "issue_list": [{"user_name": "AmitNativ1984", "datetime": "Feb 15, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 24, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4345", "issue_status": " Open\n", "issue_list": [{"user_name": "pdx97", "datetime": "Feb 8, 2022", "body": "Hi i am unable to view my subwindow in the Unreal engine for my drone even though the camera name in the code and the setting.json are same, i have read various issues and even followed them but nothing is working out .\n{\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true, \"External\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\":true}\n]}", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 10, 2022", "body": "You have External set for the first subwindow but it's not an external camera. Try removing this param", "type": "commented", "related_issue": null}, {"user_name": "pdx97", "datetime": "Feb 10, 2022", "body": " i tried that too still the subwindow doesn't appear.", "type": "commented", "related_issue": null}, {"user_name": "pdx97", "datetime": "Feb 10, 2022", "body": "{\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\": true}\n]}even with these settings it doesn't appear", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 12, 2022", "body": "Wonder why AirSim is even starting, you don't have  set in the settings, add that and it should work", "type": "commented", "related_issue": null}, {"user_name": "pdx97", "datetime": "Feb 12, 2022", "body": "No even after adding SimMode it doesn't work . {\n\"SeeDocsAt\": \"\",\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"Multirotor\",\n\"ClockSpeed\": 2,\n\"Vehicles\": {\n\"Drone1\": {\n\"VehicleType\": \"Simpleflight\",\n\"AutoCreate\": true,\n\"Sensors\": {\n\"LidarSensor1\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 16,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n},\n\"MyLidar2\": {\n\"SensorType\": 6,\n\"Enabled\": true,\n\"NumberOfChannels\": 4,\n\"PointsPerSecond\": 10000,\n\"X\": 0,\n\"Y\": 0,\n\"Z\": -1,\n\"DrawDebugPoints\": false\n}\n}\n}\n},\n\"SubWindows\": [\n{\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 1, \"CameraName\": \"1\", \"ImageType\": 5, \"VehicleName\": \"Drone1\", \"Visible\": true},\n{\"WindowID\": 2, \"CameraName\": \"1\", \"ImageType\": 0, \"VehicleName\": \"Drone1\", \"Visible\":true}\n]}", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Feb 13, 2022", "body": "The above settings are working fine for me, using the 1.7.0 release Blocks binary on Linux. Details of which platform, version, etc haven't been added in the issue description, please do so. A more minimal settings which should work as well -Also make sure that the correct settings.json is getting picked up, try removing everything and just having SettingsVersion, and Car simmode", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 23, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4344", "issue_status": " Open\n", "issue_list": [{"user_name": "HarrySoteriou", "datetime": "Feb 8, 2022", "body": "I was tweaking the settings file, trying to get the front rotors out of the simGetImage() field of view (when using a custom front camera the two front rotors would be part of the captured image no matter what I changed, i.e. pitch: -90.0, x : 3, z: 5--> NOTHING CHANGED WHAT MY simGetImage was returning )...I tried using external camera= True, in my script without modifying the settings file... this has caused a crash and even when reverting the change AirSim will not load.... After trying to fix the issue without success I have tried building everything from scratch again on 2 separate computers using the same version UE=4.27. I have a Block solution (Block.sln), that matches the UE4 version, AirSim Plugins are placed in the project but I don't get prompted to use AirSim as soon as I load the environment.Steps:Expected behaviour:\nLoad in the environment and be able to Play AirSimObserved behaviour:\nLoad in the environment and although the plugin is listed I do not have the option to run AirSim", "type": "commented", "related_issue": null}, {"user_name": "HarrySoteriou", "datetime": "Feb 8, 2022", "body": "Manage to overcome this issue by ensuring that I was using the right version of VS when building the Blocks environment and returning my settings.json to the original one (On that computer I have 2 versions of VS: 2019 and 2022.. )Then I ran the simulation using my configured settings.json/ not the default one and got the same error as Edit: Resolved this issue by return settings.json to it's default value", "type": "commented", "related_issue": null}, {"user_name": "HarrySoteriou", "datetime": "Feb 9, 2022", "body": "For the love of god make a clear tutorial on how to build the blocks environment and common issues that are faced, this is the 3rd day of me trying to return to the point I was before editing my settings file.\nThe documentation at different point says to do different things:I dont have the option to PLAY AIRSIM, when I enter my blocks environment. I have ran the  DebugGame_Editor X64 and then build using \"Develop Editor\" and  x64", "type": "commented", "related_issue": null}, {"user_name": "HarrySoteriou", "datetime": "Feb 9, 2022", "body": "followed this tutorial:  and solved my issues, if airsim crashes soon after running a script containing simGetImages ensure that the settings.json file doesnt have any invalid settings", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 22, 2022", "body": "Hi !\nThanks for keeping the issue up to date.1_ The developer command prompt is only necessary to run build.cmd\n2_ With binaries, it is not required to clone the repo\n3_ update_from_git.bat or GenerateProjectFiles.bat are not called in the case of binaries\n4_ You already solved it.Sorry about the state of the documentation. You can submit a pull request to improve it.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 22, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4322", "issue_status": " Open\n", "issue_list": [{"user_name": "MartinRossel-67", "datetime": "Jan 26, 2022", "body": "Hey, I want to use AirSim to record my one simulation.\nI need to run a \"normal\" et a \"segmentation\" record as the same time frame a camera I choose.\nI would like to call the record from Blueprint.\nI prefer not to use AirSim game mode, I got my own.The thing is: I don't know C++ and yet I didn't figure it out where I can run a \"segmentation\" record.\nIf you have any hint or any tutorial about what a would like to do, I would appreciate cause haven't figured it out on my own yet.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Feb 3, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4312", "issue_status": " Open\n", "issue_list": [{"user_name": "mmuetert", "datetime": "Jan 24, 2022", "body": "Hello,I am trying to run the segmentation.py  example from the PythonClient folder in airsim. When I try to run the script, it runs fine until the line#get segmentation image in various formats\nresponses = client.simGetImages([\nairsim.ImageRequest(\"0\", airsim.ImageType.Segmentation, True), #depth in perspective projection\nairsim.ImageRequest(\"0\", airsim.ImageType.Segmentation, False, False)])  #scene vision image in uncompressed RGBA array\nprint('Retrieved images: %d', len(responses))where I get the error\"msgpackrpc.error.RPCError: rpclib: function 'simGetImages' (called with 3 arg(s)) threw an exception. The exception is not derived from std::exception. No further information available.\"I had the same error in a custom script I was writing so I decided to test the issue with the provided python script. I test the c++ version of HelloDrone and it runs simGetImages just fine.I am running python3.6 on Ubuntu 20.04. My python packages are up to date and my Airsim install should be as well. I very recently upgraded from Ubuntu 18.04 to Ubuntu 20.04, but I never ran the python versions on 18.04, so I can't say if they worked before the upgrade.Any help would be appreciated!", "type": "commented", "related_issue": null}, {"user_name": "sudo-hello-world-2", "datetime": "Jan 25, 2022", "body": "Hi .\nThe Code above works fine fine for me. Are you sure \"0\" is your camera name?", "type": "commented", "related_issue": null}, {"user_name": "zimmy87", "datetime": "Jan 31, 2022", "body": "This might be related to , can you apply the fix in  and see if that fixes the issue for you?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 31, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4210", "issue_status": " Open\n", "issue_list": [{"user_name": "EatingEdu", "datetime": "Dec 10, 2021", "body": "{\n\"SettingsVersion\": 1.2,\n\"SimMode\": \"Multirotor\",\n\"PawnPaths\":{\n\"DefaultQuadrotor\": {\"PawnBP\": \"Class'/AirSim/DJI450/BP_FlyingPawn.BP_FlyingPawn_C'\"}\n},\n\"Vehicles\": {\n\"PX4\": {\n\"VehicleType\": \"PX4Multirotor\",\n\"QgcHostIp\": \"127.0.0.1\",\n\"QgcPort\": 14550,\n\"LocalHostIp\": \"192.168.43.64\",\n\"DefaultSensors\": {\n\"Barometer\": {\n\"SensorType\": 1,\n\"Enabled\" : true,\n\"PressureFactorSigma\": 0.001825,\n\"PressureFactorTau\": 3600,\n\"UncorrelatedNoiseSigma\": 2.7,\n\"UpdateLatency\": 0,\n\"UpdateFrequency\": 50,\n\"StartupDelay\": 0},\n\"#SubWindows\": [\n{\"WindowID\": 1, \"ImageType\": 0, \"CameraName\": \"l\", \"Visible\": true}\n]\n}I want test my control algorithm through the px4 and airsim ;\nwhen i use api moveByMotorPWMsAsync() send the pwm to  airsim,  i find the airsim env has not response.", "type": "commented", "related_issue": null}, {"user_name": "EatingEdu", "datetime": "Dec 11, 2021", "body": "in the log file ,it is a record like this:\n[2021.12.11-07.33.12:331][354]LogTemp: Not Implemented: commandMotorPWMs", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 13, 2021", "body": " moveByMotorPWMsAsync() is a low-level API. I don't see why you would use it with PX4. What are you trying to do?", "type": "commented", "related_issue": null}, {"user_name": "EatingEdu", "datetime": "Dec 14, 2021", "body": " I trained a model by RL, the print of the net is the pwm number, so I should use this API to run. I used it in software , but when i change the settings.json for px4 ,this api does not work. I do not know what is wrong.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 14, 2021", "body": "If you select PX4 then the pwm signals come from the controller... Setting the pwm signals only works with SimpleFlight as \"flight controller\".\nWhy do you need PX4?", "type": "commented", "related_issue": null}, {"user_name": "EatingEdu", "datetime": "Dec 14, 2021", "body": "I need PX4 to get the mavros message to do the HIL.  So I think px4 and pwm api can not use at the same time in the airsim , is it true?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 14, 2021", "body": "That is correct. You would need to implement  for the mavlinkmultirotorsimapi class", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 13, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5626", "issue_status": " Open\n", "issue_list": [{"user_name": "Dhruva678", "datetime": "Jul 28, 2022", "body": "import glob\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport cv2try:\nsys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\nsys.version_info.major,\nsys.version_info.minor,\n'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\nexcept IndexError:\npassimport carlaIM_WIDTH = 640\nIM_HEIGHT = 480def image(image):\nmatrix_representational_data = np.array(image.raw_data)\nreshape_of_image = matrix_representational_data.reshape((IM_HEIGHT, IM_WIDTH, 4))\nlive_feed_from_camera = reshape_of_image[:, :, :3]\nimage = cv2.cvtColor(live_feed_from_camera, cv2.COLOR_BGR2GRAY)\ncv2.imshow(\"\", image)\ncv2.waitKey(1)\nreturndef camera(get_blueprint_of_world):\ncamera_sensor = get_blueprint_of_world.find('sensor.camera.depth')\ncamera_sensor.set_attribute('image_size_x', f'{IM_WIDTH}')\ncamera_sensor.set_attribute('image_size_y', f'{IM_HEIGHT}')\ncamera_sensor.set_attribute('fov', '70')\nreturn camera_sensoractor_list = []\ntry:finally:\nprint('destroying actors')\nfor actor in actor_list:\nactor.destroy()\nprint('done.')", "type": "commented", "related_issue": null}, {"user_name": "Dhruva678", "datetime": "Jul 28, 2022", "body": "", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5591", "issue_status": " Open\n", "issue_list": [{"user_name": "yeapzexin", "datetime": "Jul 16, 2022", "body": "Hi there,I am currently using CARLA 0.9.12 to do the Autonomous Driving Software Project. I am trying to find the default maximum acceleration and deceleration of the spawn vehicle I am controlling. The config.json file that I used to spawn vehicle with via this command (roslaunch carla_ros_bridge carla_ros_bridge_with_example_ego_vehicle.launch) is shown as below{\n\"objects\":\n[\n{\n\"type\": \"sensor.pseudo.traffic_lights\",\n\"id\": \"traffic_lights\"\n},\n{\n\"type\": \"sensor.pseudo.objects\",\n\"id\": \"objects\"\n},\n{\n\"type\": \"sensor.pseudo.actor_list\",\n\"id\": \"actor_list\"\n},\n{\n\"type\": \"sensor.pseudo.markers\",\n\"id\": \"markers\"\n},\n{\n\"type\": \"sensor.pseudo.opendrive_map\",\n\"id\": \"map\"\n},\n{\n\"type\": \"vehicle.tesla.model3\",\n\"id\": \"ego_vehicle\",\n\"spawn_point\": {\"x\": -77.9, \"y\": -17.59, \"z\": 0.2, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 90.0},\n\"sensors\":\n[\n{\n\"type\": \"sensor.lidar.ray_cast\",\n\"id\": \"vlp16_1\",\n\"spawn_point\": {\"x\": -0.5, \"y\": 0.0, \"z\": 1.7, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"range\": 100,\n\"channels\": 16,\n\"points_per_second\": 300000,\n\"upper_fov\": 15,\n\"lower_fov\": -15,\n\"rotation_frequency\": 20,\n\"noise_stddev\": 0.0\n},\n{\n\"type\": \"sensor.camera.depth\",\n\"id\": \"depth_middle\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"fov\": 90.0,\n\"image_size_x\": 400,\n\"image_size_y\": 70\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_front\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_view\",\n\"spawn_point\": {\"x\": -4.5, \"y\": 0.0, \"z\": 2.8, \"roll\": 0.0, \"pitch\": 20.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0,\n\"attached_objects\":\n[\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n},\n{\n\"type\": \"sensor.camera.rgb\",\n\"id\": \"rgb_top\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 6.0, \"roll\": 0.0, \"pitch\": 90.0, \"yaw\": 0.0},\n\"image_size_x\": 800,\n\"image_size_y\": 600,\n\"fov\": 90.0,\n\"attached_objects\":\n[\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n},\n{\n\"type\": \"sensor.other.imu\",\n\"id\": \"imu\",\n\"spawn_point\": {\"x\": 2.0, \"y\": 0.0, \"z\": 2.0, \"roll\": 0.0, \"pitch\": 0.0, \"yaw\": 0.0},\n\"noise_accel_stddev_x\": 0.0, \"noise_accel_stddev_y\": 0.0, \"noise_accel_stddev_z\": 0.0,\n\"noise_gyro_stddev_x\": 0.0, \"noise_gyro_stddev_y\": 0.0, \"noise_gyro_stddev_z\": 0.0,\n\"noise_gyro_bias_x\": 0.0, \"noise_gyro_bias_y\": 0.0, \"noise_gyro_bias_z\": 0.0\n},\n{\n\"type\": \"sensor.other.gnss\",\n\"id\": \"gnss\",\n\"spawn_point\": {\"x\": 1.0, \"y\": 0.0, \"z\": 2.0},\n\"noise_alt_stddev\": 0.0, \"noise_lat_stddev\": 0.0, \"noise_lon_stddev\": 0.0,\n\"noise_alt_bias\": 0.0, \"noise_lat_bias\": 0.0, \"noise_lon_bias\": 0.0\n},\n{\n\"type\": \"sensor.other.collision\",\n\"id\": \"collision\",\n\"spawn_point\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0}\n},\n{\n\"type\": \"sensor.other.lane_invasion\",\n\"id\": \"lane_invasion\",\n\"spawn_point\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0}\n},\n{\n\"type\": \"sensor.pseudo.tf\",\n\"id\": \"tf\"\n},\n{\n\"type\": \"sensor.pseudo.objects\",\n\"id\": \"objects\"\n},\n{\n\"type\": \"sensor.pseudo.odom\",\n\"id\": \"odometry\"\n},\n{\n\"type\": \"sensor.pseudo.speedometer\",\n\"id\": \"speedometer\"\n},\n{\n\"type\": \"actor.pseudo.control\",\n\"id\": \"control\"\n}\n]\n}\n]\n}I do know that in this link: \nconsists of max acceleration and deceleration adjustment via Carla_Ackermann_Control.EgoVehicleControlInfo.msg . However, I am not using Carla_Ackermann_Control at all so what is the way to know the default value of max acceleration and deceleration of the vehicle that is spawn with config.json file as above?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5560", "issue_status": " Open\n", "issue_list": [{"user_name": "wujingda", "datetime": "Jul 4, 2022", "body": "Hi everyone,Recently we use the Roadrunner to generate a new map and import it into CARLA. The map works well with cameras but becomes weird when facing the lidar. Specifically, I use the open3d to visualize the lidar data, and the lidar data in this new map does not show the line (ray cast) of the ground. The lidar data in our new map (figure 1) and in an official map (figure 2) are provided for comparison. BTW, the lidar setting is invariant across maps.\nDoes anyone have ideas about the issue? Is it because of the road texture of the map or something... we are stuck by this issue for days. Many thanks.CARLA version: 0.9.13\nPlatform/OS: Win10\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5547", "issue_status": " Open\n", "issue_list": [{"user_name": "omega1497", "datetime": "Jun 29, 2022", "body": "'''>>> Creating camera {'x': -0.2, 'y': -0.55, 'z': 1.65, 'roll': 0, 'pitch': -10, 'yaw': 270, 'width': 1024, 'height': 768, 'fov': 90, 'sensor_label': 'camera8', 'sensor_type': 'camera'}", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5536", "issue_status": " Open\n", "issue_list": [{"user_name": "yanmiao2", "datetime": "Jun 23, 2022", "body": "Hi Developers,We are on Ubuntu 20.04 and using Carla 0.9.13. We are trying to understand how Depth Camera gives out the depth info in Carla.Here's what we did:However, we observed that the depth calculated is always around 9.3m. Although it's not far from the Ground Truth Distance, it makes us curious about how Depth Camera in Carla is calculating Distance(the only way we can think of is through ground truth, but then why would there be an error?). We check this  but doesn't quite understand.Hope someone can explain this for us, thanks!", "type": "commented", "related_issue": null}, {"user_name": "yanmiao2", "datetime": "Jun 23, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5520", "issue_status": " Open\n", "issue_list": [{"user_name": "xiao6768", "datetime": "Jun 16, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "xiao6768", "datetime": "Jun 20, 2022", "body": "Good news! This issue has been resolved . We are planning to merge into main branch, need approve from company.\nAny one please feel free to contact me if you have any questions.", "type": "commented", "related_issue": null}, {"user_name": "shr1997", "datetime": "Jul 1, 2022", "body": "I've been looking for it for a long time. This is exactly what I need. Is it convenient to share the checkboard map or code? Thank you very much", "type": "commented", "related_issue": null}, {"user_name": "AbanobSoliman", "datetime": "Jul 1, 2022", "body": "Hello everyone,\nAs part of our recent , we needed to have exact (calibrated) values for all CARLA cameras' intrinsic parameters (RGB, Depth, DVS). Accordingly, we added calibration targets (Checkerboard and AprilGrid) to the Town3 map, and calibrated RGB, and DVS cameras using SOTA algorithms. Check out the repo here: .\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "shr1997", "datetime": "Jul 4, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "xiao6768", "datetime": "Jul 4, 2022", "body": "Firstly, My compile Env. is ubuntu18.04, carla 0.9.10, and UE4 is 4.24.3.\nYou have to following get UE4 source code compile OK, then compile Carla source code.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5502", "issue_status": " Open\n", "issue_list": [{"user_name": "SoMuchSerenity", "datetime": "Jun 10, 2022", "body": "CARLA version: 0.9.12, package installation\nPlatform/OS: Windows 11\nProblem you have experienced: No rendering option still outputs camera images\nWhat you expected to happen: No rendering option should return no camera resultsSteps to reproduce:\nI attached a RGB camera to my car and I have triedand also the scripts  to disable the rendering mode, yet still got image output in my designated directory. Just wonder did I do something wrong or is it a possible bug?", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Jun 28, 2022", "body": "The no rendering mode suppresses the generation of the spectator render target. Spawning a camera creates an extra render target for that camera, that's why you still get output images. But why would you add a camera in no rendering mode?", "type": "commented", "related_issue": null}, {"user_name": "SoMuchSerenity", "datetime": "Jul 1, 2022", "body": ". Hi Scholz, the reason I added a camera in no rendering mode is that according to this tutorial:  , any camera or GPU sensor should return empty data. It also says UE engine would skip everything regarding graphics, so I was experimenting with it.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5483", "issue_status": " Open\n", "issue_list": [{"user_name": "k-nayak", "datetime": "Jun 1, 2022", "body": "Hello all,I have implemented a lane detection model to detect lanes in carla and it provides a list of xy pixel coodinates. I want to check if it is possible to get the XY map coordinates (in my case town 07) to generate the trajectory along the way for the car to drive. And if it is possible what kind of things are supposed to be taken care of.\nThe green lane is the lane i am working on and if any one can help me out as to how to get the map coordinates of the points, it would be a great help. Thank you in advance for your time and advice.Carla version: 0.9.11\nOS: Windows\nCode in Python", "type": "commented", "related_issue": null}, {"user_name": "k-nayak", "datetime": "Jun 2, 2022", "body": "Hello ,Could you please give me some feedback on if Is it possible to use the following functions to achieve this?carla.Location and carla.GeoLocationIf I have the location of the car and the points in meter wrt to the camera?Thanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "k-nayak", "datetime": "Jul 5, 2022", "body": "Hello all, i have figured some aspects of my problem so far. Instead of the above mentioned functions i have used rgb_camera.get_transform.get_matrix to get the pose of the camera wrt to world. The camera position and the lane points close to the vehicle have good coordinate values that match the vehicles trajectory location, although as the lane points go farther the coordinate values move away from the trajectory. I am also not sure if the vehicle location z=0.027961 , camera position at a height of 1.5 from the road surface has z= 1.527961, But the lane points after transformation has z values ranging from  This is somewhat unexpected, could some one explain or give any comment on this? Would mean a lot if someone can clear the doubt.Thanks", "type": "commented", "related_issue": null}, {"user_name": "k-nayak", "datetime": "Jul 5, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5455", "issue_status": " Open\n", "issue_list": [{"user_name": "knelk", "datetime": "May 19, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "jhidalgocarrio", "datetime": "May 19, 2022", "body": "Yes, I also encountered the same effects. Significantly, the aliasing effect at a low image resolution.\nHere is a video in Town10: \nI thought this issue was solved in the newest releases. Any idea ", "type": "commented", "related_issue": null}, {"user_name": "knelk", "datetime": "May 19, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5417", "issue_status": " Open\n", "issue_list": [{"user_name": "tecena", "datetime": "May 6, 2022", "body": "Hello,Carla supports a number of sensors Camera,Radar,Lidar etc. Each sensor has a number of attributes as mentioned in the link  but only the default values of attributes are mentioned in the doc, but we want to know the range of the attributes of all sensors, Where are the value ranges of attribute mentioned, From where should we get it.Thank you", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5367", "issue_status": " Open\n", "issue_list": [{"user_name": "zxiaomzxm", "datetime": "Apr 15, 2022", "body": "CARLA version: 0.19.13\nPlatform/OS: ubuntu 20.04\nProblem you have experienced:\nI use instance segmentation camera in , but capture strange results:\nrgb image\n\ninstance image\n\nIt seems that the instance camera can not see the mound in the right side and ray casting just pass through it.\nOther maps has no such phenomenon.", "type": "commented", "related_issue": null}, {"user_name": "serwansj", "datetime": "May 6, 2022", "body": "i am experiencing the same issue right now. did you manage to fix this somehow ?", "type": "commented", "related_issue": null}, {"user_name": "XGodina", "datetime": "May 24, 2022", "body": "Hi , You wrote \"CARLA version: 0.19.3\", do you mean the \" CARLA version 0.9.13\", right?. I will take a look. Thank you for letting us know", "type": "commented", "related_issue": null}, {"user_name": "zxiaomzxm", "datetime": "Jun 1, 2022", "body": "yep, it's 0.9.13. And I found town04 have similar problem.\n\n", "type": "commented", "related_issue": null}, {"user_name": "ghh116553", "datetime": "Jun 22, 2022", "body": "I meet the same issue. InstanceCamera can not show the landscape correctly.", "type": "commented", "related_issue": null}, {"user_name": "XGodina", "datetime": "May 24, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "XGodina", "datetime": "May 24, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5362", "issue_status": " Open\n", "issue_list": [{"user_name": "Chup123", "datetime": "Apr 13, 2022", "body": "CARLA version: 0.9.12\nPlatform/OS: ubuntu 18.04\nProblem you have experienced: When I move the mouse in any direction, the camera will always do the same thing. It will look at the ground and spin to the right. Even when I try to move the mouse somewhere else.\nWhat you expected to happen: When i point the mouse up, I except the camera to move to the position I put my mouse, just like in a video game\nSteps to reproduce: Move the mouse in any map. It will always move in this direction\nOther information (documentation you consulted, workarounds you tried): I tried to lower to mouse sensitivity in the DefaultInput.ini file. This made it much more clear that the mouse always moves in this direction", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5321", "issue_status": " Open\n", "issue_list": [{"user_name": "makaveli10", "datetime": "Apr 4, 2022", "body": "Hello,\nI am trying to generate 3D bounding box from ,  and . Turns out somethings is missing from what I have put together as the boxes I get are disoriented.\nThis is what I am using right now with the .", "type": "commented", "related_issue": null}, {"user_name": "makaveli10", "datetime": "Apr 11, 2022", "body": " You seem to have solved this on . So, if you could throw some pointers on how to make progress here, that would be very helpful.\nThanks", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5254", "issue_status": " Open\n", "issue_list": [{"user_name": "joako1991", "datetime": "Mar 14, 2022", "body": "CARLA version: latest\nPlatform/OS: LinuxHi everybody,I would like to know if there is any efforts in course or consideration to include the polarization state of the light as part of the simulation engine. I work with DoFP RGB cameras, and I would like to test the simulator with this modality.If there is not, could you kindly guide me where should I look at in order to implement it myself ?Thank you very much in advance!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5227", "issue_status": " Open\n", "issue_list": [{"user_name": "JiulongGao", "datetime": "Mar 3, 2022", "body": "In the Carla simulator, when the manual control mode is turned on and the camera picture is saved, the screen will be stuck and the frame drop will occur.What should be done?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5223", "issue_status": " Open\n", "issue_list": [{"user_name": "ll7", "datetime": "Mar 1, 2022", "body": "Hello,I wondered why the  uses two different ways to destroy an actor:Is this redundant because we appended the camera to the ?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 30, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 30, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5193", "issue_status": " Open\n", "issue_list": [{"user_name": "pespen", "datetime": "Feb 22, 2022", "body": "I am trying to use the Carla lidar sensor to create 2D images from 3D point cloud data(similarly to how Ouster does it), and have so far been successful with the range and intensity images. Ouster also creates what they call Ambient images, where they somehow use the reflectivity that they get from their sensor to create a more life-like image. Is it possible to get this information from Carla? I basically need to know the number of photons over time the sensor registers. See the image for reference", "type": "commented", "related_issue": null}, {"user_name": "GruNyv", "datetime": "Mar 3, 2022", "body": "Hi, sounds like we are working on pretty similar problems. However, note that the ambient images are not the same as the reflectivity image. The ambient image are created based on the photons coming from other sources than the lidar (the sun), while the reflectivity images are created by using the inverse square law for the intensity. Btw, could I ask you if you have got any realistic results for the intensity image in Carla? From my experience the simulated values are pretty unrealistic for the intensity image you get from the Ouster sensor.", "type": "commented", "related_issue": null}, {"user_name": "pespen", "datetime": "Mar 3, 2022", "body": "Alright, that makes sense about the reflectivity and ambient images. My images are not as good as the ones from Ouster, I'm not sure if it's because they do more processing or if they just have more data available. Here are some examples of range and intensity.\n\nThis is part of my Graduate degree, and my supervisor said that these images are good enough for our purpose. I might try to adjust the values closest to the camera as it is a bit hard to spot objects there", "type": "commented", "related_issue": null}, {"user_name": "GruNyv", "datetime": "Mar 3, 2022", "body": "Ouster does not project the point cloud from 3D space to 2D as I assume you do here to create the 2D image. Instead they use the values that each pixel in the sensor itself measures, since these are organized into an array of num_channels * width. The range image however contains holes, as they are not able to calculate the range for noisy points, but they still got the intensity and ambient value from the sensor.", "type": "commented", "related_issue": null}, {"user_name": "GruNyv", "datetime": "Mar 3, 2022", "body": "Btw, if you want we could talk together about this problem on an other platform, since I do not want to talk about all parts of my thesis in public. Could be interesting to get some other input", "type": "commented", "related_issue": null}, {"user_name": "pespen", "datetime": "Mar 3, 2022", "body": "Sure! Shoot me an email at , and we can take it from there (probably use Discord or something if that is OK)", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 2, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5163", "issue_status": " Open\n", "issue_list": [{"user_name": "ValeriaProiettiDante", "datetime": "Feb 14, 2022", "body": "Hello everyone!I have an Ubuntu system with an NVIDIA 3080ti with CARLA and Unreal Engine 4.26.When CARLA is not in server mode, I have 60fps, but when I run the server, the fps go down to 8.I have already tried to uncheck the \"Use less CPU\" parameter in the Unreal editor.Anyone have any suggestions?Thank you", "type": "commented", "related_issue": null}, {"user_name": "MunchZkin", "datetime": "Feb 16, 2022", "body": "I personally don't use Carla on Linux due to poor GPU support. But when you say \"run the server\" what do you mean?\nGenerally, when you have more actors in Carla you usually tend to loose fps, and these actors can be cameras, other vehicles/ walkers, streetlamps etc. So try restricting these actors needed specifically for you use case.", "type": "commented", "related_issue": null}, {"user_name": "ValeriaProiettiDante", "datetime": "Feb 17, 2022", "body": "Thanks  for reply.For \"run the server\", I mean press play on UnrealEngine editor and it starts on server mode for CARLA simulator and, in my case, ROS2.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 18, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5130", "issue_status": " Open\n", "issue_list": [{"user_name": "meso17", "datetime": "Feb 2, 2022", "body": "Carla version 0.9.5 on windows 10 python 3.7.9. But when I run the PythonAPI code examples, there is no error. but I write the code below it gives an error.camera_bp = blueprint_library.find('sensor.camera.rgb')\ncamera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\ncamera_bp.set_attribute('image_size_x', '800')\ncamera_bp.set_attribute('image_size_y', '600')\ncamera_bp.set_attribute('fov', '105')", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5102", "issue_status": " Open\n", "issue_list": [{"user_name": "cerebro19288485", "datetime": "Jan 22, 2022", "body": "Hi,\nI am trying to  to train a reinforcement learning agent. I am able to print depth but not able to store it in a variable or write onto a numpy/text file for further use.Below code lets me print Radar depth DataReports depth data like this:\nBut if i try to append Depth data to a list, its not happening. (Tried writing to numpy and text file also, but couldnt do it)Reports like this:\nI want to know if there is a way to extract radar data for every point in each frame and save it to a numpy/text file.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5100", "issue_status": " Open\n", "issue_list": [{"user_name": "kanji95", "datetime": "Jan 21, 2022", "body": "I am having trouble getting the 3D world coordinates in CARLA. Basically, I use mouse click to select the destination point (pixel coordinate) in the front-view RGB camera image and get the depth of that point through a depth sensor. Now, I get the 3D coordinate using the following,\n\nHere, K is the intrinsic camera matrix and T_cam is Camera extrinsic transformation matrix. However, the final point I get after the above transformation is not correct, it is basically a point behind the vehicle's starting position. I am unable to figure out what I am doing wrong.", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Feb 7, 2022", "body": "Correct me if I'm wrong, but I think that won't work anyway because you have zero clue as to where along its way the ray that is represented by the pixel met a surface? How do you want to extract that depth information from the image?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5078", "issue_status": " Open\n", "issue_list": [{"user_name": "werewolfdev", "datetime": "Jan 17, 2022", "body": "CARLA version: 0.9.11\nPlatform/OS: Ubuntu 18.04Problem you have experienced: Hi Team, I am trying to update the location of the ego vehicle in a scenario using location data sent from another software. The data packets are reaching at a frequency of 100 packets per second into the Python API. Hence I would like to know if I could update my location data in CARLA at 100 FPS. I tried putting  \"-benchmark -fps=100\", but the client FPS still stays around 60. I tried adjusting in config.py the delta time seconds and the end result is also not as expected. Could anyone kindly let me know if 100 FPS is possible with CARLA (I am not using any Cameras or any other sensors as of now) and if yes, what is missing from my end. Thanks a lot.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1946", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "Jul 15, 2022", "body": "With point cloud visualization coming soon (cf ), we should probably add  types capturing the default formats that our 3D visualizer will expect.Note that an alternative would be to simply expect users to use the existing  and  types and add the 3D-specified info as dynamic attributes of same/similar names as below.Here's some proposed formats, inspired by :", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jul 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#752"}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5026", "issue_status": " Open\n", "issue_list": [{"user_name": "SM1991CODES", "datetime": "Jan 3, 2022", "body": "I have gone through the basic setup and have a working simulation where there are 50 cars and my ego vehicle.\nI have a lidar mounted which I have configured at 128 channels.\nI have also configured the rotation frequency to 100 => 1ms per rotation.\nTherefore, I expect the lidar to generate a frame of point clouds every 1ms. This should in turn trigger my data collection callback, is that correct?So the idea is, if my sensor frame rate is higher than simulation frame rate (which I assume is < 100 FPS in this case), I should always get a full 360 degree frame each time the callback is triggered, is that right?I see this:(The white box is made by me)., then I had an idea and removed the power supply of my laptop, it seems, the simulator now runs at a slower speed leading to actual full frames. I see this:Does this look correct?\nSuprisingly, I still do not see any cars, even though I have 50 cars in the simulation.,I get only about 30k points on average whereas I have configured the lidar for 120k points.\nI have also reduced drop-off (please see code).Any idea what could be going wrong here?Please help!!!\nPlease find the code here:`\nimport carla\nimport numpy as np\nimport time\nimport carla_configs as ccssensor_dict = {\"imu\": \"sensor.other.imu\",\n\"gps\": \"sensor.other.gnss\",\n\"camera_rgb\": \"sensor.camera.rgb\",\n\"lidar\": \"sensor.lidar.ray_cast\"}lidar_data_buffer = []class CarlaSensors:class CarlaManager:if  == '':`", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5024", "issue_status": " Open\n", "issue_list": [{"user_name": "441599828", "datetime": "Jan 3, 2022", "body": "CARLA version:0.9.13\nPlatform/OS:Ubuntu20.04\nProblem: the parameters in sensor.camera.rgb, I changed the parameters fstop and focal_distance but nothing changed in the Images,\nfstop=0.8 focal_distance=100\n\nfstop=8 focal_distance=1000\n\nin ue4 doc, it suppose to have the blur effect for the background ( )\n\nExpected to happen: to have a bigger aperture, shallower depth field as mentioned in ue4 docSteps to reproduce:", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 17, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3131", "issue_status": " Open\n", "issue_list": [{"user_name": "NielsRogge", "datetime": "Oct 21, 2021", "body": "Instructions to add a new dataset can be found .", "type": "commented", "related_issue": null}, {"user_name": "dnaveenr", "datetime": "Mar 22, 2022", "body": "I think we can close this issue since PR  solves this.", "type": "commented", "related_issue": null}, {"user_name": "NielsRogge", "datetime": "Oct 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Dec 8, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-skainet/issues/58", "issue_status": " Open\n", "issue_list": [{"user_name": "ValeraDanger", "datetime": "Aug 14, 2022", "body": "I am using TTGO T-camera  with I2S microphone. It's pinout:This module has ESP32 (not S3) chip, so i am using . I'm trying to change i2s pins. I've founded  file, where I changed  block (not , cause it seems to be second mic). This didn't work, so I disabled all useless blocks (change  parametrs to zero). Here you can see my actual full  file:This didn't help too. I appended  in  file in  function. to see, is there any data from my mic. . There is this func's code:But monitor is empty\n\nSo, there is not any data from my microphone,\nAlso, I've noticed, this warning\n\nI think, the problem is the absence of any codec on my board (before, I took data directly from mic by DMA (this sample worked for me   ))How can I tune this code to force it working on this board?", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Aug 15, 2022", "body": "I think you should refer to  to modify your board.", "type": "commented", "related_issue": null}, {"user_name": "ValeraDanger", "datetime": "Aug 15, 2022", "body": "I have renamed  to  and tryied to flash my board. While flashing, I tooked this error, cause my chip is not S3.\n", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Aug 23, 2022", "body": "I mean you can refer to  to modify I2S setting.\nThe microphone of esp32_korvo_v1_1 is analog microphone.\nThe microphone of esp32s3-sys is digital microphone and I guess your mircophone is digital microphone too.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 14, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/839", "issue_status": " Open\n", "issue_list": [{"user_name": "loganlebanoff", "datetime": "Nov 11, 2020", "body": "I noticed that the XSum dataset has no space between sentences. This could lead to worse results for anyone training or testing on it. Here's an example (0th entry in the test set):", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5082", "issue_status": " Open\n", "issue_list": [{"user_name": "ivannson", "datetime": "Jan 18, 2022", "body": "I was wondering how the collision sensor can be configured to work in synchronous mode?I added it to the  alongside other sensors and added some exception handling to deal with empty queue when there is no data (since there are no collisions), but adding the collision sensor that way makes the simulation run really really slowly (dropping from 20 fps to 0 fps).This is what I'm currently using (I took bits which are not relevant to the collision sensor out)", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/229", "issue_status": " Open\n", "issue_list": [{"user_name": "Zikingz", "datetime": "Aug 2, 2018", "body": "Hi. I am a new one here. It is a awesome lib for CV, .\nNow,  I am encountering a problem that I trained  on my own dataset, the dataset has . After training, I got a  file. When I want to load the parmeters and predict on new images, it return an error, logs as it,\n\nThe command I used as follow,\n\n\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "Zikingz", "datetime": "Aug 3, 2018", "body": "And could you please add a function draw boxes on image then return this image? I found it is difficult for me to use cv2.imshow() show processed images in real time detection.\nThank you.", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Aug 3, 2018", "body": "because you are assigning your customized weights to voc models, the shape of class prediction is different.\nYou can load the same net in your training script.I have added a convenient getter for custom dataset just merged in masterYou can use it to create a custom network quickly.", "type": "commented", "related_issue": null}, {"user_name": "Zikingz", "datetime": "Aug 8, 2018", "body": "Thank you for your useful reply :)", "type": "commented", "related_issue": null}, {"user_name": "Zikingz", "datetime": "Aug 8, 2018", "body": "Thank you again for your reply, that really solved my problem.\nBut there is a new question that I wanna use this trained model make a webcam real-time detection. I use opencv to get the images and call cv2.imshow() to plot images with boxes.\nHow can I get the image matrix with boxes in real time?\nThanks.\n", "type": "commented", "related_issue": null}, {"user_name": "ZengyuanYu", "datetime": "Dec 4, 2018", "body": "@xDooooot Hi LaoGe, I trained SSD and get some params, when I load it and predict my test image, it is pick a issue, please you upload some code about you?", "type": "commented", "related_issue": null}, {"user_name": "Zikingz", "datetime": "Dec 12, 2018", "body": "Here is what I used.\n``\n希望能帮到你 :)", "type": "commented", "related_issue": null}, {"user_name": "Zikingz", "datetime": "Dec 12, 2018", "body": "", "type": "commented", "related_issue": null}, {"user_name": "ZengyuanYu", "datetime": "Dec 13, 2018", "body": "@xDooooot  Thank you! I got it though others ask. But use camera also funny to try it.", "type": "commented", "related_issue": null}, {"user_name": "FAFACHR", "datetime": "Mar 26, 2019", "body": "Hello, I'm new in computer vision field. I'm using gluoncv to do instance segmentation with mask-rcnn but i have some problems.\nI did my own dataset similar to coco format and I trained with this model \"mask_rcnn_resnet50_v1b_coco\", but my dataset has just 2 classes. After training i got the parameter file and when i loaded the parameters and predict a new image, the classification was wrong and still loaded all classes of the coco dataset (80 classes). So is there any solution to customize the number of classes?", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Aug 3, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Aug 28, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "narutobns", "datetime": "Jun 29, 2019", "body": [], "type": "issue", "related_issue": "#844"}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1760", "issue_status": " Open\n", "issue_list": [{"user_name": "brimoor", "datetime": "May 20, 2022", "body": "In many use cases, it makes sense for a  to have multiple versions of its media. For example:Note that we're not talking about multiple views of the scene (eg left vs right camera) here, we're talking about different versions of the exact same scene. Any  labels, for example, would be renderable on any version of the media.This could be achieved by adding a  subtype of  that allows the user to declare that the field contains a media path that should be included in a dropdown menu in the App that controls which media field is being used to source media files.The required  field would have this type, but so could new fields:Additional elementsShould the dropdown go under settings? Or somewhere else?", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jun 2, 2022", "body": " Here's some pseudocode on how this feature could be exposed in the Python API:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "May 20, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "ritch", "datetime": "Jun 1, 2022", "body": [], "type": "pull", "related_issue": "#1811"}, {"user_name": "brimoor", "datetime": "Jun 1, 2022", "body": [], "type": "issue", "related_issue": "#1520"}, {"user_name": "ritch", "datetime": "Jun 2, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "benjaminpkane", "datetime": "Jun 15, 2022", "body": [], "type": "pull", "related_issue": "#1841"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4801", "issue_status": " Closed\n", "issue_list": [{"user_name": "RafaelsNeurons", "datetime": "Aug 11, 2022", "body": "Hey :),\nI‘m trying to Train a 3D object detection Model with my custom 3D dataset created with cvat.\nBut I‘m confused I exported in Kitti formate but was Not able to do transfer-learning with it.\nCan somebody recomment a model which takes the exported Data as training?Thanks :)", "type": "commented", "related_issue": null}, {"user_name": "holtvogt", "datetime": "Aug 12, 2022", "body": "Usually, in order to train a KITTI-based model, the KITTI format requires label files for each point cloud or image frame in the following structure:000042.txt:With that being sad, the information you receive from the CVAT export is in KITTI Raw Format 1.0 which will generate a  file from which you need to convert the corresponding tag values to the KITTI format mentioned above. I recently created an issue to make sure I understood that properly. You can have a look . I hope that helps! :)", "type": "commented", "related_issue": null}, {"user_name": "RafaelsNeurons", "datetime": "Aug 14, 2022", "body": "Thank you very much! This was exactly what I was looking for. I will also try it with MMDETECTION3D.So the worksteps are:Or did I forgot a step ? :)Again, thank you very much!", "type": "commented", "related_issue": null}, {"user_name": "holtvogt", "datetime": "Aug 14, 2022", "body": "Correct. The CVAT export always provides a  which basically contains a dictionary with frame index in ascending order and its corresponding  file name as value. That comes in handy when trying to automate that conversion. I have already written a tracklet parser based on the CVAT export. You can find the link .That's actually a huge point as you'll have to dig deep into the documentation and write your own dataset configuration. But yes, that's also correct.", "type": "commented", "related_issue": null}, {"user_name": "RafaelsNeurons", "datetime": "Aug 17, 2022", "body": "Thank you very much, you are a hero!\nI really appreciate that!", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 25, 2022", "body": "That is not exactly CVAT issue, but I hope you were helped :)", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 25, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4781", "issue_status": " Open\n", "issue_list": [{"user_name": "copaah", "datetime": "Jul 27, 2022", "body": "Support rosbag format as input data source. Allow the annotator to skip through Image msgs for image/video annotation and write the annotation back into the rosbag using appropriate ROS messages.Currently you will need to extract images and annotate these and then manually write back into rosbags.Utilize the rosbag python API to load images and use appropriate ROS msgs for the supported annotation types.it is error prone and tidious to dump images from rosbags and load to CVAT and then subsequently write those annotations back into the rosbag. Annotating directly from the rosbag is less error prone and also allows to utilize other sensory information in the future such as multiple-view cameras, kinematics, IMUs etc.", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Aug 29, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Aug 29, 2022", "body": [], "type": "added this to", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4762", "issue_status": " Open\n", "issue_list": [{"user_name": "Steven-m2ai", "datetime": "Jul 15, 2022", "body": "Hello,I have images taken by a top down camera (~45 degrees looking downwards). As such, the objects have a tilt angle, and the front face of the cuboid thus will not be a \"perfect rectangle\" (it is distorted because of the object's tilt).Is there any way to label this using the cuboid?Thanks", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4587", "issue_status": " Open\n", "issue_list": [{"user_name": "xiongda777", "datetime": "Apr 22, 2022", "body": "\nunion annotate is needed.when I draw a point/box in bird -eye-view pictrue,I want project the point/box to camera pictrue with camera Intrinsics and Extrinsics(or methods I customize).\nwhat I want is drawing an obstacle in  bird -eye-view pictrue,I know the position of obstacle in camera pictrue and I know whether I annotate correctly.No such functionCan you design an Interface to get union annotation？You may  channel for community support.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4303", "issue_status": " Open\n", "issue_list": [{"user_name": "kkju", "datetime": "Feb 9, 2022", "body": "When marking the 3D box with cloud points, the  view  and  the  view , can not see the car's shape clear as follow picture:The better view to see, the cuboid need to move to the car's left:How can i move the camera view to the left side instead of using the right side to view the cloud points?\nThanks for giving some advices~", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 10, 2022", "body": "Hi,3D views are constant for now, but your request is relevant. Let me mark it as a feature for the future.", "type": "commented", "related_issue": null}, {"user_name": "kkju", "datetime": "Feb 9, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "kkju", "datetime": "Feb 9, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 10, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3939", "issue_status": " Closed\n", "issue_list": [{"user_name": "bhargav-sudo", "datetime": "Nov 23, 2021", "body": "Hi,\nWhy does annotation was not there on the downloaded dataset images ..?\nWhy the structure of datasets like market1510 , yolo ,pascal VOC etc.. in CVAT looks different when compare to original..?\nCan we able to make the structure of datasets similar to the original one..?Thanks.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": " , thanks for your comments. Could you please describe in more details and give us a couple of examples? What is the difference?", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 23, 2021", "body": "Hi,lets take some of the dataset like Market-1510.\nWhen I downloaded the Market-1510 dataset from the online.\nit has a folders of Train , test  etc..I used CVAT tool for re-identification. First I had done detection and the re-identification using auto annotation.\nWhen I downloaded that dataset from CVAT. The folder has only some images(save images from the Export dataset) and txt file.So, what's the difference here..?\nand How can I make CVAT Market-1510 dataset into original one..?Thanks.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": " , could you please look at answer?", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 24, 2021", "body": "Hi guys,\nany update..?Thanks.", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Nov 24, 2021", "body": "Hi, do you use Tag= during your annotation process in CVAT?", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 24, 2021", "body": "Thanks for the reply .Am new to this. where can i find that..?", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Nov 24, 2021", "body": "If you want to get expected result of exporting into the Market-1501 you should use Label with value  and attributes: , , . Particular use case for Market-1501 dataset it's when you have cropped images with people: one person per image. And after that using CVAT you can fill information about person for each image: , , . We implemented such logic based on .It will be interesting to get more information about your use case: what kind of images do you have? what types of annotation objects did you try to use in CVAT for getting Market-1501 dataset?", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 24, 2021", "body": "Thanks,I just annotated the data with person detection and next with reidentification.\nThen i exported the dataset from the option that you had provided.\nI didn't attached any tags.I want the dataset like you mentioned in above.", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 24, 2021", "body": "First thing I want to know how to set these tags that you said in the above message.Coming to our use case. We need the dataset from CVAT like the original one from the online.", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 24, 2021", "body": "HI, Is this is the thing that you are trying to mention.Let me know if it is wrong or right..?\nIf right what needs to be update in query attribute..?\nif right what needs to do after...?Thanks,", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Nov 24, 2021", "body": "It's better to specify type for attributes, try to use this labels definition in Raw (instead of Constructor):Also don't forget to change values for attributes  and  according your dataset (convention for list of values: )Once you create a task with a label definition like this, you can annotate your images with tags, see the documentation to read more about this   and about the attribute annotation mode .", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 25, 2021", "body": "Thanks .\nWill do this and let you know.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 25, 2021", "body": "HI \nIt works.I had a doubt what does the folder \"query\" called..?\nHow it get structured..? Does it selects one random photo from the video or it selects one random photo from one person (like 2 photos for 2 persons).Does CVAT provides any option to crop the images. ? If not how can you crop them ..?Thanks.", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 25, 2021", "body": "HI . I had tried adding my own dl models into nuclio.\nCVAT uses openvino_2020.2 version of models. I modified the code to use other versions of openvino or other type of models in the same version. (like 0031 reidentiication model instead of 0300).\nI tried for detection models too. The function has deployed completely.But it didn't got added to the list of CVAT models.\nAny reasons and how to solve it.?\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "zhiltsov-max", "datetime": "Nov 28, 2021", "body": "You can find a simple explanation of query and gallery subsets in ReID context  and . Basically, \"query\" subset means \"identities to be classified\".", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 28, 2021", "body": "Thanks .So, for every person we have one query image to match with the gallery images.", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Nov 29, 2021", "body": "Hi. Any updates..?Thanks", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Dec 9, 2021", "body": "Hi  we are still working on it. I just need the dataset of market-1501 from CVAT to be the same with the original one which we can get from online.\nwe are annotating on a video. (first detection model and reid model). I updated the labels as same as you said before. And I am not able get the folders like query, gt_query, gt_bbox when we export the dataset into market-1501.\nBut as you mentioned before about cropping images.\n(like taking cropped images and attaching the labels or how can i do that..?)", "type": "commented", "related_issue": null}, {"user_name": "zhiltsov-max", "datetime": "Jan 14, 2022", "body": "Hi, you could crop images with Datumaro. How would you like to crop them? By fixed coordinates, by a bbox or something else? From the conversation above, I can assume, you would like to auto-annotate images with a model and then crop by a bbox, is it correct?", "type": "commented", "related_issue": null}, {"user_name": "bhargav-sudo", "datetime": "Jan 16, 2022", "body": "Hi I want to crop the images using the data of person that i get from the CVAT tool.Yes, you are correct.\nwe need the data of person_reid to train the model.\nAnd we need the that data in the market-1501 format where it has  these folder structure.How can I get them..?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 2, 2022", "body": "Sorry for lack of response from our side. There are too many issues opened. I am trying to reduce them now and I will close this issue.Please, if the question is still relevant, let us know and do not hesitate to reopen.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "zhiltsov-max", "datetime": "Nov 29, 2021", "body": [], "type": "issue", "related_issue": "openvinotoolkit/datumaro#570"}, {"user_name": "bsekachev", "datetime": "Sep 2, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3387", "issue_status": " Open\n", "issue_list": [{"user_name": "hnuzhy", "datetime": "Jul 4, 2021", "body": "I have read and searched the official docs and past issues for the solution. No one had the same problem with me.I want to annotate the head orientation of people in 2D image with a standard 3D cube. Here, the head is a rigid object. A standard cube is defined as follows: three sides of any vertex are perpendicular to each other, and all twelve sides are equal in length, or in unit length.After labeling, we could get the eight projected vertices of the cube in the two-dimensional coordinate system. If three Euler angles (pitch, yaw, roll) are used to represent the orientation of the head, these precise projection points can be converted into corresponding angles.I have three suggestions or roadmaps for adding unit  label in the new version of CVAT.Here are two examples of 3D model interaction. The first is the rotation interaction of a 3D head model in mayavi. The interactive operation needs to rely on both mouse and keyboard. The second is to use the 3D image editing tool in Windows 10 to place and operate 3D models on 2D images. All you need to do is use the mouse.\nExample 1\nExample 2Looking forward to your reply. I will be willing to do whatever I can to advance this functional part.", "type": "commented", "related_issue": null}, {"user_name": "chiehpower", "datetime": "Jul 7, 2021", "body": "this is so cool feature ...", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 7, 2021", "body": " , I agree that we need to improve the functionality. Your explanation is really helpful. Could you please describe your research area and organization? Unfortunately my team has huge amount of requests and we already have an approximate roadmap for Q3'21 and Q4'21. Thus I'm trying to clarify details which will help me to increase the priority of the feature.", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "Jul 8, 2021", "body": "Yes, it is a pretty cool function which is not easy to realize :-(", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "Jul 8, 2021", "body": " Hi, I'm glad you agree to my proposal. I am a PhD student in computer department from SJTU University. My research field is the intersection of AI and education. The detailed research direction is object detection and pose estimation in computer vision. I would like to talk about the motivation of this question from two aspects.Recently, I've been studying the methods of attention detection for students in the classroom. Among them, head orientation (head pose estimation) is one of the key factors. However, as far as I know, the head pose estimation algorithm of multi-person in 2D image is not well developed. At present, there are some SOTA algorithms for head pose estimation of a single well cropped head, including  and . But their effect is not ideal, and it is not easy to extend to the case of multiple people in a single image. Most importantly, the datasets used by these algorithms are obtained by 3D head projection (), or the 3D Euler collected by depth camera in the experimental scene ().\nPrediction example 1 of FSA-Net (The input can only be a single person's head with visible face.)\nPrediction example 2 of FSA-Net (First, the head bbox of each person is detected by MTCNN, and then the single head is estimated. Therefore, this is not an efficient or essential multi-person head pose estimation algorithm.)\nPrediction example of WHE-Net (The input can only be a single person's head with wide range pose. The predictable yaw angle of the head is omnidirectional.)Dataset has always been the cornerstone of deep learning algorithms, so is head pose estimation. Therefore, I want to try to annotate the 3D head orientation, or three Euler angles of the head directly in the 2D image. As mentioned for the first time in this issue, the most accurate annotation scheme focuses on how to use 3D cube to interact freely on 2D images. In my opinion, once such a dataset is constructed, it will help promote the great progress of the corresponding algorithm research. For example, a bottom-up method could be designed to directly predict the pose of all heads in the image at one time. At the same time, compared with a single captured head image, the complete scene and human body information in the original image can assist more accurate head pose estimation.:After investigation, I didn't find tools with real 3D cube annotation. Fortunately, close functional options were found in CVAT. The first is . However, the new builded cuboid lacks rotation freedom. The second is . By annotating three consecutive non coplanar edges of a 3D cube approaching the head orientation, we can deduce the approximate Euler angle. Unfortunately, there is a great subjectivity error in this annotation process. We can't see the actual pose of the generated cube directly, unless we use a real 3D cube to annotate interactively. If we use this method reluctantly, the credibility of the final annotation will be questioned.Here are three examples of rough annotation results with . Images are all from the public  dataset. The object we annotate is the head with any orientation in the image, including the visible, occluded and invisible face. In many cases, the current method of  annotation is difficult and inaccurate.In a word, it is very useful to add interactive annotation of rigid 3D graphics (which can only be rotated, translated and scaled) to 2D images. In addition to supporting the head orientation marking, the new function can also be extended to the annotation of other rigid objects. After the construction of similar datasets about general objects, we can try to develop a simple and direct 3D object pose estimation algorithm only based on 2D images. We expect that this method can be comparable to estimation algorithms based on RGB-D or 3D point cloud.Finally, I am not good at giving the overall improvement framework of CVAT about this enhancement from UI design or code addition, but I am willing to do what I can. I sincerely thank CVAT's main contributors for their work, and hope to carefully consider adding this task to roadmap.", "type": "commented", "related_issue": null}, {"user_name": "Kucev", "datetime": "Apr 7, 2022", "body": "I support the request. We also have a need for such functionality.", "type": "commented", "related_issue": null}, {"user_name": "schliffen", "datetime": "May 28, 2022", "body": "This is a growing request from automotive industry as well, we need cuboid annotations to be done on RGB images not points clouds.", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "May 28, 2022", "body": "Yes, you are right. Actually, I have written a simple 2D head pose annotation tool using  last year. As shown below, the annotator can label one head with adding a bounding box in the 2D image, and adjust the 3D head model through mouse or keyboard in the right area to make it have the similar orientation/pose with boxed head. The co-existed 3D cube will be projected in the 2D image. For every appearing head pose status, we will record the corresponding Euler angles.\n\nHowever, this tool can only run in desktop, and is not fully as what I expected originally (refer above question for details). Then, I was busy on other things until now. I did not update and perfect this tool for a long time. If possible, I still look forward to seeing this annotation function in .", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 7, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/8", "issue_status": " Open\n", "issue_list": [{"user_name": "headdab", "datetime": "Jul 13, 2018", "body": "Another question and likely feature suggestion.When start a job, if I wait long enough, will all the frames be loaded into the browser?\nOr, are they loaded on demand as I seek through the video?\nAre they cached locally in memory?I'm working with 4k video and the interface isn't that usable, at least for my current use model, until all frames have been loaded.Based on the answer above, it would be great to have feedback as to whether the frames have all been loaded or, better, which frames have been loaded.   What I've seen that works well is using a different color on the seek bar for frames that have been loaded.If they are demand loaded, it would be nice to have a way to force it to load them all (as long as there's enough memory available).", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 13, 2018", "body": "No, they don't. To reduce server load it will try to preload next 500 frames and it will continue preload other frames as soon as necessary. After jump it will start preload next frames from the new position.Could you please describe your annotation use case? Another way to optimize your case is to resize the video before uploading to CVAT.I like the feature. I will add it into our internal roadmap. It should not be difficult to implement.Let's understand your use case. If you annotate with a reasonable speed (e.g. several seconds per image) CVAT should be fast enough to preload next frames for you.", "type": "commented", "related_issue": null}, {"user_name": "headdab", "datetime": "Jul 13, 2018", "body": "We're annotating sports video from stationary cameras.  Sometimes the players and ball are rather small in the resulting video making scaling the video down less desirable.  At the end of the day, we have to balance accuracy of the annotations with the time it takes to download and tag the videos.  I realize I can scale down the video, or compress the frames more, but need to understand the loading and caching strategy to determine the appropriate trade-offs.  How is the frame cache size determined?  Currently, I'm annotating 30s clips.  I'm guessing the segmentation options when creating jobs is so you can break big jobs into smaller ones, to handle related issues.The model I'm currently using, in interpolation mode, is to step through the video using at a coarse level (c/v keys) labeling the boxes for a given player.  I think that's the best way to get good per player tracks.  Then I use the scroll bar and single step to quickly slide back and forth through the video, stopping when necessary to add more keyframes to the track.We're still trying to find the most effective way to annotate these videos, but that's what we're doing so far.  I recall in the research that its easier to annotate one object at a time.  I use the filtering functionality to only show the boxes for the current object.  If you have other suggestions or ideas, please share.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 20, 2018", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 26, 2018", "body": [], "type": "issue", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 15, 2018", "body": [], "type": "issue", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 17, 2018", "body": [], "type": "issue", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 29, 2018", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2018", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 26, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "l3xis", "datetime": "Mar 1, 2020", "body": [], "type": "issue", "related_issue": "#1219"}, {"user_name": "nmanovic", "datetime": "Mar 21, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 15, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jun 22, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 29, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Sep 1, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 15, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Mar 10, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "korshunovdv", "datetime": "Aug 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "StellaASchlotter", "datetime": "Aug 3, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5423", "issue_status": " Closed\n", "issue_list": [{"user_name": "pooja-botpress", "datetime": "Sep 8, 2021", "body": "When a new Botpress instance is spun up, the first Super Admin Login should always be local", "type": "commented", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Sep 9, 2021", "body": "Hey,  Can you please provide a little more details on what you mean by \"the first Super Admin Login should always be \"?", "type": "commented", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": "What I understand: When an SSO strategy is enabled and the default auth strategy is disabled on first boot, the registration screen to create the super admin does not appear.", "type": "commented", "related_issue": null}, {"user_name": "JustusNBB", "datetime": "Oct 8, 2021", "body": "I would like the ability to disable the user/pass local strategy from the login page while keeping accessibility through APIs (CI User = Superadmin)", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Sep 9, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "laurentlp", "datetime": "Oct 13, 2021", "body": [], "type": "pull", "related_issue": "#5564"}, {"user_name": "laurentlp", "datetime": "Oct 14, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "EFF", "datetime": "Oct 22, 2021", "body": [], "type": "pull", "related_issue": "#5595"}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2170", "issue_status": " Closed\n", "issue_list": [{"user_name": "moeidsaleem", "datetime": "Jul 29, 2019", "body": "\nNo, it actually an accessibility feature, a nice to have. This can be for someone who doesn't understand English that well or prefer to listen or only limited to listening - Accessibility feature. A audio button with every message to say that message loud in the user selected language.\nWe can use say.js for mac and festival for Linux, a simplest implementation of this and for converting text, a language server would be good start to provide content.\nWe can use azure cognitive or IBM Watson or any other cloud service available for achieving the following task.\nAs i am already, working on implementing this with say.js / festival and also Microsoft azure cloud support.", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Aug 16, 2019", "body": "That's indeed pretty interesting. At the moment, we're focusing on improving NLU and flow editor. Indeed HITL modules needs some love and speech would be very nice but would require a decent amount of work to support multiple languages and to keep this available on-prem. Integrating with cloud providers would be a good start though.Please share your work and your progress.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 12, 2020", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "moeidsaleem", "datetime": "Jul 29, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 12, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Mar 13, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/14644", "issue_status": " Closed\n", "issue_list": [{"user_name": "mik-laj", "datetime": "Mar 6, 2021", "body": "Hello,Currently, documentation for the Docker image is available in the middle of the . This makes it difficult for the user to find this documentation. Also, this section is called \"Production deployment\", but this image is also used in the development environment. I think we should extract the image sections, but I'm not sure where. I am thinking of a few solutions:\na) create a new page/section in  package. Then the documentation will available as a new menu item in the current documentation package. See \"Content\" section on \nb) create a new documentation package. Then the documentation will be available as a new item on I prefer the second solution, but I am open to discussions.It is worth considering that I am working on creating a new package for the Helm Chart. See:Probably related: CC:      ", "type": "commented", "related_issue": null}, {"user_name": "ashb", "datetime": "Mar 6, 2021", "body": "Docs for the helm chart feel right as a new \"package\" (I really like that idea!) but docker image feels more like it is about/for apache-airflow so I favour option a.My thinking is that if you want to learn how to run Airflow, your going to be looking at the docs of apache-airflow for config reference etc, and to me docker sort of fits in there. I think the other reason why it feels wrong as separate doc package is because it's not a package we actually release.", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Mar 6, 2021", "body": "Keycloak publishes documentation for an image separately from the documentation for a project.\nKeycloak documentation index: \nKeycloak server container documentation: Jupyter has very extensive documentation for the image and publishes it separately also.\nDocumentation index: \nDocker image: Have you seen a project with non-trivial docker-images that describes building images in the main project documentation? I'd love to see it.", "type": "commented", "related_issue": null}, {"user_name": "ashb", "datetime": "Mar 7, 2021", "body": "Good question, I'll have a look around", "type": "commented", "related_issue": null}, {"user_name": "ashb", "datetime": "Mar 8, 2021", "body": "(Screenshots are of the section of the doc Table of Contents of the projects)Elastic(search):Prisma (v1, v2 has changed the architecture to become just a library so doesn't have a server component any longer:Keycloak is kind of a hybrid -- it's in the normal docs menu, it just links out to a README.md.It's a fairly uncommon case that projects document much about their docker file anyway from what I can see, jupyter seems to be the exception in having it's docker docks on a totally different url to the project docs from my search just now.", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Mar 9, 2021", "body": "I think many people are asking for docker image and have hard time finding it.  Separate section (option b) is fine). However  think there is one caveat with the current airflow/providers/split which it will only reinforce.Once you are in airflow documentation, it is not easily to realize that there are more components (providers and image when it is separated out). Most people will not go to the docs of airflow via url - they will reach it via Google search or history in the browser. Then - when you are already in airflow docs, there is no way of knowing that you should click on the airflow logo to get to the airflow + providers + image directory. And it is not very accessibility friendly. So while separating it out is fine, we should - i think also add some way for people. To know that provider/image documentation is there - other than clicking at the Airflow logo. Maybe a separate section in the table of content should be added ? Something that will clearly say hat You can reach out to full set of airflow- related docs this way?Not sure about name though? Additional documentation? Beyond Airflow ?", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Mar 9, 2021", "body": " Terraform has a drop down menu in the section under the link with the documentation.\nThey also have links to other documentation packages in the side menu.\n", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Mar 10, 2021", "body": "I would like to add a few comments. I am working on more docker-composer example files so that users can test more easily.\nSee: \nWhen we create a new package of documentation for all things related to Docker, it will be easier to put it together.Another thing that I am still missing is the description of how to use  together with Docker-compose. Many users try to use this together but have problems with it because  cannot be read by the  user. The simplest solution is: to use. See: So this documentation will increasingly describe how to deploy Airflow in a Docker/docker-compose environment much like the documentation for Helm Chart describes how to deploy Airflow in a Kubernetes environment.Also, note that the documentation that describes how to build a Docker image describes building an image for multiple versions of Airflow. You can use the same instruction to build the image for Airflow 1.10 and for Airflow 2.0. The only difference is the additional flag.", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Mar 6, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "ashb", "datetime": "Mar 6, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#14764"}, {"user_name": "mik-laj", "datetime": "Mar 17, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/13114", "issue_status": " Closed\n", "issue_list": [{"user_name": "atolopko-czi", "datetime": "Dec 16, 2020", "body": "The coloration of DAG operator statuses in the DAG \"Tree\" and \"Graph\" views are not easy to discern by color blind individuals. There is no textual representation of the status in the hover popup window.YesNo", "type": "commented", "related_issue": null}, {"user_name": "boring-cyborg", "datetime": "Dec 16, 2020", "body": "Thanks for opening your first issue here! Be sure to follow the issue template!", "type": "commented", "related_issue": null}, {"user_name": "eladkal", "datetime": "Dec 16, 2020", "body": "What Airflow version are you running? There has been improve in that area in 1.10.11\nsee issue , PR , Also in Graph View when you over one of the statuses in the legend it makes all the tasks in that status highlighted:\nCan you give more information? (maybe a mock up of what you expect?)", "type": "commented", "related_issue": null}, {"user_name": "atolopko-czi", "datetime": "Dec 16, 2020", "body": "Version : 1.10.10The Tree view status-hover highlighting is indeed helpful, and I was unaware! The Graph view doesn't provide the same feature, which would be helpful.  So it would still be helpful if these popups contained the operator status:\nUse of custom colors may also be able to address this for particular degrees of color blindness, which I will try.", "type": "commented", "related_issue": null}, {"user_name": "kaxil", "datetime": "Dec 17, 2020", "body": "You can customize those colors: ", "type": "commented", "related_issue": null}, {"user_name": "ashb", "datetime": "Dec 17, 2020", "body": "And if you can suggest new default colours we can look at changing them too.", "type": "commented", "related_issue": null}, {"user_name": "eladkal", "datetime": "Jan 29, 2021", "body": " Do you think there are still issues needs to be addressed?  If you can please describe your suggestions", "type": "commented", "related_issue": null}, {"user_name": "atolopko-czi", "datetime": "Feb 1, 2021", "body": "I still feel it is helpful if the Tree view status hover popups simply contained the operator status as text. Colors can be useful conveyors of information, but text is more accessible for the color-blind. I have not tried setting different default colors, since that would require consensus across our organization.", "type": "commented", "related_issue": null}, {"user_name": "kaxil", "datetime": "Feb 1, 2021", "body": "The tree view in 2.0.0 should already have that", "type": "commented", "related_issue": null}, {"user_name": "atolopko-czi", "datetime": "Feb 1, 2021", "body": "Will upgrade! Thanks !", "type": "commented", "related_issue": null}, {"user_name": "atolopko-czi", "datetime": "Dec 16, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "eladkal", "datetime": "Dec 16, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "atolopko-czi", "datetime": "Feb 1, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2980", "issue_status": " Closed\n", "issue_list": [{"user_name": "cengizmurat", "datetime": "Feb 20, 2020", "body": "Hello,\nI have a  in our team and he has some difficulties to use Botpress chatbot.\nHe works with two screen readers  and , and there are different issues for interacting with Botpress chatbot :Thank you very much,\nBest regards", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Feb 20, 2020", "body": " Thank you for sharing your feedback. While we were aware that Botpress' interface is not accessible, we had no idea where to start as you might imagine there's quite a lot to tackle. Thanks for sharing your thoughts, we'll take this into account and do our best to make the interface more accessible for blind people (starting with the webchat).However, if you are willing to contribute this would help a lot.", "type": "commented", "related_issue": null}, {"user_name": "patlachance", "datetime": "Feb 21, 2020", "body": " /  :  is in my team, you can count on us to help you improving botpress accessibility by submitting PR. We may need your help to pinpoint appropriate sections to improve.", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Feb 21, 2020", "body": "Hello,I am the blind developer who use your chatbot.How would you like that we work together ?Do you wish that we send any accessible code examples to you, or do you first prefer to search the informations by yourself ?Best regards.", "type": "commented", "related_issue": null}, {"user_name": "asashour", "datetime": "Feb 21, 2020", "body": ", , please don't hesitate to ask, also in  if you need further assistance I would suggest that we try to resolve the most important things first", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Apr 29, 2020", "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.", "type": "commented", "related_issue": null}, {"user_name": "SEPOL14", "datetime": "Apr 29, 2020", "body": "Hello,I tested the chatbot again, and I have seen that the buttons were accessible with a screen reader :)However, in order to help even more a blind user it would be nice to have \"heading levels\" with tags within the frame, like this :For example, we could have :After typing a message and clicking on the send button, we could have :Moreover, a big struggle still remains and it is about reading new messages from the bot. A blind user has to navigate all the way from the first message of the conversation until the newest.\nPutting new messages in a non-visible HTML tag can do the job.I have not faced to images yet, but except for logos, please keep in mind that they should have a  attribute with a short description (provided within the Admin panel).\nIf no description has been provided, keep the attribute empty like this Best regards.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Aug 17, 2021", "body": "We will close this issue.", "type": "commented", "related_issue": null}, {"user_name": "cengizmurat", "datetime": "Feb 20, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "EFF", "datetime": "Feb 20, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "slvnperron", "datetime": "Oct 22, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Aug 17, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/EconML/issues/614", "issue_status": " Closed\n", "issue_list": [{"user_name": "juandavidgutier", "datetime": "May 9, 2022", "body": "Hello ,I have a model with a X variable=Hesitant  and I can get the shap_values for my model too. The model has PolynomialFeatures(degree=3). Simillarly, I estimated the CATE for the variable Hesitant. These are the figures I obtain for shap values and CATE:My question is how to interpret the CATE results? It means that if the X0 row in the figure of shape values shows that high values of Hesitant are more likely to give high values of output variable. But in the CATE figure, the high values of Hesitant trend to reduce the effect of treatment on the output variable.I assume that the difference can be explained by the PolynomialFeatures argument, but if you can give me details, I'll appreciate it.By the way, is possible that my shap values figure shows the name of the X variable?I'll appreciate a lot your cooperation.Here is my dataset\nand here is my code:\n`\nimport os, warnings, random\nimport dowhy\nimport econml\nfrom dowhy import CausalModel\nimport pandas as pd\nimport numpy as np\nimport econml\nfrom econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML, CausalForestDML\nfrom econml.dr import DRLearner, ForestDRLearner, SparseLinearDRLearner\nfrom econml.orf import DROrthoForest, DMLOrthoForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV\nfrom econml.inference import BootstrapInference\nimport numpy as np, scipy.stats as st\nimport arviz as az\nimport scipy.stats as stats\nfrom econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nfrom zepid.graphics import EffectMeasurePlot\nfrom zepid.causal.causalgraph import DirectedAcyclicGraph\nfrom joblib import Parallel, delayed\nfrom econml.score import RScorer\nfrom plotnine import ggplot, aes, geom_line, geom_ribbon, ggtitle, labs\nimport shap\nfrom sklearn.model_selection import train_test_split\nimport warningsdef seed_everything(seed=123):\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'seed = 123\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '%.2f' % x)#import data\ndata = pd.read_csv(\"D:/dataset_covid.csv\", encoding='latin-1')\ndata = data.dropna()Constrained = data[['output50', 'Constrained', 'SVI', 'Access', 'HC_Accessibility_Barriers', 'Hesitant', 'Sociodemographic_Barriers', 'PopDensity', 'broadband']]\nprint(Constrained.std())\nConstrained.PopDensity = stats.zscore(Constrained.PopDensity)\nConstrained.Hesitant = stats.zscore(Constrained.Hesitant)Y = Constrained.output50.to_numpy() #Y = data_card['incidencia100k_cardiovasculares'].values\nT = Constrained.Constrained.to_numpy()\nW = Constrained[['SVI', 'HC_Accessibility_Barriers', 'Access']].to_numpy().reshape(-1, 3)\nX = Constrained[['Hesitant']].to_numpy().reshape(-1, 1)X_train, X_val, T_train, T_val, Y_train, Y_val, W_train, W_val = train_test_split(X, T, Y, W, test_size=.4)warnings.filterwarnings('ignore')reg1 = lambda: GradientBoostingClassifier()\nreg2 = lambda: GradientBoostingRegressor()models = [\n('ldml', LinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nlinear_first_stages=False, cv=3, random_state=123)),\n('sldml', SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)),\n('dml', DML(model_y=reg1(), model_t=reg2(), model_final=LassoCV(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3),\nlinear_first_stages=False, cv=3, random_state=123)),\n#('ortho', DMLOrthoForest(model_Y=reg1(), model_T=reg2(), model_T_final=LassoCV(), model_Y_final=LassoCV(),\n#                     discrete_treatment=False, global_res_cv=3, random_state=123)),\n('forest', CausalForestDML(model_y=reg1(), model_t=reg2(),\nfeaturizer=PolynomialFeatures(degree=3),\ndiscrete_treatment=False, cv=3, random_state=123)),\n]def fit_model(name, model):\nreturn name, model.fit(Y_train, T_train, X=X_train, W=W_train)models = Parallel(n_jobs=-1, verbose=1, backend=\"threading\")(delayed(fit_model)(name, mdl) for name, mdl in models)#Choose model with highest RScore\nscorer = RScorer(model_y=reg1(), model_t=reg2(),\ndiscrete_treatment=False, cv=3,\nmc_iters=3, mc_agg='median')scorer.fit(Y_val, T_val, X=X_val, W=W_val)rscore = [scorer.score(mdl) for _, mdl in models]\nprint(rscore)#best model SparseLinearDML#Step 1: Modeforestg the causal mechanism\nmodel_Constrained=CausalModel(\ndata = Constrained,\ntreatment=['Constrained'],\noutcome=['output50'],\ngraph= \"\"\"graph[directed 1 node[id \"Constrained\" label \"Constrained\"]\nnode[id \"output50\" label \"output50\"]\nnode[id \"SVI\" label \"SVI\"]\nnode[id \"HC_Accessibility_Barriers\" label \"HC_Accessibility_Barriers\"]\nnode[id \"Access\" label \"Access\"]\nnode[id \"Hesitant\" label \"Hesitant\"]\nedge[source \"Access\" target \"SVI\"]\nedge[source \"Access\" target \"HC_Accessibility_Barriers\"]\nedge[source \"Access\" target \"Hesitant\"]\nedge[source \"Access\" target \"output50\"]\nedge[source \"Access\" target \"Constrained\"]\nedge[source \"SVI\" target \"Constrained\"]\nedge[source \"SVI\" target \"output50\"]\nedge[source \"HC_Accessibility_Barriers\" target \"Constrained\"]\nedge[source \"HC_Accessibility_Barriers\" target \"output50\"]\nedge[source \"SVI\" target \"HC_Accessibility_Barriers\"]\nedge[source \"SVI\" target \"Hesitant\"]\nedge[source \"HC_Accessibility_Barriers\" target \"Hesitant\"]\nedge[source \"Constrained\" target \"Hesitant\"]\nedge[source \"Constrained\" target \"output50\"]\nedge[source \"Hesitant\" target \"output50\"]\n]\"\"\"\n)#view model\n#model_Constrained.view_model()#Step 2: Identifying effects\nidentified_estimand_Constrained = model_Constrained.identify_effect(proceed_when_unidentifiable=False)\nprint(identified_estimand_Constrained)#Step 3: Estimating effects\nestimate_Constrained = SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)estimate_Constrained = estimate_Constrained.dowhyestimate_Constrained.fit(Y=Y, T=T, X=X, W=W, inference='bootstrap')estimate_Constrained.effect(X)ate_Constrained = estimate_Constrained.ate(X)\nprint(ate_Constrained)ci_Constrained = estimate_Constrained.ate_interval(X)\nprint(ci_Constrained)#shap\nshap_values = estimate_Constrained.shap_values(X)\n#x0='Hesitancy'\nshap.plots.beeswarm(shap_values['Y0']['T0'])\nshap.summary_plot(shap_values['Y0']['T0'], plot_type=\"violin\")#CATE\n#range of hesitancy\nmin_Hesitant = -2.45\nmax_Hesitant = 2.35\ndelta = (max_Hesitant - min_Hesitant) / 100\nX_test = np.arange(min_Hesitant, max_Hesitant + delta - 0.001, delta).reshape(-1, 1)treatment_effects = estimate_Constrained.const_marginal_effect(X_test)te_upper, te_lower = estimate_Constrained.const_marginal_effect_interval(X_test)est2_Constrained = SparseLinearDML(model_y=reg1(), model_t=reg2(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=2, include_bias=False),\nlinear_first_stages=False, cv=3, random_state=123)est2_Constrained.fit(Y=Y, T=T, X=X, inference=\"bootstrap\")treatment_effects2 = est2_Constrained.effect(X_test)\nte_lower2_cons, te_upper2_cons = est2_Constrained.effect_interval(X_test)#plot elasticity(\nggplot(aes(x=X_test.flatten(), y=treatment_effects2))`", "type": "commented", "related_issue": null}, {"user_name": "kbattocchi", "datetime": "May 9, 2022", "body": "From your code, it looks like you're plotting the results of est2_Constrained, which has a degree 2 polynomial rather than the degree 3 polynomial that corresponds to the shap results.The plot basically says that if 'Hesitant' is 0, then a one-unit increase in T should cause a roughly 0.48 unit increase in Y, if 'Hesitant' is 2, then a one-unit increase in T should cause a roughly 0.1 unit , etc. (I'm just reading off the approximate values from the chart).As a side note, you can probably get more accurate confidence intervals from the default \"debiasedLasso\" inference rather than explicitly using \"bootstrap\".I believe you should be able to pass the feature names through like this: ", "type": "commented", "related_issue": null}, {"user_name": "juandavidgutier", "datetime": "May 10, 2022", "body": "OK , thanks for your useful cooperation.Regards", "type": "commented", "related_issue": null}, {"user_name": "juandavidgutier", "datetime": "May 10, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/EconML/issues/612", "issue_status": " Closed\n", "issue_list": [{"user_name": "juandavidgutier", "datetime": "May 7, 2022", "body": "Hello ,First, I am new in shap. I have two variables: 'Access' and 'Hesitant' in X and I can get the shap_values for my model. However, when I want to plot the shap_values with beeswarm, I see other different variables e.g. X0^3, X1^3 and others, but not my two variables of X. Why it happens?. I have shap version 0.39.0I get this plot:I'll appreciate a lot your cooperation.Here is my dataset\nand here is my code:`# importing required libraries\nimport os, warnings, random\nimport dowhy\nimport econml\nfrom dowhy import CausalModel\nimport pandas as pd\nimport numpy as np\nimport econml\nfrom econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML, CausalForestDML\nfrom econml.dr import DRLearner, ForestDRLearner, SparseLinearDRLearner\nfrom econml.orf import DROrthoForest, DMLOrthoForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV\nfrom econml.inference import BootstrapInference\nimport numpy as np, scipy.stats as st\nimport arviz as az\nimport scipy.stats as stats\nfrom econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nfrom zepid.graphics import EffectMeasurePlot\nfrom zepid.causal.causalgraph import DirectedAcyclicGraph\nfrom joblib import Parallel, delayed\nfrom econml.score import RScorer\nfrom plotnine import ggplot, aes, geom_line, geom_ribbon, ggtitle, labs\nimport shap\nfrom sklearn.model_selection import train_test_split\nimport warningsdef seed_everything(seed=123):\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'seed = 123\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '%.2f' % x)#import data\ndata = pd.read_csv(\"D:/dataset_covid.csv\", encoding='latin-1')\ndata = data.dropna()#Constrained\nConstrained = data[['output50', 'Constrained', 'SVI', 'Access', 'HC_Accessibility_Barriers', 'Hesitant', 'Sociodemographic_Barriers', 'PopDensity', 'broadband']]\nprint(Constrained.std())Yc = Constrained.output50.to_numpy()\nTc = Constrained.Constrained.to_numpy()\nWc = Constrained[['SVI', 'HC_Accessibility_Barriers']].to_numpy().reshape(-1, 2)\nXc = Constrained[['Access', 'Hesitant']].to_numpy().reshape(-1, 2)reg1 = lambda: GradientBoostingClassifier()\nreg2 = lambda: GradientBoostingRegressor()#Step 1: Modeling the causal mechanism\nmodel_Constrained=CausalModel(\ndata = Constrained,\ntreatment=['Constrained'],\noutcome=['output50'],\ngraph= \"\"\"graph[directed 1 node[id \"Constrained\" label \"Constrained\"]\nnode[id \"output50\" label \"output50\"]\nnode[id \"SVI\" label \"SVI\"]\nnode[id \"HC_Accessibility_Barriers\" label \"HC_Accessibility_Barriers\"]\nnode[id \"Access\" label \"Access\"]\nnode[id \"Hesitant\" label \"Hesitant\"]identified_estimand_Constrained = model_Constrained.identify_effect(proceed_when_unidentifiable=False)\nprint(identified_estimand_Constrained)estimate_Constrained = DML(model_y=reg1(), model_t=reg2(), model_final=LassoCV(), discrete_treatment=False,\nfeaturizer=PolynomialFeatures(degree=3),\nlinear_first_stages=False, cv=3, random_state=123)estimate_Constrained.fit(Y=Yc, T=Tc, X=Xc, W=Wc, inference='bootstrap')#shap\nshap_values = estimate_Constrained.shap_values(Xc[:200])\nshap.plots.beeswarm(shap_values['Y0']['T0'])\n`", "type": "commented", "related_issue": null}, {"user_name": "kbattocchi", "datetime": "May 9, 2022", "body": "This is because you're using  when you instantiate the estimator - this is saying that you want to estimate the effect Θ(X) as a sparse linear function of terms of up to degree 3 in the columns of X.", "type": "commented", "related_issue": null}, {"user_name": "juandavidgutier", "datetime": "May 9, 2022", "body": "Hello ,Thanks a lot for the explanation.Best regards", "type": "commented", "related_issue": null}, {"user_name": "juandavidgutier", "datetime": "May 9, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/ParlAI/issues/4642", "issue_status": " Closed\n", "issue_list": [{"user_name": "daje0601", "datetime": "Jul 1, 2022", "body": "Hello, An error occurs when installing the bart model while inference to Blenderbot 2.0, which can be searched on the Internet.\nI proceeded as follows. The Internet server downloaded the server from and used it.", "type": "commented", "related_issue": null}, {"user_name": "daje0601", "datetime": "Jul 4, 2022", "body": "I solve this problem and close the issu page~", "type": "commented", "related_issue": null}, {"user_name": "daje0601", "datetime": "Jul 4, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/219", "issue_status": " Closed\n", "issue_list": [{"user_name": "bluecamel", "datetime": "Dec 5, 2018", "body": "On a laptop touchpad, zooming is too fast to control well.  I've been poking through svg.js, but it's not clear how to change the zoom speed.  Thanks so much for any info!", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 5, 2018", "body": "Hi  ,I believe it depends on OS. For example, . In our case we use mostly mouse to annotate data. Tried a trackball but it worked bad for our team.Does it answer your question?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 8, 2018", "body": "Hi  ,I will close the issue. Feel free to reopen it if your question isn't answered.", "type": "commented", "related_issue": null}, {"user_name": "JeremyKeustersML6", "datetime": "Apr 7, 2021", "body": "Hi all,I'm opening this issue again as I'm experiencing this issue on my MacBook Pro. I already tried to change the scrolling speed in the Accessibility options, but the zoom in CVAT remains extremely sensitive when using the trackpad. Any workarounds for this? Thanks!", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 5, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Dec 8, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/582", "issue_status": " Closed\n", "issue_list": [{"user_name": "awallin", "datetime": "Nov 15, 2019", "body": "Based on user testing feedback an audio response for requests is desired by some users. In addition, this would be a powerful accessibility feature.For utterances that return card result we can provide a brief verbal response using available TTS provided by the OS.Responses for different card types are available in this doc as well as the attached mockup.\n", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 3, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "awallin", "datetime": "Jun 8, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "awallin", "datetime": "Jun 8, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "jcambre", "datetime": "Jun 29, 2020", "body": [], "type": "pull", "related_issue": "#1786"}, {"user_name": "ianb", "datetime": "Jul 1, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 1, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/560", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Nov 11, 2019", "body": "There are conflicts sometimes (with Multi-Account containers per , and with 1Password per ). We can't choose a keyboard shortcut that works for everyone, so we should just make it configurable.This would be a new field in the options page. I think we can just force people to type in the keyboard shortcut syntax per the We might leave  and make this an additional keyboard shortcut...?", "type": "commented", "related_issue": null}, {"user_name": "bnakamoto", "datetime": "Jan 15, 2020", "body": "I had mostly composed (last night) a reply to  about my feedback about Command-Period being a long-time expected keyboard shortcut on macOS, but it appears that .  had referenced an issue that referenced this issue in his reply so this seems like the best place to share my follow-up; apologies in advance if not.I surprisingly haven't been able to find any Apple developer docs about Command-Period. They have , but there's only reference to Control-Option-Command-Period (to increase screen contrast, part of ). There is evidence that Apple has standardized on Command-Period as a cancel/escape/break operation; for example, in their keyboard shortcut docs for  and . There's also third party docs such as  about Command-Period being a broad \"'cancel' feature\":Apple has associated Command-Period with \"cancel\" on Macs since before Mac OS X/macOS, the System  days. Thus, I hope that the  keyboard shortcut for Firefox Voice is changed to something other than Command-Period.Thank you!\n-Brian", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jan 17, 2020", "body": "Per the original thread disappearing: apparently our comment submission process was flagged as suspicious by GitHub, and all comments made that way were hidden. We're working on getting them back.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 11, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#550"}, {"user_name": "ianb", "datetime": "Nov 21, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 3, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 5, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 5, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/57", "issue_status": " Closed\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 8, 2019", "body": "Right now the text input seems to be a . This is extra work and has accessibility problems. We should just use an input. All the styles can still be overridden so it can look like whatever (though it takes somewhat more work).", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 12, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 30, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Sep 16, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/google/sentencepiece/issues/418", "issue_status": " Closed\n", "issue_list": [{"user_name": "blaizeberry4", "datetime": "Nov 3, 2019", "body": "Wondering when version 0.1.84 will be released to PyPI as from the , it appears the most recent version is still 0.1.83 for both  and  on PyPI. From comparing the Github release history and PyPI release history, it seems that the release date has typically been the same, so curious if the lack of accessibility via PyPI is intended. Thanks in advance for the assistance!", "type": "commented", "related_issue": null}, {"user_name": "taku910", "datetime": "Dec 16, 2019", "body": "Released 0.1.85", "type": "commented", "related_issue": null}, {"user_name": "taku910", "datetime": "Dec 16, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/2070", "issue_status": " Closed\n", "issue_list": [{"user_name": "Dronek", "datetime": "Sep 3, 2017", "body": "I would like to address the fact / the \"issue\" that the frequency of iNav (pre-)releases has been obviously decreasing since the beginning of 2017 (at least) while popularity and support have grown at the same time. To be clear up front, this is  meant to criticize or discuss the development pace of iNav whatsoever.My request is rather to increase the accessibility of experimental features to \"regular\" pilots / users. Many of us were \"brave enough\" to fly alpha releases of iNav on our aircraft in order to test new features. As far as I recall, this had a positive effect of improved contribution from non-developers in terms of functionality-oriented feedback on RCG. Over the last several months discussions on RCG have seemingly reduced towards getting supported hardware to work and configuration.Would it be feasible to make the future releases more granular / frequent, including \"alpha-releases\"?", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 4, 2017", "body": " good point. It's all comes to balance between having enough time, testing everything and desire to have as many new bells and whistles as possible in new releases.How about dropping the release-candidate cycle and changing the versioning:\nThree-digit versions will be \"unstable releases\"\nTwo digit versions will be \"stable releases\".This way we'll be able to do more frequent releases for those who are brave while still keeping somewhat adequate stable release cycle.Thoughts?", "type": "commented", "related_issue": null}, {"user_name": "stronnag", "datetime": "Sep 4, 2017", "body": "I agree witj . We should perhaps try to have no more than a month between minor releases. Now we seem to have settled into a culture of 'just one more big feature' and the cycle stretches. We seem to get a lot of '' requests on RGC. So how about a feature freeze and 1.7.3 in the new 'semi-stable' scheme by the end of the week? Do we have any show-stoppers in the works?", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 4, 2017", "body": "We don't have any show-stoppers for 1.7.2 apart from few minor features. We can have a semi-stable 1.7.3 by end of the week for sure.", "type": "commented", "related_issue": null}, {"user_name": "oleost", "datetime": "Sep 5, 2017", "body": "How about release every 4 week.So it would go like this:A stable release is done.\nAfter 3 weeks release an release candidate. Freeze new feature pullrequest, only bugfix.\nAfter 4 weeks if no outstanding critical bug, release a new stable version. If still outstanding bugs release an release candiate. It will be stuck in this stage until bug are fixed and a new stable version is out.( So this would mean it can be extended more than 4 weeks, and its noth calender based )This would leave 3 week where one can implement new features, and 1 week to fix any issues. ( Or more if necessary. )Versions numbers doesnt matter, as they will probably live they own life depending on what have been done the last 4 weeks.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 5, 2017", "body": "At the time of building it's hard to state that this will be the stable version. For me, major/minor number increase could indicate new features, while the micro number could be added for bugfix releases.\nI also vote for short release cycle, 1 month at most. In case problems are detected bugfix release(s) should follow.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 5, 2017", "body": " what about release-candidates? Releasing RC's makes release-cycle considerably longer.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 5, 2017", "body": "For me, additional RC phase could be skipped.\nEach \"new feature\" release (major/minor bump) could be the \"RC\" (regardless if we name them so or not), followed by bugfix release (could be considered \"stable\", but if more problems will occur, we may build another \"stable\" with micro number bumped).\nI'm not a fan of releasing a version named x.x-RC, and then not releasing the \"stable\" shortly (e.g. because there are no problems, or because there are some severe ones that should go to the release). It makes the impression that something is not done to the end (at least for me).", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "Sep 5, 2017", "body": "Personally I think 1 month is too short. A big new feature will take more than one month to develop and so will have to be done in a way that spans more than one release. This will discourage development of \"big\" features.What really is required is to make the latest builds available to \"test pilots\", so they can fly them and test them. I think the way to do this is to set up a \"nightly\" build server that builds development every night and makes it available. Betaflight does this with the Jenkins build server, iNav should do something similar.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 5, 2017", "body": "Nightly builds are welcome and this will offload developers from the \"please build this for me to test\" requests.\nHowever I don't get why developing a feature that takes longer than release cycle could discourage one from work...", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "Sep 5, 2017", "body": "Because then you have to spend more time doing code merges/rebases and that is always a pain...", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 5, 2017", "body": "Nightly builds are good - maybe it's possible to use same server as Betaflight to build INAV's development branch?Regarding big features - in most cases they could be done incrementally, without affecting end users much. But I agree with  - there are features that are big in terms of required coding or testing and also atomic (couldn't be split) - we had a few of those and they were a pain.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 5, 2017", "body": "Merges/rebases could be pain, I agree. But... should this block other features from being delivered?\nI see this opposite - having developed a feature, publishing PR and waiting months for this thing to be released into public could be discouraging... Just my humble opinion.", "type": "commented", "related_issue": null}, {"user_name": "giacomo892", "datetime": "Sep 5, 2017", "body": "It could be nice releasing a build when something really needs to be tested on many machines, without a fixed schedule, specifying what changed and what to test.Also inav-configurator needs to be released to match the current build. (no-go with nightly)That can really help the development with logs and bug reports.Letting users to run straight nightly builds without letting them know what's really changed can led to some dangerous situations or a led to a general lack of development awareness.When I build a firmware from development I usually take a look at every commit to understand where the project went and then I'm ready to flight.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 5, 2017", "body": " good point. Configurator has to match the build number.The way I see it: For minor releases stick to 1mo release cycle, postpone features that didn't make it to the code yet. Declare feature freeze 1 week before release and only accept bugfixes. These releases could be done straight from development branch. We'll mark these releases as \"pre-release\" on github.Minor releases would essentially be a release-candidates for a bigger release.What do you think?", "type": "commented", "related_issue": null}, {"user_name": "Dronek", "datetime": "Sep 5, 2017", "body": "I were about to suggest a similar thing to Jenkins. However, it could actually be more harmful to provide nightlies for systems with navigation / autonomous capabilities compared to BF. If so, nightlies should be rather considered for bench testing only.\nAs for release numbering, e.g. 1.7.x releases could be considered as \"testing release\" and 1.x \"stable\". Testing releases could include experimental features which might not necessarily make it into stable releases, therefore being different to a release candidate. I support the idea of getting rid of release candidates. Instead, x.x.9 could indicate that no new features will be added, being the last testing release in order to find bugs before next stable release.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 5, 2017", "body": "I'd point out that simplicity and avoiding doubts should be the principle.\nE.g. bigger number should always indicate newer version.\nIf we can provide stable releases - great. They should be clearly marked then as such. As well as bugfix releases to the stable releases in case the latter require any.", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Sep 6, 2017", "body": "OK, here are my thoughts on the topic:", "type": "commented", "related_issue": null}, {"user_name": "Dronek", "datetime": "Sep 6, 2017", "body": "I agree that defining and sticking to a fixed release cycle might be rather unrealistic for projects like this. I experience roadmap changes and postponed deadlines on a regular basis in a professional context. Not to mention how much overhead project management creates.\nHowever, please keep \"test pilots\" in mind who are also regular dudes and who like to contribute experience from the field in order to help shaping iNav's functionality... in their spare time as well.\nRegarding safety concerns: every user has the choice to enable the \"show unstable versions\" switch in the firmware flasher or to keep this switch off to stay more or less safe. As of now, this switch is quite useless.\nPrividing testing releases in a more frequent (not necessarily fixed) manner could provide a broader feedback on a comparable basis.\nThere is quite a gap between flying a stable release and bench testing development builds.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 7, 2017", "body": "My thougs on this:I agree with  that 1mo releases are unrealistic. However, I think we should make an \"unstable\" release approx. each month regardless of new features added/removed.Stable release should be made every 2-3 months. RC phase (assuming feature-freeze) for 4 weeks is probably an overkill - 2 weeks should be enough. Our release schedule will look like this:1.7.3 (unstable) --- (1mo) ---> 1.7.4 (unstable, RC) ---> (1w, some bugs fixed) --->1.7.5 --- (2 days, critical bug found and fixed) ---> 1.7.6 --- (1w, no new bugs) ---> 1.8 (stable) ---> (1mo) ---> 1.8.1 ...", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "Sep 7, 2017", "body": "If we make an \"unstable\" release then it should be clearly named as such. Ie we should not use the number to denote stability. Ie it should be 1.7.4-RC1, or 1.7.4-alpha1 or some such name.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 7, 2017", "body": "On GitHub it's possible to mark release as \"Pre release\" and in Configurator Flasher it won't be shown unless you tick the \"Show unstable releases\" switch and unstable releases will be shown as \"(release candidate)\" in the list:I think it would be enough - no need to explicitly add anything to version number.", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 7, 2017", "body": "The first impression to users could be that 1.8.1 is a bugfix release to 1.8 and not something completely new... However I don't have any constructive proposal how to make it more clear...", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "Sep 7, 2017", "body": "That assumes people are not using local files to flash with.What is the reason not to add something to the version number?", "type": "commented", "related_issue": null}, {"user_name": "krzysztofmatula", "datetime": "Sep 7, 2017", "body": "Maybe: 1.8.dev1 -> 1.8.dev2 -> 1.8 (stable) -> 1.8.1 (potential bugfix) -> 1.9.dev1I'm not sure about this \"dev\"... But anything like \"alpha\", \"beta\" or \"rc\", all have the meaning that the feature set is fully implemented, but not completely stable yet. This is not the case of development unstable snapshots.Maybe \"1.8.pre1\" like \"preview\" or \"pre-release\"...?\nI vote for including the information that build is not stable in the build name.", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "Sep 7, 2017", "body": "That is not correct. Alpha specifically means that the feature set may not be fully implemented. Once feature set is complete the software becomes beta.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 7, 2017", "body": "Local file only has 3-digit version number anyhow.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 9, 2017", "body": "Let's keep the 3-digit versions and use the following conventions:", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 9, 2017", "body": "Using this convention our next release will be unstable  and after that - stable ", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 4, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 4, 2017", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Sep 9, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/10760", "issue_status": " Closed\n", "issue_list": [{"user_name": "rohitgr7", "datetime": "Nov 25, 2021", "body": "See title:\nSimilar issues: , .. I'll keep linking more.This is a highly requested feature from the lightning community. Total training steps is being used by some of the lr schedulers, especially when using transformer models, but since there are a lot of arguments/flags involved for computing it, it's not easy for a user to create one that can work on all the possible edge cases with no-code change. Also we make updates to some of these flags and accessibility to some of its components (for eg. train_datalaoders in v1.5), so there is a possibility that a custom one created by a user might get outdated soon, and one has to write a new one which is only possible if they are well aware with the codebase internals. But we as core-contributors and maintain it with some tests of course.would like to credit  for helping out :)cc \ncc @PyTorchLightning/core-contributors", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Nov 25, 2021", "body": "I have definitely seen this question being asked over and over.So I believe we should do it !    ?", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Nov 25, 2021", "body": "The last time this was discussed, I remember somebody mentioning that the problem with this is that it will not be correct in all circumstances and there's no way for us to know it. A problem of silent correctness.Also, this property cannot be called from anywhere, the attributes used inside need to be computed first.Just pointing out possible problems, not saying we shouldn't add it.", "type": "commented", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Nov 25, 2021", "body": "I raised the concern for correctness earlier. However, I do believe it becomes more important to provide such utility because Lightning is getting more complex and harder to understand what's happening under the hood for the regular user. And if users ask for it, we should provide we should provide the most accurate estimation that is possible, then at the same time document what the unknowns are (e.g., accumulation scheduling via callback).Also, which implementation are the users asking for? The number of training steps, i.e., the number of times the training_step will be called / the size of the \"dataloader\", OR the number of optimization steps?\nBoth may be used for learning rate scheduling (?), and if you are converting from PyTorch to Lightning you may have to choose one or the other (?).", "type": "commented", "related_issue": null}, {"user_name": "SeanNaren", "datetime": "Nov 25, 2021", "body": "Thanks for picking this up  again, I'm very for this :) I've added both functionality manually into a class for Flash and Transformers and it would be nice to have a single function available through the trainer.Regarding correctness, I think the cases this function breaks down I detailed above. However in my opinion I think the gains outweigh the issue of correctness. It may be worth putting a disclaimer in the docstring that in certain cases we cannot estimate correctly!", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Nov 26, 2021", "body": "I think except for the dataloader, every other argument will already be there when we initialize the trainer. but yeah dataloader might need a few more things that might not be there for eg. dataset, batch_size... so won't be able to access it inside  (which I guess is okay because we don't expect users to do that inside ).accumulation via callback will still be available right? since we will have it during Trainer init?its number of optimization steps. We can rename the method to  if required.", "type": "commented", "related_issue": null}, {"user_name": "justusschock", "datetime": "Nov 26, 2021", "body": "AFAIK for lr scheduling you only use the number of optimization steps, since this is the only thing that may influence the model and the optimizer :) So I think it's fine to go with that and not provide the size of the \"dataloader\"", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Nov 26, 2021", "body": "In that case, after one epoch, ?", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Nov 26, 2021", "body": "I think yes.. it should be. any edge case we missed?", "type": "commented", "related_issue": null}, {"user_name": "mariomeissner", "datetime": "Nov 28, 2021", "body": "I got  when using the function provided here. Running lightning 1.5.3.", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Nov 28, 2021", "body": " in which hook/method did you call this function?", "type": "commented", "related_issue": null}, {"user_name": "mariomeissner", "datetime": "Nov 29, 2021", "body": " I call it inside  to set up the scheduler!", "type": "commented", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Nov 29, 2021", "body": "You probably need to change this line\n\nto(the error  pretty much gives it away)", "type": "commented", "related_issue": null}, {"user_name": "lukasschmit", "datetime": "Jan 19, 2022", "body": "One thing I noticed with the provided solution, if passing  to the trainer the total number of training steps will end up being negative as  will be -1 as well", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Jan 19, 2022", "body": "good catch! I think we should use  instead. Need to check whether it will hold true for TPU and DDP2 or not.", "type": "commented", "related_issue": null}, {"user_name": "SeanNaren", "datetime": "Jan 20, 2022", "body": " can we add this as a property now? Definitely would be a great feature I'm interested in :)", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Jan 20, 2022", "body": "yes! just waiting for final approvals.\ncc   ", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Jan 20, 2022", "body": "Sounds good to me.", "type": "commented", "related_issue": null}, {"user_name": "ananthsub", "datetime": "Jan 20, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Jan 20, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Jan 20, 2022", "body": "For the name, I would suggest something like  to differentiate it from count variables that get updated as training progresses", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Jan 21, 2022", "body": "I believe  and  might be less confusing for the users.", "type": "commented", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Nov 25, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Nov 25, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "tchaton", "datetime": "Nov 25, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "tchaton", "datetime": "Nov 25, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Dec 1, 2021", "body": [], "type": "issue", "related_issue": "#10275"}, {"user_name": "rohitgr7", "datetime": "Jan 24, 2022", "body": [], "type": "pull", "related_issue": "#11599"}, {"user_name": "carmocca", "datetime": "Feb 16, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "rohitgr7", "datetime": "Feb 28, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/6765", "issue_status": " Closed\n", "issue_list": [{"user_name": "erohmensing", "datetime": "Sep 23, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 23, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "m-vdb", "datetime": "Sep 23, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 28, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "erohmensing", "datetime": "Sep 28, 2020", "body": [], "type": "pull", "related_issue": "#6822"}, {"user_name": "rasabot", "datetime": "Oct 3, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/4445", "issue_status": " Closed\n", "issue_list": [{"user_name": "edenlightning", "datetime": "Oct 30, 2020", "body": "to promote NeMo/lightning further in a medium article showing accessibility and ease to make audio research headway", "type": "commented", "related_issue": null}, {"user_name": "edenlightning", "datetime": "Oct 30, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/4046", "issue_status": " Closed\n", "issue_list": [{"user_name": "tomtel", "datetime": "Jul 30, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "BreakBB", "datetime": "Jul 30, 2019", "body": "It would be great if you could add some information to your post. Moreover you should put your environment information into the place of the template instead of the title.", "type": "commented", "related_issue": null}, {"user_name": "tomtel", "datetime": "Jul 30, 2019", "body": "How to reproduce the problem\nMy error message:(base) user0001@LINUX0001:~$ pip install -U spacy[cuda]\nCollecting spacy[cuda]\nUsing cached \nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy[cuda])\nUsing cached \nCollecting cymem<2.1.0,>=2.0.2 (from spacy[cuda])\nUsing cached \nCollecting thinc<7.1.0,>=7.0.8 (from spacy[cuda])\nUsing cached \nRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in ./anaconda3/lib/python3.7/site-packages (from spacy[cuda]) (1.16.4)\nCollecting plac<1.0.0,>=0.9.6 (from spacy[cuda])\nUsing cached \nCollecting preshed<2.1.0,>=2.0.1 (from spacy[cuda])\nUsing cached \nRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.7/site-packages (from spacy[cuda]) (2.22.0)\nCollecting wasabi<1.1.0,>=0.2.0 (from spacy[cuda])\nUsing cached \nCollecting blis<0.3.0,>=0.2.2 (from spacy[cuda])\nUsing cached \nCollecting srsly<1.1.0,>=0.0.6 (from spacy[cuda])\nUsing cached \nCollecting thinc-gpu-ops<0.1.0,>=0.0.1; extra == \"cuda\" (from spacy[cuda])\nUsing cached \nCollecting cupy>=5.0.0b4; extra == \"cuda\" (from spacy[cuda])\nUsing cached \nERROR: Command errored out with exit status 1:\ncommand: /home/user0001/anaconda3/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6ku2puxn/cupy/setup.py'\"'\"'; ='\"'\"'/tmp/pip-install-6ku2puxn/cupy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)();code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, , '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\ncwd: /tmp/pip-install-6ku2puxn/cupy/\nComplete output (43 lines):\nOptions: {'package_name': 'cupy', 'long_description': None, 'wheel_libs': [], 'wheel_includes': [], 'no_rpath': False, 'profile': False, 'linetrace': False, 'annotate': False, 'no_cuda': False}ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n(base) user0001@LINUX0001:", "type": "commented", "related_issue": null}, {"user_name": "tomtel", "datetime": "Jul 30, 2019", "body": "My Environment\nOperating System:\nx86_64\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=18.04\nDISTRIB_CODENAME=bionic\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.2 LTS\"\nNAME=\"Ubuntu\"\nVERSION=\"18.04.2 LTS (Bionic Beaver)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 18.04.2 LTS\"\nVERSION_ID=\"18.04\"\nHOME_URL=\"\"\nSUPPORT_URL=\"\"\nBUG_REPORT_URL=\"\"\nPRIVACY_POLICY_URL=\"\"\nVERSION_CODENAME=bionic\nUBUNTU_CODENAME=bionicPython Version Used:\nPython 3.7.3spaCy Version Used:\nspaCy is not installedEnvironment Information:\n(base) user0001@LINUX0001:~$ printenv\nCLUTTER_IM_MODULE=xim\nCONDA_SHLVL=1\nLD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tgz=01;31:.arj=01;31:.lha=01;31:.lzh=01;31:.tlz=01;31:.tzo=01;31:.zip=01;31:.Z=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.tzst=01;31:.bz=01;31:.tbz2=01;31:.deb=01;31:.jar=01;31:.ear=01;31:.rar=01;31:.ace=01;31:.cpio=01;31:.rz=01;31:.wim=01;31:.dwm=01;31:.jpg=01;35:.mjpg=01;35:.gif=01;35:.pbm=01;35:.ppm=01;35:.xbm=01;35:.tif=01;35:.png=01;35:.svgz=01;35:.pcx=01;35:.mpg=01;35:.m2v=01;35:.webm=01;35:.mp4=01;35:.mp4v=01;35:.qt=01;35:.wmv=01;35:.rm=01;35:.flc=01;35:.fli=01;35:.gl=01;35:.xcf=01;35:.yuv=01;35:.emf=01;35:.ogx=01;35:.au=00;36:.m4a=00;36:.midi=00;36:.mp3=00;36:.ogg=00;36:.wav=00;36:.opus=00;36:.xspf=00;36:\nCONDA_EXE=/home/user0001/anaconda3/bin/conda\nLESSCLOSE=/usr/bin/lesspipe %s %s\nXDG_MENU_PREFIX=gnome-\nLANG=de_DE.UTF-8\nDISPLAY=:1\nGNOME_SHELL_SESSION_MODE=ubuntu\nCOLORTERM=truecolor\nCUDA_PATH=/usr/local/cuda-10.1\nUSERNAME=user0001\nCONDA_PREFIX=/home/user0001/anaconda3\nXDG_VTNR=2\nSSH_AUTH_SOCK=/run/user/1000/keyring/ssh\n_CE_M=\nXDG_SESSION_ID=3\nUSER=user0001\nDESKTOP_SESSION=ubuntu\nQT4_IM_MODULE=xim\nTEXTDOMAINDIR=/usr/share/locale/\nGNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/d1324337_0a68_424d_b658_e7e868284cbf\nPWD=/home/user0001\nHOME=/home/user0001\nCONDA_PYTHON_EXE=/home/user0001/anaconda3/bin/python\nTEXTDOMAIN=im-config\nSSH_AGENT_PID=1885\nQT_ACCESSIBILITY=1\nXDG_SESSION_TYPE=x11\nXDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share:/usr/share:/var/lib/snapd/desktop\n_CE_CONDA=\nXDG_SESSION_DESKTOP=ubuntu\nGJS_DEBUG_OUTPUT=stderr\nCONDA_PROMPT_MODIFIER=(base)\nGTK_MODULES=gail:atk-bridge\nWINDOWPATH=2\nTERM=xterm-256color\nSHELL=/bin/bash\nVTE_VERSION=5202\nQT_IM_MODULE=xim\nXMODIFIERS==ibus\nIM_CONFIG_PHASE=2\nXDG_CURRENT_DESKTOP=ubuntu:GNOME\nGPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1\nGNOME_TERMINAL_SERVICE=:1.114\nXDG_SEAT=seat0\nSHLVL=1\nGDMSESSION=ubuntu\nGNOME_DESKTOP_SESSION_ID=this-is-deprecated\nLOGNAME=user0001\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\nXDG_RUNTIME_DIR=/run/user/1000\nXAUTHORITY=/run/user/1000/gdm/Xauthority\nXDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg\nPATH=/usr/local/cuda-10.1/bin:/home/user0001/anaconda3/bin:/home/user0001/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nCONDA_DEFAULT_ENV=base\nGJS_DEBUG_TOPICS=JS ERROR;JS LOG\nSESSION_MANAGER=local/LINUX0001:@/tmp/.ICE-unix/1790,unix/LINUX0001:/tmp/.ICE-unix/1790\nLESSOPEN=| /usr/bin/lesspipe %s\nGTK_IM_MODULE=ibus\n_=/usr/bin/printenv", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Jul 30, 2019", "body": "This is likely not a problem with spaCy. Check out your error log:Looks like cupy (which spaCy uses) can't find CUDA. Once you've fixed that, spaCy should also work as expected.", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "Aug 29, 2019", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "tomtel", "datetime": "Jul 30, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "ines", "datetime": "Jul 30, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "no-response", "datetime": "Jul 30, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "ines", "datetime": "Jul 30, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ines", "datetime": "Jul 30, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lock", "datetime": "Aug 29, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/Lightning-AI/lightning/issues/9220", "issue_status": " Closed\n", "issue_list": [{"user_name": "awaelchli", "datetime": "Aug 31, 2021", "body": "Provide a means to share data in a stack of loops, for example in the training loop which is made of\nFitLoop > TrainingEpochLoop > TrainingBatchLoop > OptimizerLoop, where \"X > Y\"  denotes Y is a child loop of X.We currently have several attributes like running_loss, skip_backward, etc. which need to be accessed externally or from within subloops.\nThe current way is to reach into these loops like so:If a subloop needs to access a parent loop's attribute like this, then it has several drawbacks.Introduce a dataclass or dictionary:(the attributes here are for illustrative purpose, whether they get included or not is for discussion)This approach may still be too strict and it could be extended to make the data holder dynamic and let each loop register attributes at the time of instantiation.We are currently extracting an optimizer loop in  which is the origin of this RFC. There we are splitting the code path of manual and automatic optimization. In these cases, the updating logic for the attributes \nshould also be different but currently these attributes are owned by the . A data holder would greatly improve the situation there.", "type": "commented", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Aug 31, 2021", "body": "   ", "type": "commented", "related_issue": null}, {"user_name": "tchaton", "datetime": "Aug 31, 2021", "body": "Hey ,Quick questions to better understand the background:Best,\nT.C", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Aug 31, 2021", "body": "As it stands, all the instances where we need to access shared data are points where the code could be improved, perhaps with the exception of  which is really a global value to track.This is why I don't think this refactor is important right now.", "type": "commented", "related_issue": null}, {"user_name": "ananthsub", "datetime": "Sep 1, 2021", "body": " i'm curious to hear your ideas on this! needing access to the trainer to walk down the loop hierarchy to access state somewhere is very roundabout right now.i also wonder: how easy is it to read this code now vs having dedicated implementations of training loops that can be subbed into the fit loop to run. it seems like this problem arises only in the training loops compared to the evaluation and prediction loops, given the differences in training modes lightning supports.if we have pre-packaged loop implementations for training, the code/state is all in one place. the somewhat hacky part is needing a loop selector to pick the training loop implementation based on the lightning module properties.vs.with fit_loop.epoch_loop.batch_loop.optimizer_loop this is hopping across 4 different classes to trace through the execution.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Oct 1, 2021", "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!", "type": "commented", "related_issue": null}, {"user_name": "carmocca", "datetime": "Oct 6, 2021", "body": "It's a case-by-case thing where we need to check each occurrence and thing about why it was done that way and how we can group things in self-contained classes.This is a matter of perspective, if you ask me, its as easy as it has ever been, perhaps with the only disadvantage of having to jump across files. The non-fit loops don't suffer from this as much because they don't have nearly as many features and customizations.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Nov 6, 2021", "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!", "type": "commented", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Aug 31, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Aug 31, 2021", "body": [], "type": "pull", "related_issue": "#9191"}, {"user_name": "awaelchli", "datetime": "Sep 5, 2021", "body": [], "type": "pull", "related_issue": "#9317"}, {"user_name": "stale", "datetime": "Oct 1, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Oct 6, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Nov 6, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "awaelchli", "datetime": "Nov 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/5405", "issue_status": " Closed\n", "issue_list": [{"user_name": "fresejoerg", "datetime": "May 5, 2020", "body": "I am using the CLI training interface to train a custom tagger and parser. The dependency labels are a custom set of semantic labels. The training data is converted from conll format.\nI am not using the --base-model argument, so I believe I'm starting from a blank model. Also, the output directory does not exist prior to training.\nAfter training, the model sometimes outputs an unexpected dependency tag ('dep') which is not part of my training data.", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 6, 2020", "body": "I'm not entirely sure, but I think this may be a bug related to using  as a label because the parser (which is used underneath for both  and ) has some special cases related to  to process labels like  for NER.Do you have the same results if you replace  with a different dummy label like ?", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "May 6, 2020", "body": "Thanks! I replaced  with  and haven't observed the issue again, so I think your intuition was correct.", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 6, 2020", "body": "Glad to hear it! I have to look into the details about how complicated it might be to fix the parser directly (since you really should be able to use any string label), but at least for now we should show an error when training data is loaded with  in a dependency label to avoid this problem.A side note: unless you are only processing single sentences with your model when it's in use, I would recommend against using . If you use this option, the parser won't learn to split sentences because it also splits the training data up into individual sentences while training and the parser never sees any sentence boundaries.If you want to train with gold tokenization, then just remove the  texts from your training data (if you have them) and it will learn from the gold tokens without splitting up documents. (Gold tokenization and single-sentence training ended up grouped together in this option for specific kinds of parser evaluations, when separate options would been better.)", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "May 6, 2020", "body": "Thanks for the pointer regarding . I don't have the  string in my data and the inputs are relatively short, non-grammatical fragments (search queries, in particular), so I'm not concerned about sentence splitting at the moment. But I'll do some experiments to see how omitting  affects LAS.", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "May 7, 2020", "body": "The original issue just re-emerged for me. A freshly trained version of my model is predicting  as a label. I verified that no  labels are in my training or dev dataset. So it appears that the root cause is unrelated to special handling for this character.", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 14, 2020", "body": "Hmm, what is  for your model? Are there any warnings/errors for your data with ?", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "May 14, 2020", "body": "this is what I get from :which is a subset of all training labels (with the exception of , which is not in my training data.Here's the output from debugging the training data:", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 29, 2020", "body": "I noticed that one of our example scripts uses  as a label in for a similar case without issues, so it must be something else.I don't understand why  would show 21 labels but you don't end up with all of them in the model labels. How many training docs do you have? There was a minor issue where the parser peeked at the first 1000 examples instead of examining all of them when adding labels. This peeking is still in v2.2.4, but will be removed in v2.3.0 (to be released soon, change in ).", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "May 29, 2020", "body": "My sense is that the number of training examples is related to this issue, but probably not to the one you're referencing. The  output above is for a training data set with 260 examples. I've since added some examples and have re-trained the model with 344 training examples, but still well below 1,000.I noticed that there were previously two labels ( and ) which didn't make it into the model but also did not receive a  warning. These two labels are now in the model.Unfortunately, adding more training data didn't prevent  from showing up in the model.\nLooking at specific examples where the model actually predicts , I noticed that that occurs in cases where either the correct label would have been one of those  with a  warning or in edge cases where even a human expert can't confidently assign the correct label.", "type": "commented", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "Jun 2, 2020", "body": "I think I figured out what's going on. There's a minimum label frequency parameter with a default value of 30, which explains why some labels are missing in the model. You can lower this by passing the parameter  to .  uses a cutoff of 20 instead of 30, which is confusing here.The  label is coming from here as a backoff if there's no other good move:spacy v2 has a number of parameters and defaults that are spread across the code and hard to track down. The rewrite of thinc for spacy v3 uses a much better config system where models can be saved with a complete config file and there shouldn't be as many frustrating issues with hidden defaults.", "type": "commented", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "Jun 2, 2020", "body": "Thanks for sticking with this. I'm closing this issue as resolved.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Nov 5, 2021", "body": "This thread has been automatically locked since there  has not been any recent activity after it was closed.  Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "svlandeg", "datetime": "May 5, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 6, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "no-response", "datetime": "May 6, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 6, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "adrianeboyd", "datetime": "May 6, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "fresejoerg", "datetime": "Jun 2, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "github-actions", "datetime": "Nov 5, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/431", "issue_status": " Closed\n", "issue_list": [{"user_name": "nottug", "datetime": "Dec 20, 2016", "body": "I'm trying to have this running of my Jetson TX1 (Ubuntu 16.04), but I keep getting a build error when running . This is just the part where it fails, I can post the full output if necessary. I ran it with --debug and --stacktrace for a more verbose output.`14:39:33.080 [INFO] [org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter] Skipping task ':extractNatives' as task onlyIf is false.\n14:39:33.080 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':extractNatives'\n14:39:33.081 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractNatives SKIPPED\n14:39:33.081 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractNatives (Thread[main,5,main]) completed. Took 0.107 secs.\n14:39:33.082 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractUserdev (Thread[main,5,main]) started.\n14:39:33.082 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractUserdev\n14:39:33.082 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Starting to execute task ':extractUserdev'\n14:39:33.083 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: {CACHE_DIR}/{API_GROUP_DIR}/{API_NAME}/{API_VERSION}/userdev\n14:39:33.083 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: /home/ubuntu/.gradle/caches/minecraft/net/minecraftforge/forge/1.8-11.14.3.1543/userdev\n14:39:33.084 [DEBUG] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Determining if task ':extractUserdev' is up-to-date\n14:39:33.089 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.DefaultArtifactDependencyResolver] Resolving configuration ':forgeGradleUserDevPackage'\n14:39:33.091 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'forge' [58d026f73bd51b2d7316bd092d31aca0].\n14:39:33.093 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'MavenRepo' [4471c3b2f5ea2d40ffaa8b3948cdbacd].\n14:39:33.096 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'minecraft' [1a583fa30149d818dec07b24fb2d3986].\n14:39:33.098 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'deobfDeps' [78379ea6164128758068fc0273acf515].\n14:39:33.099 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] Reusing in-memory cache for repo 'TweakerMcRepo' [9bade229c9af7de517a76b0c8c6204b2].\n14:39:33.103 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting configuration com.microsoft.MalmoMod:Minecraft:unspecified(forgeGradleUserDevPackage).\n14:39:33.105 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting dependency com.microsoft.MalmoMod:Minecraft:unspecified(forgeGradleUserDevPackage) -> net.minecraftforge:forge:1.8-11.14.3.1543(forgeGradleUserDevPackage)\n14:39:33.105 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Selecting new module version net.minecraftforge:forge:1.8-11.14.3.1543\n14:39:33.106 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.RepositoryChainComponentMetaDataResolver] Attempting to resolve component for net.minecraftforge:forge:1.8-11.14.3.1543 using repositories [forge, MavenRepo, minecraft, deobfDeps, TweakerMcRepo]\n14:39:33.109 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.parser.IvyXmlModuleDescriptorParser] post 1.3 ivy file: using exact as default matcher\n14:39:33.113 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.CachingModuleComponentRepository] Using cached module metadata for module 'net.minecraftforge:forge:1.8-11.14.3.1543' in 'forge'\n14:39:33.113 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.RepositoryChainComponentMetaDataResolver] Using net.minecraftforge:forge:1.8-11.14.3.1543 from Maven repository 'forge'\n14:39:33.114 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] Visiting configuration net.minecraftforge:forge:1.8-11.14.3.1543(default).\n14:39:33.114 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.graph.DependencyGraphBuilder] net.minecraftforge:forge:1.8-11.14.3.1543(default) has no transitive incoming edges. ignoring outgoing edges.\n14:39:33.115 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.oldresult.TransientConfigurationResultsBuilder] Flushing resolved configuration data in Binary store in /tmp/gradle2012945065804096886.bin. Wrote root com.microsoft.MalmoMod:Minecraft:unspecified:forgeGradleUserDevPackage.\n14:39:33.116 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.CachingModuleComponentRepository] Found artifact 'forge-userdev.jar (net.minecraftforge:forge:1.8-11.14.3.1543)' in resolver cache: /home/ubuntu/.gradle/caches/modules-2/files-2.1/net.minecraftforge/forge/1.8-11.14.3.1543/88bee89f68318739e0c0a1936f0f0a563c0cad8d/forge-1.8-11.14.3.1543-userdev.jar\n14:39:33.117 [INFO] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Skipping task ':extractUserdev' as it is up-to-date (took 0.033 secs).\n14:39:33.118 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':extractUserdev'\n14:39:33.118 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :extractUserdev UP-TO-DATE\n14:39:33.118 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :extractUserdev (Thread[main,5,main]) completed. Took 0.036 secs.\n14:39:33.120 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :getAssetIndex (Thread[main,5,main]) started.\n14:39:33.120 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :getAssetIndex\n14:39:33.120 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Starting to execute task ':getAssetIndex'\n14:39:33.121 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: \n14:39:33.121 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: \n14:39:33.122 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolving: {CACHE_DIR}/assets/indexes/{ASSET_INDEX}.json\n14:39:33.122 [DEBUG] [net.minecraftforge.gradle.util.delayed.DelayedBase] Resolved: /home/ubuntu/.gradle/caches/minecraft/assets/indexes/1.8.json\n14:39:33.123 [DEBUG] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Determining if task ':getAssetIndex' is up-to-date\n14:39:33.131 [INFO] [org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter] Executing task ':getAssetIndex' (up-to-date check took 0.008 secs) due to:\nTask.upToDateWhen is false.\n14:39:33.131 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter] Executing actions for task ':getAssetIndex'.\n14:39:33.541 [DEBUG] [sun.net.www.protocol.http.HttpURLConnection] sun.net.www.MessageHeader@5df920896 pairs: {GET /Minecraft.Download/indexes/1.8.json HTTP/1.1: null}{User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.95 Safari/537.11}{If-None-Match: \"ec766c1642f6db8ab4469933d6aeae31\"}{Host: s3.amazonaws.com}{Accept: text/html, image/gif, image/jpeg, .tar=01;31:.arc=01;31:.taz=01;31:.lz4=01;31:.lzma=01;31:.txz=01;31:.t7z=01;31:.z=01;31:.dz=01;31:.lrz=01;31:.lzo=01;31:.bz2=01;31:.tbz=01;31:.tz=01;31:.rpm=01;31:.war=01;31:.sar=01;31:.alz=01;31:.zoo=01;31:.7z=01;31:.cab=01;31:.jpeg=01;35:.bmp=01;35:.pgm=01;35:.tga=01;35:.xpm=01;35:.tiff=01;35:.svg=01;35:.mng=01;35:.mov=01;35:.mpeg=01;35:.mkv=01;35:.ogm=01;35:.m4v=01;35:.vob=01;35:.nuv=01;35:.asf=01;35:.rmvb=01;35:.avi=01;35:.flv=01;35:.dl=01;35:.xwd=01;35:.cgm=01;35:.ogv=01;35:.aac=00;36:.flac=00;36:.mid=00;36:.mka=00;36:.mpc=00;36:.ra=00;36:.oga=00;36:.spx=00;36:*.xspf=00;36:, SHLVL=2, LESSCLOSE=/usr/bin/lesspipe %s %s, COMPIZ_CONFIG_PROFILE=ubuntu, UPSTART_JOB=unity-settings-daemon, QT_IM_MODULE=ibus, TERM=xterm-256color, XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/usr/share/upstart/xdg:/etc/xdg, LANG=en_US.UTF-8, XDG_SEAT_PATH=/org/freedesktop/DisplayManager/Seat0, GNOME_KEYRING_CONTROL=, XDG_SESSION_TYPE=x11, XDG_SESSION_ID=c1, DISPLAY=:0, GDM_LANG=en_US, XDG_GREETER_DATA_DIR=/var/lib/lightdm-data/ubuntu, UPSTART_EVENTS=started starting, DESKTOP_SESSION=ubuntu, GPG_AGENT_INFO=/home/ubuntu/.gnupg/S.gpg-agent:0:1, USER=ubuntu, QT_ACCESSIBILITY=1, VTE_VERSION=4205, XDG_SEAT=seat0, SSH_AUTH_SOCK=/run/user/1000/keyring/ssh, QT_QPA_PLATFORMTHEME=appmenu-qt5, XDG_RUNTIME_DIR=/run/user/1000, XDG_VTNR=7, HOME=/home/ubuntu, GNOME_KEYRING_PID=}\n14:39:35.892 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING\n14:39:35.896 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'.\n14:39:35.909 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED\n14:39:35.916 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java''\n14:39:35.916 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...\n14:39:37.057 [QUIET] [system.out] [14:39:37] [main/INFO] [GradleStart]: Extra: []\n14:39:37.058 [QUIET] [system.out] [14:39:37] [main/INFO] [GradleStart]: Running with arguments: [--userProperties, {}, --assetsDir, /home/ubuntu/.gradle/caches/minecraft/assets, --assetIndex, 1.8, --accessToken{REDACTED}, --version, 1.8, --tweakClass, net.minecraftforge.fml.common.launcher.FMLTweaker]\n14:39:37.114 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.151 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Using primary tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.152 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLTweaker\n14:39:37.202 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Forge Mod Loader version 11.14.3.1543 for Minecraft 1.8 loading\n14:39:37.204 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Java is OpenJDK 64-Bit Server VM, version 1.8.0_111, running on Linux:aarch64:3.10.96-tegra, installed at /usr/lib/jvm/java-8-openjdk-arm64/jre\n14:39:37.233 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Managed to load a deobfuscated Minecraft name- we are in a deobfuscated environment. Skipping runtime deobfuscation\n14:39:37.248 [QUIET] [system.out] [14:39:37] [main/INFO] [FML]: Found a command line coremod : com.microsoft.Malmo.OverclockingPlugin\n14:39:37.280 [QUIET] [system.out] [14:39:37] [main/WARN] [FML]: The coremod com.microsoft.Malmo.OverclockingPlugin does not have a MCVersion annotation, it may cause issues with this version of Minecraft\n14:39:37.291 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.293 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n14:39:37.296 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.298 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n14:39:37.299 [QUIET] [system.out] [14:39:37] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:37.497 [QUIET] [system.out] [14:39:37] [main/ERROR] [FML]: The binary patch set is missing. Either you are in a development environment, or things are not going to work!\n14:39:38.785 [QUIET] [system.out] [14:39:38] [main/ERROR] [FML]: FML appears to be missing any signature data. This is not a good thing\n14:39:38.787 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:38.799 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n14:39:38.838 [QUIET] [system.out] [14:39:38] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n14:39:39.594 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.TerminalTweaker\n14:39:39.596 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.TerminalTweaker\n14:39:39.622 [QUIET] [system.out] [14:39:39] [main/INFO] [LaunchWrapper]: Launching wrapped minecraft {net.minecraft.client.main.Main}\n14:39:39.768 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:52]: MALMO: Attempting to transform MinecraftServer\n14:39:39.779 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:147]: MALMO: Found Minecraft, attempting to transform it\n14:39:39.781 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:153]: MALMO: Found Minecraft.runGameLoop() method, attempting to transform it\n14:39:39.783 [QUIET] [system.out] [14:39:39] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:168]: MALMO: Hooked into call to Minecraft.updateDisplay()\n14:39:40.596 [QUIET] [system.out] [14:39:40] [main/ERROR] [LaunchWrapper]: Unable to launch\n14:39:40.596 [QUIET] [system.out] java.lang.reflect.InvocationTargetException\n14:39:40.596 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat net.minecraft.launchwrapper.Launch.launch(Launch.java:135) [launchwrapper-1.12.jar:?]\n14:39:40.597 [QUIET] [system.out] \tat net.minecraft.launchwrapper.Launch.main(Launch.java:28) [launchwrapper-1.12.jar:?]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_111]\n14:39:40.597 [QUIET] [system.out] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat net.minecraftforge.gradle.GradleStartCommon.launch(Unknown Source) [start/:?]\n14:39:40.598 [QUIET] [system.out] \tat GradleStart.main(Unknown Source) [start/:?]\n14:39:40.598 [QUIET] [system.out] Caused by: java.lang.UnsatisfiedLinkError: /home/ubuntu/.gradle/caches/minecraft/net/minecraft/natives/1.8/liblwjgl.so: /home/ubuntu/.gradle/caches/minecraft/net/minecraft/natives/1.8/liblwjgl.so: wrong ELF class: ELFCLASS32 (Possible cause: architecture word width mismatch)\n14:39:40.598 [QUIET] [system.out] \tat java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_111]\n14:39:40.598 [QUIET] [system.out] \tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.Runtime.loadLibrary0(Runtime.java:870) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat java.lang.System.loadLibrary(System.java:1122) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys$1.run(Sys.java:73) ~[lwjgl-2.9.1.jar:?]\n14:39:40.599 [QUIET] [system.out] \tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys.doLoadLibrary(Sys.java:66) ~[lwjgl-2.9.1.jar:?]\n14:39:40.599 [QUIET] [system.out] \tat org.lwjgl.Sys.loadLibrary(Sys.java:95) ~[lwjgl-2.9.1.jar:?]\n14:39:40.600 [QUIET] [system.out] \tat org.lwjgl.Sys.(Sys.java:112) ~[lwjgl-2.9.1.jar:?]\n14:39:40.600 [QUIET] [system.out] \tat net.minecraft.client.Minecraft.getSystemTime(Unknown Source) ~[Minecraft.class:?]\n14:39:40.600 [QUIET] [system.out] \tat net.minecraft.client.main.Main.main(Unknown Source) ~[Main.class:?]\n14:39:40.600 [QUIET] [system.out] \t... 12 more\n14:39:40.602 [ERROR] [system.err] Exception in thread \"main\" 14:39:40.604 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: java.lang.reflect.InvocationTargetException\n14:39:40.606 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n14:39:40.608 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n14:39:40.610 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n14:39:40.623 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.reflect.Method.invoke(Method.java:498)\n14:39:40.625 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraftforge.gradle.GradleStartCommon.launch(Unknown Source)\n14:39:40.626 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat GradleStart.main(Unknown Source)\n14:39:40.628 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: Caused by: net.minecraftforge.fml.relauncher.FMLSecurityManager$ExitTrappedException\n14:39:40.629 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraftforge.fml.relauncher.FMLSecurityManager.checkPermission(Unknown Source)\n14:39:40.635 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.SecurityManager.checkExit(SecurityManager.java:761)\n14:39:40.636 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.Runtime.exit(Runtime.java:107)\n14:39:40.637 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat java.lang.System.exit(System.java:971)\n14:39:40.642 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraft.launchwrapper.Launch.launch(Launch.java:138)\n14:39:40.643 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \tat net.minecraft.launchwrapper.Launch.main(Launch.java:28)\n14:39:40.649 [QUIET] [system.out] [14:39:40] [main/INFO] [STDERR]: [java.lang.Throwable$WrappedPrintStream:println:748]: \t... 6 more\n14:39:41.001 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: FAILED\n14:39:41.003 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with exit value 1 (state: FAILED)\n14:39:41.005 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':runClient'\n14:39:41.005 [LIFECYCLE] [class org.gradle.TaskExecutionLogger] :runClient FAILED\n14:39:41.006 [INFO] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] :runClient (Thread[main,5,main]) completed. Took 5.193 secs.\n14:39:41.006 [DEBUG] [org.gradle.execution.taskgraph.AbstractTaskPlanExecutor] Task worker [Thread[main,5,main]] finished, busy: 12.272 secs, idle: 0.024 secs14:39:41.023 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.030 [ERROR] [org.gradle.BuildExceptionReporter] FAILURE: Build failed with an exception.\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter] * What went wrong:\n14:39:41.031 [ERROR] [org.gradle.BuildExceptionReporter] Execution failed for task ':runClient'.\n14:39:41.032 [ERROR] [org.gradle.BuildExceptionReporter] > Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with non-zero exit value 1\n14:39:41.033 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.033 [ERROR] [org.gradle.BuildExceptionReporter] * Exception is:\n14:39:41.035 [ERROR] [org.gradle.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':runClient'.\n14:39:41.036 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69)\n14:39:41.037 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46)\n14:39:41.038 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35)\n14:39:41.038 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)\n14:39:41.045 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)\n14:39:41.046 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52)\n14:39:41.047 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)\n14:39:41.048 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53)\n14:39:41.049 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\n14:39:41.049 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203)\n14:39:41.050 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185)\n14:39:41.051 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:62)\n14:39:41.052 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50)\n14:39:41.053 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25)\n14:39:41.054 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110)\n14:39:41.055 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)\n14:39:41.056 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)\n14:39:41.056 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)\n14:39:41.057 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)\n14:39:41.058 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)\n14:39:41.059 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)\n14:39:41.059 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)\n14:39:41.060 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:155)\n14:39:41.061 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.Factories$1.create(Factories.java:22)\n14:39:41.062 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n14:39:41.062 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)\n14:39:41.063 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:152)\n14:39:41.064 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33)\n14:39:41.064 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:100)\n14:39:41.065 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:94)\n14:39:41.066 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)\n14:39:41.067 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)\n14:39:41.068 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:94)\n14:39:41.069 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83)\n14:39:41.070 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)\n14:39:41.074 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77)\n14:39:41.075 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47)\n14:39:41.076 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51)\n14:39:41.077 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28)\n14:39:41.077 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43)\n14:39:41.078 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170)\n14:39:41.079 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237)\n14:39:41.079 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210)\n14:39:41.080 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35)\n14:39:41.080 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24)\n14:39:41.082 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206)\n14:39:41.085 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)\n14:39:41.086 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.Main.doAction(Main.java:33)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)\n14:39:41.087 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.launcher.GradleMain.main(GradleMain.java:23)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:127)\n14:39:41.088 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61)\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'command '/usr/lib/jvm/java-8-openjdk-arm64/bin/java'' finished with non-zero exit value 1\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367)\n14:39:41.089 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.process.internal.DefaultJavaExecAction.execute(DefaultJavaExecAction.java:31)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.tasks.JavaExec.exec(JavaExec.java:60)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:226)\n14:39:41.090 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:219)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:208)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)\n14:39:41.091 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter] \tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter] \t... 60 more\n14:39:41.092 [ERROR] [org.gradle.BuildExceptionReporter]\n14:39:41.092 [LIFECYCLE] [org.gradle.BuildResultLogger]\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger] BUILD FAILED\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger]\n14:39:41.093 [LIFECYCLE] [org.gradle.BuildResultLogger] Total time: 22.741 secs\n14:39:41.095 [DEBUG] [org.gradle.api.internal.tasks.compile.daemon.CompilerDaemonManager] Stopping 0 compiler daemon(s).\n14:39:41.096 [INFO] [org.gradle.api.internal.tasks.compile.daemon.CompilerDaemonManager] Stopped 0 compiler daemon(s).\n14:39:41.099 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on cp_proj class cache for build file '/home/ubuntu/Malmo/Minecraft/build.gradle' (/home/ubuntu/.gradle/caches/2.7/scripts/build_28dl6d7gt773pezkarluvxmug/cp_proj).\n14:39:41.099 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on proj class cache for build file '/home/ubuntu/Malmo/Minecraft/build.gradle' (/home/ubuntu/.gradle/caches/2.7/scripts/build_28dl6d7gt773pezkarluvxmug/proj).\n14:39:41.101 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache plugin-use-metadata.bin (/home/ubuntu/.gradle/caches/2.7/plugin-resolution/plugin-use-metadata.bin)\n14:39:41.101 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on Plugin Resolution Cache (/home/ubuntu/.gradle/caches/2.7/plugin-resolution).\n14:39:41.104 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache module-metadata.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/module-metadata.bin)\n14:39:41.105 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache artifact-at-repository.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/artifact-at-repository.bin)\n14:39:41.105 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache artifact-at-url.bin (/home/ubuntu/.gradle/caches/modules-2/metadata-2.15/artifact-at-url.bin)\n14:39:41.106 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on artifact cache (/home/ubuntu/.gradle/caches/modules-2).\n14:39:41.120 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache outputFileStates.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/outputFileStates.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache fileSnapshots.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/fileSnapshots.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache fileHashes.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/fileHashes.bin)\n14:39:41.121 [DEBUG] [org.gradle.cache.internal.btree.BTreePersistentIndexedCache] Closing cache taskArtifacts.bin (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts/taskArtifacts.bin)\n14:39:41.122 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on task history cache (/home/ubuntu/Malmo/Minecraft/.gradle/2.7/taskArtifacts).\n14:39:41.123 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.CachedStoreFactory] Resolved configuration cache closed. Cache reads: 0, disk reads: 0 (avg: 0.0 secs, total: 0.0 secs)\n14:39:41.123 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.CachedStoreFactory] Resolution result cache closed. Cache reads: 0, disk reads: 0 (avg: 0.0 secs, total: 0.0 secs)\n14:39:41.124 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.resolveengine.store.ResolutionResultsStoreFactory] Deleted 2 resolution results binary files in 0.002 secs\n14:39:41.124 [DEBUG] [org.gradle.api.internal.artifacts.ivyservice.ivyresolve.memcache.InMemoryCachedRepositoryFactory] In-memory dependency metadata cache closed. Repos cached: 44, cache instances: 6, modules served from cache: 4, artifacts: 2`", "type": "commented", "related_issue": null}, {"user_name": "katja-hofmann", "datetime": "Dec 20, 2016", "body": "", "type": "", "related_issue": null}, {"user_name": "nottug", "datetime": "Dec 20, 2016", "body": "Yeah I have 3 different Java 8s installed but I've tried all of them. If I attempt to run the others, I get an immediate build failure. This is the", "type": "commented", "related_issue": null}, {"user_name": "nottug", "datetime": "Dec 20, 2016", "body": "default 64 bit sdk. The issue might be the Tegra architecture not being recognized, but I'm not sure.", "type": "commented", "related_issue": null}, {"user_name": "mfuntowicz", "datetime": "Jan 9, 2017", "body": "Hi @traw1234, are you trying to run Malmo experiment through SSH connection on the Jetson ?\nI suspect that there is no desktop environment initialized on the device, so Minecraft cannot start.Can you try starting Minecraft on a Desktop environment and launching only the agent on embedded device ?Morgan", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Jan 25, 2017", "body": "Am closing this due to inactivity; @traw1234 please re-open and provide more details if this is still a problem.", "type": "commented", "related_issue": null}, {"user_name": "nottug", "datetime": "Dec 20, 2016", "body": [], "type": "issue", "related_issue": "tambetm/minecraft-py#1"}, {"user_name": "DaveyBiggers", "datetime": "Jan 25, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1763", "issue_status": " Closed\n", "issue_list": [{"user_name": "madisonestabrook", "datetime": "Nov 26, 2018", "body": "\nThe project is not as accessible as it could be, especially to users who have disabilities:\nThe project should be accessible to all users.\n\nComplete the following steps to reproduce the accessibly issues:\n\nNotice how accessibility can be improved.\n\nI would very much like to work on this issue!", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " How do you propose to improve the accessibility of the app? As the issue, you raised falls under the category of enhancement so I guess a bit more elaboration is called for.", "type": "commented", "related_issue": null}, {"user_name": "madisonestabrook", "datetime": "Nov 27, 2018", "body": "@AakashMallik  ARIA; however, I am willing to learn whatever tools are best for the job. I am currently studying for Google's Mobile Web Specialist Certification and recently graduated from the Grow with Google Front-End Web Development Nanodegree Scholarship program.", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " Covering so many components in a single issue is not advisable. How about we start with one component at a time.", "type": "commented", "related_issue": null}, {"user_name": "madisonestabrook", "datetime": "Nov 27, 2018", "body": "@AakashMallik Okay; good point. How about doing fix search area first?", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Nov 27, 2018", "body": " Cool, go for it. Good luck.  Edit your issue description accordingly as now you would open one issue for each major componet.", "type": "commented", "related_issue": null}, {"user_name": "Pipe-Runner", "datetime": "Dec 11, 2018", "body": " Any update on this issue?", "type": "commented", "related_issue": null}, {"user_name": "mariobehling", "datetime": "May 24, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi.ai/issues/1632", "issue_status": " Closed\n", "issue_list": [{"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": "Currently, the lighthouse score is low for Performance, AccessibilityImprove itGo to Audit in inspect element, then run test for Performance, Accessibility.Current score:\nYes", "type": "commented", "related_issue": null}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "pull", "related_issue": "#1634"}, {"user_name": "fragm3", "datetime": "Oct 21, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "anshumanv", "datetime": "Oct 26, 2018", "body": [], "type": "pull", "related_issue": null}, {"user_name": "anshumanv", "datetime": "Oct 26, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "pulkit1joshi", "datetime": "Jan 20, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/fossasia/susi_server/issues/632", "issue_status": " Closed\n", "issue_list": [{"user_name": "abishekvashok", "datetime": "Dec 20, 2017", "body": "Travis CI build is not parallelizedTravis CI bulild should be parallelized\nWe should do this to reduce build time and make user interaction with the logs easy and to improve accessibility of the logs. It also helps us the pin point which component was broken.\nJob 1 - build, test, deploy.\n(we can't change this as we should deploy only when we pass all tests and compilation is successful)\nJob 2 - CodecovSee a Travis Ci build log or .travis.yml fileSure, taking it up as part of GCI.", "type": "commented", "related_issue": null}, {"user_name": "RahulMetre03", "datetime": "Dec 21, 2017", "body": "I would like to work on that issue.Plz guide about I should do", "type": "commented", "related_issue": null}, {"user_name": "abishekvashok", "datetime": "Dec 21, 2017", "body": " sorry for that, I have worked on it and opened a PR.", "type": "commented", "related_issue": null}, {"user_name": "abishekvashok", "datetime": "Dec 20, 2017", "body": [], "type": "pull", "related_issue": "#635"}, {"user_name": "the-dagger", "datetime": "Dec 30, 2017", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/Project-MONAI/MONAI/issues/3482", "issue_status": " Closed\n", "issue_list": [{"user_name": "Nic-Ma", "datetime": "Dec 13, 2021", "body": "\nThanks for the interesting technical discussion with     , as we still have many unclear requirements and unknown use cases, we plan to develop the model package feature step by step, May adjust the design based on feedback during the development.For the initial step, the core team aligned to develop a small but typical example for  first, it will use JSON config files to define environments, components and workflow, save the config and model into TorchScript model. then other projects can easily reconstruct the exact same python program and parameters to reproduce the inference. When the small MVP is ready, will share and discuss within the team for the next steps.I will try to implement the MVP referring to some existing solutions, like NVIDIA Clara MMAR, ignite online package, etc. Basic task steps:", "type": "commented", "related_issue": null}, {"user_name": "dbericat", "datetime": "Dec 13, 2021", "body": " this is the current PR to expand model metadata.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 14, 2021", "body": "Hi     ,I updated the ticket description to make it more clear.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "wyli", "datetime": "Dec 14, 2021", "body": "We've discussed that the MVP should focus on the minimal structure, including model weights, basic environment/system info, maintaining changelog/versioning, how to ensure accessibility for both human and machine.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 14, 2021", "body": "Hi  ,Sure, I added these items to the MVP task list.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "dbericat", "datetime": "Dec 14, 2021", "body": "       thoughts on proposed MVP?", "type": "commented", "related_issue": null}, {"user_name": "ericspod", "datetime": "Dec 14, 2021", "body": "We should discuss a clear definition of requirements and objectives. We want to define a format of a single file or multiples files which contains the model weights at least with secondary information describing how to use it for various use cases. This will allow a human or a program to determine what sort of model it is, how to use the model, and what tasks to use it for. For our MVP we want to consider the starting position of what the model weight storage and metadata storage would look like and if it would achieve that objective to some degree.The base level set of requirements I would suggest are:One use case for this information is a human user looking into how the model is used in a particular task. They would want a clear idea of what inputs are expected and what the outputs mean. Whatever format this information is can be either easily read by a human or easily converted into a convenient format using included tools.A second use case is a deployment environment which automatically constructs whatever infrastructure is needed around a model to present it through a standard interface. This would require generating transform sequences automatically to pre- and post-process data, and load the model through some script defining the workflow. This would used by MONAI Deploy to automatically generate a MAP from the package, or another script automatically create a Docker image to serve the model as a command line tool or through Flask, or another script to interface with existing hosting services to upload the model and whatever other information is needed.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 15, 2021", "body": "Hi  ,Thanks very much for your detailed description.\nI will try to prepare a draft model package for discussion according to your summary and our basic ideas, and then develop the necessary features to support it.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "wyli", "datetime": "Dec 16, 2021", "body": "I'd strongly recommend that the MVP should support the essential features for fast prototyping of model packages. The following uses the popular  package as an example:These are mainly to ensure a good flexibility and extensibility of the packaging.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 16, 2021", "body": "Hi  ,Thanks for your great suggestions.\nThese are very good references for config parsing logic in steps 3 and 4 of this ticket, we definitely need similar features for flexibility and extensibility.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 16, 2021", "body": "Hi     ,I tried to prepare a draft model package structure for the  of this ticket in the tutorial PR.\n\nCould you please help take a look and share some feedback for the model package structure?\n(1) I didn't implement any config-parsing related logic so far, I think that's later steps.\n(2) I am not very sure to put some description strings in  or , so I put in both sides, would like to see your ideas.Thanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "ericspod", "datetime": "Dec 23, 2021", "body": "To recap where we are with existing issues/PRs:Related issues:", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 23, 2021", "body": "Hi  ,Thanks for your great summary!\nThere is another related PR: .Thanks.", "type": "commented", "related_issue": null}, {"user_name": "wyli", "datetime": "Jan 10, 2022", "body": "for the record, we could consider this cache dataset config vs content Integrity issue .", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Jan 17, 2022", "body": "Hi    ,Another thing I want to mention is that: we can also consider to add the package format verification with predefined , for example: check the JSON syntax, check necessary fields in the meta data, check essential folders, etc.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "ericspod", "datetime": "Jan 17, 2022", "body": "We'll certainly want to check the format of specific fields in the json data so that they correctly adhere to a standard way of describing transforms and inference so that code can be generated from them. With the other data that can be added it would be preferrable that this can be as free-form as people like, so I would expect standardized elements in the metadat adhering to our format mixed in with ones that are application-specific in any format.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Jan 17, 2022", "body": " sounds good to me! We can do that when we finalized the first MVP MMAR example.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "MMelQin", "datetime": "Jan 21, 2022", "body": "As for the machine consumer of the model package, specifically TrochScript, we do have a specific one, . Triton can automatically generate runtime config for all the backends it supports, except PyTorch. Triton team had logged an issue for PyTorch, though based on the discussions we've already had here, the requested metadata are covered in the MONAI model package. Here is the ,", "type": "commented", "related_issue": null}, {"user_name": "MMelQin", "datetime": "Jan 21, 2022", "body": "For deployment of AI apps/models in clinical environment, there are a number of persona and user stories.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Jan 21, 2022", "body": "Hi  ,Thanks very much for your detailed feedback!\nThese are important information for metadata. It's a long way to finalize the metadata content and format.\nLet's try to provide a minimum metadata for the first version, then add or adjust according to users' feedback.\nWe also definitely should define the metadata , , , etc.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Feb 19, 2022", "body": "Another related PR relating to the configuration parsing: .Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Feb 21, 2022", "body": "Related PR for common training, evaluation and inference:  .Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Feb 21, 2022", "body": "As we are still developing the  in PR , I compared it with the Hydra : \nThere are several interesting features of it we still not support:I think we can try to support 3 with enhanced idea: use the \"@\" mark to define a placeholder, then parser gives the value.\nAnd we can try to support 4 to make the base config similar to \"base class with abstractmethod\" that must be overridden.For other features we may consider later.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "ericspod", "datetime": "Feb 21, 2022", "body": "I have a PR now on the specification document  which should have been a draft PR but oh well. It's very minimal compared to what  prototyped  and lacks details about intent, use cases, design expectations, etc.", "type": "commented", "related_issue": null}, {"user_name": "wyli", "datetime": "Feb 25, 2022", "body": "I think  is now intuitive and flexible after a few rounds of refactoring. We may want to provide a system-wide flag to optionally disable  because it is too powerful and unsafe..", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Feb 28, 2022", "body": "I put all the links of related PRs to the task steps in the above ticket description.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Mar 7, 2022", "body": "Related PR for inference example bundle: .Thanks.", "type": "commented", "related_issue": null}, {"user_name": "wyli", "datetime": "Mar 20, 2022", "body": "follow-up from the dev meeting:", "type": "commented", "related_issue": null}, {"user_name": "ericspod", "datetime": "Mar 21, 2022", "body": "Revising this tutorial to use our bundle would be a good exemplar: ", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Mar 22, 2022", "body": "After sharing with the internal NVIDIA Clara team, I put 2 missing features in the task feature.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Mar 30, 2022", "body": "Feedback from MONAI deploy team:Welcome more testing and feedback later.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Apr 4, 2022", "body": "Most of the tasks are completed, I created several new tickets to track the left items, and let's close this big first ticket now.Thanks.", "type": "commented", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 13, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 13, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "Project-MONAI/monai-deploy-app-sdk#212"}, {"user_name": "Nic-Ma", "datetime": "Dec 15, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ericspod", "datetime": "Dec 15, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Dec 16, 2021", "body": [], "type": "issue", "related_issue": "Project-MONAI/tutorials#486"}, {"user_name": "Nic-Ma", "datetime": "Dec 20, 2021", "body": [], "type": "pull", "related_issue": "#3518"}, {"user_name": "Nic-Ma", "datetime": "Jan 6, 2022", "body": [], "type": "pull", "related_issue": "#3593"}, {"user_name": "gigony", "datetime": "Jan 13, 2022", "body": [], "type": "issue", "related_issue": "Project-MONAI/monai-deploy-app-sdk#213"}, {"user_name": "dbericat", "datetime": "Jan 13, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "Nic-Ma", "datetime": "Feb 19, 2022", "body": [], "type": "pull", "related_issue": "#3822"}, {"user_name": "Nic-Ma", "datetime": "Feb 21, 2022", "body": [], "type": "pull", "related_issue": "#3832"}, {"user_name": "ericspod", "datetime": "Feb 21, 2022", "body": [], "type": "pull", "related_issue": "#3834"}, {"user_name": "Nic-Ma", "datetime": "Feb 28, 2022", "body": [], "type": "pull", "related_issue": "#3865"}, {"user_name": "Nic-Ma", "datetime": "Mar 2, 2022", "body": [], "type": "issue", "related_issue": "google/python-fire#356"}, {"user_name": "wyli", "datetime": "Mar 11, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#3927"}, {"user_name": "wyli", "datetime": "Mar 17, 2022", "body": [], "type": "issue", "related_issue": "#3966"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#3974"}, {"user_name": "wyli", "datetime": "Mar 22, 2022", "body": [], "type": "pull", "related_issue": "#3982"}, {"user_name": "Nic-Ma", "datetime": "Mar 24, 2022", "body": [], "type": "pull", "related_issue": "#3994"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#4029"}, {"user_name": "Nic-Ma", "datetime": "Apr 4, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/1471", "issue_status": " Closed\n", "issue_list": [{"user_name": "k-nut", "datetime": "Oct 28, 2017", "body": "I was reading the  and could not understand what the last column means. The header suggests that this is a boolean value but there are three options (red/green/gray). Does green mean that it does need a model or not? It would also be cool to avoid using only color as a means of encoding this information because it makes it difficult for colourblind red/green-blind people to judge the information.I would propose to use symbols here or at least add a legend or tooltips on hover.If you can explain the three colors to me I am more than happy to send a pull request or some mockups for how this could be displayed differently. ", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 28, 2017", "body": "Thanks for the feedback – this is super valuable! The original idea here was green = yes (based on model predictions, so needs model data), red = no, grey = neutral / \"it depends\" (which, admittedly, is a bit vague – but both serialization and training don't actually require a model to be loaded in order to work - like dependency parsing for example – but they can be used in combination with one). Either way, I totally see how this is confusing.Honestly, the more I look at this table, the more I'm like \"wtf was I thinking when I built this\", haha. It's also an accessibility nightmare btw – important information expressed with only icons and no other distinction except for colour (whyyyy?), and red/green/grey out of all colours. And  okay, I did add ARIA labels, but they were bad, so screen readers can't parse this mess either. So yeah, sorry about that. No idea why I did this.As a solution, how about replacing the icons with \"yes\" and \"no\", and maybe adding an additional note to the \"depends\" features? The same also applies to the feature comparison . And while I'm at it, I'll also add some proper ARIA attributes to the before/after code examples (e.g.  – it's more obvious visually, but still completely inaccessible).I'm happy to take care of this, since it'll require some more fixes and decisions on the front-end – but let me know if this makes sense, and if you have any other suggestions! I've been playing around with this, and the \"needs model\" thing is slightly trickier than I thought. I do think it's important to get this information across as early as possible, since it's one of the main sources of confusion for beginners. It's not immediately intuitive which functionality is built into spaCy, and what needs a statistical model. On the other hand, it's not always so black and white. The  in English uses the part-of-speech tags to predict a token's lemma – however, many languages also come with a lookup table that doesn't require a model at all. But this distinction is a little complex for one of the very first sections of the beginners guide... ", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 28, 2017", "body": "Current work in progress: now with distinctive icons and label. I also think that a better policy for categorising the features that \"need a model\" is to interpret \"need\" as \"absolutely need\", i.e. \"can't do without\" (at least, not by default in spaCy).", "type": "commented", "related_issue": null}, {"user_name": "emste", "datetime": "Oct 30, 2017", "body": "Thanks for improving the website / documentation!\nMaybe it's worth adding a sentence above the table to describe the \"needs model\" column? If I didn't know SpaCy or NLP tools in general, I might wonder what it exactly means. Does it mean SpaCy is not feature complete? The red mark looks like something is \"missing\", \"not yet supported\" or \"not available\". In conjunction with \"needs model\" it sounds like we still wait for somebody to train a model for tokenization.Maybe something like this:", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 31, 2017", "body": " Good point, thanks! I hadn't really thought about the problems with the positive/negative connotation of the icons in this context. We definitely don't want new users to get the impression that tokenization not requiring a statistical model is somehow \"bad\" or \"worse\". (In fact, the main point of the table should be to demystify the processing pipeline and move away from just reducing it to  – which I feel like hasn't particularly come across well in the past.)Maybe we should just remove the \"needs model\" column and replace it with a column that describes how spaCy \"does it\" and where the information comes from? For example, something like:I really like your text suggestion btw. We could use that for the intro, plus maybe the text that's currently in the aside on the right (where it kinda gets lost at the moment – asides should be for additional notes, not for critical information).We could still have an aside there that goes into more details about what exactly a \"statistical model\" is, what it does, where to find the models and install them etc.", "type": "commented", "related_issue": null}, {"user_name": "k-nut", "datetime": "Oct 31, 2017", "body": "I agree with the problem of the positive/negative connotations of the icons and think replacing them would be a good idea.  I like your idea of removing the column entirely. Thinking about it now: The main goal of this table is to show the reader what they can do with spacy, right? So it might not even be necessary to explain the way in which spacy does this because all the readers want to know is: What can I do with this tool? The actual implementation details could be deferred further down to the individual sections.\nOn the other hand it might be interesting to learn something about the internals in the summary table already for people that have some more knowledge of NLP and would like to understand, how spacy implements certain features...", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 31, 2017", "body": " Yes, I think you're right! The main reason I wanted to cover the models in this section was to show what's statistical and what isn't – and to get this across as early as possible.This may seem obvious to users who already have some knowledge of NLP, but to a beginner, this isn't necessarily intuitive. (And that whole \"magical AI\" narrative that's often used when talking about the technology doesn't help, either.)For example: How does spaCy assign part-of-speech tags or named entities? Are there rules? Does the code in the library do all the magic? And what does it mean if spaCy \"gets it wrong\"? I also think that the concept of loading and installing models – and choosing between the different options – becomes much more obvious once you view the models as \"binary data packs that enable spaCy to make predictions\".In terms of the 101 guide, another option could be to add this as a separate section underneath the features table and cover the components of the statistical models. Or just give a quick overview of what can be included – like, binary weights for the tagger, parser and entity recognizer, lexical entries in the vocabulary, word vectors etc.If we manage to get this across right, I hope that what a new users takes away from this will be: \"Ah, cool, so I can use spaCy to find entities in my text. All I need to do is plug in a binary data pack spaCy can use to make the predictions. And if I want different results for a different language or domain, I need to plug in a different data pack – or create my own!\"", "type": "commented", "related_issue": null}, {"user_name": "k-nut", "datetime": "Nov 1, 2017", "body": "I really like this new approach os separating the two issues! ", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Nov 1, 2017", "body": "Yay – it's all on  now and will be uploaded as soon as  is out. So I'm closing this issue for now. Thanks again for your feedback!", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "May 8, 2018", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 28, 2017", "body": [], "type": "added", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 28, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 28, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "ines", "datetime": "Oct 31, 2017", "body": [], "type": "", "related_issue": null}, {"user_name": "ines", "datetime": "Nov 1, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "lock", "datetime": "May 8, 2018", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/433", "issue_status": " Open\n", "issue_list": [{"user_name": "ArneBinder", "datetime": "Jul 24, 2020", "body": "I have written a generic dataset for corpora created with the Brat annotation tool (, ). Now I wonder how to use that to create specific dataset instances. What's the recommended way to reuse formats and loading functionality for datasets with a common format?In my case, it took a bit of time to create the Brat dataset and I think others would appreciate to not have to think about that again. Also, I assume there are other formats (e.g. conll) that are widely used, so having this would really ease dataset onboarding and adoption of the library.", "type": "commented", "related_issue": null}, {"user_name": "thomwolf", "datetime": "Jul 29, 2020", "body": "Hi , we have a few \"generic\" datasets which are intended to load data files with a predefined format:You can find more details about this way to load datasets here in the documentation: Maybe your brat loading script could be shared in a similar fashion?", "type": "commented", "related_issue": null}, {"user_name": "ArneBinder", "datetime": "Jul 29, 2020", "body": " that was also my first idea and I think I will tackle that in the next days. I separated the code and created a real abstract class  to allow to inherit from that (I've just seen that the dataset_loader loads the first non abstract class), now  is very similar in its functionality to  but inherits from .However, it is still not clear to me how to add a specific dataset (as explained in ) to your repo that uses this format/abstract class, i.e. re-using the  entry of the   object and . Again, by doing so, the only remaining entries/functions to define would be , ,  and  (which is all copy-paste stuff) and .In a lack of better ideas, I tried sth like below, but of course it does not work outside  ( is currently defined in ):Nevertheless, many thanks for tackling the dataset accessibility problem with this great library!", "type": "commented", "related_issue": null}, {"user_name": "ArneBinder", "datetime": "Jul 31, 2020", "body": "As temporary fix I've created  (contributions welcome).", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/950", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Feb 4, 2020", "body": "Since now the wakeword works when Firefox is in the background, it can often be strange to be working with the background browser. Ideally we'd raise the window.Maybe sometimes this isn't desirable, I'm not sure.One of  might do the trick, though I suspect not. If not, then it might require a webexperiment API implementation.Some accessibility advise is specifically  to do this, as it can interfere with accessibility helper tools, but I don't understand the specifics.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 6, 2020", "body": "Using  unfortunately does not work – it focuses one window among the other Firefox windows, but does not itself bring Firefox to the front.Implementing this will be more complicated than I hoped as a result. Pushing this to the backlog as a result.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 5, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 5, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 6, 2020", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 6, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 20, 2020", "body": [], "type": "issue", "related_issue": "#1037"}]},
{"issue_url": "https://github.com/explosion/spaCy/issues/3356", "issue_status": " Closed\n", "issue_list": [{"user_name": "rulai-huajunzeng", "datetime": "Mar 5, 2019", "body": "I found a bug where tokenization is completely not working with version 2.1.0a10 on python 2.7. I have reproduced this on three of my machines.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 6, 2019", "body": "Hmm that's very confusing. I don't see how our tests could be passing if this is the case. I don't suppose you're somehow getting a narrow unicode build of Python?Another thing to check is that your  line is actually running the version of pip you expect. But then if it werent't I don't see how spaCy would be in your environment. Hmm.", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 6, 2019", "body": "It's quite easy to reproduce for me. The following is another run. I also checked my python and pip and they are correct.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 6, 2019", "body": "Okay, thanks! Definitely seems bad --- will look into it.", "type": "commented", "related_issue": null}, {"user_name": "svlandeg", "datetime": "Mar 7, 2019", "body": "I can't reproduce either :(\n : what happens if you use the default English model instead?", "type": "commented", "related_issue": null}, {"user_name": "handsomezebra", "datetime": "Mar 7, 2019", "body": "Using the default English model the result is correct. But using spacy.load('en_core_web_sm') the result is the same. I am guessing maybe there are conflicts with previous installed spacy models?", "type": "commented", "related_issue": null}, {"user_name": "svlandeg", "datetime": "Mar 7, 2019", "body": "  : Any chance you could run this entire sentence through the  pipeline?\n", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 7, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 7, 2019", "body": "Thanks for your patience on this, this is definitely mysterious!And yeah, maybe what you're loading as  here isn't actually the correct model package? Could you print  and see what that returns?", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 7, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 19, 2019", "body": "I still got the same issue even using the stable release of 2.1.0", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 20, 2019", "body": "I suspect there might actually be a connection to , especially since you and  both had the same problems  we weren't able to reproduce either of them yet on any of our test machines and CI... So there must be  configuration here that we might be missing? I'm not sure, this is so confusing ", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "The tests are green in the exact same environment as produces the tokenization issues:(.env) [  6:30PM ]  [ jfelectron@madmax:~/Documents/Repos/spaCy/spacy/tests(master) ]\n$ pytest -svv\n==================================================================================================================================================================================== test session starts =====================================================================================================================================================================================\nplatform linux2 -- Python 2.7.15+, pytest-4.0.2, py-1.7.0, pluggy-0.8.0 -- /usr/bin/python\ncachedir: .pytest_cache\nmetadata: {'Python': '2.7.15+', 'Platform': 'Linux-4.18.0-10-generic-x86_64-with-Ubuntu-18.10-cosmic', 'Packages': {'py': '1.7.0', 'pytest': '4.0.2', 'pluggy': '0.8.0'}, 'Plugins': {'html': '1.19.0', 'xdist': '1.15.0', 'timeout': '1.3.3', 'metadata': '1.7.0'}}\nrootdir: /home/jfelectron/Documents/Repos/spaCy, inifile:\nplugins: xdist-1.15.0, timeout-1.3.3, metadata-1.7.0, html-1.19.0\ncollected 2123 items\n...\n1482 passed, 585 skipped, 56 xfailed in 62.04 seconds", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "In [2]: import spacyIn [3]: spacy.\nOut[3]: '2.1.1'In [4]: spacy_nlp = spacy.load('en_core_web_sm')In [5]: [d.text for d in spacy_nlp(u'I have a dog named spot')]\nOut[5]:\n[u'I',\nu'h',\nu'a',\nu'v',\nu'e',\nu'a',\nu'd',\nu'o',\nu'g',\nu'n',\nu'a',\nu'm',\nu'e',\nu'd',\nu's',\nu'p',\nu'o',\nu't']", "type": "commented", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 20, 2019", "body": "Thanks for running more tests! This is consistent with what's been reported before. The problem seems to occur when deserializing the tokenization rules from the model, not when using a blank language class like . The tests shipped with the library do not test the models, so it makes sense they succeed.If you could run the  command in your environment and post the environment variables that you have set here, that might be helpful (just double-check and make sure to remove stuff like secrets etc.). We're trying to pin down the exact configuration that causes the problems – we tried lots of different combinations but we haven't been able to reproduce it yet.", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "How about compiler tool chain or Cython? It looks like spacy leaves many of its deps unpinned to a specific version.Or how about compression/serialization tool chain?", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 20, 2019", "body": "Now I can reproduce it using miniconda docker image.First create a Dockerfile as below:Then run , you will be able to see:", "type": "commented", "related_issue": null}, {"user_name": "rulai-huajunzeng", "datetime": "Mar 20, 2019", "body": "Also tried the following base images, both can reproduce easily.", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": " what environments are you using? Linux or OS X?LC_ADDRESS=en_US.UTF-8\nXDG_CONFIG_DIRS=/etc/xdg/xdg-pop:/etc/xdg\nLC_TELEPHONE=en_US.UTF-8\nLANG=en_US.UTF-8\nDISPLAY=:1\nSHLVL=1\nLOGNAME=jfelectron\nLANGUAGE=en_US:en\nMANDATORY_PATH=/usr/share/gconf/pop.mandatory.path\nLC_NAME=en_US.UTF-8\nXDG_VTNR=2\nUSER=jfelectron\nXAUTHORITY=/run/user/1000/gdm/Xauthority\nPWD=/home/jfelectron/Documents/Repos/spaCy\nGTK_IM_MODULE=ibus\nGJS_DEBUG_TOPICS=JS ERROR;JS LOG\nXDG_SESSION_ID=3\nCOLORTERM=truecolor\nGNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/aa51ec0d_5220_4740_957e_4aa554ffa884\nDESKTOP_SESSION=pop\nXDG_SESSION_DESKTOP=pop\nGDMSESSION=pop\nGNOME_DESKTOP_SESSION_ID=this-is-deprecated\nDEFAULTS_PATH=/usr/share/gconf/pop.default.path\nWINDOWPATH=2\nPAPERSIZE=letter\nLC_MEASUREMENT=en_US.UTF-8\nLC_NUMERIC=en_US.UTF-8\nLC_MONETARY=en_US.UTF-8\nLC_PAPER=en_US.UTF-8\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\nVTE_VERSION=5401\nUSERNAME=jfelectron\nCLUTTER_IM_MODULE=xim\nGNOME_TERMINAL_SERVICE=:1.114\nQT4_IM_MODULE=xim\nXDG_DATA_DIRS=/usr/share/pop:/usr/local/share/:/usr/share/\nXDG_MENU_PREFIX=gnome-\nLC_IDENTIFICATION=en_US.UTF-8\nIM_CONFIG_PHASE=2\nSHELL=/usr/bin/zsh\nGNOME_SHELL_SESSION_MODE=pop\nQT_IM_MODULE=xim\nLC_TIME=en_US.UTF-8\nTERM=xterm-256color\nSESSION_MANAGER=local/madmax:@/tmp/.ICE-unix/2674,unix/madmax:/tmp/.ICE-unix/2674\nGJS_DEBUG_OUTPUT=stderr\nXDG_SESSION_TYPE=x11\nGTK_MODULES=gail:atk-bridge\nXDG_CURRENT_DESKTOP=pop:GNOME\nSSH_AGENT_PID=2818\nPATH=/home/jfelectron/Documents/Repos/spaCy/.env/bin:/home/jfelectron/.local/bin:/home/jfelectron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/cuda/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin\nSSH_AUTH_SOCK=/run/user/1000/keyring/ssh\nXMODIFIERS==ibus\nHOME=/home/jfelectron\nXDG_RUNTIME_DIR=/run/user/1000\nGPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1\nXDG_SEAT=seat0\nQT_ACCESSIBILITY=1\nOLDPWD=/home/jfelectron\nPYTHONPATH=:/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext\nZSH=/home/jfelectron/.oh-my-zsh\nPAGER=less\nLESS=-R\nLC_CTYPE=en_US.UTF-8\nLSCOLORS=Gxfxcxdxbxegedabagacad\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tgz=01;31:.arj=01;31:.lha=01;31:.lzh=01;31:.tlz=01;31:.tzo=01;31:.zip=01;31:.Z=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.tzst=01;31:.bz=01;31:.tbz2=01;31:.deb=01;31:.jar=01;31:.ear=01;31:.rar=01;31:.ace=01;31:.cpio=01;31:.rz=01;31:.wim=01;31:.dwm=01;31:.jpg=01;35:.mjpg=01;35:.gif=01;35:.pbm=01;35:.ppm=01;35:.xbm=01;35:.tif=01;35:.png=01;35:.svgz=01;35:.pcx=01;35:.mpg=01;35:.m2v=01;35:.webm=01;35:.mp4=01;35:.mp4v=01;35:.qt=01;35:.wmv=01;35:.rm=01;35:.flc=01;35:.fli=01;35:.gl=01;35:.xcf=01;35:.yuv=01;35:.emf=01;35:.ogx=01;35:.au=00;36:.m4a=00;36:.midi=00;36:.mp3=00;36:.ogg=00;36:.wav=00;36:.opus=00;36:.xspf=00;36:\nVIRTUAL_ENV=/home/jfelectron/Documents/Repos/spaCy/.env\nPS1=(.env) (git_prompt_info)$fg[yellow]$(rvm_prompt_info)$fg_bold[blue] ]$reset_color\n$\n_=/usr/bin/env", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "$ pip freeze\natomicwrites==1.3.0\nattrs==19.1.0\nblis==0.2.4\ncertifi==2019.3.9\nchardet==3.0.4\nconfigparser==3.7.3\ncymem==2.0.2\nCython==0.29.6\nen-core-web-sm==2.1.0\nenum34==1.1.6\nflake8==3.5.0\nfuncsigs==1.0.2\nfunctools32==3.2.3.post2\nidna==2.8\njsonschema==2.6.0\nmccabe==0.6.1\nmock==2.0.0\nmore-itertools==5.0.0\nmurmurhash==1.0.2\nnumpy==1.16.2\npathlib==1.0.1\npathlib2==2.3.3\npbr==5.1.3\nplac==0.9.6\npluggy==0.9.0\npreshed==2.0.1\npy==1.8.0\npycodestyle==2.3.1\npyflakes==1.6.0\npytest==4.0.2\npytest-timeout==1.3.3\nrequests==2.21.0\nscandir==1.10.0\nsix==1.12.0\nspacy==2.1.1\nsrsly==0.0.5\nthinc==7.0.4\ntqdm==4.31.1\nuncommon==1.0.2.14072\nuncommon-testing==1.0.2.11140\nurllib3==1.24.1\nvulcan==1.0.2.14072\nwasabi==0.1.4", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": " we are both using py27, my sense is you and  are using Py3, have you tried a py2.7 venv?", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "Otherwise, what the the lowest level thing we can check to identify the root cause here?", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 20, 2019", "body": " Yes of course we've tried Python 2...We've tried configuring the locale to ASCII, tried setting the , and different combinations of the two. We have Python2.7 in our test suite, so the CI tests Python2.7 for every pull request and every release. Thanks, I'll try that container.", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "Your test suite doesn't cover this behavior, they are green for me too, it's irrelevant unless in your CI env you are running integration tests that do load models?? If so how do we invoke them?", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 20, 2019", "body": "Haven't run the container yet, but I had a look at the definition. I think it's the use of the  locale : I'm not 100% sure but I think your locale looks broken too. Your  environment variable isn't set, which looks suspicious: ", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "LC_ALL isn't set on ANY Ubuntu based system I have access to.  So either it's a Debian/Ubuntu problem or Spacy is expecting something that most environment simply can't provide.  Why are we only seeing this on 2.1?", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 20, 2019", "body": "I  my locale, seems to have no impact.In [1]: from vulcan.text_spacy import init_spacyIn [2]: spacy_nlp = init_spacy()In [3]: [d.text for d in spacy_nlp(u'I have a dog named spot')]\nOut[3]:\n[u'I',\nu'h',\nu'a',\nu'v',\nu'e',\nu'a',\nu'd',\nu'o',\nu'g',\nu'n',\nu'a',\nu'm',\nu'e',\nu'd',\nu's',\nu'p',\nu'o',\nu't']In [4]: import osIn [5]: os.environ\nOut[5]: {'LC_NUMERIC': 'en_US.UTF-8', 'LESS': '-R', 'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated', 'GJS_DEBUG_OUTPUT': 'stderr', 'LC_CTYPE': 'en_US.UTF-8', 'XDG_CURRENT_DESKTOP': 'pop:GNOME', 'LC_PAPER': 'en_US.UTF-8', 'QT_IM_MODULE': 'xim', 'LOGNAME': 'jfelectron', 'USER': 'jfelectron', 'HOME': '/home/jfelectron', 'XDG_VTNR': '2', 'PATH': '/home/jfelectron/.local/bin:/home/jfelectron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/cuda/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin:/home/jfelectron/bin:/usr/local/bin:/usr/share/idea/bin', 'ZSH': '/home/jfelectron/.oh-my-zsh', 'DISPLAY': ':1', 'SSH_AGENT_PID': '2727', 'LANG': 'en_US.UTF-8', 'TERM': 'xterm-256color', 'SHELL': '/usr/bin/zsh', 'XAUTHORITY': '/run/user/1000/gdm/Xauthority', 'LANGUAGE': 'en_US.UTF-8', 'SESSION_MANAGER': 'local/madmax:@/tmp/.ICE-unix/2559,unix/madmax:/tmp/.ICE-unix/2559', 'SHLVL': '1', 'MANDATORY_PATH': '/usr/share/gconf/pop.mandatory.path', 'QT_ACCESSIBILITY': '1', 'QT4_IM_MODULE': 'xim', 'CLUTTER_IM_MODULE': 'xim', 'WINDOWPATH': '2', 'IM_CONFIG_PHASE': '2', 'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1', 'LC_MONETARY': 'en_US.UTF-8', 'USERNAME': 'jfelectron', 'XDG_SESSION_DESKTOP': 'pop', 'XDG_RUNTIME_DIR': '/run/user/1000', 'LC_IDENTIFICATION': 'en_US.UTF-8', 'LC_ADDRESS': 'en_US.UTF-8', 'PYTHONPATH': ':/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext:/home/jfelectron/Documents/Repos/phoenix/shared:/home/jfelectron/Documents/Repos/phoenix/plato:/home/jfelectron/Documents/Repos/phoenix/sourcerer:/home/jfelectron/Documents/Repos/phoenix/vulcan:/home/jfelectron/Documents/Repos/phoenix/zoltar:/home/jfelectron/Documents/Repos/phoenix/tests:/home/jfelectron/Documents/Repos/phoenix/olympus:/home/jfelectron/Documents/Repos/flashtext', 'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh', 'VTE_VERSION': '5401', 'GDMSESSION': 'pop', 'XMODIFIERS': '=ibus', 'GNOME_SHELL_SESSION_MODE': 'pop', 'XDG_DATA_DIRS': '/usr/share/pop:/usr/local/share/:/usr/share/', 'LC_ALL': 'en_US.UTF-8',", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 20, 2019", "body": "Dude, I said I wasn't sure. I'm working on it. Back off okay?", "type": "commented", "related_issue": null}, {"user_name": "jfelectron", "datetime": "Mar 21, 2019", "body": "Don't get me wrong, I think we want to help. The tests are uninformative, so I'm looking for the lowest entry point to break in code (Python or not) where we can figure out what's going on here.", "type": "commented", "related_issue": null}, {"user_name": "lock", "datetime": "Apr 21, 2019", "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.", "type": "commented", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 6, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 19, 2019", "body": [], "type": "issue", "related_issue": "#3438"}, {"user_name": "ines", "datetime": "Mar 19, 2019", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 19, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 19, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 20, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 22, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 22, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "honnibal", "datetime": "Mar 22, 2019", "body": [], "type": "pull", "related_issue": "#3460"}, {"user_name": "ines", "datetime": "Mar 22, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ines", "datetime": "Mar 22, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "lock", "datetime": "Apr 21, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "Kiku-Reise", "datetime": "Jun 18, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/359", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "Oct 2, 2019", "body": "Enable \"Open developer tools\" and related synonyms:", "type": "commented", "related_issue": null}, {"user_name": "lelouchB", "datetime": "Mar 19, 2020", "body": "  Is this still open??", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Mar 19, 2020", "body": "Yes, . Are you interested in working on it?", "type": "commented", "related_issue": null}, {"user_name": "lelouchB", "datetime": "Mar 19, 2020", "body": "Yes, I am", "type": "commented", "related_issue": null}, {"user_name": "lelouchB", "datetime": "Mar 19, 2020", "body": " please assign me", "type": "commented", "related_issue": null}, {"user_name": "lelouchB", "datetime": "Mar 19, 2020", "body": " Thanks, Do you have any idea on how to proceed?", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Mar 19, 2020", "body": " might be able to offer suggestions?", "type": "commented", "related_issue": null}, {"user_name": "lelouchB", "datetime": "Mar 19, 2020", "body": "Cool, I'll wait then\nthanks", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 3, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "awallin", "datetime": "Mar 19, 2020", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/102", "issue_status": " Open\n", "issue_list": [{"user_name": "Wolfman1219", "datetime": "Aug 17, 2022", "body": "C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\venv\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\nwarnings.warn(\"No audio backend is available.\")\nTraceback (most recent call last):  File \"c:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\demo\\demo.py\", line 7, in \nfrom engine import SpeechRecognitionEngine  File \"C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\engine.py\", line 10, in \nfrom neuralnet.dataset import get_featurizer\nFile \"C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\neuralnet\\dataset.py\", line 6, in \nfrom utils import TextProcess\nImportError: cannot import name 'TextProcess' from 'utils' (C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\venv\\lib\\site-packages\\utils__.py)", "type": "commented", "related_issue": null}, {"user_name": "AbdouBou", "datetime": "Aug 26, 2022", "body": "add this to dataset.py before the from utils import TextProcess Lineimport sys\nsys.path.insert(0, 'PATH_TO_NEURALNET_FOLDER') #C:\\Users\\Abduraxim\\A-Hackers-AI-Voice-Assistant\\VoiceAssistant\\speechrecognition\\neuralnetif it doesn'T resolve the error try to delete the utils (most probably you installed via pip/pip3) using pip uninstall utils.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/77", "issue_status": " Open\n", "issue_list": [{"user_name": "dharani211", "datetime": "Aug 28, 2021", "body": "I want to add these features to the voice assistantAttaching my sample voice assistant here\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/30", "issue_status": " Open\n", "issue_list": [{"user_name": "pietmlr", "datetime": "Nov 27, 2020", "body": "I constantly getting this error:\n`During handling of the above exception, another exception occurred:Traceback (most recent call last):\nFile \"/Users/pietmuller/Dokumente/code/sr.venv/speechrecognition/neuralnet/dataset.py\", line 110, in \nlabel = self.text_process.text_to_int_sequence(self.data['text'].iloc[idx])\nFile \"/Users/pietmuller/Dokumente/code/sr.venv/speechrecognition/neuralnet/utils.py\", line 51, in text_to_int_sequence\nch = self.char_map[c]\nKeyError: 'D'`\nI didn't change anything in the utils.py file.\nI'm also wondering why it says that there is a KeyError with the character: \"D\". In the variable char_map_str (in utils.py) is no capital D mentioned. Also I want to train it on a common voice dataset (German to be exact)It' not only 'D', it's 'D' and 'E' alternately but no other characters...", "type": "commented", "related_issue": null}, {"user_name": "pietmlr", "datetime": "Nov 27, 2020", "body": "Okay I could fix this by adding some characters to the map and adding .lower() at the and of the label variable in dataset.py\nBut now it says that my spectrograms are to big..", "type": "commented", "related_issue": null}, {"user_name": "NoCodeAvaible", "datetime": "Mar 5, 2021", "body": "Hey mate did you fix it?", "type": "commented", "related_issue": null}, {"user_name": "pietmlr", "datetime": "Mar 5, 2021", "body": " No, I couldn't, but now I am building my own automatic speech recognition system in Keras/Tensorflow.", "type": "commented", "related_issue": null}, {"user_name": "NoCodeAvaible", "datetime": "Mar 16, 2021", "body": "Fixed it! Add me on Discord I'am german too so I can tell u everythink about it:) Name: SheeeshForce1#8083", "type": "commented", "related_issue": null}, {"user_name": "NoCodeAvaible", "datetime": "Mar 16, 2021", "body": "If you not german I could tell you more about this because the KeyError will occur even if you lowercase.\nBut in a different way", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 12, 2021", "body": " How did you fix the error? I fixed the KeyError by changing the sentence to only lowercase letters, but I didn't fix the spectrogram which is too big. So I would be very interested how you did that.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 12, 2021", "body": " it's pretty late for me I will tell you tommorow:)", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 11, 2021", "body": " Can you tell me now?\nI originally thought I would have fixed it, but it doesn't allow the big letters. (I don't get any error, but it's not liking the big letters)", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/53", "issue_status": " Open\n", "issue_list": [{"user_name": "Jochen-sys", "datetime": "Apr 9, 2021", "body": "Hello first of all nice work!\nI wanted to ask if I could use mp3 files instead of wav files and which lines I have to change for that, if this is working?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 9, 2021", "body": "Hello  I think that's not possible.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 9, 2021", "body": " because the spectograms are made out of the wav files", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 9, 2021", "body": "  Thanks for the quick answer.\nBut I also can create  a spectogram with a mp3 file. Admitting I only tested it with tensorflow and I know it works there.\nIf it didn't work with pytorch, would it be possible to do the one task with tensorflow and the rest with pytorch?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 9, 2021", "body": " hmmm good question I actually don't know if this is possible but I think yes you can do one part with tensorflow and the other with pytorch you just need to fed the spectograms some how into pytorch.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 9, 2021", "body": " but why would you want to use mp3 files instead of wav files it's much easier to handel and format them:)", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 10, 2021", "body": " First of all I don't have so much storage on my computer.\nBut the bigger problem is my GPU, so I have to train with google colab. Wav Files are too big, mp3 files are not so big. Another idea was only to upload the spectogram to google colab, so the files wouldn't be too big.\nIs there a quality different between wav and mp3? I know that wav files have a better quality, but I don't really think this is so important in training. Or it's better, because the model has to transcript worse quality.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 10, 2021", "body": " mp3 files are like any normal files, wav files(wave files ) are constructed different they look quite different too because every sound is displayed as a wave, mp3's on the other hand not. I don't know how it would change the performance of the model or the training. U can add me on discord: SheeeshForce#8083", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 10, 2021", "body": " Thanks for explaning. I wanted to test the engine.py, but I got an error \"ImportError: cannot import name 'imsave'\". imsave is from scipy.misc and I found out that stackoverflow means it should be imageio. Now I'm confused, because I think it should work with imsave?! Could you help me out there please?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 10, 2021", "body": " I am actually not familiar with imsave and imageio", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 10, 2021", "body": " Ok but can you run engine.py without problems?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 10, 2021", "body": " at the moment not because for installing the ctcdecoder I have to switch to Linux.", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 10, 2021", "body": " Ok I'm sorry I'm an idiot, I fixed it. My problem was that I thought neuralnet would be a regular pypi package and not a special self programmed one. Why did you name scripts or folders like other existing packages on pypi :-) (there is sadly no smiley which is laughing)?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 11, 2021", "body": " Where exactly will the spectrograms be produced? There is so much code with spectrograms, I don't find the exact one.", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 15, 2021", "body": " Which version of ctcdecode do you use? (Mine worked a few days ago, but than it failed)\nWhat does the ken_lm file mean? Is this the file which did such a good transcription in the video?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 15, 2021", "body": " I don't was able to test ctcdecode yet", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 17, 2021", "body": " Ok got it.\nDo I have to use the ckpt file for training from a checkpoint (argument for --load_model_from)? And how can I get zip file in the end of training or a ckpt file? I think I need a zip file for transcription with the microphone, but I also would like to  get a ckpt file for further training in the future.", "type": "commented", "related_issue": null}, {"user_name": "NoCodeAvaible", "datetime": "Apr 18, 2021", "body": "Hey  yes you have:) The model will be saved automatically as a ckpt file:) Yes I think that you need one too(btw I need also one ) because I think without the zip we get no outputs. Could you please add me on discord please so we could talk there and speed up communication? Name:SheeeshForce1#8083", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 20, 2021", "body": "Sorry I don't have discord.\nOk thanks. I'm getting the folowing error when I use the argument --load_model_from speechrecognition.ckpt:\nRuntimeError: Error(s) in loading state_dict for SpeechModule:\nUnexpected key(s) in state_dict: \"model.cnn.0.weight\", \"model.cnn.0.bias\", \"model.cnn.1.norm.weight\", \"model.cnn.1.norm.bias\", \"model.dense.0.weight\", \"model.dense.0.bias\", \"model.dense.1.weight\", \"model.dense.1.bias\", \"model.dense.4.weight\", \"model.dense.4.bias\", \"model.dense.5.weight\", \"model.dense.5.bias\", \"model.lstm.weight_ih_l0\", \"model.lstm.weight_hh_l0\", \"model.lstm.bias_ih_l0\", \"model.lstm.bias_hh_l0\", \"model.layer_norm2.weight\", \"model.layer_norm2.bias\", \"model.final_fc.weight\", \"model.final_fc.bias\".Does anyone now what this means?Then I tried to use the argument --resume_from_checkpoint (I don't know what this argument is doing, sorry) instead of --load_model_from. But this doesn't work, too. Following error:\ncheckpoint_callbacks[-1].best_model_path = checkpoint['checkpoint_callback_best_model_path']\nKeyError: 'checkpoint_callback_best_model_path'", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 21, 2021", "body": "Ok I fixed the first error. My version of pytorch_lightning was to old.\nBut what does the --resume_from_checkpoint argument mean?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 21, 2021", "body": " It means that you insert an checkpoint file as default or truh the terminal(set required false if you set it as default) and the training is being resumed from this checkpoint. U basically use it to  resume training if you stopped the training, if you want to test the checkpoint(model that you create in optimize_graph.py) or if your pc shuts down for an unknown reason while training.", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 24, 2021", "body": " Do you know why loss could be \"nan\"? At the beginning it worked with a real float, but now I only see this string there. I researched this, but didn't find a good cause.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 24, 2021", "body": " yes Cuda and cudnn are not installed the right way. U can search on YouTube for videos for a correct cuda and cudnn installation:)", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 24, 2021", "body": " Ouh ok that's interesting thanks. I'm using my CPU.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "Apr 24, 2021", "body": " are you working with mp3 files now?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "Apr 25, 2021", "body": "With Windows, no, because it doesn't work there with mp3 files, but it is working with Linux. I'm training with my Windows system, I only have Linux as vm.", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 1, 2021", "body": " Do you know what this ken_lm is for and where I could get it? Is this the file which improved the transcription in the video so much? When not, what was the file which improved the transcription so much?", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "May 1, 2021", "body": " did you already tested the speechrecognition?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 1, 2021", "body": " Yes with the zip model. But it's not so good. But I remember that in the video he used something else, too, to get good results.", "type": "commented", "related_issue": null}, {"user_name": "CracKCatZ", "datetime": "May 1, 2021", "body": " hold up did u used the portaudio library because I think that this library is required and can give better results. Could you please tell me if you have portaudio already installed at the beginning of working with this project or if you have to install it?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 8, 2021", "body": " Sorry for late response. Yes, I think so.\nTo come back to the loss=nan problem: Why isn't loss=nan when I train the same wav file for a few epochs? Could I try to train only one wav file per training or would the result be worse?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 12, 2021", "body": "Ok I tried some things and it  seems that there is a problem with the big letters but only in the first line. The second line doesn't care about big letters.\nHave you teained a kenlm model? When yes, how? I don't understand what I have to do, I'm sorry!", "type": "commented", "related_issue": null}, {"user_name": "Botirjon2009", "datetime": "Feb 16, 2022", "body": " Hi.. I am going to set my mp3 files. .. Actually I going to know where my mp3 files should be set?  I mean which section should be linked to mp3 files.. scripts\\common_voice_json.. ? right?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/25", "issue_status": " Open\n", "issue_list": [{"user_name": "samano99", "datetime": "Oct 27, 2020", "body": "File \"/home/User/anaconda3/envs/User/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/optimizer_connector.py\", line 47, in update_learning_rates\nmonitor_key = lr_scheduler['monitor']\nKeyError: 'monitor'pytorch_lightning.utilities.exceptions.MisconfigurationException: ReduceLROnPlateau requires returning a dict from configure_optimizers with the keyword monitor=. For example:return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'your_loss'}Any suggestion on solving this?", "type": "commented", "related_issue": null}, {"user_name": "Jochen-sys", "datetime": "May 11, 2021", "body": " I don't know whether you still have this issue, but a possible solution would be here:\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/LearnedVector/A-Hackers-AI-Voice-Assistant/issues/21", "issue_status": " Open\n", "issue_list": [{"user_name": "samano99", "datetime": "Oct 14, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "LearnedVector", "datetime": "Oct 15, 2020", "body": "looks like you're trying to train a speech recognition model. I think the size of your spectrogram is too big and maybe causing python to get stuck in a loop. you can change this line of code to accept large spectrogram size to like 2000 ", "type": "commented", "related_issue": null}, {"user_name": "samano99", "datetime": "Oct 17, 2020", "body": "in the optimze graph.py, i got \"python stopped working\". it stuck while saving the file\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AI/issues/19", "issue_status": " Open\n", "issue_list": [{"user_name": "Bhoot127", "datetime": "Nov 1, 2021", "body": "import pyttsx3\nimport datetime\nimport speech_recognition as sr\nimport wikipedia\nimport smtplib\nimport webbrowser as wb\nimport os\nimport requests\nfrom pprint import pprint\nimport pyautogui\nimport pyjokesengine = pyttsx3.init()\nvoices = engine.getProperty('voices')       # getting details of current voice\nengine.setProperty('voice', voices[1].id)   # For Female Voicedef speak(audio):\nengine.say(audio)\nengine.runAndWait()def time():\nTime = datetime.datetime.now().strftime(\"%I:%M:%S\")\nprint(Time)\nspeak(\"The current Time is\")\nspeak(Time)def date():\nyear  = int(datetime.datetime.now().year)\nmonth = int(datetime.datetime.now().month)\ndates = int(datetime.datetime.now().day)\nspeak(\"The current date is\")\nprint(dates)\nprint(month)\nprint(year)\nspeak(dates)\nspeak(month)\nspeak(year)def wishme():\nspeak(\"Welcome!\")\nhour = datetime.datetime.now().hour\nif hour >= 6 and hour < 12:\nspeak(\"Good Morning Sir\")\nelif hour >=12 and hour < 18:\nspeak(\"Good Afternoon Sir\")\nelif hour >=18 and hour < 24:\nspeak(\"Good Evening Sir\")\nelse:\nspeak(\"I hope you are enjoying your Night Sir\")\nspeak(\"Friday at your service. Please tell me how can i help you \")def takeCommand():\nr = sr.Recognizer()\nwith sr.Microphone() as source:\nprint(\"Listening...\")\nr.pause_threshold = 1\naudio = r.listen(source)\ntry:\nprint(\"Recognizing...\")\nquery = r.recognize_google(audio, language='en-in')\nprint(f\"You Said: {query}\\n\")def sendEmail(to, content):\nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.ehlo()\nserver.starttls()\nserver.login('', 'Password')\nserver.sendmail('', to, content)\nserver.close()def screenshot():\nimg = pyautogui.screenshot()\nimg.save('C:/Users/Amandeep/Desktop/Friday/screenshot.png')def jokes():\nhaha = pyjokes.get_joke()\nprint(haha)\nspeak(haha)if  == \"\":\nwishme()\nwhile True:\nquery = takeCommand().lower()", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/63", "issue_status": " Open\n", "issue_list": [{"user_name": "wahyouwebid", "datetime": "Apr 16, 2021", "body": "I have used Kaldi to trigger the speech but it doesn't work, I still have to click the button first\n\nplease for the solution, thank you", "type": "commented", "related_issue": null}, {"user_name": "rubycho", "datetime": "Apr 16, 2021", "body": "Though this comment is not the answer for your question, I suggest you to hide your API KEY on your screenshot.", "type": "commented", "related_issue": null}, {"user_name": "wahyouwebid", "datetime": "Apr 16, 2021", "body": "on the github there is also API KEY, this is proof\n", "type": "commented", "related_issue": null}, {"user_name": "rubycho", "datetime": "Apr 16, 2021", "body": "Oh it was the key from the repo. I thought it was your own key. Sorry.", "type": "commented", "related_issue": null}, {"user_name": "cdhiraj40", "datetime": "Mar 18, 2022", "body": "hey,  did you find a solution for this?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/61", "issue_status": " Open\n", "issue_list": [{"user_name": "Tedcas", "datetime": "Feb 11, 2021", "body": "Hi,I'm working in a new project where I need a custom voice assistant in my android app. I tried this example  and everything works perfectly, however, if I go from MainActivity to another activity and I try to load the assistant on this new activity, tts, stt and wake word doesn't work, I press the floating button and the assitant does its animation, but thats all, my question is, How can I implement something like the example project but extending the assistant in all the activities?Thank you very much in advance.", "type": "commented", "related_issue": null}, {"user_name": "bgubanov", "datetime": "Feb 12, 2021", "body": "Hello!\nAre you initializing the Aimybox object from the Application instance, like in the example?\nCan you send code snippet with the second activity?\nI added second activity to example project, copied code of first activity and everything started correctly on each of activities.", "type": "commented", "related_issue": null}, {"user_name": "Billthebest1", "datetime": "Feb 12, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "Tedcas", "datetime": "Feb 14, 2021", "body": "Hi, first of all many thanks for your reply ¡, I'm sorry I couldn't reply you earlier, answering your question, yes I'm initializing the Aimybox object from Application instance like the example, here is my code for that I've modified it a little bit to adjust to my project:class AimyboxApplication : Application(), AimyboxProvider {And this is the class I use to create the Assistant in every class of my project:class VoiceAssistant constructor(context: Context, container: Int) : Serializable, ActivitiesFather() {So in Activity 1 I call the following methods on onCreate():And then in the next activity I do the same (I simplified this step, calling those two statements in the father of all the Activities of my project)I'm not sure what I'm doing wrong, I must admit I'm not used to Kotlin (nowadays I'm learning the language) and I could make a misstake there.Again many thanks for your help in advance.", "type": "commented", "related_issue": null}, {"user_name": "Tedcas", "datetime": "Apr 23, 2021", "body": "Hi, it's been a while since my last posr, after doing some tests I've realized that the problem only happens when I use KaldiVoiceTrigger with the TTS, STT and DialogApi together, if I disable the KaldiVoiceTrigger system, all works perfectly. I've try updating all the dependencies to the last version, but nothing change.", "type": "commented", "related_issue": null}, {"user_name": "cdhiraj40", "datetime": "Jul 5, 2022", "body": "does KaldiVoiceTrigger work perfectly for you folks?  \nFor me, it just triggers every time I say anything! Let me know thanks.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-flutter/issues/12", "issue_status": " Open\n", "issue_list": [{"user_name": "MarsadMaqsood", "datetime": "Jul 20, 2022", "body": "I'm facing an issue with Flutter - Android. whenever I run this code, the app crashes.and in the studio.alan.app, I added", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Jul 21, 2022", "body": "Hey \nWhat do you try to achieve?\nIntent  will be activated on voice phrase.  executes command locally . it should contains an object:If you want to execute command with voice you can do following. In you voice script add  with object:And add command handler to you flutter app:", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Jul 20, 2022", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4197", "issue_status": " Open\n", "issue_list": [{"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": "I start it: ./NAME.sh\nThe screen gets black (two windows opened, one 'select vehicle' and the other has a longer name.\nOnce it's killed by ctrl+windows+alt F4 ->  eg. Africa_001 all is okay and I see following error messages:And I seeDEFAULTN/ADUDE Github is crashing I can't see what I'm writing", "type": "commented", "related_issue": null}, {"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": " (Issue where I've put short information but I thought it's not related and is closed)", "type": "commented", "related_issue": null}, {"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": "Note that if I start Abandoned Park only following error message appears: ", "type": "commented", "related_issue": null}, {"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": "I discovered this newly: ", "type": "commented", "related_issue": null}, {"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": "I can steer Africa now with the arrow keys (car) but why does it not drive/fly byself?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 13, 2021", "body": "Hi and welcome @debian-user-france1! AirSim vehicles don't drive by themselves. However, AirSim provides an API that you can use to accomplish that.", "type": "commented", "related_issue": null}, {"user_name": "arch-user-france1", "datetime": "Dec 6, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 13, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5718", "issue_status": " Open\n", "issue_list": [{"user_name": "bassamtantawi-botpress", "datetime": "Nov 26, 2021", "body": "\nIF you are using a carousel, and on the carousel you are adding a button, the only way to choose a Value is by clicking it.\nSteps to reproduce the behavior:\nTo be able to deal with the carousel like how you deal with choice field", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Dec 14, 2021", "body": "Could you please give a detailed example of what the problem is.", "type": "commented", "related_issue": null}, {"user_name": "bassamtantawi-botpress", "datetime": "Dec 14, 2021", "body": "I want to type the option I want to select instead of clicking it. We had a prospect who wanted to deal with the same bot but from web channel and voice channel.From web channel, the carousel is fine since I can click the button associated to it. But when it comes to voice it will be impossible to choose the value.So the bug is: if I have a choice field with values (yes/no) I can write (yes) and then the flow continues. But in carousel we can't do this.", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Dec 14, 2021", "body": "Thanks - this is not a bug, but a feature request. Will file accordingly", "type": "commented", "related_issue": null}, {"user_name": "bassamtantawi-botpress", "datetime": "Nov 26, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5714", "issue_status": " Open\n", "issue_list": [{"user_name": "bassamtantawi-botpress", "datetime": "Nov 26, 2021", "body": "\nWhen there is a markdown in the text, the TTS reads it, in addition to the emojis.\nSteps to reproduce the behavior:\nSpecial Characters are skipped. But things like \"!\" and \"?\" can affect the tone of the voice.", "type": "commented", "related_issue": null}, {"user_name": "bassamtantawi-botpress", "datetime": "Nov 26, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/34", "issue_status": " Open\n", "issue_list": [{"user_name": "a4awesome", "datetime": "Aug 14, 2020", "body": "Do you have any plans to support Python for voice scripts?", "type": "commented", "related_issue": null}, {"user_name": "okolyachko", "datetime": "Aug 18, 2020", "body": "Hi a4awesome,Thank you for your request, we will pass it to our R&D team and reach out to you as soon as there is any news.\nAt present, voice scripts in the Alan Studio are created in JavaScript that has a pretty easy syntax.Stay tuned!", "type": "commented", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 14, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "andreyryabov", "datetime": "Aug 17, 2020", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-ionic/issues/25", "issue_status": " Open\n", "issue_list": [{"user_name": "ljudbane", "datetime": "Dec 13, 2021", "body": "We have encountered an issue where on specific Android devices the voice input is not recognized. When Alan is listening, nothing happens, as if the voice is not recorded. No amount of app re-installs or different builds have helped, the issue is always present.Affected devices are:On Samsung S20 FE we tested your reference app \"Alan Playground\" (installed from Google Play) and voice input also doesn't work in your app. We will also test your app on Note 10 device at a later time and we will update the issue once we get results.I am attaching logcat from running our Ionic app on Samsung S20 FE. In the log i've marked the approximate location when user started speaking (line 416). A bit above that, on line 398 is app's log entry when button state callback fired and state was LISTEN. So somewhere around here Alan started listening.Ionic info:", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Dec 14, 2021", "body": "Hey \nThank you for detailed issue. We will try to reproduce it.\nDid you get microphone permission popup on this device?", "type": "commented", "related_issue": null}, {"user_name": "ljudbane", "datetime": "Dec 14, 2021", "body": " Yes, the permission popup was shown and approved. We also double checked in Android settings and permission for recording audio was enabled in App settings. The microphone does work with phone calls and other apps.", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Dec 13, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-ionic/issues/23", "issue_status": " Open\n", "issue_list": [{"user_name": "ljudbane", "datetime": "Dec 2, 2021", "body": "When we call callProjectApi() from ionic application to execute a command from Alan scripts, the callback doesn't get executed when running on Android or iOS device (or simulator). It does work when running in browser (ionic serve).For example we call Alan's function  from ionic:The callback function doesn't get called and those console logs don't ever get printed. Neither is there any caught exception. When testing on Android device we didn't see anything in the logs that would indicate what's the problem. But when running on iOS there is a log entry that could point to the problem:", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Dec 2, 2021", "body": "Hey \nWe will try to reproduce your issue.", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Dec 10, 2021", "body": "Hey \nWe found how to reproduce the issue which is related to your scenario. I will let you know when the fix for your case is ready.", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Jan 13, 2022", "body": " We have a new release \nPlease take a look - that should resolve this issue.", "type": "commented", "related_issue": null}, {"user_name": "ljudbane", "datetime": "Jan 17, 2022", "body": " Thank you, i will check it out and report back if the issue is fixed for me.", "type": "commented", "related_issue": null}, {"user_name": "ljudbane", "datetime": "Jan 19, 2022", "body": "I did some testing on iOS simulator.First i updated libs to most recent versions:First call in our app to project API is:And in the log of simulator there are not any console.log outputs from callback function, nor from the catch block:Then i click on a button in our app that launches another callProjectAPI:And in the log i see the same error that was present before at the time of original issue:Every subsequent callProjectApi will trigger the error. Only the first one won't. We plan to test on Android next, so i will report those findings later.", "type": "commented", "related_issue": null}, {"user_name": "ljudbane", "datetime": "Jan 20, 2022", "body": "I've also tested on Android and the code in callback function doesn't get executed. I don't see any error in logcat.", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Jan 20, 2022", "body": " Did you run  or  after plugin update and before run the app?", "type": "commented", "related_issue": null}, {"user_name": "ljudbane", "datetime": "Feb 1, 2022", "body": " Yes i did, multiple times. Sorry for the late reply.I will try to replicate the issue in one of your example apps.", "type": "commented", "related_issue": null}, {"user_name": "snyuryev", "datetime": "Dec 2, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/alan-ai/alan-sdk-web/issues/50", "issue_status": " Open\n", "issue_list": [{"user_name": "BennieMatthee", "datetime": "Dec 22, 2021", "body": "Did you use it with some framework (Angular, React, etc)?\nalan-sdk-webI cant seem to find an example on how to send a command to Alan from a text box.\nI would like to give the user the option to either use the voice command or type in their questions, but I cant find an example on how to do this.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/2955", "issue_status": " Open\n", "issue_list": [{"user_name": "AOS55", "datetime": "Aug 20, 2020", "body": "I am working on adding a FixedWing aircraft class into Airsim, so far I have been documenting my work under issue  but as suggested by  it would be usefult to move this work to a new issue so as to improve visibility for others to work on.Please see this  for a detailed breakdown of what has been added in my . I will aim to keep updates on this issue of new files, classes, methods & functions with a view to write comprehensive documentation as the code begins to get to a state where it can be merged back into the master branch on the main repo.Since the  on  I have been updating the FixedWingLibClient files to enable communication with Ardupilot. A linked issue has been opened within the .Initially I am aiming to work to resolve all the known issues and turn all the  in the state to . Following from this I will aim to add the following functionality:\n[ ] Construct a similar Python API as in multirotor <-- hopefully a short term objective\n[ ] Implement Undercarriage for fixedWingLandings\n[ ] Deep Reinforcement Learning (DRL) based FW landings <-- main PhD objective, expect to write & publish a paper based on this\n[ ] Validate model and control on real UAV and FW aircraft, ties in with above.Principally this is for my PhD at the  where I am aiming to use a combination of computer vision and Deep Reinforcement Learning to train an autonomous fixedwing agent/aircraft to navigate to and safely land in an unknown location. This is both with and without power and on a range of fixedwing aircraft including fixedWing UAVs, gliders & multiengine light piston aircraft (<5700Kg). Not only this but when discussing AirSim with my colleagues we believe that having a fixedWing simulator capable of highfidelity graphics and SITL that is efficient will be of great benefit.Several users have already been realy helpful ( ), if anyone has any ideas about how to solve some of the issues I have described above all help is greatly appreciated. In particular if anyone is able to have any success compiling the  please let me know as I'm a little stumped  as to why it won't compile.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 21, 2020", "body": "I had a try at compiling your branch, the error message was really quite useless.\nOpened  which fixes that and brings us back to normal compilation errors with understandable messages (for now)", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 24, 2020", "body": "I have been continuing to work on the FixedWingPhysics simulation aiming to resolve compilation errors. I have vectorised the control surfaces so ideally they behave as a  tyoe vector. With this I introduced a new file  that instants each control from the  file as a PhysicsBodyVertex class. I have not introduced the thrust vector yet but this shouldn't be too much of a challenge.Aircraft links to the structs in  and applies the aero-forces via . Most fixedwing dynamic models/databases, define the forces and moments around the CG, as . setWrench applies these and then is updated in the loop by , I'm a little lost as to how the PhysicsBody class updates the aircrafts pose via . The way I've set up the classes may also be an antipattern and I might need to refactor some of the structs, please let me know your thoughts on how you might setup the structures.When I run the repo from visual studio I get the following error :Im not sure what this error is being caused by, I'm guessing there is no termination condition in the recursive call. If anyone has experienced this before I'd love to get your thoughts.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 24, 2020", "body": "That's happening cause the function is calling itself here - \nIt should instead be calling , from it's parent classNote that I haven't tested this myself yet", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 24, 2020", "body": "Thanks yep that was it was quite obvious now I think about it in more detail. It several unresolved external symbol errors now but I think there a case of sorting out further problems with the API.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 24, 2020", "body": "Does anyone have any thoughts on this error coming from , I thought it was to do with the link between the base class and Mavlink/Px4 implementation override. I can't see anything untawards though:\nLNK2001\tunresolved external symbol \"protected: virtual float __cdecl msr::airlib::FixedWingApiBase::getAutoLookahead(float,float,float,float)const \" (?getAutoLookahead@FixedWingApiBase@airlib@msr@@MEBAMMMMM@Z) Blocks C:\\Users\\quessy\\Documents\\Unreal Projects\\AirSim\\Unreal\\Environments\\Blocks\\Intermediate\\ProjectFiles\\Module.AirSim.cpp.obj\t1", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 24, 2020", "body": "Functions seem to be missing, are all the files uncommented in the ?", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 24, 2020", "body": "Thats a good point I've uncommented them now, from , but Im still getting the same link errors from the compiler. When running build.cmd I also get some errors in the command prompt that are slightly different, . These are quite clearly what I should have been looking at, I didn't appreciate the importance of the API src client, server and base files.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 24, 2020", "body": "Resolved all build errors now except for one rather unusual one:\nNot sure where this one links to as when I look at the location of the error in object.hpp which I'm not sure where it lives in the repo as dependencies folder not included in push/pull requests.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "The RPC error was due to the fact that it doesn't know how to pack the Vector3r object, need to convert it into the struct defined in RpcLibAdaptorsBase and then pass it. Opened  which fixes that, and adds the files to CMake.\nAfter this AirLib compiles, but there are still lots of errors when building the entire Blocks env, undefined references in FixedWingApiBase.cpp, due to function declarations in .hpp, and usage of commented out methods", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Great thanks very much Ill take a look at those errors today, I merged the issue back into fixedwing and have no build.cmd errors on my end too.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Any thoughts on why I'm getting the following include/visibility related build error from in visual studio? I can see the source file  is still on the repo, could it be related to cmake changes?", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "That's very strange, Windows shouldn't be affected AFAIK since it uses the .sln files. And if this were a problem, then it would have appeared way earlier since support for Rover has been present for many months\nDoes this appear when running ?", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "No thats the unusual thing build.cmd doesnt present any errors when run", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "Hmm, maybe try running  from the root directory, that should rebuild everything.\nDo commit any changes made in the files present in the Blocks directory like in the AirLib folder before running though", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Great thanks that seems to have resolved it back to linking problems probably due to decleration again.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "Nice! Still have no idea why VS decided to remove the header file\nSide note, this does somewhat bring into mind that GitHub issues might not be the best way to communicate, especially on smaller problems. Maybe something for future", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Yeah not sure its unusual.\nYes certainly, I'm surprised there isn't a microsoft/AirSim slack setup or equivalent for more direct communication. As you said something for the future.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "There is a Facebook group, but I myself haven't used it since a long time.\nAP uses Discord (earlier Gitter) and has a forum as well, similar with PX4. Maybe after there is more activity and members. Having something like this would definitely reduce the number of issues also, since more confirmed issues will be created rather than duplicate and support ones", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Yes I agree thats a good idea, the documentation for AirSim would need to reflect this too as there is not an obvious point of contact when onboarding new users/developers with the application.Also after commenting out the linking errors in the application I have managed to compile through to the blocks environment .", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "Awesome, now comes the probably more difficult part of modelling, physics and making sure everything works!\nHaving a more direct method of communication will also increase community interaction a lot. Which platform to select, etc will have to be decided by the maintainers though, might require collecting some opinions from the users", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Yes I agree, would you like to open up a communication issue, I'm not sure if you can do a poll on github?", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "Don't think I'm the best person to open such a issue, a maintainer should do this probably. They'll also have to setup the platform and maybe maintain it (no other word is coming to my mind right now)\nGithub doesn't have polls AFAIK", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 25, 2020", "body": "Yes that makes sense, a moderator or project manager sort of thing. Perhaps if you know anyone you could  them into this issue so they can set it up. Ok sure you could always make the proposed solution as a comment and get people to vote   or  on the comment.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 25, 2020", "body": "    Do you think having another (more chat & voice type) communication platform will be worthwhile? This will definitely increase community interaction, and will probably reduce support issues.\nJust wanted to know your opinions, and definitely don't want to increase your (already heavy) workload too much.\nLooking forward to hearing from you!", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 26, 2020", "body": " AirLib is actually not compiling for me, getting some undeclared identifier errors for the function declarations which have been commented out. Here's a Travis build - , even the Windows build fails.\nDid some files not get committed when commenting out the API methods?Would also recommend activating Travis on your Github repositories (atleast Airsim), that has saved me a few times when making some changes which don't compile on Windows, or breaking something else. (Side note, I did break the Windows build in my first contribution to Airsim (and others later on also), that was a big motivation to get Travis setup properly on Airsim)", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 26, 2020", "body": " hmm ok, this is interesting could you please send me the identifier errors you are getting I will cross-check them on my local machine, I just ran update_to_git.bat and had no further changes and I also ran update_from_git.bat in Blocks and it still compiles ok on my end. A couple of thoughts:Nice Idea, I have setup , and it also appears to fail. Looking at the job log it has the following on line 1457. I wonder if this is unity related or just what the  script should print to the command line.", "type": "commented", "related_issue": null}, {"user_name": "rajat2004", "datetime": "Aug 26, 2020", "body": "Very strange that it works on your machine, these are the errors I'm getting -AirLib compilation doesn't depend on UE, Travis also doesn't have UE, it just tests whether AirLib, ROS wrapper compiles, and there's the recently added Azure Pipelines CI which compiles the entire Blocks project on Ubuntu and Windows and packages it.Yup, I'm rebuilding from the root directory, .That's just Travis printing the entire command which failed to run, the errors can be seen above it, starting from ", "type": "commented", "related_issue": null}, {"user_name": "seanmcleod", "datetime": "Oct 19, 2020", "body": "I mentioned that option, although I suggested using JSBSim for running the physics/aerodynamics, see my comment a couple of comments back - ", "type": "commented", "related_issue": null}, {"user_name": "xxEoD2242", "datetime": "Nov 18, 2020", "body": "Have you thought about changing this value in the Fast Physics Engine? I linked to the place where  is instantiated. I'd be really careful with this, as the time step calculations for this process may be heavily dependent on a sufficient timestep. Have you thought about taking the underpinnings of JBSim and modifying the FastPhysicsEngine to be something like kindFastPhysicsEngine with more realism?Myself and a team are looking to implement a fixed wing drone for a drone competition but we also want to update the physics models to be a bit more realistic for dynamics information so that we can implement our state-space models more efficiently then writing them externally in Python.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Nov 18, 2020", "body": "Hi  thanks for this. I think a JSBSim physics engine is what we should be heading towards it makes a lot of sense and prevents us reinventing the wheel. I have been working on a python implementation over the last month. But having it native to airsim in C++ would be ideal. The problem I've begun to realise over the past couple of days is that by pose forwarding JSBSim to airsim in CV-mode the collisions cannot be easily calculated from unreal as there is no physical mesh geometry like you get in the multirotor or car pawn.I'd certainly be keen to work together on this though. A native C++ framework is certainly the way to go long term, with a python API similar to multirotor. I think it might be best to let JSBSim deal with all of the fixedwing physics. JSBSim also has a ground interaction component, but I'm not entirely sure how it works yet, the aircraft I have defined so far does not have any ground-reactions in the aircraft XML file.", "type": "commented", "related_issue": null}, {"user_name": "xxEoD2242", "datetime": "Nov 18, 2020", "body": " That makes sense. I think the appropriate solution would be to implement JBSim natively in the build process with AirSim and have the selection of JBSim for dynamics information. I work with Purdue University researchers a lot so I'll see if there would be interest from the faculty there in exploring how we would achieve this. Signal routing through the same FastPhysicsEngine routes would be my biggest concern, especially from a memory standpoint to ensure that AirSim stays as optimized as possible. If we decide to go down this route, I'll let you know and possibly fork your branch since you have so much done.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Nov 18, 2020", "body": "The documentation and tests leave a little to be desired at the moment but you might find my  repo useful, it just implements the pose forwarding with CV mode described above and then runs JSBSim for the flight physics.The key classes are in ", "type": "commented", "related_issue": null}, {"user_name": "xxEoD2242", "datetime": "Nov 18, 2020", "body": "Awesome. Thanks!I think there is a desire to implement the physical drone as well and I have a teammate who is working on creating custom environments and scenarios. If we can figure out the blueprints and meshes correctly, we will try to implement that as well, which should help with the issue you are having in CV mode.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Nov 18, 2020", "body": "Great that would be really useful and its always useful to get others opinion on how to best implement this. Yes I'm not sure on the difference in efficiency JSBSim presents  may be able to help there.Great also would be very helpful", "type": "commented", "related_issue": null}, {"user_name": "seanmcleod", "datetime": "Nov 18, 2020", "body": "Yep, I see a feature request was raised for collision detection in CV-mode in Aug 2020 - So for now you would need to make use of JSBSim's collision detection via it's ground callback mechanism etc.I had originally suggested the JSBSim pose forwarding idea to Airsim in CV mode as a quick and fairly easy mechanism to implement. Linking JSBSim into AirSim and having some integration and switching mechanism with Airsim's physics engine is definitely doable but a lot more work.Would the Airsim repo maintainers be interested in incorporating JSBSim in this way? Or would you have to maintain your own fork?", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Nov 30, 2020", "body": "Hey a couple of questions for the airsim maintainers and perhaps JSBSim too. I have been looking at integrating JSBSim into AirSim  over the past week, using the car and multirotor pawns as a base. Im not sure how best to structure the code to get it to work with airsim.As  stated above jsbsim has a collision model built in for a STRUCTURE and BOGEY (undercarriage) element. There is a wrapper class in JSBSim  that is used to instantiate a jsbsim object and then the simulation can be advanced with run and properties accessed and set.Im not sure how to connect the class of JSBSim to an Airsim UE4 pawn object necessary for physics and then send back the terrain information (this can probably come later though). I believe I need to call the wrapper class on a class derived on AirSim's\n. I have created a simple class  that should serve this purpose. I'm not sure how to call this method with a pawn, I initially followed a similar structure to the car api but did not realize it used Unreals inbuilt physics engine. I'm not sure where multirotor builds its FastPhysicsEngine model either.I believe the model needs to do the following and I have started making classes for each of these but am unsure of the interaction:Does anyone have any ideas about the best way to link the JSBSim physics library with the airsim UE4 pawn library. The collision information from UE4 would ideally be sent to JSBSim's ground callback mechanics or we could do it in UE4 and set the acceleration/state. Also not sure if the JSBSim python instance can be used this way I think the best way to interface with the running program will be to setup a pythonclient the same as airsim's multirotor and fixedwing not sure what the way to do that will be.Are there any UML type diagrams for AirSim the maintainers or microsoft have they might be able to publish to show the derived class interaction for the Car and the Multirotor this might make the task of adding a new vehicle class easier in the future too. Thanks for everyone's help.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Dec 1, 2020", "body": "Following up from my previous post, I have been working on the JSBSim addition today. Im still not 100% how information is supposed to be sent from the Pawn to the API. In particular I have 2 structs  and .I believe the interaction between JSBSim's wrapper FGFDMExec should be something similar to the block diagram shown below:\n\nWith  responsible for getting state information from JSBSim along with calling JSBSim update (via run) and instantiating JSBSim. The class should also pass information to JSBSimPawn to set the aircraft's pose in UE4. Information from JSBSimPawnSim should then be passed to JSBSimApiBase used as the base for a plane specific api (JSBSimPlaneApi) in here all the autopilot functions can be created to control the aircraft and update the JSBSim aircraft. I have the following outstanding questions and it would great to get anyones thoughts:", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Dec 9, 2020", "body": "If anyone has any ideas on what Im missing in  that would be really useful I am trying to sort out linking the static JSBSim library to AirSim but I am having problems with visual studio.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Dec 30, 2020", "body": "Hi ! Thanks for the amazing work you are driving here! I think we can simplify something in your design. I don't know if there is a simple controller for JSBSim (like SimpleFlight), but it is integrated with the autopilots. So it would seem like a good idea to leave the control on the JSBSim side and leave AirSim alone with perception and reporting collisions. What do you all think?", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Jan 5, 2021", "body": "Hi  thanks a lot. Yes there is xml files for a lot of the aircraft that sets the gains for controllers defined inside JSBSim, . I have also found it useful to directly control the aircraft using a python script which I did for the . I expect to use the same script to control the aircraft but with collisions also being calculated by airsim, i.e.  should also return a collision value and the connection between airsim and jsbsim will happen in the airsim C++ side rather than in the python script, this should be what .", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 6, 2021", "body": "Great! This development would have an incredible impact if it could be used for multirotors as well. Why is it that I don't see much work done in JSBSim for that? It is not the best option?", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Jan 6, 2021", "body": "Thanks, yes certainly there are quite a few useful physics simulations included with JSBSim too such as heavier than air powered rotorcraft (helicopters), airships etc that I haven't looked at but I'm sure others would find useful.  My best guess as to why multi rotors haven't been extensively developed is probably age? I think JSBSim started in the late 90s or early 2000s and they just weren't considered significantly at this time except for rc aircraft  may know more?", "type": "commented", "related_issue": null}, {"user_name": "seanmcleod", "datetime": "Jan 7, 2021", "body": "In terms of JSBSim and multirotors take a look at the following - ", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 11, 2021", "body": "Thanks, ! I opened a discussion about integrating with other simulators here: . It would be great if anyone of you can join.", "type": "commented", "related_issue": null}, {"user_name": "tpool2", "datetime": "Feb 16, 2021", "body": "Hi all. I'm very interested in the integration of AirSim with JSBSim physics in order to model transitioning vehicles. Just wondering what the current status of this project is and how I can help move it forward.", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Feb 19, 2021", "body": "Hi  thanks for your interest I'll put a general summary out of where I have gotten to with JSBSim integration with AirSim:I spent a while getting JSBSim to compile and link in Visual Studio and with mac/linux by adding the relevant commands to the  and  or .When first building in Unreal I've had to add  to a couple of the jsbsim header files, I thought I added the relevant code to the airlib project file [https://github.com/AOS55/AirSim-1/blob/FixedWing-JSBSim/AirLib/AirLib.vcxproj] but I've tried playing around with it and can't seem to get it to work.When run with settings.json as:The programme runs until it gets the following error:Looking at the call stack this appears to come from here the  function. I haven't made much more progress past this unfortunately and its proven to be a real challenge.", "type": "commented", "related_issue": null}, {"user_name": "nikitabeebe", "datetime": "May 28, 2021", "body": "Has there been any update on the status of this? I'm looking to implement a fixed wing UAV for a camera GPS project I'm working on, and need to do it via airsim.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jun 1, 2021", "body": "Hi ! you can use  fork: ", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Jun 1, 2021", "body": "Hi  sorry for the delay in getting back to you the branch jony suggested is still wip I haven’t looked at it in a while hope to get back soon you might find the fixedwing-airsim repo of more use which uses a python backend with the client pose forwarding. I’m going to put some more documentation in this week happy to setup a meeting to talk through it too.", "type": "commented", "related_issue": null}, {"user_name": "Jaeyoung-Lim", "datetime": "Jul 27, 2021", "body": " Thanks for the great work on getting fixedwing working with Airsim!I am interested in supporting fixedwing vehicles in Airsim with a autopilot (PX4) as a SITL simulation.I have looked into , but it doesn't seem very obvious how this would work with the implementation using python bindings of jsbsim. Would you have any insights how you envision this would work and how this would maybe fit the jsbsim integration on PX4 side as in  ?Thank you in advance", "type": "commented", "related_issue": null}, {"user_name": "redhat2299", "datetime": "Dec 4, 2021", "body": "  really great work ,I like to know what is the progress ,when i tried to open fixed fixed airsim's block environment in unreal I'm getting errors ,Sir please help me out", "type": "commented", "related_issue": null}, {"user_name": "airsimdevdd", "datetime": "Dec 8, 2021", "body": "Fixed wing in Airsim , Any Modified version of Airsim  to work with fixed wing please help   \n       ", "type": "commented", "related_issue": null}, {"user_name": "AOS55", "datetime": "Aug 20, 2020", "body": [], "type": "issue", "related_issue": "#2508"}, {"user_name": "jonyMarino", "datetime": "Aug 20, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Nov 16, 2020", "body": [], "type": "issue", "related_issue": "#3141"}, {"user_name": "AOS55", "datetime": "Dec 3, 2020", "body": [], "type": "issue", "related_issue": "#3181"}, {"user_name": "ahmed-elsaharti", "datetime": "Dec 16, 2020", "body": [], "type": "issue", "related_issue": "#3203"}, {"user_name": "Jaeyoung-Lim", "datetime": "Aug 23, 2021", "body": [], "type": "pull", "related_issue": "#3978"}, {"user_name": "firemount", "datetime": "Jun 22, 2022", "body": [], "type": "issue", "related_issue": "#4588"}]},
{"issue_url": "https://github.com/apache/airflow/issues/22532", "issue_status": " Open\n", "issue_list": [{"user_name": "KulykDmytro", "datetime": "Mar 25, 2022", "body": "2.2.4 (latest released)When inspecting metadata in airflow tables about sla stuff found thatthis prevents to create any custom SLA analisys dashboards/metrics because unable to join  and / in direct and clear wayclicking on daily task's link on SLA Miss dashboard forwarding to task instance which was never executed (because it is in \"future\") and have no runtime metrics in All screenshots are taken @ 2022-03-25 17:xx:xx Z\n should be same as  in all casessetup a dag with any  daily schedule\nsetup an SLA on task\nwait  to trigger\nobserve in  dashboard - incorrect incorrect execution dateamazon linuxOfficial Apache Airflow Helm Chart", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Mar 31, 2022", "body": "SLA feature is genreally broken in many ways and need to be completely rewritten. I think there is no point to fix it", "type": "commented", "related_issue": null}, {"user_name": "uranusjr", "datetime": "Apr 1, 2022", "body": "We should advertise 2.4 as “SLA now actually works!”", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Apr 1, 2022", "body": "Providing that we will fix it :)", "type": "commented", "related_issue": null}, {"user_name": "sarinarw", "datetime": "Jun 15, 2022", "body": "Bumping this as we're also experiencing this and it seems to align with data interval end here as well. (v2.2.3)", "type": "commented", "related_issue": null}, {"user_name": "argibbs", "datetime": "Jul 18, 2022", "body": "  Can I check if the above is tongue in cheek, or if there is a rewrite of SLAs planned for 2.4(.x)?I'm hitting an issue in 2.3.3 with SLAs preventing processing of the dags (similar to  which I raised a while back - the way in which it fails has changed, but the behaviour is the same; dags don't get recreated, etc. etc.)I ask because I'm trying to judge whether I should write SLAs off in 2.3.3 and try again in 2.4, or if it's worth putting effort in to work out what's going wrong...", "type": "commented", "related_issue": null}, {"user_name": "argibbs", "datetime": "Jul 18, 2022", "body": "Ok, I got nerd sniped, and dug into it anyway, and now have a reasonable idea of how I might fix it.So, the question still stands: is it worth me attempting to patch this locally for myself (and then raising the change as an MR if successful) or is this pointless since it's already being worked on?", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Jul 18, 2022", "body": "If you have smple fix that is easy to review you can make a PR fixing it. Happy to review. For now no-one works on rewritinug SLA as far as I know and it is unlikely to happen in 2.4. You can also see discussion I started on it -  - seems like everyone agrees SLA is broken, but there is not enough will (or courage maybe ?) to deprecate it nor incentive to rewrite it, So we are a bit in a limbo - and anything you might want to improve there with very little cost is welcome.Feel free also to chime in the discussion if you have strong feelings about it  - I think more stronger voices are needed if we want to improve the situation a bit more than just patch here or there (or maybe by patching it here and there we might get it to a usable state - who knows :) ?", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Jul 18, 2022", "body": "Actully - If you are much into it and REALLY want to dig into it and see if you can incrementally improve it - I think that might be a super-valuable contribution to the community :). So if you have the will, I am happy to help with reviews and comments :). Just ping me in the PRs.", "type": "commented", "related_issue": null}, {"user_name": "KulykDmytro", "datetime": "Mar 25, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "argibbs", "datetime": "Aug 2, 2022", "body": [], "type": "pull", "related_issue": "#25489"}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/446", "issue_status": " Open\n", "issue_list": [{"user_name": "ubilrodriguez", "datetime": "Mar 2, 2022", "body": "excuse a query. in the annyang.min.js library you can also manipulate the coordinates in 3D? For example, I have an avartar project that captures those coordinates and with the annyang.min.js library I want to move with the voice.\n\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/435", "issue_status": " Open\n", "issue_list": [{"user_name": "josephpramodh", "datetime": "Jun 4, 2021", "body": "Sir, I'm trying to work on a voice based mail for blind using Annyang JS but I'm failing to implement it, so kindly help me to resolve it!!!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/445", "issue_status": " Open\n", "issue_list": [{"user_name": "vesper8", "datetime": "Jan 30, 2022", "body": "I'm using annyang to input chess moves and I'm finding it very challenging to have it correctly recognize commands such as \"D 2\" and \"B 2\", \"E 2\" and so on.Is there a browser in particular that might do a better job than others? Clearly my own voice and pronunciation is a factor in this, I'm just finding it borderline unfeasible at all with how often it incorrectly understands my input, as simple as those inputs are meant to be (64 sounds ranging from \"A 1\" to \"H 8\"", "type": "commented", "related_issue": null}, {"user_name": "limury", "datetime": "Aug 10, 2022", "body": "Hi! So different browsers will definitely perform differently since Annyang uses the Web Speech API for processing, and the algorithm used by the Web Speech API is different across browsers. However, I unfortunately do not think that there are any reports on what browser is the best and the worst. Also since the functionality is very niche, your best bet is probably just to test them out yourself.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/441", "issue_status": " Open\n", "issue_list": [{"user_name": "inglesuniversal", "datetime": "Aug 22, 2021", "body": "With the option to extend and use external cloud voices from AWS, GCP, AZURE, IBMBest regards", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/439", "issue_status": " Open\n", "issue_list": [{"user_name": "Baudin999", "datetime": "Jun 29, 2021", "body": "Annyang does not seem to work on in my browser (see details below).  The Annyang main page (demo app) does not work and I think it's the same reason as the reason why my own project does not work. The problem is made visible through SpeechKITT because I see the recording starting and stopping.What I'm trying in my own application:I am trying to get annyang (with SpeechKITT) added to my app. I use the following code:I expect, when I press the \"mic icon\" and start voice that I can talk and the voice commands get recorded and handled.What happens is that I click start and it immediately stops. If I add a  handler it immediately get's fired after the  handler.\nAnnyang get's configured only once, I can read out the .I've tried: , but to no avail.\nI am working in a svelte app. At first I thought it was because the app rerenders, but this wasn't it. I've tried the main index page and also a separate page without anything on it, the behavior is the same.I'm working on an app where voice commands would help people who have trouble typing, this component is an important part of the app.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/438", "issue_status": " Open\n", "issue_list": [{"user_name": "ralphignacio13", "datetime": "Jun 20, 2021", "body": "When I am using requestFullscreen() function to make a video player fullscreen using voice commands, I am getting this \"(index):379 Failed to execute 'requestFullscreen' on 'Element': API can only be initiated by a user gesture.\"Video player should be fullscreenedReturning an error: (index):379 Failed to execute 'requestFullscreen' on 'Element': API can only be initiated by a user gesture.Maybe changing the height and width but I tried that it only changes the dimensions of the video the video controls are not included in the change sizeI am very thankful for the opportunity to be able to add voice recognition easily on web application, and I am currently using this for my college thesis.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/393", "issue_status": " Open\n", "issue_list": [{"user_name": "scoy028", "datetime": "Mar 21, 2019", "body": "Repo branch with logs: \nRelevant files: client/components/recipe-step.js, client/annyangCommands.js\nBrowser we are using: ChromeMy team and I have been working on adding annyang with Web Speech API to recognize multiple commands, some of which trigger navigation. We are using the browser console to test. We speak a number of commands and get responses, but after a few commands, the console stops recognizing the speech event for the command. We have logged within the methods and the \"heard\" command and we are getting the correct logs, we just aren't hearing anything. We are also logging commands twice. The more commands we give, we start to eventually log the commands more than twice. Is this a garbage collection issue? We believe this is a bug with how we are integrating our annyang commands and the logic of how we trigger our command methods. Perhaps the same annyang event is persisting. Would love some help if possible!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/436", "issue_status": " Open\n", "issue_list": [{"user_name": "josephpramodh", "datetime": "Jun 4, 2021", "body": "How can I make my music button play or pause, using my voice commands, 'play music ' and 'pause music'I want to add submit option using the submit voice command, how can I write the code for the submit option\nHelp with these two issues ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/346", "issue_status": " Open\n", "issue_list": [{"user_name": "ceides9497", "datetime": "Jun 11, 2018", "body": "I want to say pause and let annyang stop until I say resume and start listening againis it possible?", "type": "commented", "related_issue": null}, {"user_name": "jrsarath", "datetime": "Jun 21, 2018", "body": "You might want to combine annyang with native speech-api..\ntell speech-api to wake up annyang on a specific word/hotword and shutdown native speech-api..\nand for pause function add a pause command to annyang execute pause function as start native speech-api again and stop annyang..", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/313", "issue_status": " Open\n", "issue_list": [{"user_name": "AdamMiltonBarker", "datetime": "Oct 20, 2017", "body": "Hi guys, an issue that is becoming a pretty big problem for us is that once a user has voice rec on, and then stops using abort(), if they restart again, it will continue hearing everything twice resulting in querying our NLU twice, displaying what it heard twice, and displaying the response twice, any suggestions to stop this happening?Here is the related code:", "type": "commented", "related_issue": null}, {"user_name": "AdnanHafeez", "datetime": "Dec 18, 2017", "body": "Not sure if I am missing something here, but why are you using 'setTimeOut' to call Bootup every 4 seconds?", "type": "commented", "related_issue": null}, {"user_name": "AdamMiltonBarker", "datetime": "Dec 18, 2017", "body": "I think you are missing something, setTimeOut executes only once, setInterval would execute multiple times. In the code setTimeOut makes the script wait 4 seconds before executing bootup.", "type": "commented", "related_issue": null}, {"user_name": "AdnanHafeez", "datetime": "Dec 18, 2017", "body": "Yes, you are correct. My bad! I confused the two.", "type": "commented", "related_issue": null}, {"user_name": "AdamMiltonBarker", "datetime": "Dec 18, 2017", "body": "No worries ;)", "type": "commented", "related_issue": null}, {"user_name": "Adrianjewell91", "datetime": "Dec 31, 2017", "body": "Has this been resolved? I'm looking to start contributing to an open source project, and would like permission to tackle this one.", "type": "commented", "related_issue": null}, {"user_name": "TalAter", "datetime": "Dec 31, 2017", "body": "Thank you \nIf you are able to replicate the issue please go ahead Have you been able to isolate the issue and understand what's the root cause?", "type": "commented", "related_issue": null}, {"user_name": "Adrianjewell91", "datetime": "Jan 3, 2018", "body": "Thanks. , I was not able to replicate the bug. It looks like your code requires a certain HTML element called #TOA_Recognition_Window. If you can post a description on how to replicate the issue along with the complete code, I'm still happy to take a deep look.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/TalAter/annyang/issues/288", "issue_status": " Open\n", "issue_list": [{"user_name": "Hakim2014", "datetime": "Jun 4, 2017", "body": "Hi,I am working on a small project on raspberry pi, which have voice functionality. I am using electron + annyang. I have included annyang as  const annyang = require('annyang') in my index.html and provided google cloud speech api key , client key and secret key in main.js of electron. I have attached a call back function to annyang and it always give me network error on my windows machine and on my raspberry pi it gives me audio-capture error. I also tried to setup the keys in environment variable but no luck. May you help me out figuring the issue.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5641", "issue_status": " Closed\n", "issue_list": [{"user_name": "SalahSoliman", "datetime": "Aug 1, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "Shreyas-dotcom", "datetime": "Aug 3, 2022", "body": "Hey Salah,No - Carla does not yet support Unreal Engine 5. It was discussed on discord by German Ros.He said:\nHope this Helps!", "type": "commented", "related_issue": null}, {"user_name": "SalahSoliman", "datetime": "Aug 3, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "Shreyas-dotcom", "datetime": "Aug 3, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "SalahSoliman", "datetime": "Aug 3, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "glopezdiest", "datetime": "Sep 27, 2022", "body": "Closing this as it has been answered already", "type": "commented", "related_issue": null}, {"user_name": "glopezdiest", "datetime": "Sep 27, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1585", "issue_status": " Open\n", "issue_list": [{"user_name": "nsubiron", "datetime": "Apr 29, 2019", "body": "Make a Dockerfile that allows us building an image with a Windows 10 environment set up for building Carla.The main complication is having Unreal built without redistributing the code/binaries.See also .", "type": "commented", "related_issue": null}, {"user_name": "cmpute", "datetime": "Jul 7, 2019", "body": "I guess this tool can help with dockerize UE4: I didn't have enough resources to build the image so I didn't try it. But I think it can be useful for building a docker image for CARLA under windows and linux ()", "type": "commented", "related_issue": null}, {"user_name": "cmpute", "datetime": "Jul 7, 2019", "body": "Also, make a docker image can make building on windows more easily... Definitely hope to have a docker image, and happy to help with it", "type": "commented", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jun 26, 2020", "body": "I would love to see this done too.. does anybody want to help?", "type": "commented", "related_issue": null}, {"user_name": "TKeutgens", "datetime": "Sep 23, 2020", "body": "I'm no Docker expert but I'm currently trying to build the Docker image on Windows. I managed to install all pre-requesites. Unfortunately the docker build process takes very very long. Currently I'm already waiting for almost 24 h. I feel like the process is stuck during building the UE4 EditorDoes anyone has any suggestions?", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Apr 29, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jun 26, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/4776", "issue_status": " Open\n", "issue_list": [{"user_name": "albertvillanova", "datetime": "Aug 1, 2022", "body": "Current version of  (0.12.0) raises a RuntimeError when trying to use  backend but non-Python dependency  is not installed:\nMaybe we should raise a more actionable error message so that the user knows how to fix it.UPDATE:TODO:Related to:", "type": "commented", "related_issue": null}, {"user_name": "fxtentacle", "datetime": "Aug 8, 2022", "body": "Requiring torchaudio<0.12.0 isn't really a viable solution because that implies torch<0.12.0 which means no sm_86 CUDA support which means no RTX 3090 support in PyTorch.But in my case, the error only occurs if  resolves to  inside torchaudio 0.12.0 which is only the case if FFMPEG initialization failed:  That means the proper solution for torchaudio>=0.12.0 is to check  and if it is False, then we need to remind the user to install a dynamically linked ffmpeg 4.1.8 and then maybe call  to force a user-visible exception showing the missing ffmpeg dynamic library name.On my system, installingfrom ffmpeg 4.1.8 made HF datasets 2.3.2 work just fine with  torchaudio 0.12.1+cu116:", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 24, 2022", "body": "Related: ", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Aug 1, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "huggingface/transformers#18379"}, {"user_name": "albertvillanova", "datetime": "Aug 2, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Aug 3, 2022", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 26, 2022", "body": [], "type": "issue", "related_issue": "pytorch/audio#2652"}, {"user_name": "polinaeterna", "datetime": "Aug 31, 2022", "body": [], "type": "pull", "related_issue": "#4923"}, {"user_name": "albertvillanova", "datetime": "Sep 21, 2022", "body": [], "type": "issue", "related_issue": "#3663"}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/4634", "issue_status": " Closed\n", "issue_list": [{"user_name": "moro23", "datetime": "Jul 5, 2022", "body": "common_voice_train = load_dataset(\"common_voice\", \"ha\", split=\"train+validation\")", "type": "commented", "related_issue": null}, {"user_name": "khushmeeet", "datetime": "Jul 15, 2022", "body": "Could you provide the error details. It is difficult to debug otherwise. Also try other config.  is not a valid.", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Sep 13, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4089", "issue_status": " Open\n", "issue_list": [{"user_name": "HankYuan1101", "datetime": "Apr 14, 2021", "body": "Hi, there\nI follow the Linux build instruction and I got an error while executing make launch.\nThe error shown as follow:OS: ubuntu 18.04\ncarla version: 0.9.11\nunreal version: 4.24any suggestion is welcome, thanks!", "type": "commented", "related_issue": null}, {"user_name": "onebitme", "datetime": "Apr 15, 2021", "body": "Can you please check the version of gcc,\nI am currently using v 7.5.0", "type": "commented", "related_issue": null}, {"user_name": "HankYuan1101", "datetime": "Apr 19, 2021", "body": "My gcc version is also 7.5.0\ngcc --version\ngcc (Ubuntu 7.5.0-3ubuntu118.04) 7.5.0Are those difference with your version?", "type": "commented", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Apr 20, 2021", "body": "Hi  could you attach the full output of any error you get after running", "type": "commented", "related_issue": null}, {"user_name": "HankYuan1101", "datetime": "Apr 23, 2021", "body": "Hi , thanks for your feedback.There is a lot of \"can't find file\" in the output. I'm not sure the connection between missing files and this error message.\nBut I export the UnrealUngine directory in the bashrc. Is there other directory setting?Here is my full output after running your suggestion", "type": "commented", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Apr 23, 2021", "body": "When you ran the command from this part of the , did Unreal start up successfully?And you downloaded all the assets correctly and they were extracted to ?", "type": "commented", "related_issue": null}, {"user_name": "seuwcs", "datetime": "Jun 17, 2021", "body": "Hello, I have met a question that after run the code:\ncd ~/UnrealEngine_4.24/Engine/Binaries/Linux && ./UE4Editor\nin the terminal. The Unreal Engine could not start up, and the output is as below.Could you give me some suggestions on solving this question?  Thank you !", "type": "commented", "related_issue": null}, {"user_name": "ryandoren", "datetime": "Jul 1, 2021", "body": "Hello , I ran into the same problem as yours, have you solved it yet?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 28, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "HankYuan1101", "datetime": "Apr 14, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Apr 20, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Apr 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 28, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/4163", "issue_status": " Open\n", "issue_list": [{"user_name": "TristanThrush", "datetime": "Apr 13, 2022", "body": "\nA clear and concise description of what the problem is.We now have hate speech datasets on the hub, like this one: I'm wondering if there is an option to select a content warning message that appears before the dataset preview? Otherwise, people immediately see hate speech when clicking on this dataset.\nA clear and concise description of what you want to happen.Implementation of a content warning message that separates users from the dataset preview until they click out of the warning.\nA clear and concise description of any alternative solutions or features you've considered.Possibly just a way to remove the dataset preview completely? I think I like the content warning option better, though.\nAdd any other context about the feature request here.", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "May 2, 2022", "body": "Hi! You can use the  YAML field in a dataset card for displaying custom messages/warnings that the user must accept before gaining access to the actual dataset. This option also keeps the viewer hidden until the user agrees to terms.", "type": "commented", "related_issue": null}, {"user_name": "HannahKirk", "datetime": "May 10, 2022", "body": "Hi , thanks for explaining how to add this feature.If the current dataset yaml is:Can you provide a minimal working example of how to added the gated prompt?Thanks!", "type": "commented", "related_issue": null}, {"user_name": "leondz", "datetime": "May 10, 2022", "body": "+ enable  under the Settings pane.There's a brief guide here  , and you can see the field in action here,  (you need to agree the terms in the Dataset Card pane to be able to access the files pane, so this comes up 403 at first).And a working example here!  :) Great to be able to mitigate harms in text.", "type": "commented", "related_issue": null}, {"user_name": "leondz", "datetime": "May 10, 2022", "body": "-- is there a way to gate content anonymously, i.e. without registering which users access it?", "type": "commented", "related_issue": null}, {"user_name": "Breakend", "datetime": "Jun 9, 2022", "body": "+1 to 's question. One scenario is if you don't want the dataset to be indexed by search engines or viewed in browser b/c of upstream conditions on data, but don't want to collect emails. Some ability to turn off the dataset viewer or add a gating mechanism without emails would be fantastic.", "type": "commented", "related_issue": null}, {"user_name": "TristanThrush", "datetime": "Apr 13, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3978", "issue_status": " Open\n", "issue_list": [{"user_name": "kingabzpro", "datetime": "Mar 21, 2022", "body": " Am I the one who added this dataset ? Yes", "type": "commented", "related_issue": null}, {"user_name": "severo", "datetime": "Mar 24, 2022", "body": "the dataset viewer is working on this dataset. I imagine the issue is that we would expect to be able to listen to the audio files in the  column, right?maybe  or  could help", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Mar 29, 2022", "body": "The structure of the dataset is not supported. Only the CSV file is parsed and the audio files are ignored.We're working on supporting audio datasets with a specific structure in ", "type": "commented", "related_issue": null}, {"user_name": "kingabzpro", "datetime": "Mar 29, 2022", "body": "Got it.", "type": "commented", "related_issue": null}, {"user_name": "kingabzpro", "datetime": "Mar 21, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "severo", "datetime": "Mar 21, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "kingabzpro", "datetime": "Mar 29, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "severo", "datetime": "Apr 4, 2022", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "severo", "datetime": "Apr 4, 2022", "body": [], "type": "removed their assignment", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3720", "issue_status": " Open\n", "issue_list": [{"user_name": "aasem", "datetime": "Feb 14, 2022", "body": "Missing language in Common Voice dataset I tried to call the Urdu dataset using  but couldn't due to builder configuration not found. I checked the source file here for the languages support:and Urdu isn't included there. I assume a quick update will fix the issue as Urdu speech is now available at the Common Voice dataset.Am I the one who added this dataset? No", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Feb 15, 2022", "body": "Hi , thanks for reporting.Please note that currently Commom Voice is hosted on our Hub as a community dataset by the Mozilla Foundation. See all Common Voice versions here: Maybe we should add an explaining note in our \"legacy\" Common Voice canonical script? What do you think   ?", "type": "commented", "related_issue": null}, {"user_name": "aasem", "datetime": "Feb 15, 2022", "body": "Thank you, , for the quick response. I am not sure about the exact flow but I guess adding the following lines under the  dictionary definition in  might resolve the issue. I guess the dataset is recently made available so the file needs updating.", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Feb 15, 2022", "body": " for compliance reasons, we are no longer updating the  script.We agreed with Mozilla Foundation to use their community datasets instead, which will ask you to accept their terms of use:In order to use e.g. their Common Voice dataset version 8.0, please:", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Feb 15, 2022", "body": "Yes, I agree we should have a deprecation notice in the canonical script to redirect users to the new script.", "type": "commented", "related_issue": null}, {"user_name": "aasem", "datetime": "Feb 15, 2022", "body": ",\nI now get the following error after downloading my access token from the huggingface and passing it to  call:Any quick pointer on how it might be resolved?", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Feb 15, 2022", "body": " What version of  are you using? We renamed that attribute from  to  fairly recently, so updating to the newest version should resolve the issue:", "type": "commented", "related_issue": null}, {"user_name": "aasem", "datetime": "Feb 15, 2022", "body": "Thanks a lot, . That completely resolved the issue.", "type": "commented", "related_issue": null}, {"user_name": "aasem", "datetime": "Feb 14, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Feb 15, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/2730", "issue_status": " Open\n", "issue_list": [{"user_name": "yjernite", "datetime": "Jul 29, 2021", "body": "Instructions to add a new dataset can be found .", "type": "commented", "related_issue": null}, {"user_name": "yjernite", "datetime": "Jul 29, 2021", "body": "cc ?", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 7, 2021", "body": "Does anybody know if there is a bundled link, which would allow direct data download instead of manual?\nSomething similar to:  ? cc ", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 7, 2021", "body": "Also see: ", "type": "commented", "related_issue": null}, {"user_name": "yjernite", "datetime": "Jul 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 7, 2021", "body": [], "type": "pull", "related_issue": "#2771"}, {"user_name": "DewiBrynJones", "datetime": "Sep 17, 2021", "body": [], "type": "issue", "related_issue": "common-voice/common-voice#3262"}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3577", "issue_status": " Open\n", "issue_list": [{"user_name": "omarespejel", "datetime": "Jan 13, 2022", "body": "Instructions to add a new dataset can be found .", "type": "commented", "related_issue": null}, {"user_name": "omarespejel", "datetime": "Jan 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Jan 27, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3393", "issue_status": " Open\n", "issue_list": [{"user_name": "wiedymi", "datetime": "Dec 7, 2021", "body": "Instructions to add a new dataset can be found .", "type": "commented", "related_issue": null}, {"user_name": "wiedymi", "datetime": "Dec 7, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Dec 9, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3909", "issue_status": " Open\n", "issue_list": [{"user_name": "aliceinland", "datetime": "Mar 14, 2022", "body": "When loading the Common_Voice dataset, by downloading it directly from the Hugging Face hub, some files can not be opened.The common voice dataset downloaded and correctly loaded whit the use of the hugging face datasets library.The error is:", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Mar 28, 2022", "body": "Hi ! It could an issue with torchaudio, which version of torchaudio are you using ? Can you also try updating  to 2.0.0 and see if it works ?", "type": "commented", "related_issue": null}, {"user_name": "Zeboku", "datetime": "Mar 28, 2022", "body": "I  have a similar issue. I'm trying to use the librispeech_asr dataset and read it with soundfile.The code is taken directly from \"\".The short error code is \"RuntimeError: Error opening '6930-75918-0000.flac': System error.\" (it can't find the first file), and I agree, I can't find the file either. The dataset has downloaded correctly (it says), but on the location, there are only \".arrow\" files, no \".flac\" files.", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Mar 29, 2022", "body": "Hi ! In  2.0 can access the audio array with  already, no need to use . See our documentation on  :)cc  we will need to update the readme at  as well as ", "type": "commented", "related_issue": null}, {"user_name": "Zeboku", "datetime": "Mar 29, 2022", "body": "Thanks!And sorry for posting this problem in what turned on to be an unrelated thread.I rewrote the code, and the model works. The WER is 0.137 however, so I'm not sure if I have missed a step. I will look further into that at a later point. The transcriptions look good through manual inspection.The rewritten code:", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Mar 30, 2022", "body": "I think the issue comes from the fact that you set  while  still returns a list of strings for \"transcription\". You can fix it by adding  at the end of this line to get the string:", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Apr 5, 2022", "body": "Updating as many model cards now as I can find", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Apr 5, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "aliceinland", "datetime": "Mar 14, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/3663", "issue_status": " Closed\n", "issue_list": [{"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": "The path should be the complete absolute path to the downloaded audio file not some relative path.", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": "Having talked to , I see that this feature is no longer supported.I really don't think this was a good idea. It is a major breaking change and one for which we don't even have a working solution at the moment, which is bad for PyTorch as we don't want to force people to have  decode audio files automatically, but  bad for Tensorflow and Flax where we  even use  to load  files - e.g.  doesn't work anymore in a TF training script. Note this worked perfectly fine before making the change (think it was done  no?)IMO, it's really important to think about a solution here and I strongly favor to make a difference here between loading a dataset in streaming mode and in non-streaming mode, so that in non-streaming mode the actual downloaded file is displayed. It's really crucial for people to be able to analyse the original files IMO when the dataset is not downloaded in streaming mode.There are the following reasons why it is paramount to have access to the  audio file in my opinion (in non-streaming mode):=> IMO, it's a  big priority to again have the correct absolute path in non-streaming mode. The other solution of providing a path-like object derived from the bytes stocked in the  file is not nearly as user-friendly, but better than nothing.", "type": "commented", "related_issue": null}, {"user_name": "cahya-wirawan", "datetime": "Feb 1, 2022", "body": "Agree that we need to have access to the original sound files. Few days ago I was looking for these original files because I suspected there is bug in the audio resampling (confirmed in ) and I want to do my own resampling to workaround the bug, which is now not possible anymore due to the unavailability of the original files.", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Feb 7, 2022", "body": "Just to clarify, here you describe the approach that uses the  attribute to access the underlying bytes?I'd assume this is because we use  as a backend for decoding. However, soon we should be able to use , which supports path-like objects, for MP3 ().Your concern is reasonable, but there are situations where we can only serve bytes (see  for instance). IMO it makes sense to fix the affected datasets for now, but I don't think we should care too much whether we rely on local paths or bytes after soundfile adds support for MP3 as long as our examples work (shouldn't be too hard to update the  functions) and we properly document how to access the underlying path/bytes for custom decoding (via ).", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Feb 7, 2022", "body": "Related to this discussion: in  I propose how we could change  to work for streaming and also return local paths (as it used too !). I'd love your opinions on this", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 8, 2022", "body": "Yes!Yes this might be, but I highly doubt that  is the go-to library for audio then.  and I have tried out a bunch of different audio loading libraries (, , , pure , , ...). One thing that was pretty clear  to me is that there is just no \"de-facto standard\" library and they all have pros and cons. None of the libraries really supports \"batch\"-ed audio loading. Some depend on PyTorch.  is 100x faster (really!) than  fallback on MP3.  often has problems with multi-proessing, ... Also we should keep in mind that resampling is similarly not as simple as reading a text file. It's a pretty complex signal processing transform and people very well might want to use special filters, etc...at the moment we just hard-code  or  default filter when doing resampling.=> All this to say that we  care about whether we rely on local paths or bytes IMO. We don't want to loose all users that are forced to use  decoding or resampling or have to built a very much not intuitive way of loading bytes into a numpy array. It's much more intuitive to be able to inspect a local file. I feel pretty strongly about this and am happy to also jump on a call. Keeping libraries flexible and lean as well as exposing internals is very important IMO (this philosophy has worked quite well so far with Transformers).", "type": "commented", "related_issue": null}, {"user_name": "mariosasko", "datetime": "Feb 8, 2022", "body": "Thanks a lot for the very detailed explanation. Now everything makes much more sense.", "type": "commented", "related_issue": null}, {"user_name": "lhoestq", "datetime": "Feb 22, 2022", "body": "From  the Common Voice dataset now gives access to the local audio files as before", "type": "commented", "related_issue": null}, {"user_name": "albertz", "datetime": "Apr 21, 2022", "body": "I understand the argument that it is bad to have a breaking change. How to deal with the introduction of breaking changes is a topic of its own and not sure how you want to deal with that (or is the policy this is never allowed, and there must be a  or so if you really want to introduce a breaking change?).Regardless of whether it is a breaking change, however, I don't see the other arguments.I don't exactly understand this. Why not?Why does the HF dataset on-the-fly decoding mechanism not work? Why is it anyway specific to PyTorch or TensorFlow? Isn't this independent?But even if you just provide the raw bytes to TF, on TF you could just use sth like  or  or ?I don't really understand the arguments (despite that it maybe breaks existing code). You anyway have the original audio files but it is just embedded in the dataset? I don't really know about any library which cannot also load the audio from memory (i.e. from the dataset).Btw, on librosa being slow for decoding audio files, I saw that as well, so we have this comment RETURNN:Resampling is also a separate aspect, which is also less straightforward and with different compromises between speed and quality. So there the different tradeoffs and different implementations can make a difference.However, I don't see how this is related to the question whether there should be the raw bytes inside the dataset or as separate local files.", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Apr 21, 2022", "body": "Thanks for your comments here  - cool to get your input!Answering a bit here between the lines:The problem with decoding on the fly is that we currently rely on  for this now which relies on  which is not necessarily something people would like to install when using  or . Therefore we cannot just rely on people using the decoding on the fly method. We just didn't find a library that is ML framework independent and fast enough for all formats.  is currently in our opinion by far the best here.So for TF and Flax it's important that users can load audio files or bytes they way the want to - this might become less important if we find (or make) a good library with few dependencies that is fast for all kinds of platforms / use cases.Now the question is whether it's better to store audio data as a path to a file or as raw bytes I guess.\nMy main arguments for storing the audio data as a path to a file is pretty much all about users experience - I don't really expect our users to understand the inner workings of datasets:But the argument that the audio should be loadable directly from memory is good - haven't thought about this too much.\nI guess it's still very much possible for the user to do this:Guess the question is more a bit about what should be the default case?", "type": "commented", "related_issue": null}, {"user_name": "albertz", "datetime": "Apr 21, 2022", "body": "But how is this relevant for this issue here? I thought this issue here is about having the (correct) path in the dataset or having raw bytes in the dataset.How did TF users use it at all then? Or they just do not use on-the-fly decoding? I did not even notice this problem (maybe because I had  installed). But what do they use instead?But as I outlined before, they could just use  and co, where it would be more natural if you already provide the raw bytes.I was not really familiar with . It seems that they really don't provide an easy/direct API to operate on raw bytes. Which is very strange and unfortunate because as far as I can see, all the underlying backend libraries (e.g. soundfile) easily allow that. So I would say that this is the fault of  then. But despite, if you anyway use  with  backend, why not just use  directly. It's very simple to use and crossplatform.But ok, now we are just discussing how to handle the on-the-fly decoding. I still think this is a separate issue and having raw bytes in the dataset instead of local files should just be fine as well.I think nobody who writes code is scared by seeing the raw bytes content of a binary file. :)In , you said/proposed that this  is not needed anymore and  could do it automatically (maybe via some option)?Yea this is up to you. I'm happy as long as we can get it the way we want easily and this is a well supported use case. :)", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Apr 21, 2022", "body": "Yes! Should be super easy now see discussion here: Thanks for the super useful input :-)", "type": "commented", "related_issue": null}, {"user_name": "DCNemesis", "datetime": "Jul 13, 2022", "body": "Despite the comments that this has been fixed, I am finding the exact same problem is occurring again (with datasets version 2.3.2)", "type": "commented", "related_issue": null}, {"user_name": "DCNemesis", "datetime": "Jul 13, 2022", "body": "It appears downgrading to torchaudio 0.11.0 fixed this problem.", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Aug 23, 2022", "body": ", sorry which problem exactly is occuring again? Also cc   here", "type": "commented", "related_issue": null}, {"user_name": "DCNemesis", "datetime": "Aug 23, 2022", "body": "   I was unable to load audio from Common Voice using  with the current version of torchaudio, but downgrading to torchaudio 0.11.0 fixed it. This is probably more of a torch problem than a Hugging Face problem.", "type": "commented", "related_issue": null}, {"user_name": "polinaeterna", "datetime": "Aug 24, 2022", "body": " that's interesting, could you please share the error message if you still can access it?", "type": "commented", "related_issue": null}, {"user_name": "DCNemesis", "datetime": "Aug 24, 2022", "body": " I believe it is the same exact error as above. It occurs on other .mp3 sources as well, but the problem is with torchaudio > 0.11.0. I've created a short colab notebook that reproduces the error, and the fix here: ", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Sep 21, 2022", "body": "Hi ,Your issue was slightly different from the original one in this issue page. Yours seems related to a change in the backend used by  ( instead of ). Refer to the issue page here:Normally, it should be circumvented with the patch made by  in:", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Sep 21, 2022", "body": "I think the original issue reported here was already fixed by:Otherwise, feel free to reopen.", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Feb 1, 2022", "body": [], "type": "issue", "related_issue": "#3662"}, {"user_name": "anton-l", "datetime": "Feb 1, 2022", "body": [], "type": "pull", "related_issue": "#3664"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#4184"}, {"user_name": "polinaeterna", "datetime": "Aug 31, 2022", "body": [], "type": "pull", "related_issue": "#4923"}, {"user_name": "albertvillanova", "datetime": "Sep 21, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Sep 21, 2022", "body": [], "type": "pull", "related_issue": "#3736"}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/166", "issue_status": " Open\n", "issue_list": [{"user_name": "stone-tong", "datetime": "Mar 10, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Mar 11, 2022", "body": "Can you use IDF version release/v4.2 and try again?", "type": "commented", "related_issue": null}, {"user_name": "stone-tong", "datetime": "Mar 11, 2022", "body": "change esp-idf version to 4.2 and after compile, getting the following error\nso this function is not defined.", "type": "commented", "related_issue": null}, {"user_name": "arunrkumaran", "datetime": "Jun 9, 2022", "body": "undefined reference to `xTaskNotify'\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/164", "issue_status": " Open\n", "issue_list": [{"user_name": "chaitanya-smartrotamac", "datetime": "Dec 15, 2021", "body": "amazon_alexa with Set ALEXA_BT=1 enabled.\nBut with LyraT board i am getting this error.E (1955) spi_flash: Detected size(8192k) smaller than the size in the binary image header(16384k). Probe failed.\nassertion \"flash_ret == ESP_OK\" failed: file \"C:/Users/LENOVO/Desktop/esp-idf/components/esp32/cpu_start.c\", line 472, function: start_cpu0_defaultabort() was called at PC 0x40201500 on core 0\n0x40201500: __assert_func at /builds/idf/crosstool-NG/.build/HOST-x86_64-w64-mingw32/xtensa-esp32-elf/src/newlib/newlib/libc/stdlib/assert.c:62 (discriminator 8)I think we cant enable bluetooth with alexa in LyraT board is it true?? due to memory..Can I use memory card someway for this?", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 15, 2021", "body": "Hi \nIf you want to have OTA support as well, you might need a board with 16MB flash.\nIf not, you can remove ota_1 partition from partition.csv in examples directory and set flash size to 8MB via menuconfig->Serial flash config option.", "type": "commented", "related_issue": null}, {"user_name": "Marcoz08", "datetime": "Dec 16, 2021", "body": "I have the same error. I did what you said, and removed ota_1 but now it doesn't compile the code.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 17, 2021", "body": "can you provide your updated partitions.csv and the error log?", "type": "commented", "related_issue": null}, {"user_name": "chaitanya-smartrotamac", "datetime": "Dec 17, 2021", "body": "Thanks \nBut with this, I lost the Alexa wake word feature.\nBoard is not responding for wakeword. But responding to button.\nBluetooth working now.Log :\nentry 0x40080710\nI (29) boot: ESP-IDF v4.2.2-dirty 2nd stage bootloader\nI (29) boot: compile time 10:06:55\nI (29) boot: chip revision: 3\nI (32) qio_mode: Enabling default flash chip QIO\nI (38) boot.esp32: SPI Speed      : 80MHz\nI (42) boot.esp32: SPI Mode       : QIO\nI (47) boot.esp32: SPI Flash Size : 8MB\nI (51) boot: Enabling RNG early entropy source...\nI (57) boot: Partition Table:\nI (60) boot: ## Label            Usage          Type ST Offset   Length\nI (68) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (75) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (83) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (90) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (98) boot:  4 ota_0            OTA app          00 10 00020000 00480000\nI (105) boot: End of partition table\nI (109) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1cde7c (1891964) map\nI (676) esp_image: segment 1: paddr=0x001edea4 vaddr=0x3ffbdb60 size=0x02174 (  8564) load\nI (679) esp_image: segment 2: paddr=0x001f0020 vaddr=0x400d0020 size=0x141b88 (1317768) map\n0x400d0020: _stext at ??:?I (1070) esp_image: segment 3: paddr=0x00331bb0 vaddr=0x3ffbfcd4 size=0x036f0 ( 14064) load\nI (1075) esp_image: segment 4: paddr=0x003352a8 vaddr=0x40080000 size=0x18a50 (100944) load\n0x40080000: _WindowOverflow4 at C:/Users/LENOVO/Desktop/esp-idf/components/freertos/xtensa/xtensa_vectors.S:1730I (1126) boot: Loaded app from partition at offset 0x20000\nI (1127) boot: Disabling RNG early entropy source...\nI (1127) psram: This chip is ESP32-D0WD\nI (1132) spiram: Found 64MBit SPI RAM device\nI (1137) spiram: SPI RAM mode: flash 80m sram 80m\nI (1142) spiram: PSRAM initialized, cache is in low/high (2-core) mode.\nI (1149) cpu_start: Pro cpu up.\nI (1153) cpu_start: Application information:\nI (1158) cpu_start: Project name:     avs\nI (1163) cpu_start: App version:      1.0\nI (1167) cpu_start: Compile time:     Dec 17 2021 10:06:19\nI (1174) cpu_start: ELF file SHA256:  e9ffccbdbd4e6b33...\nI (1180) cpu_start: ESP-IDF:          v4.2.2-dirty\nI (1185) cpu_start: Starting app cpu, entry point is 0x40081dc8\n0x40081dc8: call_start_cpu1 at C:/Users/LENOVO/Desktop/esp-idf/components/esp32/cpu_start.c:287I (0) cpu_start: App cpu up.\nI (1683) spiram: SPI SRAM memory test OK\nI (1684) heap_init: Initializing. RAM available for dynamic allocation:\nI (1684) heap_init: At 3FFAFF10 len 000000F0 (0 KiB): DRAM\nI (1689) heap_init: At 3FFB6388 len 00001C78 (7 KiB): DRAM\nI (1696) heap_init: At 3FFB9A20 len 00004108 (16 KiB): DRAM\nI (1702) heap_init: At 3FFC7878 len 00018788 (97 KiB): DRAM\nI (1708) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\nI (1715) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\nI (1721) heap_init: At 40098A50 len 000075B0 (29 KiB): IRAM\nI (1727) cpu_start: Pro cpu start user code\nI (1732) spiram: Adding pool of 4077K of external SPI memory to heap allocator\nI (1753) spi_flash: detected chip: generic\nI (1753) spi_flash: flash io: qio\nI (1753) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (1762) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (1772) [app_main]: ==== Voice Assistant SDK version: v1.2.2 ====\nI (1782) [time_utils]: Initializing SNTP.\nI (1782) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (1822) wifi:wifi driver task: 3ffd5e34, prio:23, stack:6656, core=0\nI (1822) system_api: Base MAC address is not set\nI (1822) system_api: read default base MAC address from EFUSE\nI (1832) wifi:wifi firmware version: bb6888c\nI (1832) wifi:wifi certification version: v7.0\nI (1832) wifi:config NVS flash: enabled\nI (1832) wifi:config nano formating: disabled\nI (1842) wifi:Init data frame dynamic rx buffer num: 20\nI (1842) wifi:Init management frame dynamic rx buffer num: 20\nI (1852) wifi:Init management short buffer num: 32\nI (1852) wifi:Init static tx buffer num: 32\nI (1862) wifi:Init tx cache buffer num: 32\nI (1862) wifi:Init static rx buffer size: 1600\nI (1862) wifi:Init static rx buffer num: 12\nI (1872) wifi:Init dynamic rx buffer num: 20\nI (1872) wifi_init: rx ba win: 16\nI (1882) wifi_init: tcpip mbox: 32\nI (1882) wifi_init: udp mbox: 6\nI (1882) wifi_init: tcp mbox: 12\nI (1892) wifi_init: tcp tx win: 14360\nI (1892) wifi_init: tcp rx win: 14360\nI (1902) wifi_init: tcp mss: 1440\nI (1902) wifi_init: WiFi/LWIP prefer SPIRAM\nI (1912) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (1912) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (1922) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (1942) esp_codec_es8388: Initialising esp_codec\nI (1942) gpio: GPIO[21]| InputEn: 0| OutputEn: 1| OpenDrain: 0| Pullup: 0| Pulldown: 0| Intr:0\nI (2402) button_driver_gpio: Initialising button driver\nI (2402) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2402) gpio: GPIO[39]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2412) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2422) led_driver_esp_ledc: Initialising led driver\nI (2432) [scli]: Initialising UART on port 0\nI (2432) [diag_cli]: Registering command: up-time\nI (2432) uart: queue free spaces: 8I (4912) wifi<1,0>, old:<1,0>, ap:<255,255>, sta:<1,0>, prof:1\nI (4912) wifi:state: init -> auth (b0)\nI (4942) wifi:state: auth -> assoc (0)\nI (4942) wifi:state: assoc -> run (10)\nI (4962) wifi:connected with Office, aid = 5, channel 1, BW20, bssid = 5a:96:1d:f4:75:db\nI (4962) wifi:security: WPA2-PSK, phy: bgn, rssi: -43\nI (4972) wifi:pm start, type: 1I (5062) wifi:AP's beacon interval = 102400 us, DTIM period = 3\n[app_wifi]: Connected with IP Address: 192.168.137.196\nI (5812) esp_netif_handlers: sta ip: 192.168.137.196, mask: 255.255.255.0, gw: 192.168.137.1\nI (5812) [va_nvs_utils]: No value set for: friendlyname\nI (5822) SSDP: Network Interface List (1):\nI (5822) [time_utils]: SNTP already initialized.\nI (5822) SSDP: 1. wifi  : 192.168.137.196\nI (5832) LSSDP: create SSDP socket 57\nI (5852) [tls_certification]: Done setting global CA store\nI (5852) auth-delegate-config: Client ID or Refresh token found in NVS\nI (5862) [va_nvs_utils]: No value set for: clientSecret\nI (5862) auth-delegate-config: Returning auth delegate subsequent auth configuration\n[alexa]: Authentication done\nI (5872) [network_diagnostics]: Registering command: ping\nI (5882) [app_va_cb]: Dialog state is: 8\n[dialog]: Entering VA_IDLE\nI (5922) [alexa_playback_controller_cli]: Registering command: button\nI (5932) [alexa_bt]: Min. Ever Free Size        78316   1931664\nI (5942) [bluetooth]: Enabling BT host and controller\nI (5942) BTDM_INIT: BT controller compile version [ba56601]\nI (5952) phy_init: phy_version 4660,0162888,Dec 23 2020\nI (6312) [va_nvs_utils]: No value set for: locale\nI (6312) [va_nvs_utils]: No value set for: second_locale\nE (6312) BT_AV: a2dp invalid cb event: 4\n[speaker]: Volume changed to 59\nI (6352) [audio_codec]: Starting mp3_decoder codec\nI (6352) [alexa_player]: Init done\nI (6362) audio_stream: Starting http_stream stream\nI (6362) [audio_codec]: Starting mp3_decoder codec\nI (6362) [va_nvs_utils]: No value set for: dnd\nI (6372) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (20792) [time_utils]: The current time is: Fri Dec 17 04:56:25 2021 +0000[GMT], DST: No.\nI (21372) [time_utils]: The current time is: Fri Dec 17 04:56:25 2021 +0000[GMT], DST: No.\nI (21372) [va_nvs_utils]: No value set for: gateway\n[apigateway_handler]: Cannot find endpoint URL in NVS. Setting default: \nI (21382) [va_nvs_utils]: No value set for: endpoint\n[apigateway_handler]: AVS endpoint: \nI (21392) [http_transport]: Connecting to server: \nI (21402) [sh2lib]: [sh2-connect] Setting default tls_cfg parameter for alpn_proto.\nI (23422) [http_transport]: HTTP2 Connection done\nI (23422) [http_transport]: Set AVS connection packet priority to Voice\n[http_stream]: [stream_new]: Internal: 20464, External: 982728\n[auth-delegate]: Token will be refreshed after 3000 seconds.\n[http_stream]: [stream_get: 1]: /v20160207/directives\n[http_stream]: [sid: 1] Response code: 200\nI (23812) [http_stream]: [sid: 1] Response content type is multipart/related; boundary=------abcde123; type=application/json\nI (23812) [http_stream]: Final boundary:------abcde123\nI (23822) [http_stream]: Speculatively stopping capture\nI (23832) [http_stream]: Part begin\n[http_transport]: AVS level connction has now been established: \nI (23842) [http_transport]: Enqueueing SynchronizeState to the front\n[http_stream]: [stream_new]: Internal: 19896, External: 979140\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"Bluetooth\",\"name\":\"BluetoothState\"},\"payload\":{\"alexaDevice\":{\"friendlyName\":\"ESP32-0E0\"},\"pairedDevices\": [{\"uniqueDeviceId\":\"d246b1df-2144-42b1-a408-192eebe497b7\",\"friendlyName\":\"ChaituK20Pro\",\"supportedProfiles\":[{\"name\":\"A2DP-SOURCE\",\"version\":\"\"}]}]}},{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"FINISHED\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"System\",\"name\":\"SynchronizeState\",\"messageId\":\"261180f9-4e69-6a0b-d1bd-d237c8dda093\"},\"payload\":{}}}\n[http_stream]: [stream_post: 3]: /v20160207/events\n[http_stream]: [sid: 3] Response code: 204\nI (24522) [http_transport]: [sid: 3] stream close\n[22 seconds]: [http_stream]: [stream_delete: 3] Internal: 19324, External: 980496, min ever internal: 17880, largest free block: 17844\n[http_stream]: [stream_new]: Internal: 19484, External: 980704\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"Bluetooth\",\"name\":\"BluetoothState\"},\"payload\":{\"alexaDevice\":{\"friendlyName\":\"ESP32-0E0\"},\"pairedDevices\": [{\"uniqueDeviceId\":\"d246b1df-2144-42b1-a408-192eebe497b7\",\"friendlyName\":\"ChaituK20Pro\",\"supportedProfiles\":[{\"name\":\"A2DP-SOURCE\",\"version\":\"\"}]}]}},{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":59,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"FINISHED\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"Alexa.ApiGateway\",\"name\":\"VerifyGateway\",\"messageId\":\"e21a08ae-f65a-d521-e6ce-4c111c31c485\"},\"payload\":{}}}\n[http_stream]: [stream_post: 5]: /v20160207/events\n[http_stream]: [sid: 5] Response code: 204\n[alexa_discovery]: Capabilities unchanged\nI (25052) [esp_dsp]: Created I2S audio stream\nI (25052) audio_stream: Starting i2s_reader stream\nI (25062) audio_stream: Starting audio stream i2s_reader\nI (25062) audio_stream: Stream i2s_reader Event Started\nI (25072) [esp_dsp]: Reader stream event 2\nI (25082) audio_stream: Stopping audio stream i2s_reader\nI (25082) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (25102) audio_stream: Starting audio stream i2s_reader\nW (25152) wifi:m f nullI (25362) [va_nvs_utils]: No value set for: alert_0_data\nI (25362) [va_nvs_utils]: No value set for: alert_1_data\nI (25362) [va_nvs_utils]: No value set for: alert_2_data\nI (25362) [va_nvs_utils]: No value set for: alert_3_data\nI (25372) [alerts_nvs]: Alerts loaded from NVS\n############## Alexa is ready ##############\nI (26152) [http_transport]: [sid: 5] stream close\n[24 seconds]: [http_stream]: [stream_delete: 5] Internal: 7392, External: 867036, min ever internal: 92, largest free block: 7016\nI (61882) [network_diagnostics]: Network rssi: -41", "type": "commented", "related_issue": null}, {"user_name": "chaitanya-smartrotamac", "datetime": "Dec 17, 2021", "body": "How to use my own certificates for this example amazon_alexa ?", "type": "commented", "related_issue": null}, {"user_name": "Marcoz08", "datetime": "Dec 18, 2021", "body": "Thanks, I resolved the partitions.csv, code build and flash successfully but i can't connect to app.Log:`entry 0x40080710\nI (29) boot: ESP-IDF v4.2.2-dirty 2nd stage bootloader\nI (29) boot: compile time 17:53:31\nI (29) boot: chip revision: 3\nI (32) boot_comm: chip revision: 3, min. bootloader chip revision: 0\nI (40) qio_mode: Enabling default flash chip QIO\nI (45) boot.esp32: SPI Speed      : 80MHz\nI (49) boot.esp32: SPI Mode       : QIO\nI (54) boot.esp32: SPI Flash Size : 8MB\nI (59) boot: Enabling RNG early entropy source...\nI (64) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (82) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (90) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (97) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (105) boot:  4 ota_0            OTA app          00 10 00020000 00480000\nI (112) boot: End of partition table\nI (117) boot: No factory image, trying OTA 0\nI (121) boot_comm: chip revision: 3, min. application chip revision: 0\nI (129) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1cfb04 (1899268) map\nI (766) esp_image: segment 1: paddr=0x001efb2c vaddr=0x3ffbdb60 size=0x004ec (  1260) load\nI (767) esp_image: segment 2: paddr=0x001f0020 vaddr=0x400d0020 size=0x150f18 (1380120) map\n0x400d0020: _stext at ??:?I (1229) esp_image: segment 3: paddr=0x00340f40 vaddr=0x3ffbe04c size=0x0590c ( 22796) load\nI (1238) esp_image: segment 4: paddr=0x00346854 vaddr=0x40080000 size=0x1dde0 (122336) load\n0x40080000: _WindowOverflow4 at C:/Users/MARCOZ/Desktop/esp-idf/components/freertos/xtensa/xtensa_vectors.S:1730I (1304) boot: Loaded app from partition at offset 0x20000\nI (1304) boot: Set actual ota_seq=1 in otadata[0]\nI (1304) boot: Disabling RNG early entropy source...\nI (1310) psram: This chip is ESP32-D0WD\nI (1314) spiram: Found 64MBit SPI RAM device\nI (1319) spiram: SPI RAM mode: flash 80m sram 80m\nI (1324) spiram: PSRAM initialized, cache is in low/high (2-core) mode.\nI (1332) cpu_start: Pro cpu up.\nI (1336) cpu_start: Application information:\nI (1340) cpu_start: Project name:     avs\nI (1345) cpu_start: App version:      1.0\nI (1350) cpu_start: Compile time:     Dec 17 2021 17:53:00\nI (1356) cpu_start: ELF file SHA256:  da111424e87f7aac...\nI (1362) cpu_start: ESP-IDF:          v4.2.2-dirty\nI (1368) cpu_start: Starting app cpu, entry point is 0x40082010\n0x40082010: call_start_cpu1 at C:/Users/MARCOZ/Desktop/esp-idf/components/esp32/cpu_start.c:287I (0) cpu_start: App cpu up.\nI (1876) spiram: SPI SRAM memory test OK\nI (1878) heap_init: Initializing. RAM available for dynamic allocation:\nI (1878) heap_init: At 3FFAFF10 len 000000F0 (0 KiB): DRAM\nI (1883) heap_init: At 3FFB6388 len 00001C78 (7 KiB): DRAM\nI (1889) heap_init: At 3FFB9A20 len 00004108 (16 KiB): DRAM\nI (1896) heap_init: At 3FFC7E40 len 000181C0 (96 KiB): DRAM\nI (1902) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\nI (1908) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\nI (1915) heap_init: At 4009DDE0 len 00002220 (8 KiB): IRAM\nI (1921) cpu_start: Pro cpu start user code\nI (1926) spiram: Adding pool of 4077K of external SPI memory to heap allocator\nI (1946) spi_flash: detected chip: generic\nI (1947) spi_flash: flash io: qio\nI (1947) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (1955) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nI (1965) [app_main]: ==== Voice Assistant SDK version: v1.2.2 ====\nI (1975) [time_utils]: Initializing SNTP.\nI (1975) [time_utils]: Waiting for time to be synchronized. This may take time.\nI (2025) wifi:wifi driver task: 3ffd63fc, prio:23, stack:6656, core=0\nI (2025) system_api: Base MAC address is not set\nI (2025) system_api: read default base MAC address from EFUSE\nI (2035) wifi:wifi firmware version: bb6888c\nI (2045) wifi:wifi certification version: v7.0\nI (2045) wifi:config NVS flash: enabled\nI (2045) wifi:config nano formating: disabled\nI (2045) wifi:Init data frame dynamic rx buffer num: 20\nI (2045) wifi:Init management frame dynamic rx buffer num: 20\nI (2055) wifi:Init management short buffer num: 32\nI (2065) wifi:Init static tx buffer num: 32\nI (2065) wifi:Init tx cache buffer num: 32\nI (2065) wifi:Init static rx buffer size: 1600\nI (2075) wifi:Init static rx buffer num: 12\nI (2075) wifi:Init dynamic rx buffer num: 20\nI (2085) wifi_init: rx ba win: 16\nI (2085) wifi_init: tcpip mbox: 32\nI (2085) wifi_init: udp mbox: 6\nI (2095) wifi_init: tcp mbox: 12\nI (2095) wifi_init: tcp tx win: 14360\nI (2095) wifi_init: tcp rx win: 14360\nI (2105) wifi_init: tcp mss: 1440\nI (2105) wifi_init: WiFi/LWIP prefer SPIRAM\nI (2115) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (2125) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=5\nI (2125) I2S: PLL_D2: Req RATE: 48000, real rate: 48076.000, BITS: 16, CLKM: 13, BCK: 8, MCLK: 12292917.167, SCLK: 1538432.000000, diva: 64, divb: 1\nI (2145) esp_codec_es8388: Initialising esp_codec\nI (2145) gpio: GPIO[21]| InputEn: 0| OutputEn: 1| OpenDrain: 0| Pullup: 0| Pulldown: 0| Intr:0\nI (2175) button_driver_gpio: Initialising button driver\nI (2175) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2175) gpio: GPIO[39]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2185) gpio: GPIO[36]| InputEn: 1| OutputEn: 0| OpenDrain: 0| Pullup: 1| Pulldown: 1| Intr:4\nI (2195) led_driver_esp_ledc: Initialising led driver\nI (2205) [scli]: Initialising UART on port 0\nI (2205) [diag_cli]: Registering command: up-time\nI (2205) uart: queue free spaces: 8[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\n[app_wifi]: Disconnect event: 5, reason: 201\nI (32595) [va_nvs_utils]: No value set for: friendlyname\nI (32595) SSDP: Network Interface List (1):\nI (32595) SSDP: 1. wifi  : 0.0.0.0\nI (32595) [time_utils]: SNTP already initialized.\nI (32595) LSSDP: create SSDP socket 57\nI (32615) [tls_certification]: Done setting global CA store\nI (32625) [va_nvs_utils]: No value set for: clientId\nI (32625) [va_nvs_utils]: No value set for: refreshToken\nI (32625) [alexa]: Checking for companion app based authentication...[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36105) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36115) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\nE (36125) LSSDP: sendto wifi (0.0.0.0) failed, errno = Host is unreachable (118)\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[alexa]: Waiting for authentication tokens.\n[app_wifi]: Disconnect event: 5, reason: 201`", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/160", "issue_status": " Open\n", "issue_list": [{"user_name": "yusufk", "datetime": "Oct 19, 2021", "body": "Logged this on esp-idf and was asked to move it here ()Build fails for  with an error:Build succeededmake failed with exit code 2`", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Nov 12, 2021", "body": "The IDF branch you are using seems to be v4.4. But in the top level readme, it mentions to checkout the release/v4.2 branch.Can you check the development setup () and try again?", "type": "commented", "related_issue": null}, {"user_name": "yusufk", "datetime": "Oct 19, 2021", "body": [], "type": "issue", "related_issue": "espressif/esp-idf#7711"}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/155", "issue_status": " Open\n", "issue_list": [{"user_name": "Nalininalu", "datetime": "Sep 23, 2021", "body": "In readme they mentioned that this only support Lyra board..is it?", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 23, 2021", "body": "Which board do you have and which example are you trying to build?", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 27, 2021", "body": "I'm trying to do Alexa example ( with music and playback support ) by using vaquita board)", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 29, 2021", "body": "In the release yesterday, we have added the supported boards along with all the supported voice assistants on the master branch. The branches of other repos and the build commands have also been updated. Check the new READMEs for more info.", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 29, 2021", "body": "Could u share the link for that exact branch plz?", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 29, 2021", "body": "\nCheck the readme here.", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 29, 2021", "body": "Hi thank you so much..what are all the music app supported by this sdk", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 30, 2021", "body": "Hi.. this SDK  does not support Spotify ?", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 30, 2021", "body": "The default AVS product here does not support the music services which require approval. You can create your own AVS product, enable the music services that you want and use that AVS product instead of the default one. (You will also have to modify the phone app to use your AVS product and build it again.)Apart from Spotify, all the music services supported by Alexa should work.", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 30, 2021", "body": "Hi  ...thanks.. actually I flashed amazon_aia and it supported music except Spotify, pandora and some others", "type": "commented", "related_issue": null}, {"user_name": "Nalininalu", "datetime": "Sep 23, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-skainet/issues/48", "issue_status": " Closed\n", "issue_list": [{"user_name": "ghost", "datetime": "Mar 10, 2022", "body": "I am using a custom board with 2 I2S microphones. Havn't set up microphones yet, compiling the WakeWord example with ESP32-S3-EYE set as board in menuconfig. I have Disable MultiNet since I have only 2MB of RAM available, but this should be ample for WakeNet, correct? Then I don't underestand why I get \"Item psram alloc failed\" from model.  Am I running out of PSRAM?? I adjusted many settings in menuconfig, but still get same error.ELF file SHA256: 90725b566f0ff608Rebooting...\nxESP-ROM:esp32s3-20210327\nBuild:Mar 27 2021\nrst:0xc (RTC_SW_CPU_RST),boot:0x8 (SPI_FAST_FLASH_BOOT)\nSaved PC:0x403799e0\nSPIWP:0xee\nmode:DIO, clock div:1\nload:0x3fcd0108,len:0x17b4\nload:0x403b6000,len:0xe28\nload:0x403ba000,len:0x30d0\nentry 0x403b6274\nI (29) boot: ESP-IDF v4.4-348-gc45dee0a7a 2nd stage bootloader\nI (29) boot: chip revision: 0\nI (32) qio_mode: Enabling default flash chip QIO\nI (37) boot.esp32s3: Boot SPI Speed : 80MHz\nI (42) boot.esp32s3: SPI Mode       : QIO\nI (47) boot.esp32s3: SPI Flash Size : 8MB\nI (52) boot: Enabling RNG early entropy source...\nI (57) boot: Partition Table:\nI (61) boot: ## Label            Usage          Type ST Offset   Length\nI (68) boot:  0 factory          factory app      00 00 00010000 00271000\nI (75) boot:  1 model            Unknown data     01 82 00281000 0050c000\nI (83) boot: End of partition table\nI (87) esp_image: segment 0: paddr=00010020 vaddr=3c060020 size=1f420h (128032) map\nI (115) esp_image: segment 1: paddr=0002f448 vaddr=3fc9a800 size=00bd0h (  3024) load\nI (116) esp_image: segment 2: paddr=00030020 vaddr=42000020 size=5e6a8h (386728) map\nI (179) esp_image: segment 3: paddr=0008e6d0 vaddr=3fc9b3d0 size=01c4ch (  7244) load\nI (181) esp_image: segment 4: paddr=00090324 vaddr=40378000 size=127f4h ( 75764) load\nI (199) esp_image: segment 5: paddr=000a2b20 vaddr=50000000 size=00010h (    16) load\nI (207) boot: Loaded app from partition at offset 0x10000\nI (207) boot: Disabling RNG early entropy source...\nI (220) spiram: Found 16MBit SPI RAM device\nI (220) spiram: SPI RAM mode: sram 80m\nI (220) spiram: PSRAM initialized, cache is in normal (1-core) mode.\nI (225) cpu_start: Pro cpu up.\nI (228) cpu_start: Starting app cpu, entry point is 0x4037932c\nI (213) cpu_start: App cpu up.\nI (464) spiram: SPI SRAM memory test OK\nI (465) spiram: Instructions copied and mapped to SPIRAM\nI (510) spiram: Read only data copied and mapped to SPIRAM\nI (525) cache: SPIRAM wrap enabled, size = 32.\nI (534) cpu_start: Pro cpu start user code\nI (534) cpu_start: cpu freq: 160000000\nI (534) cpu_start: Application information:\nI (537) cpu_start: Project name:     wake_word_detection\nI (543) cpu_start: App version:      v0.3.0-105-g4847f52-dirty\nI (555) cpu_start: ELF file SHA256:  90725b566f0ff608...\nI (561) cpu_start: ESP-IDF:          v4.4-348-gc45dee0a7a\nI (568) heap_init: Initializing. RAM available for dynamic allocation:\nI (575) heap_init: At 3FC9F058 len 00040FA8 (259 KiB): D/IRAM\nI (581) heap_init: At 3FCE0000 len 0000EE34 (59 KiB): STACK/DRAM\nI (588) heap_init: At 3FCF0000 len 00008000 (32 KiB): DRAM\nI (594) heap_init: At 600FE000 len 00002000 (8 KiB): RTCRAM\nI (600) spiram: Adding pool of 1536K of external SPI memory to heap allocator\nI (608) spi_flash: detected chip: gd\nI (612) spi_flash: flash io: qio\nI (617) sleep: Configure to isolate all GPIO pins in sleep state\nI (623) sleep: Enable automatic switching of GPIO sleep configuration\nI (630) cpu_start: Starting scheduler on PRO CPU.\nI (0) cpu_start: Starting scheduler on APP CPU.\nI (641) spiram: Reserving pool of 32K of internal memory for DMA/internal allocations\nInitializing SPIFFS\nPartition size: total: 4857101, used: 874233\nI (1121) I2S: DMA Malloc info, datalen=blocksize=640, dma_buf_count=6\nmodel_name: hiesp8q8 model_data: /srmodel/hiesp8q8/wn8q8_data\nItem psram alloc failed. Size: 163860 = 81920 x 2 + 16 + 4\nitem buff malloc fail\nGuru Meditation Error: Core  0 panic'ed (StoreProhibited). Exception was unhandled.Core  0 register dump:\nPC      : 0x40056fcc  PS      : 0x00060f30  A0      : 0x82008934  A1      : 0x3fcf3f10\nA2      : 0x00000000  A3      : 0x3dede104  A4      : 0x00028000  A5      : 0x00000000\nA6      : 0x0101fb0f  A7      : 0x0a000101  A8      : 0xf90a060a  A9      : 0x3fcf3ec0\nA10     : 0x0000000a  A11     : 0x00000002  A12     : 0x00014000  A13     : 0x00000002\nA14     : 0x00000010  A15     : 0x00000004  SAR     : 0x00000010  EXCCAUSE: 0x0000001d\nEXCVADDR: 0x00000000  LBEG    : 0x40056fc5  LEND    : 0x40056fe7  LCOUNT  : 0x000027ffBacktrace:0x40056fc9:0x3fcf3f10 |<-CORRUPTED", "type": "commented", "related_issue": null}, {"user_name": "LiTongXue98", "datetime": "Mar 11, 2022", "body": "menuconfig 里面的 CONFIG_ESP32S3_SPIRAM_SUPPORT=y 有没有配置呢？", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 11, 2022", "body": "Yes, I have already done that:%\n% ESP32S3-Specific\n%\n% CONFIG_ESP32S3_DEFAULT_CPU_FREQ_80 is not set\nCONFIG_ESP32S3_DEFAULT_CPU_FREQ_160=y\n% CONFIG_ESP32S3_DEFAULT_CPU_FREQ_240 is not set\nCONFIG_ESP32S3_DEFAULT_CPU_FREQ_MHZ=160%\n% Cache config\n%\n% CONFIG_ESP32S3_INSTRUCTION_CACHE_16KB is not set\nCONFIG_ESP32S3_INSTRUCTION_CACHE_32KB=y\nCONFIG_ESP32S3_INSTRUCTION_CACHE_SIZE=0x8000\n% CONFIG_ESP32S3_INSTRUCTION_CACHE_4WAYS is not set\nCONFIG_ESP32S3_INSTRUCTION_CACHE_8WAYS=y\nCONFIG_ESP32S3_ICACHE_ASSOCIATED_WAYS=8\nCONFIG_ESP32S3_INSTRUCTION_CACHE_LINE_32B=y\nCONFIG_ESP32S3_INSTRUCTION_CACHE_LINE_SIZE=32\nCONFIG_ESP32S3_INSTRUCTION_CACHE_WRAP=y\n% CONFIG_ESP32S3_DATA_CACHE_16KB is not set\nCONFIG_ESP32S3_DATA_CACHE_32KB=y\n% CONFIG_ESP32S3_DATA_CACHE_64KB is not set\nCONFIG_ESP32S3_DATA_CACHE_SIZE=0x8000\n% CONFIG_ESP32S3_DATA_CACHE_4WAYS is not set\nCONFIG_ESP32S3_DATA_CACHE_8WAYS=y\nCONFIG_ESP32S3_DCACHE_ASSOCIATED_WAYS=8\n% CONFIG_ESP32S3_DATA_CACHE_LINE_16B is not set\nCONFIG_ESP32S3_DATA_CACHE_LINE_32B=y\n% CONFIG_ESP32S3_DATA_CACHE_LINE_64B is not set\nCONFIG_ESP32S3_DATA_CACHE_LINE_SIZE=32\nCONFIG_ESP32S3_DATA_CACHE_WRAP=y\n% end of Cache configCONFIG_ESP32S3_SPIRAM_SUPPORT=y%\n% SPI RAM config\n%\nCONFIG_SPIRAM_MODE_QUAD=y\n% CONFIG_SPIRAM_MODE_OCT is not set\nCONFIG_SPIRAM_TYPE_AUTO=y\n% CONFIG_SPIRAM_TYPE_ESPPSRAM16 is not set\n% CONFIG_SPIRAM_TYPE_ESPPSRAM32 is not set\n% CONFIG_SPIRAM_TYPE_ESPPSRAM64 is not set\nCONFIG_SPIRAM_SIZE=-1%\n% PSRAM Clock and CS IO for ESP32S3\n%\nCONFIG_DEFAULT_PSRAM_CLK_IO=30\nCONFIG_DEFAULT_PSRAM_CS_IO=26\n% end of PSRAM Clock and CS IO for ESP32S3CONFIG_SPIRAM_FETCH_INSTRUCTIONS=y\nCONFIG_SPIRAM_RODATA=y\n% CONFIG_SPIRAM_SPEED_120M is not set\nCONFIG_SPIRAM_SPEED_80M=y\n% CONFIG_SPIRAM_SPEED_40M is not set\nCONFIG_SPIRAM=y\nCONFIG_SPIRAM_BOOT_INIT=y\n% CONFIG_SPIRAM_IGNORE_NOTFOUND is not set\n% CONFIG_SPIRAM_USE_MEMMAP is not set\n% CONFIG_SPIRAM_USE_CAPS_ALLOC is not set\nCONFIG_SPIRAM_USE_MALLOC=y\nCONFIG_SPIRAM_MEMTEST=y\nCONFIG_SPIRAM_MALLOC_ALWAYSINTERNAL=131072\nCONFIG_SPIRAM_TRY_ALLOCATE_WIFI_LWIP=y\nCONFIG_SPIRAM_MALLOC_RESERVE_INTERNAL=32768\n% end of SPI RAM config% CONFIG_ESP32S3_TRAX is not set\nCONFIG_ESP32S3_TRACEMEM_RESERVE_DRAM=0x0\n% CONFIG_ESP32S3_ULP_COPROC_ENABLED is not set\nCONFIG_ESP32S3_ULP_COPROC_RESERVE_MEM=0\nCONFIG_ESP32S3_DEBUG_OCDAWARE=y\nCONFIG_ESP32S3_BROWNOUT_DET=y\nCONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_7=y\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_6 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_5 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_4 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_3 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_2 is not set\n% CONFIG_ESP32S3_BROWNOUT_DET_LVL_SEL_1 is not set\nCONFIG_ESP32S3_BROWNOUT_DET_LVL=7\nCONFIG_ESP32S3_TIME_SYSCALL_USE_RTC_FRC1=y\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_RTC is not set\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_FRC1 is not set\n% CONFIG_ESP32S3_TIME_SYSCALL_USE_NONE is not set\nCONFIG_ESP32S3_RTC_CLK_SRC_INT_RC=y\n% CONFIG_ESP32S3_RTC_CLK_SRC_EXT_CRYS is not set\n% CONFIG_ESP32S3_RTC_CLK_SRC_EXT_OSC is not set\n% CONFIG_ESP32S3_RTC_CLK_SRC_INT_8MD256 is not set\nCONFIG_ESP32S3_RTC_CLK_CAL_CYCLES=1024\nCONFIG_ESP32S3_DEEP_SLEEP_WAKEUP_DELAY=2000\n% CONFIG_ESP32S3_NO_BLOBS is not set\n% CONFIG_ESP32S3_RTCDATA_IN_FAST_MEM is not set\n% CONFIG_ESP32S3_USE_FIXED_STATIC_RAM_SIZE is not set\n% end of ESP32S3-Specific", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Mar 14, 2022", "body": "8bit-wakenet8 takes about 850KB, Audio-Front-End module takes about 500 KB.\nWe haven't tested the R8N2, but it's possible to run out of memory.", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Mar 14, 2022", "body": "Next month, we will release a wakenet9 in v2.0 which takes about 300 KB.\nIf you don't want to change ESP32-S3 module, you can wait for the latest wakenet9.", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 14, 2022", "body": "Thanks, I wish I knew about this, now I am left with custom boards that are no good , I made them specifically to use a single wake word (i.e. Alexa)\nI read in\n\nand\nthat RAM usage for WakeWord is <100KB, hence choosing N8R2. Anyways, please support \"Alexa\" and \"Hi, ESP\" in WakeNet9. Good to know that there is still some hope.", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Mar 15, 2022", "body": "Hi eyewy，\nI think I need to explain the details about memory for different wakenet.\nwakenet5(ESP32): we read all parameters from flash and do not save a backup in PSRAM, so we only need RAM < 100KB.\nwakenet7&wakenet8(ESP32S3), When we initialize the model, parameters are read into PSRAM from flash(the parameter of wakenet8 is about 800KB), then we use the parameters in PSRAM to calculate.Why do we do that on ESP32-S3?\nThe main reason is that Octal PSRAM is faster than QIO flash. And Octal flash is too expensive.", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 16, 2022", "body": "Tested on ESP32-S3-DevKitC-1-N32R8V Development Board, and there is no RAM issue any more. This board says:\nI (729) spiram: Adding pool of 8192K of external SPI memory to heap allocator\nAfter running the program I check RAM usage by:\nprintf(\"RAM left %d\\n\", esp_get_free_heap_size());\nand I get:\nRAM left 6718923\nSo actual ram usage is: 1473KBThis confirms  calculation of 850KB + 500KB =1350KBNow here is my question: given ~1.5MB usage which is <2MB, how come I receive error on N8R2?? Is there a temperorary peak usage while loading the model? Is there something that can be done here to optimize instantaneous RAM usage?Also, I don't use Wifi and BT, is there a way to free up little bit more memory?If Answer to above questions is no, I guess I can only wait for WakeNet9 to become available, I would be happy to test it on N8R2 as soon as you have a beta version available.", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Apr 8, 2022", "body": "Yes, when loading the model, it does require more memory.\nI am sorry to tell you wakenet9 will be delayed.  Colleagues in Shanghai can not upload new codes due to COVID-19.", "type": "commented", "related_issue": null}, {"user_name": "feizi", "datetime": "Aug 11, 2022", "body": "Hi @eyewy ， wakenet9 has been updated in the master branch.", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": "Mar 10, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "github-actions", "datetime": "Mar 10, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "feizi", "datetime": "Aug 23, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/151", "issue_status": " Open\n", "issue_list": [{"user_name": "HowardHonig", "datetime": "Sep 7, 2021", "body": "After about 5 minutes, Vaquita will crash and fall into gdb with the message \"Entering gdb stub now\".I've enabled RainMaker.  All code is the original, that is not modified.  Before the crash, everything seems to work well and is really quite impressive.\nWhen RainMaker is not enabled it also crashes but takes a little longer.Any ideas?The last printed messages from the serial port are:[0;32mI (2970557) opus_decoder: Total decoded frames = 240[0m\n[0;32mI (2970557) opus_decoder: Closed[0m\n[0;32mI (2970567) [audio_codec]: Codec opus_decoder Event Stopped[0m\n[2968 seconds]: [http_transport]: Event data: {\"events\":[{\"header\":{\"name\":\"SpeakerClosed\",\"messageId\":\"4377508f-c7be-66d2-a922-bbc1119a0733\"},\"payload\":{\"offset\":69280}}]}\n[0;32mI (2970587) [http_transport]: Sending to topic: $aws/alexa/ais/v1/517f70d2-c7f5-44af-8486-eab66f93135e/event[0m\n[0;32mI (2970597) [basic_player]: Waiting for decoder to stop line 116[0m\n[basic_player]: Codec event: Codec Stopped\n[0;32mI (2970607) opus_decoder: Closed[0m\n[0;32mI (2970617) [audio_codec]: Codec opus_decoder Event Stopped[0m\n[basic_player]: Codec event: Codec Stopped\n[0;32mI (2970627) [audio_codec]: Closing codec opus_decoder without decoding any data[0m\n[attachment]: Free ais_attachment_3\n[dialog]: Speech done\n[2970 seconds]: [http_transport]: Free Memory Internal: 51836, External: 2480240\n[directive_proc]: Json data: {\"directives\":[{\"header\":{\"name\":\"SetAttentionState\",\"messageId\":\"dd02e871-f5ca-4847-a105-952f0e738d45\"},\"payload\":{\"state\":\"IDLE\"}}]}\n[0;32mI (2972877) [directive_proc]: Name: SetAttentionState\n[0m\n[dialog]: Stream finished\n[dialog]: Event STREAM_CLOSED unsupported in SPEAKING state\n[0;32mI (2972897) [app_va_cb]: Dialog state is: 8[0m\n[dialog]: Speaking state timeout. Releasing focus\n[0;32mI (2975397) [focus_manager]: Release channel: Dialog SpeechSynthesizer[0m\n[0;32mI (2975397) [focus_manager]: Releasing SpeechSynthesizer[0m\n[0;32mI (2975397) [sys_playback]: Release[0m\n[dialog]: Entering VA_IDLE\n[auth-delegate]: Queue receive returned false for 3000 seconds. It timed out. Getting new credentials.\nGuru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.\nCore 0 register dump:\nPC      : 0x4015ff24  PS      : 0x00060e30  A0      : 0x80160897  A1      : 0x3f829410  \nA2      : 0x0000003c  A3      : 0x00000000  A4      : 0x3f983b1c  A5      : 0x3f829540  \nA6      : 0x3fff498c  A7      : 0x00000000  A8      : 0x00000004  A9      : 0x3f8293d0  \nA10     : 0x00000000  A11     : 0x00000fff  A12     : 0x00001005  A13     : 0x00000001  \nA14     : 0x00000008  A15     : 0x3f807574  SAR     : 0x00000018  EXCCAUSE: 0x0000001c  \nEXCVADDR: 0x00000004  LBEG    : 0x4000c2e0  LEND    : 0x4000c2f6  LCOUNT  : 0xffffffff  \n\nELF file SHA256: 5659eb0ee866b947\n\nBacktrace: 0x4015ff21:0x3f829410 0x40160894:0x3f8294e0 0x4011345c:0x3f829510 0x400daa6d:0x3f829530 0x400dab69:0x3f8295a0 0x400dae0c:0x3f8295c0\n\nEntering gdb stub now.\n$T0b#e6", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 8, 2021", "body": "Could you share the IDF branch/commit you are using?\nCould you also share another log of the device crashing?", "type": "commented", "related_issue": null}, {"user_name": "HowardHonig", "datetime": "Sep 8, 2021", "body": "Thank you,\n\nI couldn't figure out how to tell the version number of IDF but I originally loaded it from the instructions:$ git clone --recursive \n$ cd esp-idf; git checkout release/v4.0; git submodule init; git submodule update --init --recursive;Here is the output of the  file:\nSetting IDF_PATH: C:\\Users\\howar\\Documents\\Alexa\\iot\\esp-vaquita\\esp-idfAdding ESP-IDF tools to PATH...\nC:\\Users\\howar.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\\bin\nC:\\Users\\howar.espressif\\tools\\esp32ulp-elf\\2.28.51.20170517\\esp32ulp-elf-binutils\\bin\nC:\\Users\\howar.espressif\\tools\\cmake\\3.13.4\\bin\nC:\\Users\\howar.espressif\\tools\\openocd-esp32\\v0.10.0-esp32-20210721\\openocd-esp32\\bin\nC:\\Users\\howar.espressif\\tools\\mconf\\v4.6.0.0-idf-20190628\nC:\\Users\\howar.espressif\\tools\\ninja\\1.9.0\nC:\\Users\\howar.espressif\\tools\\idf-exe\\1.0.1\nC:\\Users\\howar.espressif\\tools\\ccache\\3.7\nC:\\Users\\howar.espressif\\python_env\\idf4.0_py3.9_env\\Scripts\nC:\\Users\\howar\\Documents\\Alexa\\iot\\esp-vaquita\\esp-idf\\toolsand when I do a  from the esp-idf directory:\nv4.0.3-144-gd64bf0dfbeI could try updating to v4.1.12 if you think that will help.\nThis was captured 3x with the same result.  It is taking a little longer to crash today, about 1/2 hr or so.  It enters gdb.  When gdb is exited, it looks like vaquito tries to boot up but doesn't.  Hitting a reset does boot vaquito.\nAnd here is the log of the last crash. Note that the SSDP is my AV amp and I know that isn't supported:\n''\n40e4::urn:schemas-upnp-org:service:RenderingControl:1\nW (2486257) SSDP: --end--\nI (2487267) SSDP: ---SSDP List----\nI (2487267) SSDP: 1.id=, ip=, name=uuid:5f9ec1b3-ed59-1900-4530-00a0dea340e4::urn:schemas-upnp-org:service:ConnectionManager:1\nW (2487277) SSDP: --end--\nI (2487567) SSDP: ---SSDP List----\nI (2487567) SSDP: 1.id=, ip=, name=uuid:5f9ec1b3-ed59-1900-4530-00a0dea340e4::urn:schemas-upnp-org:service:AVTransport:1\nW (2487577) SSDP: --end--\nW (2538097) LSSDP: remove timeout SSDP neighbor:  () (50522ms)\nI (2538097) SSDP: ---SSDP List----\nW (2538097) SSDP: Empty\nW (2598387) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2598387) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2719017) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2719017) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2778647) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2778647) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\nW (2838877) httpd_uri: httpd_uri: URI '/rootDesc.xml' not found\nW (2838877) httpd_txrx: httpd_resp_send_err: 404 Not Found - This URI does not exist\n[auth-delegate]: Queue receive returned false for 3000 seconds. It timed out. Getting new credentials.\nGuru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.\nCore 0 register dump:\nPC      : 0x4015ff24  PS      : 0x00060d30  A0      : 0x80160897  A1      : 0x3f829410\n0x4015ff24: esp_tcp_connect at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:163\n(inlined by) esp_tls_low_level_conn at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:616A2      : 0x0000003c  A3      : 0x00000000  A4      : 0x3f8519ec  A5      : 0x3f829540\nA6      : 0x3fff4984  A7      : 0x00000000  A8      : 0x00000004  A9      : 0x3f8293d0\nA10     : 0x00000000  A11     : 0x00000fff  A12     : 0x00001005  A13     : 0x00000001\nA14     : 0x00000008  A15     : 0x3f807574  SAR     : 0x00000018  EXCCAUSE: 0x0000001c\nEXCVADDR: 0x00000004  LBEG    : 0x4000c2e0  LEND    : 0x4000c2f6  LCOUNT  : 0xffffffffELF file SHA256: 5659eb0ee866b947Backtrace: 0x4015ff21:0x3f829410 0x40160894:0x3f8294e0 0x4011345c:0x3f829510 0x400daa6d:0x3f829530 0x400dab69:0x3f8295a0 0x400dae0c:0x3f8295c0\n0x4015ff21: esp_tcp_connect at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:163\n(inlined by) esp_tls_low_level_conn at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:6160x40160894: esp_tls_conn_new at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-idf/components/esp-tls/esp_tls.c:7170x4011345c: http_connection_new at C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-va-sdk/components/httpc/httpc.c:1560x400daa6d: get_auth_response_with_header at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:830x400dab69: get_auth_response at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:1410x400dae0c: refresh_auth_task at /Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/auth_delegate.c:303Entering gdb stub now.\n$T0b#e6GNU gdb (crosstool-NG esp-2020r3) 8.1.0.20180627-git\nCopyright (C) 2018 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later \nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"--host=x86_64-host_w64-mingw32 --target=xtensa-esp32-elf\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n.\nFind the GDB manual and other documentation resources online at:\n.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from c:\\users\\howar\\documents\\alexa\\iot\\esp-vaquita\\esp-va-sdk\\examples\\amazon_aia\\build\\aia.elf...done.\nRemote debugging using COM7\nIgnoring packet error, continuing...\nwarning: unrecognized item \"timeout\" in \"qSupported\" response\nRemote replied unexpectedly to 'vMustReplyEmpty': vMustReplyEmpty\n(gdb)\n''", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 9, 2021", "body": "From your logs, it seems to crash after:The device connects to the server and gets new access tokens. And the crash seems to be when connecting. Let me check and get back. What I can think of, is it could be because of lack of memory or lack of sockets.Have you also checked with the default repository?", "type": "commented", "related_issue": null}, {"user_name": "HowardHonig", "datetime": "Sep 9, 2021", "body": "Thanks again.  Not sure what you identify as the default repository.   Just tried building with the recent tools version v4.3.1 of the tools and got a linking error, undefined reference.  I did change the project.cmake file to add  statement.Got the below link error message.  Could not find where xTaskNofity is defined, except in a library in the recommended downloaded but not in v4.3.1 tools.  Did find a number of places in which the function is called.FAILED: aia.elf\ncmd.exe /C \"cd . && C:\\Users\\howar.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\\bin\\xtensa-esp32-elf-g++.exe  -mlongcalls -Wno-frame-address   @CMakeFiles\\aia.elf.rsp  -o aia.elf  && cd .\"\nc:/users/howar/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld.exe: C:/Users/howar/Documents/Alexa/iot/esp-vaquita/esp-va-sdk/components/voice_assistant/lib/libaia.a(dialog.o):(.literal.dialog_sm_thread+0x30): undefined reference to dialog_sm_thread':\n/Users/ganesh/work/gitlab/esp-alexa/components/voice_assistant/common/src/dialog.c:324: undefined reference to `xTaskNotify'\ncollect2.exe: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 13, 2021", "body": "The esp-idf version or branch should be release/v4.0 for the amazon_aia example. It is not expected to work with other IDF versions since the libraries built are version specific. Can you try with the release/v4.0 branch for esp-idf? (From the logs in previous comments, I think you were already using this branch and getting the crashes on this branch. We have a planned release for this week with release/v4.2 version and it has a few stability fixes for AIA.)By 'default repository', I mean without any changes (apart from the patches required to compile) that you may have done.", "type": "commented", "related_issue": null}, {"user_name": "HowardHonig", "datetime": "Sep 20, 2021", "body": "Thank you chiragatal.\nI deleted and reinstalled all the need support drivers, libraries, and packages.  Made sure it was version 4.0 of the esp-idf.  Still crashes in the same way. Did not try the \"future aia/beta\" version.\nLooked for version esp-va-sdk 4.2 and it is an older release so I'll wait for the \"planned release for this week of v4.2\".I also eliminated the SSDP sources in the local network, except for the other Alexa's which also sends out SSDP responses, still crashed.\nTried a couple of times to delete the build directory and did an \"idf.py fullclean\" to be thorough.What I don't understand is why I'm seeing this problem since this board is not new.  Doesn't anyone else have this same problem?  If not, perhaps my board is defective and I'm willing to buy another one to see if it is.  If not, I'll wait for the new release but need more information on which driver and support package is being released so I can  download and try it when ready.", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Sep 21, 2021", "body": "The new release would be for esp-va-sdk. It will use esp-idf v4.2 and will not use esp-aws-iot at all. When released, just installing/updating the esp-idf drivers (running install.sh) should be enough.", "type": "commented", "related_issue": null}, {"user_name": "HowardHonig", "datetime": "Sep 9, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "HowardHonig", "datetime": "Sep 9, 2021", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/161", "issue_status": " Open\n", "issue_list": [{"user_name": "yusufk", "datetime": "Oct 19, 2021", "body": "Getting the following error on console:", "type": "commented", "related_issue": null}, {"user_name": "s95i", "datetime": "Dec 14, 2021", "body": "I have the same problem. Did you find the solution?", "type": "commented", "related_issue": null}, {"user_name": "yusufk", "datetime": "Dec 14, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 15, 2021", "body": "Hi  could you please tell which app are you trying and provide complete logs for the same?\nIt appears that we might need to update some certificate in our library.", "type": "commented", "related_issue": null}, {"user_name": "yusufk", "datetime": "Dec 15, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "s95i", "datetime": "Dec 15, 2021", "body": "Hi , I had the same issue with the GVA app.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 16, 2021", "body": "I believe this is GVA app and not the Alexa app. We will check and get back.", "type": "commented", "related_issue": null}, {"user_name": "yusufk", "datetime": "Dec 16, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "s95i", "datetime": "Jan 11, 2022", "body": "Hi, , Is there any way to add a new certificate to solve this issue?", "type": "commented", "related_issue": null}, {"user_name": "andrewwinter", "datetime": "Jan 18, 2022", "body": "I'm still having this issue does anyone have a solution? I believe that the problem is that the CA certifications are hardcoded into the lib files (found at esp-va-sdk/voice_assistant/*.a), I found an older google CA certification in them, however i believe that they're expired. Could you please recompile these libraries with the new CA certificates? \nThanks", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Feb 10, 2022", "body": "Hi \nSorry for the delay. We will provide updated library by next week.", "type": "commented", "related_issue": null}, {"user_name": "s95i", "datetime": "Mar 1, 2022", "body": "Hi  ,\nWhen can we expect the updated library?\nThanks", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/143", "issue_status": " Open\n", "issue_list": [{"user_name": "KalashQK", "datetime": "Aug 19, 2021", "body": "Hello!\nPlease tell me if there are any methods of Wi-Fi authentication for Google Voice Assistant on LyraT board besides NVS input? For example, how is it implemented in the Alexa variant using Bluetooth, but only in the GVA? Perhaps not yet implemented, but planned?\nIf not planned, please tell me or advise where to start for development of such a solution?I also notice at GVA that if do not turn on the board and do not use it for more than a week, it stops responding to commands. After pronouncing WakeWord, the board reacts, there is a characteristic sound and light, but there is no response. The board need to be reflash again. How can fix this problem? I am using 2 LyraT boards, both have this problem.Thanks in advance!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/132", "issue_status": " Open\n", "issue_list": [{"user_name": "jassi00713", "datetime": "Mar 25, 2021", "body": "Trying to implement Alexa on ESP32 Vaquita DSPG and using branch feature/aia-betaFollowed instructions on the below link:-\nBuild is failing.\nError caused due to -> multiple definition of `set_xpd_sar'\nKindly help with the soltion to this.Below are logs on running idf.py build.\nLOGS-idf.py build\nChecking Python dependencies...\nPython requirements from /home/javivek/espaia2/esp-idf/requirements.txt are satisfied.\nExecuting action: all (aliases: build)\nRunning ninja in directory /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build\nExecuting \"ninja all\"...\n[211/1168] cd /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-...****************************************************************************\"\nPartition table binary generated. Contents:nvs,data,nvs,0x9000,24K,\nphy_init,data,phy,0xf000,4K,\nfctry_aia,data,nvs,0x10000,24K,\nfctry,data,nvs,0x16000,24K,\notadata,data,ota,0x1c000,8K,\nota_0,app,ota_0,0x20000,4032K,\nota_1,app,ota_1,0x410000,4032K,[304/1168] Performing configure step for 'bootloader'\n-- Component directory /home/javivek/espaia2/esp-idf/components/cbor does not contain a CMakeLists.txt file. No component will be added\n-- Component directory /home/javivek/espaia2/esp-idf/components/cmock does not contain a CMakeLists.txt file. No component will be added\n-- Component directory /home/javivek/espaia2/esp-idf/components/tinyusb does not contain a CMakeLists.txt file. No component will be added\n-- Project version: v4.0.2-442-g41efdb0b3\n-- Building ESP-IDF components for target esp32\n-- Project sdkconfig file /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/sdkconfig\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp32/ld/esp32.peripherals.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main/esp32.bootloader.ld\n-- Adding linker script /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main/esp32.bootloader.rom.ld\n-- Components: bootloader bootloader_support efuse esp32 esp_common esp_rom esptool_py log main micro-ecc partition_table soc spi_flash xtensa\n-- Component paths: /home/javivek/espaia2/esp-idf/components/bootloader /home/javivek/espaia2/esp-idf/components/bootloader_support /home/javivek/espaia2/esp-idf/components/efuse /home/javivek/espaia2/esp-idf/components/esp32 /home/javivek/espaia2/esp-idf/components/esp_common /home/javivek/espaia2/esp-idf/components/esp_rom /home/javivek/espaia2/esp-idf/components/esptool_py /home/javivek/espaia2/esp-idf/components/log /home/javivek/espaia2/esp-idf/components/bootloader/subproject/main /home/javivek/espaia2/esp-idf/components/bootloader/subproject/components/micro-ecc /home/javivek/espaia2/esp-idf/components/partition_table /home/javivek/espaia2/esp-idf/components/soc /home/javivek/espaia2/esp-idf/components/spi_flash /home/javivek/espaia2/esp-idf/components/xtensa\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/bootloader\n[331/1168] Performing build step for 'bootloader'\n[1/2] Linking C executable bootloader.elf\n[2/2] Generating binary image from built executable\nesptool.py v2.9-dev\nGenerated /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/bootloader/bootloader.bin\n[1067/1168] Building C object esp-idf/streams/CMakeFiles/__idf_streams.dir/i2s_stream/i2s_stream.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/streams/i2s_stream/i2s_stream.c: In function 'parse_i2s_config':\n/home/javivek/espaia2/esp-va-sdk/components/streams/i2s_stream/i2s_stream.c:45:15: warning: unused variable 'ret' [-Wunused-variable]\nesp_err_t ret = ESP_OK;\n^~~\n[1078/1168] Building C object esp-idf/httpc/CMakeFiles/__idf_httpc.dir/httpc.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c: In function 'http_connection_new':\n/home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c:156:5: warning: 'esp_tls_conn_new' is deprecated [-Wdeprecated-declarations]\ntls = esp_tls_conn_new(&url[u->field_data[UF_HOST].off], u->field_data[UF_HOST].len,\n^~~\nIn file included from /home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.h:18,\nfrom /home/javivek/espaia2/esp-va-sdk/components/httpc/httpc.c:20:\n/home/javivek/espaia2/esp-idf/components/esp-tls/esp_tls.h:271:12: note: declared here\nesp_tls_t )(void)' {aka 'int ()(uint8_t)' {aka 'int ()(int)' [-Wincompatible-pointer-types]\nmedia_hal->audio_codec_control_volume = es8311_set_volume;\n^\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/media_hal_codec_init.c:47:39: warning: assignment to 'esp_err_t ()(_Bool)'} from incompatible pointer type 'int (*)(int)' [-Wincompatible-pointer-types]\nmedia_hal->audio_codec_set_mute = es8311_set_mute;\n^\n[1146/1168] Building C object esp-idf/codec_es8311/CMakeFiles/__idf_codec_es8311.dir/es8311.c.obj\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c: In function 'es8311_init':\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:321:9: warning: unused variable 'port_num' [-Wunused-variable]\nint port_num = media_hal_conf->port_num;\n^~~~~~~~\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:320:28: warning: unused variable 'es8388_dac_output' [-Wunused-variable]\nmedia_hal_dac_output_t es8388_dac_output = media_hal_conf->dac_output;\n^~~~~~~~~~~~~~~~~\n/home/javivek/espaia2/esp-va-sdk/components/audio_hal/esp_codec/es8311/components/codec_es8311/es8311.c:319:27: warning: unused variable 'es8311_adc_input' [-Wunused-variable]\nmedia_hal_adc_input_t es8311_adc_input = media_hal_conf->adc_input;\n^~~~~~~~~~~~~~~~\n[1167/1168] Linking CXX executable aia.elf\nFAILED: aia.elf\n: && /home/javivek/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++  -mlongcalls -Wno-frame-address  -nostdlib CMakeFiles/aia.elf.dir/project_elf_src.c.obj  -o aia.elf  esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/driver/libdriver.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/efuse/libefuse.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/app_update/libapp_update.a esp-idf/spi_flash/libspi_flash.a esp-idf/nvs_flash/libnvs_flash.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/lwip/liblwip.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/esp_event/libesp_event.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/esp32/libesp32.a esp-idf/xtensa/libxtensa.a esp-idf/esp_common/libesp_common.a esp-idf/esp_rom/libesp_rom.a esp-idf/soc/libsoc.a esp-idf/log/liblog.a esp-idf/heap/libheap.a esp-idf/freertos/libfreertos.a esp-idf/vfs/libvfs.a esp-idf/newlib/libnewlib.a esp-idf/cxx/libcxx.a esp-idf/app_trace/libapp_trace.a esp-idf/asio/libasio.a esp-idf/bt/libbt.a esp-idf/coap/libcoap.a esp-idf/console/libconsole.a esp-idf/nghttp/libnghttp.a esp-idf/esp-tls/libesp-tls.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/esp_gdbstub/libesp_gdbstub.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/esp_https_ota/libesp_https_ota.a esp-idf/protobuf-c/libprotobuf-c.a esp-idf/protocomm/libprotocomm.a esp-idf/mdns/libmdns.a esp-idf/esp_local_ctrl/libesp_local_ctrl.a esp-idf/esp_websocket_client/libesp_websocket_client.a esp-idf/expat/libexpat.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/fatfs/libfatfs.a esp-idf/freemodbus/libfreemodbus.a esp-idf/jsmn/libjsmn.a esp-idf/json/libjson.a esp-idf/libsodium/liblibsodium.a esp-idf/mqtt/libmqtt.a esp-idf/openssl/libopenssl.a esp-idf/spiffs/libspiffs.a esp-idf/ulp/libulp.a esp-idf/unity/libunity.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/main/libmain.a esp-idf/json_parser/libjson_parser.a esp-idf/httpc/libhttpc.a esp-idf/streams/libstreams.a esp-idf/utils/libutils.a esp-idf/media_hal/libmedia_hal.a esp-idf/led_pattern/libled_pattern.a esp-idf/misc/libmisc.a esp-idf/audio_hal/libaudio_hal.a esp-idf/audio_pipeline/libaudio_pipeline.a esp-idf/sys_playback/libsys_playback.a esp-idf/basic_player/libbasic_player.a esp-idf/esp_httpd_ota/libesp_httpd_ota.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/qrcode/libqrcode.a esp-idf/sh2lib/libsh2lib.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/common/libcommon.a esp-idf/alexa_equalizer/libalexa_equalizer.a esp-idf/app_cloud/libapp_cloud.a esp-idf/app_smart_home/libapp_smart_home.a esp-idf/custom_player/libcustom_player.a esp-idf/esp-aws-iot/libesp-aws-iot.a esp-idf/audio_board_dspg_avs_kit_m/libaudio_board_dspg_avs_kit_m.a esp-idf/neo_pixel_led/libneo_pixel_led.a esp-idf/codec_es8311/libcodec_es8311.a -Wl,--cref -Wl,--Map=/home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/aia.map -fno-rtti -fno-lto /home/javivek/espaia2/esp-va-sdk/components/voice_assistant/lib/libaia.a /home/javivek/espaia2/esp-va-sdk/components/codecs/lib/libcodecs.a esp-idf/utils/libutils.a esp-idf/audio_hal/libaudio_hal.a esp-idf/media_hal/libmedia_hal.a esp-idf/streams/libstreams.a esp-idf/misc/libmisc.a esp-idf/led_pattern/libled_pattern.a esp-idf/sys_playback/libsys_playback.a /home/javivek/espaia2/esp-va-sdk/components/esp-ssdp/lib/libesp-ssdp.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/dbmd5-ipc/lib/libdspg-ipc.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/va_dsp/lib/libva_dsp.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/basic_player/libbasic_player.a esp-idf/main/libmain.a esp-idf/streams/libstreams.a esp-idf/utils/libutils.a esp-idf/media_hal/libmedia_hal.a esp-idf/led_pattern/libled_pattern.a esp-idf/misc/libmisc.a esp-idf/audio_hal/libaudio_hal.a esp-idf/audio_pipeline/libaudio_pipeline.a esp-idf/sys_playback/libsys_playback.a esp-idf/basic_player/libbasic_player.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/common/libcommon.a esp-idf/alexa_equalizer/libalexa_equalizer.a esp-idf/app_smart_home/libapp_smart_home.a esp-idf/custom_player/libcustom_player.a esp-idf/audio_board_dspg_avs_kit_m/libaudio_board_dspg_avs_kit_m.a esp-idf/neo_pixel_led/libneo_pixel_led.a esp-idf/codec_es8311/libcodec_es8311.a /home/javivek/espaia2/esp-va-sdk/components/voice_assistant/lib/libaia.a /home/javivek/espaia2/esp-va-sdk/components/codecs/lib/libcodecs.a esp-idf/utils/libutils.a esp-idf/audio_hal/libaudio_hal.a esp-idf/media_hal/libmedia_hal.a esp-idf/streams/libstreams.a esp-idf/misc/libmisc.a esp-idf/led_pattern/libled_pattern.a esp-idf/sys_playback/libsys_playback.a /home/javivek/espaia2/esp-va-sdk/components/esp-ssdp/lib/libesp-ssdp.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/dbmd5-ipc/lib/libdspg-ipc.a /home/javivek/espaia2/esp-va-sdk/components/audio_hal/dsp_driver/dspg_driver/components/va_dsp/lib/libva_dsp.a esp-idf/uuid-gen/libuuid-gen.a esp-idf/basic_player/libbasic_player.a esp-idf/main/libmain.a esp-idf/qrcode/libqrcode.a esp-idf/spiffs/libspiffs.a esp-idf/sh2lib/libsh2lib.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/libsodium/liblibsodium.a esp-idf/fatfs/libfatfs.a esp-idf/httpc/libhttpc.a esp-idf/json_parser/libjson_parser.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/driver/libdriver.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/efuse/libefuse.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/app_update/libapp_update.a esp-idf/spi_flash/libspi_flash.a esp-idf/nvs_flash/libnvs_flash.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/lwip/liblwip.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/esp_event/libesp_event.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/esp32/libesp32.a esp-idf/xtensa/libxtensa.a esp-idf/esp_common/libesp_common.a esp-idf/esp_rom/libesp_rom.a esp-idf/soc/libsoc.a esp-idf/log/liblog.a esp-idf/heap/libheap.a esp-idf/freertos/libfreertos.a esp-idf/vfs/libvfs.a esp-idf/newlib/libnewlib.a esp-idf/cxx/libcxx.a esp-idf/app_trace/libapp_trace.a esp-idf/asio/libasio.a esp-idf/bt/libbt.a esp-idf/coap/libcoap.a esp-idf/console/libconsole.a esp-idf/nghttp/libnghttp.a esp-idf/esp-tls/libesp-tls.a esp-idf/esp_adc_cal/libesp_adc_cal.a esp-idf/esp_gdbstub/libesp_gdbstub.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/esp_https_ota/libesp_https_ota.a esp-idf/esp_http_client/libesp_http_client.a esp-idf/protobuf-c/libprotobuf-c.a esp-idf/protocomm/libprotocomm.a esp-idf/mdns/libmdns.a esp-idf/esp_local_ctrl/libesp_local_ctrl.a esp-idf/esp_websocket_client/libesp_websocket_client.a esp-idf/expat/libexpat.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/fatfs/libfatfs.a esp-idf/wear_levelling/libwear_levelling.a esp-idf/sdmmc/libsdmmc.a esp-idf/freemodbus/libfreemodbus.a esp-idf/jsmn/libjsmn.a esp-idf/json/libjson.a esp-idf/libsodium/liblibsodium.a esp-idf/mqtt/libmqtt.a esp-idf/tcp_transport/libtcp_transport.a esp-idf/openssl/libopenssl.a esp-idf/spiffs/libspiffs.a esp-idf/ulp/libulp.a esp-idf/unity/libunity.a esp-idf/wifi_provisioning/libwifi_provisioning.a esp-idf/protocomm/libprotocomm.a esp-idf/bt/libbt.a -L/home/javivek/espaia2/esp-idf/components/bt/controller/lib -lbtdm_app esp-idf/protobuf-c/libprotobuf-c.a esp-idf/mdns/libmdns.a esp-idf/json/libjson.a esp-idf/json_parser/libjson_parser.a esp-idf/httpc/libhttpc.a /home/javivek/espaia2/esp-va-sdk/components/esp-downmix/lib/libesp-downmix.a esp-idf/esp_httpd_ota/libesp_httpd_ota.a esp-idf/esp_http_server/libesp_http_server.a esp-idf/multipart_parser/libmultipart_parser.a esp-idf/qrcode/libqrcode.a esp-idf/sh2lib/libsh2lib.a esp-idf/esp-tls/libesp-tls.a esp-idf/nghttp/libnghttp.a -L /home/javivek/espaia2/esp-va-sdk/components/speech_recog/lib -lesp_wwe -lspeech_recog -lc_speech_features -ldl_lib -lnn_model esp-idf/app_cloud/libapp_cloud.a esp-idf/console/libconsole.a esp-idf/esp-aws-iot/libesp-aws-iot.a esp-idf/jsmn/libjsmn.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a esp-idf/cxx/libcxx.a esp-idf/newlib/libnewlib.a esp-idf/freertos/libfreertos.a esp-idf/heap/libheap.a esp-idf/log/liblog.a esp-idf/soc/libsoc.a esp-idf/esp_rom/libesp_rom.a esp-idf/esp_common/libesp_common.a esp-idf/xtensa/libxtensa.a esp-idf/esp32/libesp32.a esp-idf/esp_ringbuf/libesp_ringbuf.a esp-idf/lwip/liblwip.a esp-idf/mbedtls/mbedtls/library/libmbedtls.a esp-idf/mbedtls/mbedtls/library/libmbedcrypto.a esp-idf/mbedtls/mbedtls/library/libmbedx509.a esp-idf/bootloader_support/libbootloader_support.a esp-idf/spi_flash/libspi_flash.a esp-idf/efuse/libefuse.a esp-idf/app_update/libapp_update.a esp-idf/wpa_supplicant/libwpa_supplicant.a esp-idf/nvs_flash/libnvs_flash.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcoexist.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libcore.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libespnow.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libmesh.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libnet80211.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libphy.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libpp.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/librtc.a /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32/libsmartconfig.a esp-idf/esp_event/libesp_event.a esp-idf/tcpip_adapter/libtcpip_adapter.a esp-idf/driver/libdriver.a esp-idf/vfs/libvfs.a esp-idf/esp_wifi/libesp_wifi.a esp-idf/esp_eth/libesp_eth.a esp-idf/app_trace/libapp_trace.a esp-idf/pthread/libpthread.a esp-idf/espcoredump/libespcoredump.a -u __cxa_guard_dummy -lstdc++ esp-idf/pthread/libpthread.a -u __cxx_fatal_exception esp-idf/newlib/libnewlib.a -u newlib_include_locks_impl -u newlib_include_heap_impl -u newlib_include_syscalls_impl -u newlib_include_pthread_impl -Wl,--undefined=uxTopUsedPriority -L /home/javivek/espaia2/esp-idf/components/esp_rom/esp32/ld -T esp32.rom.ld -T esp32.rom.libgcc.ld -T esp32.rom.syscalls.ld -T esp32.rom.newlib-data.ld -Wl,--gc-sections /home/javivek/espaia2/esp-idf/components/xtensa/esp32/libhal.a -L /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-idf/esp32 -T esp32_out.ld -u app_main -L /home/javivek/espaia2/esp-idf/components/esp32/ld -T esp32.extram.bss.ld -L /home/javivek/espaia2/esp-va-sdk/examples/amazon_aia/build/esp-idf/esp32/ld -T esp32.project.ld -T esp32.peripherals.ld -u call_user_start_cpu0 -u ld_include_panic_highint_hdl -mfix-esp32-psram-cache-issue -mfix-esp32-psram-cache-strategy=memw -u esp_app_desc -u vfs_include_syscalls_impl -L /home/javivek/espaia2/esp-idf/components/esp_wifi/lib_esp32 -lgcov -lc -lm -lgcc -u pthread_include_pthread_impl -u pthread_include_pthread_cond_impl -u pthread_include_pthread_local_storage_impl && :\n/home/javivek/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/esp_wifi/libesp_wifi.a(wifi_init.c.obj): in function set_xpd_sar'; esp-idf/common/libcommon.a(app_wifi.c.obj):/home/javivek/espaia2/esp-va-sdk/examples/common/app_wifi.c:185: first defined here\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\nninja failed with exit code 1", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Mar 25, 2021", "body": "I think this is because of a recent commit in esp-idf. Could you comment the  API in esp-va-sdk/examples/common/app_wifi.c:184 and try again?", "type": "commented", "related_issue": null}, {"user_name": "jassi00713", "datetime": "Mar 25, 2021", "body": "Thanks a lot. It worked.If i could get one more help. Firmware flashes fine. I have generated the new ESP Alexa Credentials.\nmfg_config.csv file i have received in the mail has sme extra feilds as compared to the one already present in SDK.mfg_config.csv in SDK :-\nkey,type,encoding,value\ndevice,namespace,,\nclient_cert,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/device.crt\nclient_key,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/device.key\nserver_cert,file,binary,/path/to/esp-alexa/examples/amazon_aia/main/certs/server.crtmfg_config.csv Received in mail:-\nkey,type,encoding,value\ndevice,namespace,,\ndevice_id,file,binary,/path/to/device.info\naccount_id,file,binary,/path/to/account.info\nmqtt_host,file,binary,/path/to/endpoint.info\nclient_cert,file,binary,/path/to/device.cert\nclient_key,file,binary,/path/to/device.key\nserver_cert,file,binary,/path/to/server.certThanks.", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Mar 25, 2021", "body": "All the fields in the downloaded one are required.\nThe one already present in the SDK only has some of those as the SDK provides other options (like menuconfig) to set the other fields.", "type": "commented", "related_issue": null}, {"user_name": "jassi00713", "datetime": "Mar 28, 2021", "body": "cool ", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#147"}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/117", "issue_status": " Open\n", "issue_list": [{"user_name": "deepak141189", "datetime": "Oct 7, 2020", "body": "Connected ESp32 board with Android phone over bluetooth and played audio songs. But voice cracks on speaker.\nWhen asked alexa to play song via internet directly then songs plays ok without any crack in sound, so speaker is ok it seems.issue comes only when song is played over bluetooth.I have external speaker connected at \"Audio OUT\" of ESP32 board. Speaker is powered externally.", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Oct 7, 2020", "body": "Hi Deepak,\nRelease here uses WW engine which runs on ESP32 board. Dedicated DSP is not used.To use dedicated DSP and go for production with lyratd-dspg, please contact Espressif.Thanks.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/115", "issue_status": " Open\n", "issue_list": [{"user_name": "deepak141189", "datetime": "Sep 23, 2020", "body": "I am using ESP32_LyraTD_DSPG_A_V3.1 & B_V1.1 .   Edit (using Window 10 + mingw32)Used these 6 commands to flash it.\n`1. cd esp-va-sdk/examples/amazon_alexaCode start building but in end get these error:esp_avrc_tg_init'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x54): undefined reference to esp_avrc_psth_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x5c): undefined reference to esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o):(.literal.bluetooth_internal_start+0x64): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x141): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x175): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x18f): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x19c): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1aa): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1b7): undefined reference to bluetooth_internal_start':\n(.text.bluetooth_internal_start+0x1c4): undefined reference to esp_avrc_psth_bit_mask_operation' follow\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_tg_set_psth_cmd_filter'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bluetooth_internal.o): In function esp_avrc_tg_set_rn_evt_cap'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.literal.bt_app_av_notify_vol_change+0xc): undefined reference to esp_avrc_ct_send_get_rn_capabilities_cmd'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.text.bt_av_new_track+0x13): undefined reference to esp_avrc_rn_evt_bit_mask_operation'\nC:/msys32/home/bansal1/esp-va-sdk/components/voice_assistant/lib/libalexa.a(bt_app_av.o):(.text.bt_av_play_pos_changed+0xc): undefined reference to bt_app_av_notify_vol_change':\n(.text.bt_app_av_notify_vol_change+0x3e): undefined reference to bt_app_rc_ct_cb':\n(.text.bt_app_rc_ct_cb+0x2a): undefined reference to bt_app_rc_tg_cb':\n(.text.bt_app_rc_tg_cb+0x58): undefined reference to `esp_avrc_tg_send_rn_rsp'\ncollect2.exe: error: ld returned 1 exit status\nmake: *** [/C/msys32/home/bansal1/esp-idf/make/project.mk:461: /home/bansal1/esp-va-sdk/examples/amazon_alexa/build/alexa.elf] Error 1`", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/113", "issue_status": " Open\n", "issue_list": [{"user_name": "Berosco", "datetime": "Sep 19, 2020", "body": "Hello!I'm having a problem using the example of integration with the dialogflow. I followed the steps correctly until the end. I performed the program build and flash, adding the necessary data, but when the program starts and returns the status \"Assistant is ready\", and then the\" REC \"button is pressed the following return is displayed, and after that nothing happens.", "type": "commented", "related_issue": null}, {"user_name": "KalashQK", "datetime": "Mar 18, 2021", "body": "Hello! Please tell me if you have solved the problem? I had exactly the same situation, but with an Google Assistant, and not with a Dialogflow. Thank you in advance!", "type": "commented", "related_issue": null}, {"user_name": "KalashQK", "datetime": "Mar 24, 2021", "body": [], "type": "issue", "related_issue": "#131"}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/107", "issue_status": " Open\n", "issue_list": [{"user_name": "klisevich", "datetime": "Jul 17, 2020", "body": "Hi!\nI evaluated voice assistant with Espressif AWS account and now want it run on my account. I created an AVS product and security profile but couldn't  get my device registered. I called   and set product ID to 'esp_vaq_va' and when I was pushing \"Allow\" button in amazon shopping app the following error floated up:\n\nAfter rolling back the Product ID to \"esp_alexa_open\" the console outputsand error wasn't floating up. What should I do to run with a custom product ID?", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Jul 17, 2020", "body": "Hi  , As mentioned in the Readme(), if you want to use your custom AVS Product ID, you will have to generate the phone apps with your AVS Product.\nHowever you can use your own AWS Account without creating a new AVS Product and continue using the existing phone apps.", "type": "commented", "related_issue": null}, {"user_name": "klisevich", "datetime": "Jul 17, 2020", "body": "Thank you for responding! For me, it is important to work with a custom AVS Product. I'm not an experienced Android developer, so could you kindly provide example files with changes(what and where) that I should make in the Android project.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/106", "issue_status": " Open\n", "issue_list": [{"user_name": "klisevich", "datetime": "Jul 15, 2020", "body": "Hi!\nWe want to use ESP-VA-SDK as a code reference for developing a demo for ESP-Vaquita board appliance. In our project we want to utilize  and  functionality to connect to AWS IoT Core. Are there possible ways for us to substitute ais_mqtt_init() with custom ais_mqtt_over ws_init() function while preserving voice assistant functionality?", "type": "commented", "related_issue": null}, {"user_name": "chiragatal", "datetime": "Jul 15, 2020", "body": "Can you explain your use-case a bit more? Is it something that can't be supported with the existing implementation?", "type": "commented", "related_issue": null}, {"user_name": "klisevich", "datetime": "Jul 15, 2020", "body": "Sure! The point of this demo is to replace MQTT protocol with MQTT over WebSocket protocol. As AIA works over the MQTT protocol, it requires managing security certificates when used over TLS. This can create considerable friction in the provisioning procedure. MQTT can use the WebSocket protocol as a transport and it allows for a significantly simpler authentication procedure. We going to utilize pre-signed URL scheme used by AWS for opening the connection over WebSocket. It would be cool if you add to the SDK an option to work with MQTT over WebSocket protocol. Unfortunately, we couldn't find any information regarding MQTT over TCP support from AWS.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/102", "issue_status": " Open\n", "issue_list": [{"user_name": "tcpipchip", "datetime": "Jul 5, 2020", "body": "Working good!I loved!When will we have news about ESP-VA-SDK ?", "type": "commented", "related_issue": null}, {"user_name": "tcpipchip", "datetime": "Jul 6, 2020", "body": "Blog using your ESP-VA-SDK on LYRAT", "type": "commented", "related_issue": null}, {"user_name": "kroggen", "datetime": "Jul 7, 2020", "body": "*assistant", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Jul 15, 2020", "body": "Hi \nThat's great work. If you'd like to try any further, esp-va-sdk also offers solution for inbuilt Google voice assistant. Look forward to hear back from you. :)", "type": "commented", "related_issue": null}, {"user_name": "tcpipchip", "datetime": "Jul 15, 2020", "body": "Hi how are you ?But what i did different ?What means the INBUILT ?Do you mean ALEXA ?Thank you!!!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/93", "issue_status": " Open\n", "issue_list": [{"user_name": "skylli", "datetime": "May 9, 2020", "body": "I'm building a robot which can do someting with voice and answer some question like Alexa.\nI know i can build a alexa custom skill to do that, but i have to start with 'Alexa ask robot to do somthing ' which experience is very poor.\nCan Google Dialogflow and Alexa work at the same time? If it can, how i can make it?", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "May 11, 2020", "body": "Hi \nUnfortunately it's not possible to have multiple built-in voice assistants as of yet.\nBesides from the usecase perspective, how would you identify which query to be sent to the Alexa and which one to be sent to the DialogFlow?\nIt would either require multiple WW support (e.g 'OK google' and 'Alexa') and some way for DSPs to notify the host of which WW has been detected. Or it needs some sort of query/speech processing locally. Both of which would require significant time and effort. :)", "type": "commented", "related_issue": null}, {"user_name": "skylli", "datetime": "May 15, 2020", "body": "\nThanks for your answer.\nSo the multiple WW  is wake Aws-Alexa with 'Alexa', and wake DialogFlow with 'Ok Google' or 'Hey, rootbot'. How can we  to achieve that.\nCan you provide an example of the implementatio.\nI have connect Espressif, and they have send me an sdk of gitlab, but it oes not help.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "May 15, 2020", "body": "Hi \nLike I said, it requires significant time and efforts. Multiple WW support (in fact we only have  WW support right now), and support for multiple voice assistant clouds is yet to be implemented, which is quite some work.", "type": "commented", "related_issue": null}, {"user_name": "skylli", "datetime": "May 16, 2020", "body": "Are there any plans to achieve this function？", "type": "commented", "related_issue": null}, {"user_name": "skylli", "datetime": "May 16, 2020", "body": "\nI just read Google Assistant Doc, and i find we can \nDoes our esp-va-sdk can do that ?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/83", "issue_status": " Open\n", "issue_list": [{"user_name": "lxdemon", "datetime": "Jan 20, 2020", "body": "你好，我想通过Google Voice Assistant来控制开发板的硬件，我看了Google Assistant SDK的Device Actions的指南，在Actions Console中添加了一个Traits，但是sdk这边不知道怎么做，能指导一下吗？另外也想问一下Alexa如果也想实现相同的功能的话该怎么做，大致的流程是怎样的，感谢！", "type": "commented", "related_issue": null}, {"user_name": "skylli", "datetime": "Jun 8, 2020", "body": "我也希望 sdk 能开发出 gva 的 build in 命令，这样我们就可以定制.\n目前我是通过 Google Dialogflow 实现，可是没有智能对话啊。", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/78", "issue_status": " Open\n", "issue_list": [{"user_name": "DuHeLong", "datetime": "Dec 31, 2019", "body": "Hello,\nI'm using the latest SDK. During the test, I found that during the wake-up and dialogue of ALEXA, only the right microphone works, and the left microphone does not affect the function even if I short-circuit it. Why is this? Isn't it a dual microphone?\n        I remember using the early SDK, shorting any microphone, ALEXA can work;", "type": "commented", "related_issue": null}, {"user_name": "DuHeLong", "datetime": "Dec 31, 2019", "body": "I tested it directly on the development board (esp32_lyrat).", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Dec 31, 2019", "body": "For upload path, as per specification, data is down-channeled and down sampled to 16K sampling rate.\nHence, another channel simply is irrelevant and data for that is ignored.", "type": "commented", "related_issue": null}, {"user_name": "DuHeLong", "datetime": "Jan 6, 2020", "body": "According to the specifications provided by your company, the function is a dual microphone function, and I remember that when testing in February last year, shielding any one microphone can work, can you tell us why? Did we test wrong before?Refer to the following specifications：\nKey Supported Features:\n•\tVarious mainstream, both lossy and lossless, compressed audio formats, including M4A, AAC, FLAC, OGG, OPUS, MP3, etc.\n•\tOne-key configuration and wake-up from the standby mode.\n•\tSoftAP and Station mode.\n•\tVarious wireless protocols：Wi-Fi 802.11b/g/n, Classic BT and Bluetooth LE.\n•\tA series of audio inputs, including Wi-Fi, BT-audio, DLNA, Line-in, etc.\n•\tBluetooth LE network configuration, and smart network configuration with apps, such as WeChat.\n•\tTwo microphones for the development of near-field and far-field voice recognition applications.\n•\tPeripherals for differentiated demands.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1908", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 10, 2020", "body": "To reproduce:It shouldn't match at all, but it is also broken when it matches. The problematic code is here: I don't understand what  is supposed to be about, and it actually breaks things. I think I was trying to be clever, but the cleverness was never used.Maybe the entire logic needs to be removed.", "type": "commented", "related_issue": null}, {"user_name": "Simpcyclassy", "datetime": "Aug 11, 2020", "body": "I was about to raise an issue on the page name routine being broken as well as I just explained to you and saw this. Looks like it's a related issue affecting ", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/PCCoE-Hacktoberfest-21/FRIDAY/issues/4", "issue_status": " Open\n", "issue_list": [{"user_name": "rohitjoshi6", "datetime": "Oct 1, 2021", "body": "User: What's 5+5 ?\nFRIDAY (Voice-assistant) : It's 10", "type": "commented", "related_issue": null}, {"user_name": "SaiNarra8", "datetime": "Oct 2, 2021", "body": "Hi can I work on this?", "type": "commented", "related_issue": null}, {"user_name": "Abhishek-Rath", "datetime": "Oct 2, 2021", "body": "Can I work on this ?", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 2, 2021", "body": "Hey  I have already assigned you with another issue.\nWill it be fine if i assign this issue to  ?", "type": "commented", "related_issue": null}, {"user_name": "harshadbhere", "datetime": "Oct 2, 2021", "body": " can I work on this?", "type": "commented", "related_issue": null}, {"user_name": "SaiNarra8", "datetime": "Oct 2, 2021", "body": "Sure sure", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 3, 2021", "body": "Alright  I am assigning this issue to you\nGo ahead", "type": "commented", "related_issue": null}, {"user_name": "ChetansMittal", "datetime": "Oct 16, 2021", "body": "Hey I want to work on it", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 1, 2021", "body": [], "type": "issue", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 3, 2021", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1907", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 10, 2020", "body": "Frequently (usually?) the browser won't identify any preferred/default voice, and so we'll have to pick one. Right now we pick the alphabetically first voice.We should choose a voice more intelligently. For instance my alphabetically first one is Agnes, which is not a good voice. Samantha however is a good voice.To see the available voices:My \"default\" is Alex, which I've never chosen. My system default is Samantha. Note that Samantha has the string \"premium\" in the .There may be heuristics on Windows and Linux, but we'll have to look more closely to determine them.Note that internally we should use null as the preference, and then determine the voice dynamically – and if the user sets an explicit preference we respect that.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/PCCoE-Hacktoberfest-21/FRIDAY/issues/3", "issue_status": " Open\n", "issue_list": [{"user_name": "rohitjoshi6", "datetime": "Oct 1, 2021", "body": "If the command is \"Where is Italy?\" then the Voice Assistant must open Google maps and display Italy", "type": "commented", "related_issue": null}, {"user_name": "SaiNarra8", "datetime": "Oct 2, 2021", "body": "Hey I would like to work on this issue!", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 2, 2021", "body": "Hello  . I am assigning this to you", "type": "commented", "related_issue": null}, {"user_name": "yashdabhade1", "datetime": "Oct 23, 2021", "body": "Hello can you please assign this to me?", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 23, 2021", "body": "Go ahead ", "type": "commented", "related_issue": null}, {"user_name": "ChetansMittal", "datetime": "Oct 26, 2021", "body": "IS the work going on, I want to work on these issue.", "type": "commented", "related_issue": null}, {"user_name": "yashdabhade1", "datetime": "Oct 26, 2021", "body": "I am pushing my commit by the EOD", "type": "commented", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 1, 2021", "body": [], "type": "issue", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 2, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 5, 2021", "body": [], "type": "issue", "related_issue": "#31"}, {"user_name": "rohitjoshi6", "datetime": "Oct 23, 2021", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "rohitjoshi6", "datetime": "Oct 23, 2021", "body": [], "type": "issue", "related_issue": "#58"}, {"user_name": "rohitjoshi6", "datetime": "Oct 23, 2021", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1913", "issue_status": " Open\n", "issue_list": [{"user_name": "julienreszka", "datetime": "Sep 16, 2020", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "Extarys", "datetime": "Sep 22, 2020", "body": "Yeah I discovered this like 5 minutes ago on the . Damn.I'll start donating to Mozilla. Can't believe I never donated.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Jan 30, 2021", "body": "I was not aware until I began seeking information in the absence of release notes for : does not exist.Re:  for a different (but related) extension, I assume that the release note for   will be similar to the release note for  :Please: where, exactly? (undated) is quite vague:– but there's no explanation for the decommissioning. I can guess a reason, but I'd prefer the explanation to come from Mozilla.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Feb 5, 2021", "body": "Cross reference:", "type": "commented", "related_issue": null}, {"user_name": "andrenatal", "datetime": "Feb 9, 2021", "body": "Hi . I'm a Mozilla employee and have created Voice Fill, Firefox Voice, our speech-proxy server, and numerous other voice projects here and was the one responsible for decommissioning the addons.The reason is simple and transparent: we are decommissioning the addons purely because the use of Google's STT backend won't scale for us, and we never managed to train production ready models through projects like Deep Speech and Common Voice with the quality required for VF and FxVoice.", "type": "commented", "related_issue": null}, {"user_name": "Extarys", "datetime": "Feb 9, 2021", "body": " I really appreciate you take the time to provide this explanation and it's totally understandable.", "type": "commented", "related_issue": null}, {"user_name": "grahamperrin", "datetime": "Feb 22, 2021", "body": "Likewise,  much appreciated.(After shifting the discussion to Discourse, I found some key posts on the subject; linked/quoted there. It's most useful to have your comment here as well.)", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/fairseq/issues/4045", "issue_status": " Open\n", "issue_list": [{"user_name": "patrickvonplaten", "datetime": "Nov 30, 2021", "body": "Hey Quintong, , ,Thanks a lot for open-sourcing the model weights of your recent paper !\nI can run the models well and the output seems coherent. E.g., on a LibriSpeech speech input with the transcription :The following models decode this input audio to the following phoneme sequence:Note that the models logits does not predict a word splitting character (the ) since it's also not in the dictionary. Now if I use the  phonemizer with the  library as follows:I get the output:  which is more or less the same as the prediction of the model - see:=> So I'm assuming that the phonemizer command:is correct here. However I couldn't find any file that confirms this. Could you take a look to see whether the phonemizer command is correct? This would make training such a model possible :-) Also if I now want to decode a French input sample, I would simple replace  with  no (I've tried it and it also gave me very good results)?Also I had to more questions:Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "xuqiantong", "datetime": "Nov 30, 2021", "body": "Hi Patrick. Yes, your command is correct, and changing  option is enough to phonemize different languages. We usually use  to separate phonemes, in order not to do any manual parsing afterwards.", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Dec 8, 2021", "body": "Thanks a mille  - that's super useful! One last question regarding what was said above:Why was the \"|\" not added as a token to the dictionary and the token embeddings? Wouldn't it make sense to train the model with \"|\" so that it's easier to later map a phoneme sequence to a word sequence (since \"|\" would allow one to know what phonemes belong to the same word).Do you think leaving out \"|\" in the dictionary improves phoneme error rate?Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "xuqiantong", "datetime": "Dec 10, 2021", "body": "Hi , this is a good question, inserting \"|\" between words will definitely help in decoding to words. We didn't do this in our work simply because we only focused on phonome recognition.Feel free to try all the other possibilities :)", "type": "commented", "related_issue": null}, {"user_name": "patrickvonplaten", "datetime": "Nov 30, 2021", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/fairseq/issues/3275", "issue_status": " Open\n", "issue_list": [{"user_name": "PhilipMay", "datetime": "Feb 24, 2021", "body": "Hi,\nI want to translate german voice to text and use the XLSR-53 model.\nThat model is mentioned here: But the usage example from here:\n\nIs not realy helpful.Can you please help me how to lead and use the XLSR-53 model to convert german .wav file to text?Thanks\nPhilip", "type": "commented", "related_issue": null}, {"user_name": "olafthiele", "datetime": "Feb 24, 2021", "body": "Hi Philip,\nnice to see you here, you will need to finetune the model with good German material before you can use it for sth useful.  has a good overview and scripts how to do that.", "type": "commented", "related_issue": null}, {"user_name": "guillefix", "datetime": "Feb 26, 2021", "body": "I imagine the recommended amounts of data mentioned in that repo won't appy for XLSR-53, as they used wav2vec_small? With XLS3-53, we could bypass all the language-specific pretraining no?\nAlso not sure why they use a language model for. I think in the ASR implementations I've seen using wav2vec, they just use a simple linear classifier on top of the wav2vec represenation, trained on the small supervised dataset. I could be wrong, but it seems like it should be much simpler than what that repo does, given XLSR-53", "type": "commented", "related_issue": null}, {"user_name": "PhilipMay", "datetime": "Mar 5, 2021", "body": "Hi  - nice to meet you here. :-)Do you have any experience with these models for german language?", "type": "commented", "related_issue": null}, {"user_name": "olafthiele", "datetime": "Mar 5, 2021", "body": "Yep, just trained a couple of models with different params. Have written you an email. For all the others and reference, use the repo mentioned above (thanks ) to begin with and adapt params to your needs. And as always: gold in, gold out or s...t in, s...t out :-)", "type": "commented", "related_issue": null}, {"user_name": "Gorodecki", "datetime": "Mar 10, 2021", "body": ", please, share which model was used as a base. What parameters were set for fine-tuning? How many hours of audio? Whether augmentation was used?\nI want to train a model for the Russian language. I have 50 hrs labeled noise audio.", "type": "commented", "related_issue": null}, {"user_name": "olafthiele", "datetime": "Mar 10, 2021", "body": ", we used the published XSLR-53 model together with the 100h base config values for first tests with fewer steps. But that depends on your data. We are testing anything between 10 mins and 500 hours with different configs. No augmentation. Our data is not very noisy, maybe clean it in advance? And if you do Russian, have you tried  ?", "type": "commented", "related_issue": null}, {"user_name": "Gorodecki", "datetime": "Mar 10, 2021", "body": " thank you!  This  it is dataset, but not models.", "type": "commented", "related_issue": null}, {"user_name": "olafthiele", "datetime": "Mar 10, 2021", "body": "Sorry, not writing a book here :-) Check him, there are models somewhere. Not wav2vec, but Russian and easy to start with last time I checked. And maybe the data helps you too.", "type": "commented", "related_issue": null}, {"user_name": "PhilipMay", "datetime": "Mar 19, 2021", "body": "Hey \nare you aware of this: They claim to have a WER (word error rate) of 18.5 %\non the german commen voice corpus (test set).But I do not know how they trained it...", "type": "commented", "related_issue": null}, {"user_name": "olafthiele", "datetime": "Mar 19, 2021", "body": "Common Voice is a good start, but if possible, find better material for finetuning or maybe use a a more suitable language model.", "type": "commented", "related_issue": null}, {"user_name": "alexeib", "datetime": "Mar 29, 2021", "body": "trying out the hugging face implementation would be a great start. their demo is super nice. in terms of \"how they trained it\" - they actually just took our published model.one downside is i dont think they use a language model for decoding but that can be added on top after you get the argmax decoding working", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 28, 2021", "body": "This issue has been automatically marked as stale.  (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!", "type": "commented", "related_issue": null}, {"user_name": "PhilipMay", "datetime": "Feb 24, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lematt1991", "datetime": "Mar 1, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "lematt1991", "datetime": "Mar 1, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 28, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/espressif/esp-va-sdk/issues/68", "issue_status": " Open\n", "issue_list": [{"user_name": "jhpark555", "datetime": "Oct 29, 2019", "body": "Can we use bluetooth a2dp and alexa/google service at the same time?\nI tried to port a2dp sink demo code on alexa example, but there was an overflow error.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Oct 31, 2019", "body": "Hi \nPlease wait for a few days, we are working on integrating A2DP sink with Alexa.\nNext release would have A2DP sink support.\nThanks", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 2, 2019", "body": "Is A2DP with Alexa still ongoing?\nThanks.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 9, 2019", "body": "Hi \nAlexa with A2DP sink support has been pushed in the latest release. You can try that out. May I know which evaluation board are you using? Since it would require the flash of size > 4MB.Thanks", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 9, 2019", "body": "Hello.  I'm using ESP32_LyraT board.  By the way I'm having a reboot problem.\nI enabled BT classic and a2dp sink on the menuconfig and make ALEXA_BT=1, the\nbelow reboot is happening. What am I missing?I (62) boot: Chip Revision: 1\nI (65) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 14:38:57\nI (43) boot: Enabling RNG early entropy source...\nI (45) qio_mode: Enabling QIO for flash chip GD\nI (50) boot: SPI Speed      : 80MHz\nI (54) boot: SPI Mode       : QIO\nI (58) boot: SPI Flash Size : 4MB\nI (62) boot: Partition Table:\nI (66) boot: ## Label            Usage          Type ST Offset   Length\nI (73) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (81) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (88) boot:  2 factory          factory app      00 00 00010000 00390000\nI (96) boot: End of partition table\nI (100) boot_comm: mismatch chip revision, expect 1, found 0\nI (106) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (684) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (691) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (692) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (703) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1082) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1116) esp_image: Image length 3887296 doesn't fit in partition length 3735552\nE (1116) boot: Factory app partition is not bootable\nE (1119) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 10, 2019", "body": "Hi \nAs mentioned in the changelog , A2DP sink requires flash size > 4MB. Since size of the Alexa binary increases substantially.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 10, 2019", "body": "Thanks. I understood.   Then, I need to buy esp32-lyratd-syna with 8MB flash board.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 16, 2019", "body": "Even if I replaced the flash to 8mb. Still it is crashing.\nPlease let me know how I can fix.I (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 14:29:37\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (57) boot: SPI Mode       : QIO\nI (61) boot: SPI Flash Size : 8MB\nI (65) boot: Partition Table:\nI (69) boot: ## Label            Usage          Type ST Offset   Length\nI (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (91) boot:  2 factory          factory app      00 00 00010000 007d0000\nI (98) boot: End of partition table\nI (102) boot_comm: mismatch chip revision, expect 1, found 0\nI (109) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (687) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (693) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (694) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (706) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1085) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 17, 2019", "body": "does the device crash with default partitions.csv as well?", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 17, 2019", "body": "Yes, I think so.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 17, 2019", "body": "can you post a complete failure log with that? Since from the above logs, I am not really able to identify the actual error. Booting up seems to be happening normally.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 17, 2019", "body": "Okay. I'm at home now. I'll post full log message on tomorrow morning.   Thanks.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 17, 2019", "body": "I changed to use default partition table by blocking partitions_4mb_flash_csv in audio_board.mk.  Below is the full log data.     Thanks.MONITOR\n--- idf_monitor on /dev/ttyUSB0 115200 ---\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nets Jun  8 2016 00:22:57rst:0x1 (POWERON_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 10:18:03\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (56) boot: SPI Mode       : QIO\nI (60) boot: SPI Flash Size : 8MB\nI (65) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (83) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (90) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (98) boot:  3 factory          factory app      00 00 00010000 00100000\nI (105) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (113) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (120) boot: End of partition table\nI (125) boot: Defaulting to factory image\nI (129) boot_comm: mismatch chip revision, expect 1, found 0\nI (136) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (714) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (720) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (721) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (733) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1112) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1146) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1146) boot: Factory app partition is not bootable\nE (1148) esp_image: image at 0x110000 has invalid magic byte\nE (1155) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1161) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1168) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1175) boot: OTA app partition slot 0 is not bootable\nE (1180) esp_image: image at 0x210000 has invalid magic byte\nE (1187) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1193) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1200) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1207) esp_image: image at 0x210000 has invalid SPI size 11\nE (1213) boot: OTA app partition slot 1 is not bootable\nE (1219) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (91) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1151) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1185) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1185) boot: Factory app partition is not bootable\nE (1187) esp_image: image at 0x110000 has invalid magic byte\nE (1194) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1200) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1207) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1214) boot: OTA app partition slot 0 is not bootable\nE (1220) esp_image: image at 0x210000 has invalid magic byte\nE (1226) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1232) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1240) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1246) esp_image: image at 0x210000 has invalid SPI size 11\nE (1253) boot: OTA app partition slot 1 is not bootable\nE (1258) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (90) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?I (1151) esp_image: segment 5: paddr=0x003aa030 vaddr=0x40082da0 size=0x1b068 (110696) load\n0x40082da0: psram_cache_init at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:850\n(inlined by) psram_enable at /home/ppark/esp/esp-idf/components/esp32/spiram_psram.c:765E (1185) esp_image: Image length 3887296 doesn't fit in partition length 1048576\nE (1185) boot: Factory app partition is not bootable\nE (1187) esp_image: image at 0x110000 has invalid magic byte\nE (1194) boot_comm: mismatch chip ID, expect 0, found 65297\nE (1200) boot_comm: can't run on lower chip revision, expect 1, found 46\nW (1207) esp_image: image at 0x110000 has invalid SPI mode 47\nE (1214) boot: OTA app partition slot 0 is not bootable\nE (1220) esp_image: image at 0x210000 has invalid magic byte\nE (1226) boot_comm: mismatch chip ID, expect 0, found 22049\nE (1232) boot_comm: can't run on lower chip revision, expect 1, found 47\nW (1240) esp_image: image at 0x210000 has invalid SPI mode 75\nW (1246) esp_image: image at 0x210000 has invalid SPI size 11\nE (1253) boot: OTA app partition slot 1 is not bootable\nE (1258) boot: No bootable app partitions in the partition table\nets Jun  8 2016 00:22:57rst:0x3 (SW_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (105) boot: Chip Revision: 1\nI (127) boot_comm: mismatch chip revision, expect 1, found 0\nI (76) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (77) boot: compile time 10:18:03\nI (90) boot: Enabling RNG early entropy source...\nI (90) qio_mode: Enabling default flash chip QIO\nI (90) boot: SPI Speed      : 80MHz\nI (95) boot: SPI Mode       : QIO\nI (99) boot: SPI Flash Size : 8MB\nI (103) boot: Partition Table:\nI (107) boot: ## Label            Usage          Type ST Offset   Length\nI (114) boot:  0 nvs              WiFi data        01 02 00009000 00004000\nI (122) boot:  1 otadata          OTA data         01 00 0000d000 00002000\nI (129) boot:  2 phy_init         RF data          01 01 0000f000 00001000\nI (137) boot:  3 factory          factory app      00 00 00010000 00100000\nI (145) boot:  4 ota_0            OTA app          00 10 00110000 00100000\nI (152) boot:  5 ota_1            OTA app          00 11 00210000 00100000\nI (160) boot: End of partition table\nI (164) boot: Defaulting to factory image\nI (169) boot_comm: mismatch chip revision, expect 1, found 0\nI (175) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x22804c (2261068) map\nI (753) esp_image: segment 1: paddr=0x00238074 vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (759) esp_image: segment 2: paddr=0x0023d268 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (760) esp_image: segment 3: paddr=0x0023d670 vaddr=0x40080400 size=0x029a0 ( 10656) load\nI (772) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x16a010 (1482768) map\n0x400d0018: _stext at ??:?", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 19, 2019", "body": "Hi \nPartition size of ota0 and ota1 that you've set (0x100000) is insufficient for the image (as indicated by the error message \"E (1146) esp_image: Image length 3887296 doesn't fit in partition length 1048576\"). The default partition table, , sets this to 0x420000, since binary size is much larger.If it isn't fitting in 8MB flash, you may remove ota_1 partition for now (this would only affect OTA functionality).", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Dec 20, 2019", "body": "Hello,\nWhen I used defalut partitions.csv and removed ota_1, the alexa worked fine but the bluetooth didn't work.     Can you tell me which board you used ? I need to prepare for demo for our customer in a month.   Thanks.", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Dec 23, 2019", "body": "It shouldn't be board specific.\nDid you compile with ALEXA_BT=1 ? It is mentioned ", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 7, 2020", "body": "Even if I enabled ALEXA_BT=1 while compiling, I can't find bluetooth supported device from the PC or phone.  Only Amazon can works.rst:0x1 (POWERON_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6740\nload:0x40078000,len:10016\nload:0x40080400,len:7072\nentry 0x40080750\nI (61) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (38) boot: compile time 10:46:40\nI (52) boot: Enabling RNG early entropy source...\nI (52) qio_mode: Enabling default flash chip QIO\nI (52) boot: SPI Speed      : 80MHz\nI (56) boot: SPI Mode       : QIO\nI (60) boot: SPI Flash Size : 8MB\nI (64) boot: Partition Table:\nI (68) boot: ## Label            Usage          Type ST Offset   Length\nI (75) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (90) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (98) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (105) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (113) boot: End of partition table\nI (117) boot: No factory image, trying OTA 0\nI (122) boot_comm: mismatch chip revision, expect 1, found 0\nI (128) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x1fc714 (2082580) map\nI (662) esp_image: segment 1: paddr=0x0021c73c vaddr=0x3ffbdb60 size=0x038d4 ( 14548) load\nI (666) esp_image: segment 2: paddr=0x00220018 vaddr=0x400d0018 size=0x166bb0 (1469360) map\n0x400d0018: _stext at ??:?I (1038) esp_image: segment 3: paddr=0x00386bd0 vaddr=0x3ffc1434 size=0x017c8 (  6088) load\nI (1040) esp_image: segment 4: paddr=0x003883a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (1046) esp_image: segment 5: paddr=0x003887a8 vaddr=0x40080400 size=0x1da08 (121352) load\nI (1110) boot: Loaded app from partition at offset 0x20000\nI (1110) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1578420829\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: AVS endpoint: \nW (1994) I2S: I2S driver already installed\nW (2014) I2S: I2S driver already installed\n[http_stream]: [stream_new]: Internal: 54100, External: 1278256\n[auth-delegate]: Token will be refreshed after 3000 seconds.\n[http_stream]: [stream_get: 1]: /v20160207/directives\n[http_stream]: [sid: 1] Response code: 200\n[http_transport]: AVS level connction has now been established: \n[http_stream]: [stream_new]: Internal: 53052, External: 1270920\n[http_transport]: New stream event: {\"context\":[{\"header\":{\"namespace\":\"Speaker\",\"name\":\"VolumeState\"},\"payload\":{\"volume\":40,\"muted\":false}},{\"header\":{\"namespace\":\"SpeechRecognizer\",\"name\":\"RecognizerState\"},\"payload\":{\"wakeword\":\"ALEXA\"}},{\"header\":{\"namespace\":\"SpeechSynthesizer\",\"name\":\"SpeechState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"PLAYING\"}},{\"header\":{\"namespace\":\"AudioPlayer\",\"name\":\"PlaybackState\"},\"payload\":{\"token\":\"\",\"offsetInMilliseconds\":0,\"playerActivity\":\"IDLE\"}},{\"header\":{\"namespace\":\"AudioActivityTracker\",\"name\":\"ActivityState\"},\"payload\":{}},{\"header\":{\"namespace\":\"Alerts\",\"name\":\"AlertsState\"},\"payload\":{\"allAlerts\":[],\"activeAlerts\":[]}},{\"header\":{\"namespace\":\"Notifications\",\"name\":\"IndicatorState\"},\"payload\":{\"isEnabled\":false,\"isVisualIndicatorPersisted\":false}}],\"event\":{\"header\":{\"namespace\":\"System\",\"name\":\"SynchronizeState\",\"messageId\":\"9bc366a8-091f-4ff9-8d36-60553287cb1d\"},\"payload\":{}}}\n[http_stream]: [stream_post: 3]: /v20160207/events\n[http_stream]: [sid: 3] Response code: 204\n############## Alexa is ready ##############\n[3 seconds]: [http_stream]: [stream_delete: 3] Internal: 52564, External: 1278240, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54480, External: 1278660\n[http_stream]: [stream_get: 5]: /ping\n[http_stream]: [sid: 5] Response code: 204\n[184 seconds]: [http_stream]: [stream_delete: 5] Internal: 52444, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 53220, External: 1278660\n[http_stream]: [stream_get: 7]: /ping\n[http_stream]: [sid: 7] Response code: 204\n[364 seconds]: [http_stream]: [stream_delete: 7] Internal: 54076, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54244, External: 1278660\n[http_stream]: [stream_get: 9]: /ping\n[http_stream]: [sid: 9] Response code: 204\n[544 seconds]: [http_stream]: [stream_delete: 9] Internal: 53960, External: 1278448, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 52372, External: 1278496\n[http_stream]: [stream_get: 11]: /ping\n[http_stream]: [sid: 11] Response code: 204\n[725 seconds]: [http_stream]: [stream_delete: 11] Internal: 53844, External: 1278284, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_new]: Internal: 54008, External: 1278496\n[http_stream]: [stream_get: 13]: /ping\n[http_stream]: [sid: 13] Response code: 204\n[905 seconds]: [http_stream]: [stream_delete: 13] Internal: 53728, External: 1278284, min ever internal: 45080, largest free block: 25012\n[http_stream]: [stream_get: 15]: /ping: 52140, External: 1278496\n[http_stream]: [sid: 15] Response code: 204\n[1085 seconds]: [http_stream]: [stream_delete: 15] Internal: 53612, External: 1278284, min ever internal: 45080, largest free block: 25012", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 7, 2020", "body": "HelloNew issues happens to me. I erased flash in order to write again. At this time, I have a credential and token issues.  Please look at below log data.   Constantly repetitive message are appearing.[conn_mgr_prov]: Scan results:\nS.N. SSID                             RSSI\n[ 0] ESS-Sound-2.4G                    -34\n[ 1] ChromecastAudio7999.a             -47\n[ 2] ESSNETGR88                        -64\n[ 3] SOUND ROOM                        -71\n[ 4] EERO                              -83\n[ 5] ESSNETGR87                        -86\n[ 6] USR5461                           -87\n[ 7] AP5                               -92\n[avs_config]: APP Got: ANRjiVJkZIhlzUMekyvx, amzn1.application-oa2-client.e8c06545d9764a05b8b6d91eecfeb23a, amzn-com.espressif.provbleavs://?methodName=signin, abcd1234\n[auth-delegate]: Auth delegate with comp app: ANRjiVJkZIhlzUMekyvx, amzn1.application-oa2-client.e8c06545d9764a05b8b6d91eecfeb23a, amzn-com.espressif.provbleavs://?methodName=signin, abcd1234E (38681) esp-tls: couldn't get hostname for :api.amazon.com:\nE (38691) esp-tls: Failed to open new connection\nE (38701) httpc: Failed to create a new TLS connection\nE (38701) [auth-delegate]: Failed to connect to Auth URL \"\"\nE (38711) [auth-delegate]: Authentication attempt failed. Retrying. Please check connectivity and/or credentials. In case of authentication failure try resetting the device to factory mode and restart provisioning\n[conn_mgr_prov_handler]: WiFi Credentials Received:\nssid: ESS-Sound-2.4G\npassword: phoenixplus\nW (38921) wifi: alloc eb len=76 type=2 fail, heap:4143112W (38921) wifi: m f probe req l=0W (39041) wifi: alloc eb len=76 type=2 fail, heap:4143968W (39041) wifi: m f probe req l=0W (39161) wifi: alloc eb len=76 type=2 fail, heap:4143968W (39161) wifi: m f probe req l=0E (39231) esp-tls: couldn't get hostname for :api.amazon.com:\nE (39231) esp-tls: Failed to open new connection\nE (39231) httpc: Failed to create a new TLS connection\nE (39231) [auth-delegate]: Failed to connect to Auth URL \"\"\nE (39241) [auth-delegate]: Authentication attempt failed. Retrying. Please check connectivity and/or credentials. In case of authentication failure try resetting the device to factory mode and restart provisioning\nW (39281) wifi: alloc eb len=76 type=2 fail, heap:4143968", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Jan 7, 2020", "body": "For BT not working issue...If you had previously build without ALEXA_BT=1, you will need a clean build. i.e., you need to remove build/ sdkconfig and sdkconfig.old.\nAnd then do a build with ALEXA_BT=1", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 7, 2020", "body": "I followed your instruction but I have reboot error like below. And continuously it reboots.Rebooting...\nets Jun  8 2016 00:22:57rst:0xc (SW_CPU_RESET),boot:0x3f (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 15:24:06\nI (52) boot: Enabling RNG early entropy source...\nI (52) boot: SPI Speed      : 80MHz\nI (52) boot: SPI Mode       : DIO\nI (56) boot: SPI Flash Size : 8MB\nI (60) boot: Partition Table:\nI (63) boot: ## Label            Usage          Type ST Offset   Length\nI (70) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (78) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (85) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (93) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (100) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (108) boot: End of partition table\nI (112) boot: No factory image, trying OTA 0\nI (117) boot_comm: mismatch chip revision, expect 1, found 0\nI (123) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (739) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (746) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (747) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (763) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1165) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1216) boot: Loaded app from partition at offset 0x20000\nI (1216) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1578440038\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (1952) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (2832) I2S: I2S driver already installed\nW (2862) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Jan 8, 2020", "body": "Can you try pulling latest IDF v3.2 branch?\nJust to confirm, you are trying with the default sdkconfig.bt.defaults right?", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 8, 2020", "body": "I'm using sdkconfig. Below is this  configuration.  In the menuconfig, I selected bluetooth dual mode and enabled classic bluetooth. then I compiled with ALEXA_BT=1 option. Do I have wrong configuration?\nThanks.CONFIG_IDF_FIRMWARE_CHIP_ID=0x0000CONFIG_TOOLPREFIX=\"xtensa-esp32-elf-\"\nCONFIG_PYTHON=\"python\"\nCONFIG_MAKE_WARN_UNDEFINED_VARIABLES=yCONFIG_LOG_BOOTLOADER_LEVEL_NONE=\nCONFIG_LOG_BOOTLOADER_LEVEL_ERROR=\nCONFIG_LOG_BOOTLOADER_LEVEL_WARN=\nCONFIG_LOG_BOOTLOADER_LEVEL_INFO=y\nCONFIG_LOG_BOOTLOADER_LEVEL_DEBUG=\nCONFIG_LOG_BOOTLOADER_LEVEL_VERBOSE=\nCONFIG_LOG_BOOTLOADER_LEVEL=3\nCONFIG_BOOTLOADER_SPI_WP_PIN=7\nCONFIG_BOOTLOADER_VDDSDIO_BOOST_1_9V=y\nCONFIG_BOOTLOADER_FACTORY_RESET=\nCONFIG_BOOTLOADER_APP_TEST=\nCONFIG_BOOTLOADER_WDT_ENABLE=y\nCONFIG_BOOTLOADER_WDT_DISABLE_IN_USER_CODE=\nCONFIG_BOOTLOADER_WDT_TIME_MS=9000CONFIG_SECURE_SIGNED_APPS_NO_SECURE_BOOT=\nCONFIG_SECURE_BOOT_ENABLED=\nCONFIG_FLASH_ENCRYPTION_ENABLED=CONFIG_ESPTOOLPY_PORT=\"/dev/ttyUSB0\"\nCONFIG_ESPTOOLPY_BAUD_115200B=\nCONFIG_ESPTOOLPY_BAUD_230400B=\nCONFIG_ESPTOOLPY_BAUD_921600B=y\nCONFIG_ESPTOOLPY_BAUD_2MB=\nCONFIG_ESPTOOLPY_BAUD_OTHER=\nCONFIG_ESPTOOLPY_BAUD_OTHER_VAL=115200\nCONFIG_ESPTOOLPY_BAUD=921600\nCONFIG_ESPTOOLPY_COMPRESSED=y\nCONFIG_FLASHMODE_QIO=y\nCONFIG_FLASHMODE_QOUT=\nCONFIG_FLASHMODE_DIO=\nCONFIG_FLASHMODE_DOUT=\nCONFIG_ESPTOOLPY_FLASHMODE=\"dio\"\nCONFIG_ESPTOOLPY_FLASHFREQ_80M=y\nCONFIG_ESPTOOLPY_FLASHFREQ_40M=\nCONFIG_ESPTOOLPY_FLASHFREQ_26M=\nCONFIG_ESPTOOLPY_FLASHFREQ_20M=\nCONFIG_ESPTOOLPY_FLASHFREQ=\"80m\"\nCONFIG_ESPTOOLPY_FLASHSIZE_1MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_2MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_4MB=\nCONFIG_ESPTOOLPY_FLASHSIZE_8MB=y\nCONFIG_ESPTOOLPY_FLASHSIZE_16MB=\nCONFIG_ESPTOOLPY_FLASHSIZE=\"8MB\"\nCONFIG_ESPTOOLPY_FLASHSIZE_DETECT=y\nCONFIG_ESPTOOLPY_BEFORE_RESET=y\nCONFIG_ESPTOOLPY_BEFORE_NORESET=\nCONFIG_ESPTOOLPY_BEFORE=\"default_reset\"\nCONFIG_ESPTOOLPY_AFTER_RESET=y\nCONFIG_ESPTOOLPY_AFTER_NORESET=\nCONFIG_ESPTOOLPY_AFTER=\"hard_reset\"\nCONFIG_MONITOR_BAUD_9600B=\nCONFIG_MONITOR_BAUD_57600B=\nCONFIG_MONITOR_BAUD_115200B=y\nCONFIG_MONITOR_BAUD_230400B=\nCONFIG_MONITOR_BAUD_921600B=\nCONFIG_MONITOR_BAUD_2MB=\nCONFIG_MONITOR_BAUD_OTHER=\nCONFIG_MONITOR_BAUD_OTHER_VAL=115200\nCONFIG_MONITOR_BAUD=115200CONFIG_ALEXA_PRODUCT_ID=\"esp_avs_open\"\nCONFIG_ALEXA_REMOVE_SHORT_ALERT_SUPPORT=\nCONFIG_ALEXA_ENABLE_OTA=\nCONFIG_ALEXA_ENABLE_LOCAL_PLAYER=\nCONFIG_ALEXA_ENABLE_EQUALIZER=\nCONFIG_ALEXA_ENABLE_AWS_IOT=CONFIG_PARTITION_TABLE_SINGLE_APP=\nCONFIG_PARTITION_TABLE_TWO_OTA=\nCONFIG_PARTITION_TABLE_CUSTOM=y\nCONFIG_PARTITION_TABLE_CUSTOM_FILENAME=\"partitions.csv\"\nCONFIG_PARTITION_TABLE_FILENAME=\"partitions.csv\"\nCONFIG_PARTITION_TABLE_OFFSET=0x8000\nCONFIG_PARTITION_TABLE_MD5=yCONFIG_OPTIMIZATION_LEVEL_DEBUG=\nCONFIG_OPTIMIZATION_LEVEL_RELEASE=y\nCONFIG_OPTIMIZATION_ASSERTIONS_ENABLED=y\nCONFIG_OPTIMIZATION_ASSERTIONS_SILENT=\nCONFIG_OPTIMIZATION_ASSERTIONS_DISABLED=\nCONFIG_CXX_EXCEPTIONS=\nCONFIG_STACK_CHECK_NONE=y\nCONFIG_STACK_CHECK_NORM=\nCONFIG_STACK_CHECK_STRONG=\nCONFIG_STACK_CHECK_ALL=\nCONFIG_STACK_CHECK=\nCONFIG_WARN_WRITE_STRINGS=\nCONFIG_DISABLE_GCC8_WARNINGS=CONFIG_ESP32_APPTRACE_DEST_TRAX=\nCONFIG_ESP32_APPTRACE_DEST_NONE=y\nCONFIG_ESP32_APPTRACE_ENABLE=\nCONFIG_ESP32_APPTRACE_LOCK_ENABLE=y\nCONFIG_AWS_IOT_SDK=y\nCONFIG_AWS_IOT_MQTT_HOST=\"\"\nCONFIG_AWS_IOT_MQTT_PORT=8883\nCONFIG_AWS_IOT_MQTT_TX_BUF_LEN=512\nCONFIG_AWS_IOT_MQTT_RX_BUF_LEN=512\nCONFIG_AWS_IOT_MQTT_NUM_SUBSCRIBE_HANDLERS=5\nCONFIG_AWS_IOT_MQTT_MIN_RECONNECT_WAIT_INTERVAL=1000\nCONFIG_AWS_IOT_MQTT_MAX_RECONNECT_WAIT_INTERVAL=128000CONFIG_AWS_IOT_OVERRIDE_THING_SHADOW_RX_BUFFER=\nCONFIG_AWS_IOT_SHADOW_MAX_SIZE_OF_UNIQUE_CLIENT_ID_BYTES=80\nCONFIG_AWS_IOT_SHADOW_MAX_SIMULTANEOUS_ACKS=10\nCONFIG_AWS_IOT_SHADOW_MAX_SIMULTANEOUS_THINGNAMES=10\nCONFIG_AWS_IOT_SHADOW_MAX_JSON_TOKEN_EXPECTED=120\nCONFIG_AWS_IOT_SHADOW_MAX_SHADOW_TOPIC_LENGTH_WITHOUT_THINGNAME=60\nCONFIG_AWS_IOT_SHADOW_MAX_SIZE_OF_THING_NAME=40CONFIG_BT_ENABLED=yCONFIG_BTDM_CONTROLLER_MODE_BLE_ONLY=\nCONFIG_BTDM_CONTROLLER_MODE_BR_EDR_ONLY=\nCONFIG_BTDM_CONTROLLER_MODE_BTDM=y\nCONFIG_BTDM_CONTROLLER_BLE_MAX_CONN=3\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_ACL_CONN=2\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_SYNC_CONN=0\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_HCI=\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_PCM=y\nCONFIG_BTDM_CONTROLLER_BR_EDR_SCO_DATA_PATH_EFF=1\nCONFIG_BTDM_CONTROLLER_BLE_MAX_CONN_EFF=3\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_ACL_CONN_EFF=2\nCONFIG_BTDM_CONTROLLER_BR_EDR_MAX_SYNC_CONN_EFF=0\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE_0=y\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE_1=\nCONFIG_BTDM_CONTROLLER_PINNED_TO_CORE=0\nCONFIG_BTDM_CONTROLLER_HCI_MODE_VHCI=y\nCONFIG_BTDM_CONTROLLER_HCI_MODE_UART_H4=CONFIG_BTDM_CONTROLLER_MODEM_SLEEP=y\nCONFIG_BTDM_MODEM_SLEEP_MODE_ORIG=y\nCONFIG_BTDM_MODEM_SLEEP_MODE_EVED=\nCONFIG_BTDM_LPCLK_SEL_MAIN_XTAL=y\nCONFIG_BLE_SCAN_DUPLICATE=y\nCONFIG_SCAN_DUPLICATE_BY_DEVICE_ADDR=y\nCONFIG_SCAN_DUPLICATE_BY_ADV_DATA=\nCONFIG_SCAN_DUPLICATE_BY_ADV_DATA_AND_DEVICE_ADDR=\nCONFIG_SCAN_DUPLICATE_TYPE=0\nCONFIG_DUPLICATE_SCAN_CACHE_SIZE=200\nCONFIG_BLE_MESH_SCAN_DUPLICATE_EN=\nCONFIG_BLE_ADV_REPORT_FLOW_CONTROL_SUPPORTED=y\nCONFIG_BLE_ADV_REPORT_FLOW_CONTROL_NUM=100\nCONFIG_BLE_ADV_REPORT_DISCARD_THRSHOLD=20\nCONFIG_BLUEDROID_ENABLED=y\nCONFIG_BLUEDROID_PINNED_TO_CORE_0=y\nCONFIG_BLUEDROID_PINNED_TO_CORE_1=\nCONFIG_BLUEDROID_PINNED_TO_CORE=0\nCONFIG_BTC_TASK_STACK_SIZE=3072\nCONFIG_BLUEDROID_MEM_DEBUG=\nCONFIG_CLASSIC_BT_ENABLED=y\nCONFIG_A2DP_ENABLE=y\nCONFIG_A2DP_SINK_TASK_STACK_SIZE=2048\nCONFIG_A2DP_SOURCE_TASK_STACK_SIZE=2048\nCONFIG_BT_SPP_ENABLED=\nCONFIG_HFP_ENABLE=\nCONFIG_GATTS_ENABLE=y\nCONFIG_GATTS_SEND_SERVICE_CHANGE_MANUAL=\nCONFIG_GATTS_SEND_SERVICE_CHANGE_AUTO=y\nCONFIG_GATTS_SEND_SERVICE_CHANGE_MODE=0\nCONFIG_GATTC_ENABLE=y\nCONFIG_GATTC_CACHE_NVS_FLASH=\nCONFIG_BLE_SMP_ENABLE=y\nCONFIG_SMP_SLAVE_CON_PARAMS_UPD_ENABLE=\nCONFIG_BT_STACK_NO_LOG=CONFIG_HCI_TRACE_LEVEL_NONE=\nCONFIG_HCI_TRACE_LEVEL_ERROR=\nCONFIG_HCI_TRACE_LEVEL_WARNING=y\nCONFIG_HCI_TRACE_LEVEL_API=\nCONFIG_HCI_TRACE_LEVEL_EVENT=\nCONFIG_HCI_TRACE_LEVEL_DEBUG=\nCONFIG_HCI_TRACE_LEVEL_VERBOSE=\nCONFIG_HCI_INITIAL_TRACE_LEVEL=2\nCONFIG_BTM_TRACE_LEVEL_NONE=\nCONFIG_BTM_TRACE_LEVEL_ERROR=\nCONFIG_BTM_TRACE_LEVEL_WARNING=y\nCONFIG_BTM_TRACE_LEVEL_API=\nCONFIG_BTM_TRACE_LEVEL_EVENT=\nCONFIG_BTM_TRACE_LEVEL_DEBUG=\nCONFIG_BTM_TRACE_LEVEL_VERBOSE=\nCONFIG_BTM_INITIAL_TRACE_LEVEL=2\nCONFIG_L2CAP_TRACE_LEVEL_NONE=\nCONFIG_L2CAP_TRACE_LEVEL_ERROR=\nCONFIG_L2CAP_TRACE_LEVEL_WARNING=y\nCONFIG_L2CAP_TRACE_LEVEL_API=\nCONFIG_L2CAP_TRACE_LEVEL_EVENT=\nCONFIG_L2CAP_TRACE_LEVEL_DEBUG=\nCONFIG_L2CAP_TRACE_LEVEL_VERBOSE=\nCONFIG_L2CAP_INITIAL_TRACE_LEVEL=2\nCONFIG_RFCOMM_TRACE_LEVEL_NONE=\nCONFIG_RFCOMM_TRACE_LEVEL_ERROR=\nCONFIG_RFCOMM_TRACE_LEVEL_WARNING=y\nCONFIG_RFCOMM_TRACE_LEVEL_API=\nCONFIG_RFCOMM_TRACE_LEVEL_EVENT=\nCONFIG_RFCOMM_TRACE_LEVEL_DEBUG=\nCONFIG_RFCOMM_TRACE_LEVEL_VERBOSE=\nCONFIG_RFCOMM_INITIAL_TRACE_LEVEL=2\nCONFIG_SDP_TRACE_LEVEL_NONE=\nCONFIG_SDP_TRACE_LEVEL_ERROR=\nCONFIG_SDP_TRACE_LEVEL_WARNING=y\nCONFIG_SDP_TRACE_LEVEL_API=\nCONFIG_SDP_TRACE_LEVEL_EVENT=\nCONFIG_SDP_TRACE_LEVEL_DEBUG=\nCONFIG_SDP_TRACE_LEVEL_VERBOSE=\nCONFIG_SDP_INITIAL_TRACE_LEVEL=2\nCONFIG_GAP_TRACE_LEVEL_NONE=\nCONFIG_GAP_TRACE_LEVEL_ERROR=\nCONFIG_GAP_TRACE_LEVEL_WARNING=y\nCONFIG_GAP_TRACE_LEVEL_API=\nCONFIG_GAP_TRACE_LEVEL_EVENT=\nCONFIG_GAP_TRACE_LEVEL_DEBUG=\nCONFIG_GAP_TRACE_LEVEL_VERBOSE=\nCONFIG_GAP_INITIAL_TRACE_LEVEL=2\nCONFIG_BNEP_TRACE_LEVEL_NONE=\nCONFIG_BNEP_TRACE_LEVEL_ERROR=\nCONFIG_BNEP_TRACE_LEVEL_WARNING=y\nCONFIG_BNEP_TRACE_LEVEL_API=\nCONFIG_BNEP_TRACE_LEVEL_EVENT=\nCONFIG_BNEP_TRACE_LEVEL_DEBUG=\nCONFIG_BNEP_TRACE_LEVEL_VERBOSE=\nCONFIG_BNEP_INITIAL_TRACE_LEVEL=2\nCONFIG_PAN_TRACE_LEVEL_NONE=\nCONFIG_PAN_TRACE_LEVEL_ERROR=\nCONFIG_PAN_TRACE_LEVEL_WARNING=y\nCONFIG_PAN_TRACE_LEVEL_API=\nCONFIG_PAN_TRACE_LEVEL_EVENT=\nCONFIG_PAN_TRACE_LEVEL_DEBUG=\nCONFIG_PAN_TRACE_LEVEL_VERBOSE=\nCONFIG_PAN_INITIAL_TRACE_LEVEL=2\nCONFIG_A2D_TRACE_LEVEL_NONE=\nCONFIG_A2D_TRACE_LEVEL_ERROR=\nCONFIG_A2D_TRACE_LEVEL_WARNING=y\nCONFIG_A2D_TRACE_LEVEL_API=\nCONFIG_A2D_TRACE_LEVEL_EVENT=\nCONFIG_A2D_TRACE_LEVEL_DEBUG=\nCONFIG_A2D_TRACE_LEVEL_VERBOSE=\nCONFIG_A2D_INITIAL_TRACE_LEVEL=2\nCONFIG_AVDT_TRACE_LEVEL_NONE=\nCONFIG_AVDT_TRACE_LEVEL_ERROR=\nCONFIG_AVDT_TRACE_LEVEL_WARNING=y\nCONFIG_AVDT_TRACE_LEVEL_API=\nCONFIG_AVDT_TRACE_LEVEL_EVENT=\nCONFIG_AVDT_TRACE_LEVEL_DEBUG=\nCONFIG_AVDT_TRACE_LEVEL_VERBOSE=\nCONFIG_AVDT_INITIAL_TRACE_LEVEL=2\nCONFIG_AVCT_TRACE_LEVEL_NONE=\nCONFIG_AVCT_TRACE_LEVEL_ERROR=\nCONFIG_AVCT_TRACE_LEVEL_WARNING=y\nCONFIG_AVCT_TRACE_LEVEL_API=\nCONFIG_AVCT_TRACE_LEVEL_EVENT=\nCONFIG_AVCT_TRACE_LEVEL_DEBUG=\nCONFIG_AVCT_TRACE_LEVEL_VERBOSE=\nCONFIG_AVCT_INITIAL_TRACE_LEVEL=2\nCONFIG_AVRC_TRACE_LEVEL_NONE=\nCONFIG_AVRC_TRACE_LEVEL_ERROR=\nCONFIG_AVRC_TRACE_LEVEL_WARNING=y\nCONFIG_AVRC_TRACE_LEVEL_API=\nCONFIG_AVRC_TRACE_LEVEL_EVENT=\nCONFIG_AVRC_TRACE_LEVEL_DEBUG=\nCONFIG_AVRC_TRACE_LEVEL_VERBOSE=\nCONFIG_AVRC_INITIAL_TRACE_LEVEL=2\nCONFIG_MCA_TRACE_LEVEL_NONE=\nCONFIG_MCA_TRACE_LEVEL_ERROR=\nCONFIG_MCA_TRACE_LEVEL_WARNING=y\nCONFIG_MCA_TRACE_LEVEL_API=\nCONFIG_MCA_TRACE_LEVEL_EVENT=\nCONFIG_MCA_TRACE_LEVEL_DEBUG=\nCONFIG_MCA_TRACE_LEVEL_VERBOSE=\nCONFIG_MCA_INITIAL_TRACE_LEVEL=2\nCONFIG_HID_TRACE_LEVEL_NONE=\nCONFIG_HID_TRACE_LEVEL_ERROR=\nCONFIG_HID_TRACE_LEVEL_WARNING=y\nCONFIG_HID_TRACE_LEVEL_API=\nCONFIG_HID_TRACE_LEVEL_EVENT=\nCONFIG_HID_TRACE_LEVEL_DEBUG=\nCONFIG_HID_TRACE_LEVEL_VERBOSE=\nCONFIG_HID_INITIAL_TRACE_LEVEL=2\nCONFIG_APPL_TRACE_LEVEL_NONE=\nCONFIG_APPL_TRACE_LEVEL_ERROR=\nCONFIG_APPL_TRACE_LEVEL_WARNING=y\nCONFIG_APPL_TRACE_LEVEL_API=\nCONFIG_APPL_TRACE_LEVEL_EVENT=\nCONFIG_APPL_TRACE_LEVEL_DEBUG=\nCONFIG_APPL_TRACE_LEVEL_VERBOSE=\nCONFIG_APPL_INITIAL_TRACE_LEVEL=2\nCONFIG_GATT_TRACE_LEVEL_NONE=\nCONFIG_GATT_TRACE_LEVEL_ERROR=\nCONFIG_GATT_TRACE_LEVEL_WARNING=y\nCONFIG_GATT_TRACE_LEVEL_API=\nCONFIG_GATT_TRACE_LEVEL_EVENT=\nCONFIG_GATT_TRACE_LEVEL_DEBUG=\nCONFIG_GATT_TRACE_LEVEL_VERBOSE=\nCONFIG_GATT_INITIAL_TRACE_LEVEL=2\nCONFIG_SMP_TRACE_LEVEL_NONE=\nCONFIG_SMP_TRACE_LEVEL_ERROR=\nCONFIG_SMP_TRACE_LEVEL_WARNING=y\nCONFIG_SMP_TRACE_LEVEL_API=\nCONFIG_SMP_TRACE_LEVEL_EVENT=\nCONFIG_SMP_TRACE_LEVEL_DEBUG=\nCONFIG_SMP_TRACE_LEVEL_VERBOSE=\nCONFIG_SMP_INITIAL_TRACE_LEVEL=2\nCONFIG_BTIF_TRACE_LEVEL_NONE=\nCONFIG_BTIF_TRACE_LEVEL_ERROR=\nCONFIG_BTIF_TRACE_LEVEL_WARNING=y\nCONFIG_BTIF_TRACE_LEVEL_API=\nCONFIG_BTIF_TRACE_LEVEL_EVENT=\nCONFIG_BTIF_TRACE_LEVEL_DEBUG=\nCONFIG_BTIF_TRACE_LEVEL_VERBOSE=\nCONFIG_BTIF_INITIAL_TRACE_LEVEL=2\nCONFIG_BTC_TRACE_LEVEL_NONE=\nCONFIG_BTC_TRACE_LEVEL_ERROR=\nCONFIG_BTC_TRACE_LEVEL_WARNING=y\nCONFIG_BTC_TRACE_LEVEL_API=\nCONFIG_BTC_TRACE_LEVEL_EVENT=\nCONFIG_BTC_TRACE_LEVEL_DEBUG=\nCONFIG_BTC_TRACE_LEVEL_VERBOSE=\nCONFIG_BTC_INITIAL_TRACE_LEVEL=2\nCONFIG_OSI_TRACE_LEVEL_NONE=\nCONFIG_OSI_TRACE_LEVEL_ERROR=\nCONFIG_OSI_TRACE_LEVEL_WARNING=y\nCONFIG_OSI_TRACE_LEVEL_API=\nCONFIG_OSI_TRACE_LEVEL_EVENT=\nCONFIG_OSI_TRACE_LEVEL_DEBUG=\nCONFIG_OSI_TRACE_LEVEL_VERBOSE=\nCONFIG_OSI_INITIAL_TRACE_LEVEL=2\nCONFIG_BLUFI_TRACE_LEVEL_NONE=\nCONFIG_BLUFI_TRACE_LEVEL_ERROR=\nCONFIG_BLUFI_TRACE_LEVEL_WARNING=y\nCONFIG_BLUFI_TRACE_LEVEL_API=\nCONFIG_BLUFI_TRACE_LEVEL_EVENT=\nCONFIG_BLUFI_TRACE_LEVEL_DEBUG=\nCONFIG_BLUFI_TRACE_LEVEL_VERBOSE=\nCONFIG_BLUFI_INITIAL_TRACE_LEVEL=2\nCONFIG_BT_ACL_CONNECTIONS=4\nCONFIG_BT_ALLOCATION_FROM_SPIRAM_FIRST=\nCONFIG_BT_BLE_DYNAMIC_ENV_MEMORY=\nCONFIG_BLE_HOST_QUEUE_CONGESTION_CHECK=\nCONFIG_SMP_ENABLE=y\nCONFIG_BLE_ACTIVE_SCAN_REPORT_ADV_SCAN_RSP_INDIVIDUALLY=\nCONFIG_BLE_ESTABLISH_LINK_CONNECTION_TIMEOUT=30\nCONFIG_BT_RESERVE_DRAM=0xdb5cCONFIG_IO_GLITCH_FILTER_TIME_MS=50CONFIG_ADC_FORCE_XPD_FSM=\nCONFIG_ADC2_DISABLE_DAC=yCONFIG_SPI_MASTER_IN_IRAM=\nCONFIG_SPI_MASTER_ISR_IN_IRAM=y\nCONFIG_SPI_SLAVE_IN_IRAM=\nCONFIG_SPI_SLAVE_ISR_IN_IRAM=yCONFIG_ESP32_REV_MIN_0=y\nCONFIG_ESP32_REV_MIN_1=\nCONFIG_ESP32_REV_MIN_2=\nCONFIG_ESP32_REV_MIN_3=\nCONFIG_ESP32_REV_MIN=0\nCONFIG_ESP32_DPORT_WORKAROUND=y\nCONFIG_ESP32_DEFAULT_CPU_FREQ_80=\nCONFIG_ESP32_DEFAULT_CPU_FREQ_160=\nCONFIG_ESP32_DEFAULT_CPU_FREQ_240=y\nCONFIG_ESP32_DEFAULT_CPU_FREQ_MHZ=240\nCONFIG_SPIRAM_SUPPORT=yCONFIG_SPIRAM_BOOT_INIT=y\nCONFIG_SPIRAM_IGNORE_NOTFOUND=\nCONFIG_SPIRAM_USE_MEMMAP=\nCONFIG_SPIRAM_USE_CAPS_ALLOC=\nCONFIG_SPIRAM_USE_MALLOC=y\nCONFIG_SPIRAM_TYPE_AUTO=y\nCONFIG_SPIRAM_TYPE_ESPPSRAM32=\nCONFIG_SPIRAM_TYPE_ESPPSRAM64=\nCONFIG_SPIRAM_SIZE=-1\nCONFIG_SPIRAM_SPEED_40M=\nCONFIG_SPIRAM_SPEED_80M=y\nCONFIG_SPIRAM_MEMTEST=y\nCONFIG_SPIRAM_CACHE_WORKAROUND=y\nCONFIG_SPIRAM_BANKSWITCH_ENABLE=y\nCONFIG_SPIRAM_BANKSWITCH_RESERVE=8\nCONFIG_SPIRAM_MALLOC_ALWAYSINTERNAL=8190\nCONFIG_WIFI_LWIP_ALLOCATION_FROM_SPIRAM_FIRST=y\nCONFIG_SPIRAM_MALLOC_RESERVE_INTERNAL=32768\nCONFIG_SPIRAM_ALLOW_STACK_EXTERNAL_MEMORY=y\nCONFIG_SPIRAM_ALLOW_BSS_SEG_EXTERNAL_MEMORY=\nCONFIG_SPIRAM_OCCUPY_HSPI_HOST=\nCONFIG_SPIRAM_OCCUPY_VSPI_HOST=yCONFIG_D0WD_PSRAM_CLK_IO=17\nCONFIG_D0WD_PSRAM_CS_IO=16CONFIG_D2WD_PSRAM_CLK_IO=9\nCONFIG_D2WD_PSRAM_CS_IO=10CONFIG_PICO_PSRAM_CS_IO=10\nCONFIG_MEMMAP_TRACEMEM=\nCONFIG_MEMMAP_TRACEMEM_TWOBANKS=\nCONFIG_ESP32_TRAX=\nCONFIG_TRACEMEM_RESERVE_DRAM=0x0\nCONFIG_ESP32_ENABLE_COREDUMP_TO_FLASH=\nCONFIG_ESP32_ENABLE_COREDUMP_TO_UART=\nCONFIG_ESP32_ENABLE_COREDUMP_TO_NONE=y\nCONFIG_ESP32_ENABLE_COREDUMP=\nCONFIG_TWO_UNIVERSAL_MAC_ADDRESS=\nCONFIG_FOUR_UNIVERSAL_MAC_ADDRESS=y\nCONFIG_NUMBER_OF_UNIVERSAL_MAC_ADDRESS=4\nCONFIG_SYSTEM_EVENT_QUEUE_SIZE=32\nCONFIG_SYSTEM_EVENT_TASK_STACK_SIZE=4096\nCONFIG_MAIN_TASK_STACK_SIZE=6144\nCONFIG_IPC_TASK_STACK_SIZE=1024\nCONFIG_TIMER_TASK_STACK_SIZE=4096\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_CRLF=y\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_LF=\nCONFIG_NEWLIB_STDOUT_LINE_ENDING_CR=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_CRLF=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_LF=\nCONFIG_NEWLIB_STDIN_LINE_ENDING_CR=y\nCONFIG_NEWLIB_NANO_FORMAT=\nCONFIG_CONSOLE_UART_DEFAULT=y\nCONFIG_CONSOLE_UART_CUSTOM=\nCONFIG_CONSOLE_UART_NONE=\nCONFIG_CONSOLE_UART_NUM=0\nCONFIG_CONSOLE_UART_BAUDRATE=115200\nCONFIG_ULP_COPROC_ENABLED=\nCONFIG_ULP_COPROC_RESERVE_MEM=0\nCONFIG_ESP32_PANIC_PRINT_HALT=\nCONFIG_ESP32_PANIC_PRINT_REBOOT=y\nCONFIG_ESP32_PANIC_SILENT_REBOOT=\nCONFIG_ESP32_PANIC_GDBSTUB=\nCONFIG_ESP32_DEBUG_OCDAWARE=y\nCONFIG_ESP32_DEBUG_STUBS_ENABLE=\nCONFIG_INT_WDT=y\nCONFIG_INT_WDT_TIMEOUT_MS=800\nCONFIG_INT_WDT_CHECK_CPU1=y\nCONFIG_TASK_WDT=y\nCONFIG_TASK_WDT_PANIC=\nCONFIG_TASK_WDT_TIMEOUT_S=5\nCONFIG_TASK_WDT_CHECK_IDLE_TASK_CPU0=y\nCONFIG_TASK_WDT_CHECK_IDLE_TASK_CPU1=y\nCONFIG_BROWNOUT_DET=y\nCONFIG_BROWNOUT_DET_LVL_SEL_0=y\nCONFIG_BROWNOUT_DET_LVL_SEL_1=\nCONFIG_BROWNOUT_DET_LVL_SEL_2=\nCONFIG_BROWNOUT_DET_LVL_SEL_3=\nCONFIG_BROWNOUT_DET_LVL_SEL_4=\nCONFIG_BROWNOUT_DET_LVL_SEL_5=\nCONFIG_BROWNOUT_DET_LVL_SEL_6=\nCONFIG_BROWNOUT_DET_LVL_SEL_7=\nCONFIG_BROWNOUT_DET_LVL=0\nCONFIG_REDUCE_PHY_TX_POWER=y\nCONFIG_ESP32_TIME_SYSCALL_USE_RTC_FRC1=y\nCONFIG_ESP32_TIME_SYSCALL_USE_RTC=\nCONFIG_ESP32_TIME_SYSCALL_USE_FRC1=\nCONFIG_ESP32_TIME_SYSCALL_USE_NONE=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_INTERNAL_RC=y\nCONFIG_ESP32_RTC_CLOCK_SOURCE_EXTERNAL_CRYSTAL=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_EXTERNAL_OSC=\nCONFIG_ESP32_RTC_CLOCK_SOURCE_INTERNAL_8MD256=\nCONFIG_ESP32_RTC_CLK_CAL_CYCLES=1024\nCONFIG_ESP32_DEEP_SLEEP_WAKEUP_DELAY=2000\nCONFIG_ESP32_XTAL_FREQ_40=y\nCONFIG_ESP32_XTAL_FREQ_26=\nCONFIG_ESP32_XTAL_FREQ_AUTO=\nCONFIG_ESP32_XTAL_FREQ=40\nCONFIG_DISABLE_BASIC_ROM_CONSOLE=\nCONFIG_ESP_TIMER_PROFILING=\nCONFIG_COMPATIBLE_PRE_V2_1_BOOTLOADERS=\nCONFIG_ESP_ERR_TO_NAME_LOOKUP=y\nCONFIG_ESP32_DPORT_DIS_INTERRUPT_LVL=5CONFIG_SW_COEXIST_ENABLE=y\nCONFIG_SW_COEXIST_PREFERENCE_WIFI=\nCONFIG_SW_COEXIST_PREFERENCE_BT=\nCONFIG_SW_COEXIST_PREFERENCE_BALANCE=y\nCONFIG_SW_COEXIST_PREFERENCE_VALUE=2\nCONFIG_ESP32_WIFI_STATIC_RX_BUFFER_NUM=10\nCONFIG_ESP32_WIFI_DYNAMIC_RX_BUFFER_NUM=32\nCONFIG_ESP32_WIFI_STATIC_TX_BUFFER=y\nCONFIG_ESP32_WIFI_TX_BUFFER_TYPE=0\nCONFIG_ESP32_WIFI_STATIC_TX_BUFFER_NUM=32\nCONFIG_ESP32_WIFI_CSI_ENABLED=\nCONFIG_ESP32_WIFI_AMPDU_TX_ENABLED=y\nCONFIG_ESP32_WIFI_TX_BA_WIN=6\nCONFIG_ESP32_WIFI_AMPDU_RX_ENABLED=y\nCONFIG_ESP32_WIFI_RX_BA_WIN=16\nCONFIG_ESP32_WIFI_NVS_ENABLED=y\nCONFIG_ESP32_WIFI_TASK_PINNED_TO_CORE_0=y\nCONFIG_ESP32_WIFI_TASK_PINNED_TO_CORE_1=\nCONFIG_ESP32_WIFI_SOFTAP_BEACON_MAX_LEN=752\nCONFIG_ESP32_WIFI_IRAM_OPT=y\nCONFIG_ESP32_WIFI_MGMT_SBUF_NUM=32CONFIG_ESP32_PHY_CALIBRATION_AND_DATA_STORAGE=y\nCONFIG_ESP32_PHY_INIT_DATA_IN_PARTITION=\nCONFIG_ESP32_PHY_MAX_WIFI_TX_POWER=20\nCONFIG_ESP32_PHY_MAX_TX_POWER=20CONFIG_PM_ENABLE=CONFIG_ADC_CAL_EFUSE_TP_ENABLE=y\nCONFIG_ADC_CAL_EFUSE_VREF_ENABLE=y\nCONFIG_ADC_CAL_LUT_ENABLE=yCONFIG_EVENT_LOOP_PROFILING=CONFIG_ESP_HTTP_CLIENT_ENABLE_HTTPS=yCONFIG_HTTPD_MAX_REQ_HDR_LEN=512\nCONFIG_HTTPD_MAX_URI_LEN=512\nCONFIG_HTTPD_PURGE_BUF_LEN=32\nCONFIG_HTTPD_LOG_PURGE_DATA=CONFIG_DMA_RX_BUF_NUM=10\nCONFIG_DMA_TX_BUF_NUM=10\nCONFIG_EMAC_L2_TO_L3_RX_BUF_MODE=y\nCONFIG_EMAC_CHECK_LINK_PERIOD_MS=2000\nCONFIG_EMAC_TASK_PRIORITY=20\nCONFIG_EMAC_TASK_STACK_SIZE=3072CONFIG_FATFS_CODEPAGE_DYNAMIC=\nCONFIG_FATFS_CODEPAGE_437=y\nCONFIG_FATFS_CODEPAGE_720=\nCONFIG_FATFS_CODEPAGE_737=\nCONFIG_FATFS_CODEPAGE_771=\nCONFIG_FATFS_CODEPAGE_775=\nCONFIG_FATFS_CODEPAGE_850=\nCONFIG_FATFS_CODEPAGE_852=\nCONFIG_FATFS_CODEPAGE_855=\nCONFIG_FATFS_CODEPAGE_857=\nCONFIG_FATFS_CODEPAGE_860=\nCONFIG_FATFS_CODEPAGE_861=\nCONFIG_FATFS_CODEPAGE_862=\nCONFIG_FATFS_CODEPAGE_863=\nCONFIG_FATFS_CODEPAGE_864=\nCONFIG_FATFS_CODEPAGE_865=\nCONFIG_FATFS_CODEPAGE_866=\nCONFIG_FATFS_CODEPAGE_869=\nCONFIG_FATFS_CODEPAGE_932=\nCONFIG_FATFS_CODEPAGE_936=\nCONFIG_FATFS_CODEPAGE_949=\nCONFIG_FATFS_CODEPAGE_950=\nCONFIG_FATFS_CODEPAGE=437\nCONFIG_FATFS_LFN_NONE=y\nCONFIG_FATFS_LFN_HEAP=\nCONFIG_FATFS_LFN_STACK=\nCONFIG_FATFS_FS_LOCK=0\nCONFIG_FATFS_TIMEOUT_MS=10000\nCONFIG_FATFS_PER_FILE_CACHE=yCONFIG_MB_QUEUE_LENGTH=20\nCONFIG_MB_SERIAL_TASK_STACK_SIZE=2048\nCONFIG_MB_SERIAL_BUF_SIZE=256\nCONFIG_MB_SERIAL_TASK_PRIO=10\nCONFIG_MB_CONTROLLER_SLAVE_ID_SUPPORT=\nCONFIG_MB_CONTROLLER_NOTIFY_TIMEOUT=20\nCONFIG_MB_CONTROLLER_NOTIFY_QUEUE_SIZE=20\nCONFIG_MB_CONTROLLER_STACK_SIZE=4096\nCONFIG_MB_EVENT_QUEUE_TIMEOUT=20\nCONFIG_MB_TIMER_PORT_ENABLED=y\nCONFIG_MB_TIMER_GROUP=0\nCONFIG_MB_TIMER_INDEX=0CONFIG_FREERTOS_UNICORE=\nCONFIG_FREERTOS_NO_AFFINITY=0x7FFFFFFF\nCONFIG_FREERTOS_CORETIMER_0=y\nCONFIG_FREERTOS_CORETIMER_1=\nCONFIG_FREERTOS_HZ=100\nCONFIG_FREERTOS_ASSERT_ON_UNTESTED_FUNCTION=y\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_NONE=\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_PTRVAL=\nCONFIG_FREERTOS_CHECK_STACKOVERFLOW_CANARY=y\nCONFIG_FREERTOS_WATCHPOINT_END_OF_STACK=\nCONFIG_FREERTOS_INTERRUPT_BACKTRACE=y\nCONFIG_FREERTOS_THREAD_LOCAL_STORAGE_POINTERS=1\nCONFIG_FREERTOS_ASSERT_FAIL_ABORT=y\nCONFIG_FREERTOS_ASSERT_FAIL_PRINT_CONTINUE=\nCONFIG_FREERTOS_ASSERT_DISABLE=\nCONFIG_FREERTOS_IDLE_TASK_STACKSIZE=1024\nCONFIG_FREERTOS_ISR_STACKSIZE=1536\nCONFIG_FREERTOS_LEGACY_HOOKS=\nCONFIG_FREERTOS_MAX_TASK_NAME_LEN=16\nCONFIG_SUPPORT_STATIC_ALLOCATION=y\nCONFIG_ENABLE_STATIC_TASK_CLEAN_UP_HOOK=\nCONFIG_TIMER_TASK_PRIORITY=1\nCONFIG_TIMER_TASK_STACK_DEPTH=6144\nCONFIG_TIMER_QUEUE_LENGTH=10\nCONFIG_FREERTOS_QUEUE_REGISTRY_SIZE=0\nCONFIG_FREERTOS_USE_TRACE_FACILITY=y\nCONFIG_FREERTOS_USE_STATS_FORMATTING_FUNCTIONS=\nCONFIG_FREERTOS_GENERATE_RUN_TIME_STATS=\nCONFIG_FREERTOS_DEBUG_INTERNALS=CONFIG_HEAP_POISONING_DISABLED=y\nCONFIG_HEAP_POISONING_LIGHT=\nCONFIG_HEAP_POISONING_COMPREHENSIVE=\nCONFIG_HEAP_TRACING=CONFIG_HTTP_CLIENT_MAX_HDR_VAL_LEN=50CONFIG_STATUS_LED_QUICK_BLINK_FREQ=5\nCONFIG_STATUS_LED_SLOW_BLINK_FREQ=1\nCONFIG_USE_LEDC_HIGHSPEED_MODE=y\nCONFIG_USE_LEDC_LOWSPEED_MODE=\nCONFIG_STATUS_LED_SPEED_MODE=0\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_0=y\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_1=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_2=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_3=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_4=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_5=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_6=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_7=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL_DEF=\nCONFIG_STATUS_LED_QUICK_BLINK_CHANNEL=0\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_0=y\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_1=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_2=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_3=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER_DEF=\nCONFIG_STATUS_LED_QUICK_BLINK_TIMER=0\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_0=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_1=y\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_2=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_3=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_4=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_5=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_6=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_7=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL_DEF=\nCONFIG_STATUS_LED_SLOW_BLINK_CHANNEL=1\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_0=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_1=y\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_2=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_3=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER_DEF=\nCONFIG_STATUS_LED_SLOW_BLINK_TIMER=1\nCONFIG_STATUS_LED_NIGHT_MODE_ENABLE=y\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_0=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_1=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_2=y\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_3=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_4=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_5=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_6=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_7=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL_DEF=\nCONFIG_STATUS_LED_NIGHT_MODE_CHANNEL=2\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_0=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_1=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_2=y\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_3=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER_DEF=\nCONFIG_STATUS_LED_NIGHT_MODE_TIMER=2CONFIG_LIBSODIUM_USE_MBEDTLS_SHA=yCONFIG_LOG_DEFAULT_LEVEL_NONE=\nCONFIG_LOG_DEFAULT_LEVEL_ERROR=\nCONFIG_LOG_DEFAULT_LEVEL_WARN=y\nCONFIG_LOG_DEFAULT_LEVEL_INFO=\nCONFIG_LOG_DEFAULT_LEVEL_DEBUG=\nCONFIG_LOG_DEFAULT_LEVEL_VERBOSE=\nCONFIG_LOG_DEFAULT_LEVEL=2\nCONFIG_LOG_COLORS=yCONFIG_L2_TO_L3_COPY=\nCONFIG_LWIP_IRAM_OPTIMIZATION=\nCONFIG_LWIP_MAX_SOCKETS=9\nCONFIG_USE_ONLY_LWIP_SELECT=y\nCONFIG_LWIP_SO_REUSE=y\nCONFIG_LWIP_SO_REUSE_RXTOALL=y\nCONFIG_LWIP_SO_RCVBUF=\nCONFIG_LWIP_DHCP_MAX_NTP_SERVERS=3\nCONFIG_LWIP_IP_FRAG=\nCONFIG_LWIP_IP_REASSEMBLY=\nCONFIG_LWIP_STATS=\nCONFIG_LWIP_ETHARP_TRUST_IP_MAC=y\nCONFIG_ESP_GRATUITOUS_ARP=y\nCONFIG_GARP_TMR_INTERVAL=60\nCONFIG_TCPIP_RECVMBOX_SIZE=32\nCONFIG_LWIP_DHCP_DOES_ARP_CHECK=y\nCONFIG_LWIP_DHCP_RESTORE_LAST_IP=CONFIG_LWIP_DHCPS_LEASE_UNIT=60\nCONFIG_LWIP_DHCPS_MAX_STATION_NUM=8\nCONFIG_LWIP_AUTOIP=\nCONFIG_LWIP_NETIF_LOOPBACK=y\nCONFIG_LWIP_LOOPBACK_MAX_PBUFS=8CONFIG_LWIP_MAX_ACTIVE_TCP=8\nCONFIG_LWIP_MAX_LISTENING_TCP=8\nCONFIG_TCP_MAXRTX=12\nCONFIG_TCP_SYNMAXRTX=6\nCONFIG_TCP_MSS=1436\nCONFIG_TCP_MSL=60000\nCONFIG_TCP_SND_BUF_DEFAULT=14360\nCONFIG_TCP_WND_DEFAULT=14360\nCONFIG_TCP_RECVMBOX_SIZE=12\nCONFIG_TCP_QUEUE_OOSEQ=y\nCONFIG_ESP_TCP_KEEP_CONNECTION_WHEN_IP_CHANGES=\nCONFIG_TCP_OVERSIZE_MSS=y\nCONFIG_TCP_OVERSIZE_QUARTER_MSS=\nCONFIG_TCP_OVERSIZE_DISABLE=\nCONFIG_LWIP_WND_SCALE=CONFIG_LWIP_MAX_UDP_PCBS=16\nCONFIG_UDP_RECVMBOX_SIZE=6\nCONFIG_TCPIP_TASK_STACK_SIZE=3072\nCONFIG_TCPIP_TASK_AFFINITY_NO_AFFINITY=y\nCONFIG_TCPIP_TASK_AFFINITY_CPU0=\nCONFIG_TCPIP_TASK_AFFINITY_CPU1=\nCONFIG_TCPIP_TASK_AFFINITY=0x7FFFFFFF\nCONFIG_PPP_SUPPORT=CONFIG_LWIP_MULTICAST_PING=\nCONFIG_LWIP_BROADCAST_PING=CONFIG_LWIP_MAX_RAW_PCBS=16CONFIG_MBEDTLS_INTERNAL_MEM_ALLOC=y\nCONFIG_MBEDTLS_EXTERNAL_MEM_ALLOC=\nCONFIG_MBEDTLS_DEFAULT_MEM_ALLOC=\nCONFIG_MBEDTLS_CUSTOM_MEM_ALLOC=\nCONFIG_MBEDTLS_SSL_MAX_CONTENT_LEN=16384\nCONFIG_MBEDTLS_ASYMMETRIC_CONTENT_LEN=\nCONFIG_MBEDTLS_DEBUG=\nCONFIG_MBEDTLS_HARDWARE_AES=y\nCONFIG_MBEDTLS_HARDWARE_MPI=\nCONFIG_MBEDTLS_HARDWARE_SHA=\nCONFIG_MBEDTLS_HAVE_TIME=y\nCONFIG_MBEDTLS_HAVE_TIME_DATE=y\nCONFIG_MBEDTLS_TLS_SERVER_AND_CLIENT=y\nCONFIG_MBEDTLS_TLS_SERVER_ONLY=\nCONFIG_MBEDTLS_TLS_CLIENT_ONLY=\nCONFIG_MBEDTLS_TLS_DISABLED=\nCONFIG_MBEDTLS_TLS_SERVER=y\nCONFIG_MBEDTLS_TLS_CLIENT=y\nCONFIG_MBEDTLS_TLS_ENABLED=yCONFIG_MBEDTLS_PSK_MODES=\nCONFIG_MBEDTLS_KEY_EXCHANGE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_DHE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ELLIPTIC_CURVE=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDHE_RSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDHE_ECDSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDH_ECDSA=y\nCONFIG_MBEDTLS_KEY_EXCHANGE_ECDH_RSA=y\nCONFIG_MBEDTLS_SSL_RENEGOTIATION=y\nCONFIG_MBEDTLS_SSL_PROTO_SSL3=\nCONFIG_MBEDTLS_SSL_PROTO_TLS1=y\nCONFIG_MBEDTLS_SSL_PROTO_TLS1_1=y\nCONFIG_MBEDTLS_SSL_PROTO_TLS1_2=y\nCONFIG_MBEDTLS_SSL_PROTO_DTLS=\nCONFIG_MBEDTLS_SSL_ALPN=y\nCONFIG_MBEDTLS_SSL_SESSION_TICKETS=yCONFIG_MBEDTLS_AES_C=y\nCONFIG_MBEDTLS_CAMELLIA_C=\nCONFIG_MBEDTLS_DES_C=\nCONFIG_MBEDTLS_RC4_DISABLED=y\nCONFIG_MBEDTLS_RC4_ENABLED_NO_DEFAULT=\nCONFIG_MBEDTLS_RC4_ENABLED=\nCONFIG_MBEDTLS_BLOWFISH_C=\nCONFIG_MBEDTLS_XTEA_C=\nCONFIG_MBEDTLS_CCM_C=y\nCONFIG_MBEDTLS_GCM_C=y\nCONFIG_MBEDTLS_RIPEMD160_C=CONFIG_MBEDTLS_PEM_PARSE_C=y\nCONFIG_MBEDTLS_PEM_WRITE_C=y\nCONFIG_MBEDTLS_X509_CRL_PARSE_C=y\nCONFIG_MBEDTLS_X509_CSR_PARSE_C=y\nCONFIG_MBEDTLS_ECP_C=y\nCONFIG_MBEDTLS_ECDH_C=y\nCONFIG_MBEDTLS_ECDSA_C=y\nCONFIG_MBEDTLS_ECP_DP_SECP192R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP224R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP256R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP384R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP521R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP192K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP224K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_SECP256K1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP256R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP384R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_BP512R1_ENABLED=y\nCONFIG_MBEDTLS_ECP_DP_CURVE25519_ENABLED=y\nCONFIG_MBEDTLS_ECP_NIST_OPTIM=yCONFIG_MDNS_MAX_SERVICES=10CONFIG_MQTT_PROTOCOL_311=y\nCONFIG_MQTT_TRANSPORT_SSL=y\nCONFIG_MQTT_TRANSPORT_WEBSOCKET=y\nCONFIG_MQTT_TRANSPORT_WEBSOCKET_SECURE=y\nCONFIG_MQTT_USE_CUSTOM_CONFIG=\nCONFIG_MQTT_TASK_CORE_SELECTION_ENABLED=\nCONFIG_MQTT_CUSTOM_OUTBOX=CONFIG_OPENSSL_DEBUG=\nCONFIG_OPENSSL_ASSERT_DO_NOTHING=y\nCONFIG_OPENSSL_ASSERT_EXIT=CONFIG_ESP32_PTHREAD_TASK_PRIO_DEFAULT=4\nCONFIG_ESP32_PTHREAD_TASK_STACK_SIZE_DEFAULT=16000\nCONFIG_PTHREAD_STACK_MIN=768CONFIG_SPI_FLASH_VERIFY_WRITE=\nCONFIG_SPI_FLASH_ENABLE_COUNTERS=\nCONFIG_SPI_FLASH_ROM_DRIVER_PATCH=y\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_ABORTS=y\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_FAILS=\nCONFIG_SPI_FLASH_WRITING_DANGEROUS_REGIONS_ALLOWED=\nCONFIG_SPI_FLASH_YIELD_DURING_ERASE=CONFIG_SPIFFS_MAX_PARTITIONS=3CONFIG_SPIFFS_CACHE=y\nCONFIG_SPIFFS_CACHE_WR=y\nCONFIG_SPIFFS_CACHE_STATS=\nCONFIG_SPIFFS_PAGE_CHECK=y\nCONFIG_SPIFFS_GC_MAX_RUNS=10\nCONFIG_SPIFFS_GC_STATS=\nCONFIG_SPIFFS_PAGE_SIZE=256\nCONFIG_SPIFFS_OBJ_NAME_LEN=32\nCONFIG_SPIFFS_USE_MAGIC=y\nCONFIG_SPIFFS_USE_MAGIC_LENGTH=y\nCONFIG_SPIFFS_META_LENGTH=4\nCONFIG_SPIFFS_USE_MTIME=yCONFIG_SPIFFS_DBG=\nCONFIG_SPIFFS_API_DBG=\nCONFIG_SPIFFS_GC_DBG=\nCONFIG_SPIFFS_CACHE_DBG=\nCONFIG_SPIFFS_CHECK_DBG=\nCONFIG_SPIFFS_TEST_VISUALISATION=CONFIG_IP_LOST_TIMER_INTERVAL=120\nCONFIG_TCPIP_LWIP=yCONFIG_SUPPRESS_SELECT_DEBUG_OUTPUT=y\nCONFIG_SUPPORT_TERMIOS=yCONFIG_WL_SECTOR_SIZE_512=\nCONFIG_WL_SECTOR_SIZE_4096=y\nCONFIG_WL_SECTOR_SIZE=4096", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Jan 8, 2020", "body": "I see quite a few differences when in terms of the configuration. (pinned core for BT task, Debug logs for BT enabled etc.).\nI suggest please try removing all older artifacts (build, sdkconfig, sdkconfig.old) and try once with . See if it works and gradually work up with other changes.\nRight now, it's really tight in terms of internal memory availability when BT and Alexa are ran simultaneously.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 14, 2020", "body": "I followed your recommendation and at this time I'm having a error and reboot again continuously.E (2440) [bluetooth-internal]: Error reading paired device list from NVSIs that something to do with flash size? I used 8mb.  Can you let me know what the error means?", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Jan 14, 2020", "body": "The error just indicates that you don't have any paired devices yet.Just go ahead and pair a device.", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 14, 2020", "body": "Before I can try paring, the system already went crash. only one time , when the wifi has failure, the bluetooth seems to be started pairing.   Do I need 16mb memory instead of 8mb?", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Jan 14, 2020", "body": "Hi ", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 15, 2020", "body": "Yes.  here is.--- idf_monitor on /dev/ttyUSB0 115200 ---\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\nets Jun  8 2016 00:22:57rst:0x1 (POWERON_RESET),boot:0x3e (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (63) boot_comm: mismatch chip revision, expect 1, found 0\nI (38) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (38) boot: compile time 11:49:06\nI (51) boot: Enabling RNG early entropy source...\nI (51) boot: SPI Speed      : 80MHz\nI (51) boot: SPI Mode       : DIO\nI (55) boot: SPI Flash Size : 8MB\nI (59) boot: Partition Table:\nI (62) boot: ## Label            Usage          Type ST Offset   Length\nI (69) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (77) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (84) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (92) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (99) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (107) boot: End of partition table\nI (111) boot: No factory image, trying OTA 0\nI (116) boot_comm: mismatch chip revision, expect 1, found 0\nI (122) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (738) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (745) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (746) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (762) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1164) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1215) boot: Loaded app from partition at offset 0x20000\nI (1215) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1579044379\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (2461) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (3361) I2S: I2S driver already installed\nW (3381) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506Rebooting...\nets Jun  8 2016 00:22:57rst:0xc (SW_CPU_RESET),boot:0x3e (SPI_FAST_FLASH_BOOT)\nconfigsip: 0, SPIWP:0xee\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\nmode:DIO, clock div:1\nload:0x3fff0018,len:4\nload:0x3fff001c,len:6388\nload:0x40078000,len:9324\nload:0x40080400,len:6456\nentry 0x40080738\nI (59) boot: Chip Revision: 1\nI (64) boot_comm: mismatch chip revision, expect 1, found 0\nI (39) boot: ESP-IDF v3.2.3-35-g3bb4b4db4-dirty 2nd stage bootloader\nI (39) boot: compile time 11:49:06\nI (52) boot: Enabling RNG early entropy source...\nI (52) boot: SPI Speed      : 80MHz\nI (52) boot: SPI Mode       : DIO\nI (56) boot: SPI Flash Size : 8MB\nI (60) boot: Partition Table:\nI (63) boot: ## Label            Usage          Type ST Offset   Length\nI (70) boot:  0 nvs              WiFi data        01 02 00009000 00006000\nI (78) boot:  1 phy_init         RF data          01 01 0000f000 00001000\nI (85) boot:  2 fctry            WiFi data        01 02 00010000 00006000\nI (93) boot:  3 otadata          OTA data         01 00 00016000 00002000\nI (100) boot:  4 ota_0            OTA app          00 10 00020000 00420000\nI (108) boot: End of partition table\nI (112) boot: No factory image, trying OTA 0\nI (117) boot_comm: mismatch chip revision, expect 1, found 0\nI (123) esp_image: segment 0: paddr=0x00020020 vaddr=0x3f400020 size=0x214684 (2180740) map\nI (739) esp_image: segment 1: paddr=0x002346ac vaddr=0x3ffbdb60 size=0x051ec ( 20972) load\nI (746) esp_image: segment 2: paddr=0x002398a0 vaddr=0x40080000 size=0x00400 (  1024) load\n0x40080000: _WindowOverflow4 at /home/ppark/esp/esp-idf/components/freertos/xtensa_vectors.S:1779I (747) esp_image: segment 3: paddr=0x00239ca8 vaddr=0x40080400 size=0x06368 ( 25448) load\nI (763) esp_image: segment 4: paddr=0x00240018 vaddr=0x400d0018 size=0x15ff34 (1441588) map\n0x400d0018: _stext at ??:?I (1165) esp_image: segment 5: paddr=0x0039ff54 vaddr=0x40086768 size=0x17700 ( 96000) load\n0x40086768: ram_set_txcap_reg at /home/aiqin/git_tree/chip7.1_phy/chip_7.1/board_code/app_test/pp/phy/phy_chip_v7_cal.c:2458 (discriminator 1)I (1216) boot: Loaded app from partition at offset 0x20000\nI (1216) boot: Disabling RNG early entropy source...\n[conn_mgr_prov]: Found ssid: ESS-Sound-2.4G\n[conn_mgr_prov]: Found password: phoenixplus\n[va_button]: button pressed: 36\n[app_main]: Connected with IP Address: 192.168.101.169\n[alexa]: Waiting for time to be updated\n[alexa]: Done getting current time: 1579044383\n[alexa]: Authentication done\n[dialog]: Entering VA_IDLE\nE (1932) [bluetooth-internal]: Error reading paired device list from NVS\n[speaker]: Volume changed to 40\n[capabilities]: Capabilities unchanged\n[endpoint_handler]: Cannot find endpoint URL in NVS. Setting default: \n[endpoint_handler]: AVS endpoint: \nW (2802) I2S: I2S driver already installed\nW (2822) I2S: I2S driver already installed\n/home/ppark/esp/esp-idf/components/freertos/tasks.c:684 (xTaskCreateStaticPinnedToCore)- assert failed!\nabort() was called at PC 0x40091733 on core 0\n0x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:4691Backtrace: 0x40093937:0x3ffbc750 0x40093c69:0x3ffbc770 0x40091733:0x3ffbc790 0x4011bafe:0x3ffbc7d0 0x4010396a:0x3ffbc800 0x400d0e43:0x3ffbc920\n0x40093937: invoke_abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40093c69: abort at /home/ppark/esp/esp-idf/components/esp32/panic.c:7070x40091733: xTaskCreateStaticPinnedToCore at /home/ppark/esp/esp-idf/components/freertos/tasks.c:46910x4011bafe: xTaskCreateStatic at /home/ppark/esp/esp-idf/components/freertos/include/freertos/task.h:608\n(inlined by) va_dsp_init at /home/ppark/esp/esp-va-sdk/board_support_pkgs/lyrat/dsp_driver/lyrat_driver/components/va_dsp/va_dsp.c:2740x4010396a: app_main at /home/ppark/esp/esp-va-sdk/examples/amazon_alexa/main/app_main.c:2120x400d0e43: main_task at /home/ppark/esp/esp-idf/components/esp32/cpu_start.c:506", "type": "commented", "related_issue": null}, {"user_name": "avsheth", "datetime": "Jan 15, 2020", "body": "Hi \nCan you try with below changes:", "type": "commented", "related_issue": null}, {"user_name": "jhpark555", "datetime": "Jan 15, 2020", "body": "Hi Amit and Vikram,It worked as I followed your last instruction. Many Thanks.   I can demo now for my customer.\nBy the way I have one more question.\nI want to use AAC decoder on Bluetooth a2dp sink. Does esp32 can support it?", "type": "commented", "related_issue": null}, {"user_name": "vikramdattu", "datetime": "Jan 15, 2020", "body": "Unfortunately, ESP32 bluetooth stack support only SBC for now.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1869", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 3, 2020", "body": "When the extension is updated, users should get a \"NEW\"  on the toolbar icon.When the person next activates Firefox Voice, a new tab should open that shows the updates. This will be a page within the extension (design to be done in a different issue).The latest version of the feature list should be kept in browser storage. Not every release will necessarily have updates (e.g., a bugfix-only release won't need a proactive release page), so we should explicitly say what version last had an update and test against that. (The current version can be fetched via getManifest().)We may want to use a different icon instead of a badge.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Aug 3, 2020", "body": [], "type": "issue", "related_issue": "#1870"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1900", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "Aug 10, 2020", "body": "Immediately after installing the extension the onboarding screens should be displayed for users to make choices about telemetry and be educated on how to use Firefox Voice.Currently there appears to be a bug where new installations do not see the onboarding screens until after they press the microphone icon.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Aug 10, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1898", "issue_status": " Open\n", "issue_list": [{"user_name": "leo-lb", "datetime": "Aug 9, 2020", "body": "Hello!The initiative that Mozilla has started is interesting, but I am a bit troubled that this requires \"cloud\" services. I think Mozilla's initiative is especially interesting if it's offline and can operate locally, with the same performance as if it was running in a \"cloud\". I am personally not at ease sending my voice over the Internet even with promises of it not being saved.I am curious what are blockers to this and propose tracking them here.Probably those could be around:How does this exactly work on the \"cloud\" side? What would it take moving the \"cloud\" side entirely on the client side?I would imagine it's possible to train a model using \"cloud\" resources and later execute that model locally.Thank you!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1897", "issue_status": " Open\n", "issue_list": [{"user_name": "novellac", "datetime": "Aug 7, 2020", "body": "\nOn the  and  pages, the button element in the header marked \"Install Now\" doesn't do anything.\nNot quite sure!  - If an actual action isn't required here, perhaps use the same link which is used on the  on the .", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1896", "issue_status": " Open\n", "issue_list": [{"user_name": "novellac", "datetime": "Aug 7, 2020", "body": "\nThe Privacy Policy and Lexicon links at the top of the Lexicon page currently lead to 404s.\nEach link should lead to a valid page.\nIn partials/pageHeader, the links should be amended:", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1893", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Aug 6, 2020", "body": "If you are in the options page and select a new voice, then we should immediately play a sample using that voice. It could be as simple as saying \"this is a test\" in the new voice.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1843", "issue_status": " Open\n", "issue_list": [{"user_name": "12people", "datetime": "Jul 23, 2020", "body": "It'd be great if Firefox Voice could run  runs.\nMyCroft's skills all appear to be open-source.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 24, 2020", "body": "I believe MyCroft is running everything on a device with Python. So that's two barriers: a browser is... well, it's a device of its own, different than Linux. And it doesn't run Python (without a lot of work).I think our emphasis is on routing things to web-based services. So if we could send an utterance to a MyCroft device, then that would be quite sensible. You'd need to configure your personal MyCroft device. But if MyCroft has a local web page where you can type in some text (or put it in the query string) then the configuration could be as simple as saying \"this is my MyCroft page\" with the appropriate page open.That would open a second question: which phrases do we forward to MyCroft? The easy thing is to give the device a name and require the user to say \"tell mycroft to [something]\". We could also consider a configurable fallback service (Google is essentially our fallback service), but I doubt MyCroft would be useful that way – it doesn't handle random queries particularly well. The fanciest would be for MyCroft to either publish some phrases, or dynamically tell us if it thinks it can parse a phrase well. There's some danger there in MyCroft being greedy about handling phrases, but it would be an interesting question to try to figure out.Lastly, we just take some of those skills and replicate them here. Whenever possible we'd like to forward things to a web-based service, though in some cases implementing something natively is helpful, for instance there's reasons for us to have native timers.", "type": "commented", "related_issue": null}, {"user_name": "lb803", "datetime": "Aug 24, 2020", "body": "Someone on Mycroft forum asked how to submit utterances via api requests. Would this work for the present case?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1876", "issue_status": " Open\n", "issue_list": [{"user_name": "lisa-wolfgang", "datetime": "Aug 4, 2020", "body": "Firefox Voice's default keyboard shortcut is also the keyboard shortcut for opening the Firefox Multi-Account Containers extension.Furthermore, this cannot be changed during the onboarding process, which may turn away Containers users.", "type": "commented", "related_issue": null}, {"user_name": "GeraldNDA", "datetime": "Aug 5, 2020", "body": "Probs a dupe of  ?", "type": "commented", "related_issue": null}, {"user_name": "strorozhsergeich", "datetime": "Aug 5, 2020", "body": "The same issue and more. I am not able to change the default keyboard shortcut. Input box does not register my input. If, however, I am starting the new shortcut with a Key instead of MOD character ( e.g. A ) it detects my input and displays a message suggesting to start with one of three valid MOD buttons Ctrl, Alt or Shift. Should I report it as a new issue, perhaps?", "type": "commented", "related_issue": null}, {"user_name": "lisa-wolfgang", "datetime": "Aug 5, 2020", "body": "You have to type out the modifier keys with your keyboard: I believe actual key detection is a feature being worked on.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1828", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 22, 2020", "body": "This is an incomplete concept and will require some discussion.The cards we get for many search results are quite valuable, but limited to Google search results. What if you could teach Firefox Voice to display other cards? While this is connected to the popup UI, it's also a start for generating voice, and for operating in the background and extracting the most relevant information.For instance, let's say I want to make a card from the . It looks like:But the card might look like this:This one is a little bit of a pain, as there's no good element that contains the card, I'm hoping usually there is.This card might be a static response to a particular phrase, like \"check irobot\". Maybe to create this you'd open the page, say \"create card check irobot\" and then select the card by dragging or hovering.Or, if we know  how to load these cards, then we can respond to \"search yahoo finance for irobot\" by loading that page in the background, and showing the card if it's available (focusing the page if not). We'd have to know that these pages should be loaded in the background.We don't have any way to create a simple alias for something like \"search yahoo finance for irobot\" as \"check stocks for irobot\", especially since it includes a slot. That might be nice, especially since Yahoo Finance could have a bang search but accessing other sites may be harder.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1826", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 21, 2020", "body": "One reason you might open the popup and say nothing is because Firefox Voice is listening to the wrong microphone. If we don't detect any speech we should display the microphone name so the user can understand if that's the problem.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 27, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1822", "issue_status": " Open\n", "issue_list": [{"user_name": "shelbyKiraM", "datetime": "Jul 21, 2020", "body": "Pretty straight forward? I don't want to give them ANY data I don't have to, that's a big part of why I use Firefox! When I do a thing like say \"Open DuckDuckGo\", it redirects through  for some reason!", "type": "commented", "related_issue": null}, {"user_name": "shelbyKiraM", "datetime": "Jul 21, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 27, 2020", "body": "@marv3lls Firefox Voice should respect the default search engine you've selected in the browsers preference. Do you have DuckDuckGo set as your default search engine?", "type": "commented", "related_issue": null}, {"user_name": "Vinnl", "datetime": "Jul 28, 2020", "body": " I have the same issue and yes, it's my default search engine:", "type": "commented", "related_issue": null}, {"user_name": "shelbyKiraM", "datetime": "Jul 30, 2020", "body": "Yes  ,  's comment shows basically the same as my setup.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 30, 2020", "body": "@marv3lls  to clarify when you do a voice search such as  the results you see are from your default search engine (DuckDuckGo) but when you navigate using a phrase such as  Google is acting as the referral and you'd prefer your default search engine be used in that instance?", "type": "commented", "related_issue": null}, {"user_name": "Vinnl", "datetime": "Jul 30, 2020", "body": "That phrase goes to Google as well, but doesn't execute a search:(Sorry, my computer was struggling while I recorded that.)", "type": "commented", "related_issue": null}, {"user_name": "Duckbilled", "datetime": "Aug 14, 2020", "body": "Yes I have the same issue as Vinnl.Also, when I say \"use duckduckgo to search for Chickens\" It opens one empty google page and one google page that searches for the whole sentence.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 30, 2020", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1819", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "Jul 17, 2020", "body": "Audio output creates a more hands free experience which we've seen as a compelling use case for voice interactions on desktop. To ensure the features isn't overlooked because it's buried in the preferences it should be enabled by default with an option to turn off from the popup window.Changes to make for audio output", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 17, 2020", "body": "Attaching speaker icons for enabled/disabled states\n", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 17, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1808", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jul 13, 2020", "body": "\"This is a test\" gives this:The background of the checkmark isn't great.Also the popup should probably stay open slightly longer than it does currently.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Jul 13, 2020", "body": "Can we use the existing checkmark image that's typically seen when an intent is completed?Seen here: ", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 24, 2020", "body": "Yes, I think we could put that in, that would require using the Zap component instead of an image.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1752", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "Jun 18, 2020", "body": "We should add a boolean about whether there was voice output, and another about whether the pref is on (since it may be on, but the intent didn't resolve into any audio).I'm thinking maybe both should be text fields, NULL for none, and the  of audio generation indicates their was audio (\"card\" being the first kind). And maybe also for the pref: what, if anything, indicates that we should have audio output? That could be a pref, a wakeword, or a phrase modifier.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 29, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "PascalUlor", "datetime": "Jul 7, 2020", "body": [], "type": "issue", "related_issue": "#1794"}, {"user_name": "ianb", "datetime": "Jul 29, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1704", "issue_status": " Open\n", "issue_list": [{"user_name": "awallin", "datetime": "May 29, 2020", "body": "Users want to skip forward and back while listening to an article in reader mode. We should support the following commands in reader mode:Bonus points if we can supportSee There's also the question of context as 'back' and 'forward' triggered page navigation in the above case.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 18, 2020", "body": "As a first pass, let's just let \"forward/backword\" execute a click on these controls that are already available in reader mode:", "type": "commented", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 14, 2020", "body": " As per your suggestion I have added Intents for Foreword and Backward in reader mode.\nCan you please check it once? Thanks.\n", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 18, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Jun 23, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 14, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 15, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 15, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Aug 4, 2020", "body": [], "type": "removed their assignment", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1681", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "May 27, 2020", "body": "Watching Firefox close down, I notice a lot of error messages about message handlers coming from Firefox Voice. I think this might be cases where sendMessage is called, and the promise hangs or doesn't return.If we put in place some more structured message handling, we could also put in warnings about slow message responses. These might be cases where we simply never resolve a message, and it waits indefinitely for the response.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 18, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 9, 2020", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1663", "issue_status": " Open\n", "issue_list": [{"user_name": "ianb", "datetime": "May 20, 2020", "body": "This doesn't technically involve \"voice\", but it's relatively easy to imagine configuring new hotkeys to run a voice command (which could itself be a routine).Inspired some by , and ", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1645", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "May 13, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA Google Drive account is logged in and a Google Docs or Google Slides file is opened and made the default notes taking tab with the \"Make notes here\" command.Note is written in the notes taking tab.Nothing happens.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-12).\nNot reproduced if Google Sheets is made the default notes taking tab.", "type": "commented", "related_issue": null}, {"user_name": "Ishakikani9117", "datetime": "Jun 14, 2020", "body": " can I work on this issue?", "type": "commented", "related_issue": null}, {"user_name": "Ishakikani9117", "datetime": "Jun 21, 2020", "body": " I worked through this issue. For adding notes we are focusing on \"document.activeElement\" and this element is either iframe or body. We also cannot access the content of the iframe due to the\nsame origin policy.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 13, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1642", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "May 13, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA tab in narration mode is made active, like a New York Times article where the \"Read\" command is made.If tab is not one of the available music services, a corresponding message is displayed.\"Internal error: TypeError: service.mute is not a function\" is displayed in the doorhanger.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-12).\n\"Internal error: TypeError: service.unmute is not a function\" is displayed in the doorhanger, if the \"Unmute audio\" or \"Unmute music\" command is made (error appears either if tab is manually muted or not).", "type": "commented", "related_issue": null}, {"user_name": "AbhiVaidya95", "datetime": "Jul 14, 2020", "body": " The right utterance is mute which refer to Muting.mute intent, when we are using Mute music its going to music intent so we need to do error checking in Music mute intent is this the right approach?I have done some changes let me know if the approach is correct so I will commit the code.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 13, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1626", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nYoutube is made active and a video is playing.Muted video player is unmuted.Muted video player is not unmuted.Reproduced on Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nNot reproduced if player audio icon is manually muted and the \"Unmute music\" command is made.", "type": "commented", "related_issue": null}, {"user_name": "Ishakikani9117", "datetime": "Jun 16, 2020", "body": "  I think my pull request   can solve this issue.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 8, 2020", "body": [], "type": "issue", "related_issue": "#1625"}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1624", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\nA non music related tab is made active.If no music is playing, display a message in the doorhanger that volume can't be decreased or increased.A new tab opens with the most used music service or the default music service chosen in the \"Preferences\" page.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nIf most used or default music tab is already opened, the commands make the tab active.\nDon't know if it's possible to make the commands act more generally like the \"Mute\" command, where they act upon a playing tab, even if the command is made from another tab.Reproduced also for the \"Mute music\" and \"Unmute music\" commands when music tabs are closed. On Soundcloud the \"Mute music\" command opens the tab but it also performs the action. If most used or default music tab is already opened, the commands act as expected. (2nd gif)Spotify gives various errors which will be filed separately.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1623", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": "Mic permissions and autoplay are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.Youtube opens in a new tab and plays the first playlist found corresponding to \"rock playlist\".\"Internal error: TypeError: service.playPlaylist is not a function\" is displayed in the doorhanger.Reproduced on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-06).\nFiled because it can work similar to commands like \"Play rock album on Youtube\" which are working to open the first playlist from the Youtube search. Also the term \"Playlist\" is frequently used on Youtube.", "type": "commented", "related_issue": null}, {"user_name": "melvin2016", "datetime": "May 9, 2020", "body": "let me look into it.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 7, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/2641", "issue_status": " Open\n", "issue_list": [{"user_name": "aisensiy", "datetime": "Jul 9, 2022", "body": "\nA clear and concise description of what the bug is.I am trying to use label-studio with the feature of  in a template of . But I found that the same prediction from my ML backend will be triggered again and again.I am not sure if this is the LabelStudio's responsibility or my ML backend's responsibility to avoid this behavior.\nSteps to reproduce the behavior:\nA clear and concise description of what you expected to happen.I think LabelStudio show also know that the current task is already get annoated by the ML backend so that the same annotation will not be added twice.\nIf applicable, add screenshots to help explain your problem.Here is a screen record to show the behavior (no voice).In the video you can see no matter how many times I annotated the text, the same prediction result from ML backend will be added to the text.\nAdd any other context about the problem here.", "type": "commented", "related_issue": null}, {"user_name": "KonstantinKorotaev", "datetime": "Jul 12, 2022", "body": "Hi \nCould you please check what your ML service returned in last calls?", "type": "commented", "related_issue": null}, {"user_name": "aisensiy", "datetime": "Jul 12, 2022", "body": "Do you mean the predict result? The result looks like this:And here is the  in my label-studio-ml-backend:It is based on the . I just use the first three letters to generate a dummy result and return.", "type": "commented", "related_issue": null}, {"user_name": "KonstantinKorotaev", "datetime": "Jul 12, 2022", "body": "Yes, do you have a sequence of calls that lead to duplicated results?", "type": "commented", "related_issue": null}, {"user_name": "aisensiy", "datetime": "Jul 12, 2022", "body": "Yes. I do have a sequence of actions to manually add some more annotations not covered by the ml backend. But I think it is not necessary to return the prediction multi times. Every time when I do some actions (even when I delete some annotations) the same prediction result will be added.", "type": "commented", "related_issue": null}, {"user_name": "aisensiy", "datetime": "Jul 18, 2022", "body": "Any process about this issue?", "type": "commented", "related_issue": null}, {"user_name": "KonstantinKorotaev", "datetime": "Jul 19, 2022", "body": "Hi \nI have created a new feature request for it.", "type": "commented", "related_issue": null}, {"user_name": "aisensiy", "datetime": "Jul 9, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 19, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/3902", "issue_status": " Open\n", "issue_list": [{"user_name": "avgorbi", "datetime": "Oct 4, 2018", "body": "It`s look like no action on battery discharge.Add autolanding when battery lowWhen battery capacity bellow \"Critical Capacity (%)\", activate autolanding procedure.", "type": "commented", "related_issue": null}, {"user_name": "teckel12", "datetime": "Oct 4, 2018", "body": "In my opinion, I'm not so sure this would be helpful. First, you'd really need a current sensor due to voltage sag. Secondly, RTH is way slower than I can fly home. It also couldn't be tied to only critical capacity as distance from home would also make a huge difference.On my Mavic the auto RTH mostly works well for two reasons.What I've found that works very well with INAV is just knowing what the battery capacity is while flying. This can easily be accomplished from the transmitter via telemetry. You can then use a Lua script or functions to give voice announcements of your battery level and warnings.Basically, even if this feature existed, I don't think is turn it on as it wouldn't really be as useful as it seems. And cause more problems (like flying till the battery is at 10% critical level from 1km away, causing a power failure half way back).", "type": "commented", "related_issue": null}, {"user_name": "avgorbi", "datetime": "Oct 25, 2018", "body": "Of course, any function will work well only if it is properly configured.But it will be better for mine if the copter will land softly, even though far from home, than it will fall like a stone when cut off at the speed regulator.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Dec 24, 2018", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "Hennyr", "datetime": "May 13, 2019", "body": "I think this feature helps to prevent accidents", "type": "commented", "related_issue": null}, {"user_name": "teckel12", "datetime": "May 13, 2019", "body": " If it was realistically possible.", "type": "commented", "related_issue": null}, {"user_name": "shellixyz", "datetime": "May 15, 2019", "body": "I guess it can be done with the sag compensated battery voltage as source. I probably won't use this feature but if people want to...", "type": "commented", "related_issue": null}, {"user_name": "teckel12", "datetime": "May 16, 2019", "body": " There's a lot of variables, calculations, and total \"fudge factor\" to consider. Different kinds of RTH, different RTH speeds (not just flying, but also up and down), distance to home, wind speed, etc.  I just don't see it as being even remotely accurate.I feel the only way it can work is to trigger RTH at an unrealistic voltage, like 4.0 volts, just to be sure. And in that case, no one would use it.If we had known hardware like DJI it would be possible, but I think it would need to overestimate so much it would be useless. But I'd love to be proved wrong.", "type": "commented", "related_issue": null}, {"user_name": "shellixyz", "datetime": "May 16, 2019", "body": " Why are you talking about RTH ? The OP only asked for landing in case the battery is almost empty to at least try to get it back in one piece. But I'm not convinced this is very useful either.", "type": "commented", "related_issue": null}, {"user_name": "teckel12", "datetime": "May 16, 2019", "body": " Sorry, serves me right for browsing quickly and assuming.  There were previous requests for RTH for low battery, and I just incorrectly assumed.  \"auto land\" is never an option for me, as my primary flying locations are near/over water, so \"land\" guarantees a lost/destroyed model for me.", "type": "commented", "related_issue": null}, {"user_name": "SeanTheITGuy", "datetime": "Sep 13, 2019", "body": "This is stale, but to add my 2c.  Either having the option to trigger failsafe or even just scaling the throttle harshly when the critical voltage level is hit would be a huge benefit.  I can't find reference to any specific feature or code in Betaflight that actions this, but I have the feeling that when I get to near the end of a pack in Betaflight, it seems to scale the throttle to where even at full, the quad is still descending.  (Though this could just be the result of voltage drop of my smaller mah packs) This would be far preferable to a sudden out of control tumble when you over discharge a pack accidentally.", "type": "commented", "related_issue": null}, {"user_name": "michalelektryk", "datetime": "Apr 5, 2020", "body": "I'm using this option but implemented as hack / workaround in taranis, where one channel is sending back battery voltage and FC is using it to trigger landing. Not really well done but it's the only way", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Dec 24, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "shellixyz", "datetime": "Jan 1, 2019", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/jovotech/jovo-framework/issues/1237", "issue_status": " Open\n", "issue_list": [{"user_name": "JRMeyer", "datetime": "Feb 16, 2022", "body": " hi there!Would be great to be able to test and debug a voice bot  an internet connection. Offline STT and TTS (from ) would make this possible using the existing UX from the new Currently there's no offline STT or TTS", "type": "commented", "related_issue": null}, {"user_name": "jankoenig", "datetime": "Feb 16, 2022", "body": "Hi there. Thank you.This is not on our immediate roadmap, but would be a great community contribution.Coqui STT could be implemented as .", "type": "commented", "related_issue": null}, {"user_name": "JRMeyer", "datetime": "Feb 17, 2022", "body": "Hi  -- just looked into the integration with Lex, and it would be considerably different with Coqui because the user would have their own server running. For example, the user might be running a simple server on their local desktop or they might have spun up a server on their AWS cloud, and using endpoints there. In either case, the API syntax and integration would be identical, but there would be an expectation that the user spins up the server themselves. Not too difficult, but I'm not sure if that's something the Jovo crowd would be interested in.I think the biggest value add for Jovo users would be to be able to test out their voicebots locally, without having an ASR backend running on one of the providers (like Lex).Thoughts?", "type": "commented", "related_issue": null}, {"user_name": "jankoenig", "datetime": "Feb 18, 2022", "body": "This could work similar to our  where people also have to run their own servers.An integration like this would also be useful for our web starters:", "type": "commented", "related_issue": null}, {"user_name": "JRMeyer", "datetime": "Feb 18, 2022", "body": "Yeah, I think a general setup mirroring the Snips approach would work nicely. You know of anyone in your community who might like to hack on this? We're happy to offer support/guidance for using the Coqui tools.", "type": "commented", "related_issue": null}, {"user_name": "rubenaeg", "datetime": "Feb 18, 2022", "body": "I think I could give this a spin :)", "type": "commented", "related_issue": null}, {"user_name": "rmtuckerphx", "datetime": "Aug 26, 2022", "body": " Are there any developer docs on the Coqui APIs for STT and TTS using Node.js or REST?", "type": "commented", "related_issue": null}, {"user_name": "jankoenig", "datetime": "Feb 16, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jankoenig", "datetime": "Feb 16, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/361", "issue_status": " Open\n", "issue_list": [{"user_name": "nxturistic", "datetime": "Mar 23, 2022", "body": "Hey Louis, an excellent project by the way. Can you please help me with this Azure TTS implementation?\nI'm having some issues with the  file. I followed the steps mentioned in .But whenever I pass the string input in the browser, I get the following error when leon starts talking.Can you please look into it and let me know what's the issue, I want to contribute to your project in this way .", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Mar 25, 2022", "body": "Hi ,Thanks for willing to contribute! Sure I can check, would you mind to open a draft PR? ", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Mar 26, 2022", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "nxturistic", "datetime": "Mar 23, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#362"}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/344", "issue_status": " Open\n", "issue_list": [{"user_name": "superflewis", "datetime": "Feb 2, 2022", "body": "pi@alchemyvoice:~/.leon $ npm run check.: CHECKING :. node --version\n v17.1.0 npm --version\n 8.1.2 pipenv --version\n pipenv, version 2022.1.8 pipenv --where\n /home/pi/.leon/bridges/python pipenv run python --version\n Python 3.7.3 pipenv run python bridges/python/main.py scripts/assets/query-object.json\n {\"package\": \"leon\", \"module\": \"randomnumber\", \"action\": \"run\", \"lang\": \"en\", \"input\": \"Give me a random number\", \"entities\": [], \"output\": {\"type\": \"end\", \"codes\": [\"success\"], \"speech\": 23, \"options\": {}}} NLP model state\n Found and valid Amazon Polly TTS\n Configured Google Cloud TTS/STT\n Configured Watson TTS\n Watson TTS is not yet configured Offline TTS\n Cannot find bin/flite/flite. You can setup the offline TTS by running: \"npm run setup:offline-tts\" Watson STT\n Watson STT is not yet configured Offline STT\n Cannot find bin/deepspeech/lm.binary. You can setup the offline STT by running: \"npm run setup:offline-stt\".: REPORT :. Here is the diagnosis about your current setup\n Run\n Run modules\n Reply you by texting\n Amazon Polly text-to-speech\n Google Cloud text-to-speech\n Watson text-to-speech\n Offline text-to-speech\n Google Cloud speech-to-text\n Watson speech-to-text\n Offline speech-to-text Hooray! Leon can run correctlyI typed a greeting, Leon responded with text. But did not produce an audio response.My setup is a RaspberryPi with an Adafruit Voice Bonnet", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Feb 2, 2022", "body": "Hi  ,Thanks for opening this issue.Now to answer your questions:Yes, if it is correctly configured you should expect an audio response.As long as you allow your browser to access your audio input, it should work.", "type": "commented", "related_issue": null}, {"user_name": "superflewis", "datetime": "Feb 2, 2022", "body": "Hi , thanks for your fast reply!Browser console was showing:\nMediaDevices.getUserMedia() is not supported on your browser.\nI've fixed that. Currently...1.) No audio from Pi or browser when this shows:\n.: LEON :.\n Talking...2.) The mic icon activates. When it stops, only this is showing:\n.: ASR :.\n Encoding WebM file to WAVE file...\n Encoding done\n Parsing WAVE file...No errors in console, tested browsers in Win/Linux.\nMic tested at mic-test.comJust to confirm, is this not a correct expected behavior?:", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Feb 3, 2022", "body": "Thanks for sharing more details!This should work. Can you please try to install Leon on your PC with the current configuration and see if it works by using the PC only? Maybe there is an issue about the audio source/output when streaming from a Pi.Also, can you please share the content of your  file?", "type": "commented", "related_issue": null}, {"user_name": "superflewis", "datetime": "Feb 2, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/206", "issue_status": " Open\n", "issue_list": [{"user_name": "dhequex", "datetime": "Nov 20, 2020", "body": "I would like to help with translations for Japanese language.", "type": "commented", "related_issue": null}, {"user_name": "arlyxiao", "datetime": "Mar 28, 2021", "body": "TTS and STT both only support English right now.", "type": "commented", "related_issue": null}, {"user_name": "Becker-Asano", "datetime": "Aug 6, 2021", "body": "German language support would also be nice.\nTTS from Google-Services also supports German and Japanese: \nSST is also supported by Google-Cloud-Services in German and Japanese: Anyone here interestes in teaming up to get these to languages running?", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Dec 28, 2021", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Dec 28, 2021", "body": "Hi ,Thanks all for willing to contribute with more languages support. I know this is a late answer, but I'm back on working on the core of Leon.I plan to improve a better support of languages scaling. Also, I took note so that we can work together if you are still interested.", "type": "commented", "related_issue": null}, {"user_name": "blokhin", "datetime": "Feb 7, 2022", "body": "I'd be definitely interested in digging into the Turkish or Russian language support, even without the TTS/STT, but just the basic NLP/NER.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Feb 8, 2022", "body": "Thanks for willing to support! I will let you know once more languages will be added. First I need to rework some architecture to make Leon more scalable in terms of language.", "type": "commented", "related_issue": null}, {"user_name": "dhequex", "datetime": "Nov 20, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/198", "issue_status": " Open\n", "issue_list": [{"user_name": "andresdev16", "datetime": "Sep 10, 2020", "body": "Hi, I've been working in the code and i added a new free TTS engine for windows, i used  like alternative, but in the code i had many questions that i realized in the all TTS engine's code and it's the need of save the audio in a .mp3 on the /tmp/ folder, Why we need that? and i can work with that TTS engine without save that file?", "type": "commented", "related_issue": null}, {"user_name": "Becker-Asano", "datetime": "Aug 6, 2021", "body": " , why would gTTS be necessary? Isn't it just using Google-Voices in its backend, which is already possible to configure in Leon anyways?", "type": "commented", "related_issue": null}, {"user_name": "andresdev16", "datetime": "Sep 10, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Divlo", "datetime": "Apr 24, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 25, 2021", "body": [], "type": "removed their assignment", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/363", "issue_status": " Open\n", "issue_list": [{"user_name": "aaronchantrill", "datetime": "Jun 21, 2022", "body": "Like most Voice Assistants, Naomi's intents serve multiple purposes. First, they are used by the speech to text system to prepare a dictionary of words to recognize. Next they are converted into a language model to help the speech to text system guess what it is most likely hearing given the likelihoods of different arrangements of words. Finally, it is used by the text to intent system to figure out which intent to trigger.When developing the format for creating grammars for Naomi speechhandler plugins, I created a structure format where the grammar is split into keywords and phrases, with keywords providing a list of options in a phrase. This was similar to the way grammars are constructed for intent parsing systems I have looked at and was a simple way to move Naomi from simply spotting keywords to reacting to more complex utterances, but has a few big problems:There are a few grammar formats out there; JSGF, Nuance, ANTLR, SRGS, etc. SRGS seems to be a W3C specification, but I see very little support for it, . JSGF has been around a long time and there is a pyJSGF library on PyPI which could be helpful. DeepSpeech/Coqui can use JSGF files directly, so I propose that we use JSGF grammar format for building Naomi intents, unless someone has a reason to prefer a different format.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Jun 26, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/leon-ai/leon/issues/234", "issue_status": " Open\n", "issue_list": [{"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": "Support for modern TTS models for various languages without the need for external TTS APIs.Consider giving a go to Silero TTS models. These are published under an open license assuming non-commercial / personal usage. Please see our TTS models here -  (corresponding article ).What is most important our TTS models can run on one CPU thread / core decently and depend mostly only on PyTorch.", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": "Also please note that this is just a  release, models will be much faster in future", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 2, 2021", "body": "Hello  ,Thanks for suggesting, it looks promising!May I know if you have any Node.js binding? As Leon's core is built on the top of Node.js.", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": "We just base our models off PyTorch and / or ONNX\nAs far as I know there are no actively maintained node-js bindings for PyTorch\nThere are though for ONNX, but we could not yet port our TTS models to ONNX", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": "Internally in such cases (where the controlling app and the inference engine are not the same) we just use rabbit-mq communication with a model in a separate container", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 2, 2021", "body": "I see. For the moment Leon does not rely on a broker for such operation but directly on Node.js binding. However, it can be a good path to explore. I'll add it to the roadmap.", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 2, 2021", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 2, 2021", "body": "For reference, do you have any online demo of the output that you can share?", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": "Please see this article - it has plenty of audios - \nOr just use the colab - Since we do not have web developers, we do not active develop fully online web demos\nColab can be considered \"online\" since it works in real-time in a notebook", "type": "commented", "related_issue": null}, {"user_name": "jankapunkt", "datetime": "May 4, 2021", "body": " is it possible to have these voices being rendered into ? Then there would no issue which backend had been used to create the voices, right?", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "May 4, 2021", "body": "Hi,I could not really understand from their example where / how the actual speech synthesis is run / storedThere is an example code here -  - but I do not see any models here", "type": "commented", "related_issue": null}, {"user_name": "snakers4", "datetime": "Apr 2, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "louistiti", "datetime": "Apr 2, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/267", "issue_status": " Open\n", "issue_list": [{"user_name": "aaronchantrill", "datetime": "May 3, 2020", "body": "Naomi should be able to respond differently to different users. If a family member asks \"do I have any emails\" it should not be necessary for Naomi to ask \"who are you?\" This would allow the user's voice to act as a sort of authorization. As part of the speech to text training, ultimately I would like to train a different acoustic model for each member of the family. Being able to identify the speaker by voice before selecting the acoustic model would make it possible to use an acoustic model optimized for the speaker, which should lead to better recognition overall.This could start allowing a database to be built around the user, and also help improve speech recognition", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Nov 27, 2020", "body": "I'm looking at using this project for an initial test: I already have had the NaomiSTTTrainer.py allowing you to enter a name for a while, so I have a database with a bunch of recordings labeled with my own name and just a few with other people's names. It would be interesting to see how many recordings are needed to differentiate between two individuals, and also how much audio is required to do a check.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Aug 30, 2021", "body": "I've been working with  and have a test project at . This package is easy to install on x86_64 systems (pip install speaker-verification-toolkit) but a pain on ARM (Raspberry Pi). To install it on ARM, you need to install version 11 of llvm first, which isn't really obvious from the error messages. Also, when building the package from source it is import to build it as type=Release or else you will run out of memory during the linking step.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 3, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 3, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Jul 11, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Jul 12, 2020", "body": [], "type": "added this to the", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/265", "issue_status": " Open\n", "issue_list": [{"user_name": "fracpete", "datetime": "Apr 27, 2020", "body": "So far, I've only come across readily available language models etc for the various STT/TTS plugins.\nMy question is, what steps are necessary in order to add a completely new language, e.g., an indigenous one, to Naomi? Code and/or configuration changes? What would be necessary to use DeepSpeech in such a scenario (I presume some form of training on a new audio corpus)?", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 28, 2020", "body": "Hey Peter. That's a good question. There is a sort of answer here:  with regard to adding Esperanto specifically, although nobody has yet gone through the whole process of adding a whole new language.As far as changes to the core naomi code, you would have to add your language as an option to the get_language method in naomi/commandline.py (this really isn't the right place for this function, so I imagine it will move eventually, and it should also be modified to key off of the list of .po files in naomi/data/locale instead of a hard-coded list) if you want people to be able to select it during the initial configuration (once a language is selected, Naomi should switch to communicating in that language, but the translation files for French and German currently need to be updated - see pull request ).The process of adding a new language could be broken down into three projects: STT (speech to text), TTI (text to intent), and TTS (text to speech).With Speech to Text, you first have to decide what engine you want to work with. Pocketsphinx and Deepspeech are both good choices for a more or less complete solution, and Kaldi is good for a solution once you are more comfortable with the concepts used in STT. They all have tutorials where building a new speech recognition model is discussed:Speech to text is generally broken down into three main concepts of acoustic model, phoneme to grapheme, and language model. The language model flows directly into the next project, Text to Intent, since it is using expectations to determine what it most likely heard.For Text to intent, you currently have to modify the intents() methods in each of the speechhandler plugins, and also generate new .po gettext translation files for the core and plugins.The intents() methods return a list of things the user might say to activate the plugin, which then get fed into whichever Text to Intent plugin you are using. Since there are different numbers of ways to say things in different languages, it just didn't work to use literal translations here. This gives an intent author better control of how an intent is constructed in a specific language, but does require someone who is adding a new language to do a lot more work, and modify every speechhandler plugin. There are instructions for writing intents here: I have considered defining a JSON format file for holding intents, so that someone adding a new translation would be adding new files, not modifying the intents() method of the plugin itself. One benefit of using that kind of file structure is that it could provide a fairly easy method of identifying plugins in the Naomi Plugin Exchange by the locales they are configured to work with, and possibly even allow people to attach additional translations to remotely hosted plugins without having to modify the plugin itself. Currently there is no indication on the Naomi Plugin Exchange of which languages a plugin supports.Generating the .po files is simply a matter of running the \"update_languages.sh -l <locale_identifier>\" with the locale identifier. If there is no locale identifier for the language you want to add, you can just make it up. It only has to be consistent within Naomi. This generates a bunch of files called <local_identifier>.po. Unfortunately, you have to go in and manually translate all the phrases in those .po files. This allows Naomi to translate its responses into another language to either display on the screen or say to the user.Last, you need Naomi to be able to say the response, so you need a Text to Speech system which is trained to speak your language so is able to pronounce the words being fed to it. If you have a mismatch between the locale and the voice, it can be difficult to understand (as an analog, I have been told by a Swiss friend that pronouncing Maori words correctly is much easier if you try to pronounce them in German than in English). Voice building can be a pretty complex task. Here are instructions for building a new voice for the Mary TTS system:  and Festival: So it is certainly not easy, but it can be done. If someone would be interested in doing this and documenting their progress, I think that would be incredibly helpful to others. The whole process is a lot easier if you can find a ready made STT model and TTS voice. If you are generating a new language from scratch then you will need a lot of labeled recordings. Naomi is able to help with that, especially if you can find a cloud provider that already provides STT and TTS in the target language. Then, once you have customized the intents and built translation files, you could use Naomi normally with audiolog enabled to build up a collection of labeled samples which could be used to build both the STT models and TTS voice.Please let me know if you have more questions or if you see any mistakes above. I hope that's helpful and not overly dense. I could go into a lot more detail, and would be willing to work directly with someone attempting to do this, especially if they would be willing to help document the process. Do you have a specific use case in mind?", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "Apr 30, 2020", "body": "Thanks for the detailed reply, I will have to mull over that a bit, as there is quite a bit of work involved. A possible use case would have been to add Maori as a language.\nBTW What do you think of meta-STT and meta-TTS wrappers?\nFor example, you could take the output of your base STT and push that through Google translate (from Maori to English) to avoid having to update all your plugins for handling this language. Any English text that would have to be spoken could be translated again, e.g., through Google translate again, before pushing it out through a Maori TTS. I could imagine that this kind of translation approach might work relatively well, as long as the incoming and outgoing text is relatively simple and short.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 1, 2020", "body": "I agree, it's a lot of work. I don't know that google translate could help all that much. My experience with google translate is that it works well enough to get the intent across, but rarely beyond pidgin level language.My biggest concern is with regard to 3rd party speechhandler plugins. I'd like to have an easy way for someone to add a translation \"pack\" to some else's plugin consisting of a .po translation file and an intent file, and for another user to then download the plugin with the added translation files.I have thought about expanding the \"update_translations.py\" program to use Google translate to generate the translation files, which should work fine in our current state without leaking usage data to Google. That way a user could generate all the translation files needed for their whole system to get Naomi up and running in a new language quickly and later go and fix the translations.For the time being, I see Naomi as more of a development kit than a finished product. My hope is that through being able to experiment with different technologies, we will eventually be able to come up with an effective platform that runs locally rather than in the cloud. Once something really seems to be working, Naomi could be used as a template to write a system optimized for the specific plugins.Have you ever used a piece of software called Simon ()? It used to be part of the KDE desktop, but the lead developer went to work for Apple's Siri division a few years ago and the project sort of fell apart after that. The idea was to have a simple means of controlling the desktop using voice, sort of like the scene in Bladerunner where Harrison Ford is zooming in on an image on his computer. One thing it included was the ability to train STT systems, especially the Julius STT engine. One of my visions for Naomi is to provide that same ability to train speech recognition engines which can then be re-applied to your own projects.You would still need both Speech to Text and Text to Speech systems with acoustic models optimized for the specific language, which I consider to be the most difficult part. Doing the actual translations is pretty straightforward.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": " can you recommend what form you would like this project to take? Do you want some specific documentation around it? This is kind of a big question, and given that Naomi uses plugins, it's impossible to give a specific set of instructions that cover every use case. At the same time, there are some definite steps that would always have to be done that can be documented along with some vague information about generating new STT and TTS models for those who need to. I'm just not sure how to resolve this.", "type": "commented", "related_issue": null}, {"user_name": "fracpete", "datetime": "May 4, 2020", "body": " had a discussion around this today. For the time being, we will concentrate on getting a handle on STT (DeepSpeech) and TTS (MaryTTS), with building up a speech corpus as the first step. Once model performance is satisfactory, we will look into a tighter integration into Naomi.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": "Wow, that's awesome. Definitely keep me informed.What I have learned from working with Naomi, though, is that you don't need perfect recognition to get good comprehension at the intent level, especially if you are willing to accept a pretty simple model where only one intent is triggered at a time.Adding some humorous responses can be a good strategy for generating some good will with users and keeping them engaged when the computer is having some trouble understanding them, as long as they don't happen too often.The point is that speech recognition will never be perfect, since even you are processing language at multiple levels to make sense of it. I have some hearing loss, so often what I actually hear is garbled, but I can usually work out the speaker's intent from context.A good illustration of the process of of the process our brains engage in for listening would be the \"Mares eat oats\" song which can sound like gibberish until you get to the \"wouldn't you?\" part of the song and realize that the whole thing has been in english the whole time.Since I started verifying/correcting transcriptions with the NaomiSTTTrainer.py software, I have gained a lot of understanding of how exactly the computer hears things and the process of matching the sounds up with a meaningful sentence. Often a little nudging in the language model is enough to get much better comprehension, and that is why edit distances and soundex type matching can be a huge help when dealing with spoken language.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 4, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Aug 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 20, 2021", "body": [], "type": "removed  the", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/212", "issue_status": " Open\n", "issue_list": [{"user_name": "aaronchantrill", "datetime": "Sep 17, 2019", "body": "This is an issue having to do with the new asynchronous mic.say_thread() method. Now that Naomi can talk and listen at the same time, Naomi can trigger itself by saying its own wakeword. For instance, this is what it looks like when Naomi first starts up now:So it is hearing \"Naomi\" and recognizing the wake word using the keyword vocabulary, then totally borking the \"How can I be of service?\" because the default vocabulary doesn't actually contain any of those words. This only showed up after I switched to using the SeeedStudio respeaker microphone with an external speaker. I have used several usb audio cards with Naomi without having this issue, so I assume most of them have some hardware echo cancelling built in. In this case, though, the respeaker is unaware of the audio out stream.Naomi should either recognize that it is hearing itself talk by recognizing its own voice, or just not hear itself at all, which would reflect a sort of selective attention. This could be accomplished by monitoring the audio out and applying a negative waveform to audio in. The goal is for Naomi to continue to hear what other people are saying as it is talking without really hearing itself. I want Naomi to react the same as when it is speaking to me through headphones, or when using the conference phone, which had noise cancellation built in.Naomi hears itself talking, and can trigger itself with wake words.Supposedly the jack audio library has some built in tools for this. Jack also has some other advantages that might be helpful. I'm going to attempt to write a jack audio engine.I was trying to use my current Naomi setup. The only real effect so far is that Naomi immediately declares that it doesn't understand a command upon starting up. After that, Naomi doesn't say Naomi again, so no longer reacts to its own voice, however, it does affect its ability to hear what I am saying when it is talking.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Mar 11, 2020", "body": "I have tried using the pulseaudio module-echo-cancel, but it seems to be more about noise cancellation than echo cancellation. I'll keep playing with it, but in the meantime I will modify Naomi to block listening while talking by default and provide an option to allow listening while talking which can be switched on by user either on the command line or in the yaml settings.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Mar 11, 2020", "body": "I have been having this issue with the Google AIY Voice kit, which I have been using by default for a while now. The device that worked well for me was a cheap little conference phone usb device and also a cheap little external usb sound card with 3.5mm input and output jacks. Neither of them were hearing themselves, and were quite capable of reacting to \"Naomi, stop\". The conference phone makes sense to have hardware correction, but I am impressed that the little usb sound card was able to work without knowing how loud the output would actually be.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 22, 2020", "body": "There are numerous solutions out there. Pulseaudio has \"module-echo-cancel\" which needs to be configured with an echo-cancelling engine like \"webrtc-audio-processing\", so it's not a simple setup and requires that the user be using pulseaudio.\nProbably the best thing to \"bake into\" naomi would be .", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Jan 31, 2022", "body": "After a lot of playing with ec, I have discovered that the pulseaudio module-echo-cancel is much easier to work with and does a good job of providing echo cancellation with default parameters. To get this to work, Naomi needs to be set up to use the pyaudio audio engine with the \"pulse\" device for both the input and output device.The module can be loaded in the Naomi function created by the Noami.sh script by adding the following:where the sink_master can be selected from  and the source_master can be selected from ", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Aug 1, 2022", "body": "This is basically solved. Naomi speaks synchronously by default, which prevents it from hearing anything when speaking. By setting the listen_while_speaking setting to \"true\", Naomi can listen while speaking, which allows the user to interrupt.There are several solutions to keeping Naomi from hearing what it is saying. The best is to use a speaker/microphone with integrated hardware echo cancellation - like a USB conference phone. The second best is to use the Pulseaudio module_echo_cancel module with an aec_method of \"webrtc\". If that fails to work with minimal effort, it would probably be better to purchase different hardware than to try to get echo cancellation working.If you come up with a better method or a method that works with specific hardware, please let us know.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 21, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 21, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Oct 4, 2019", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Oct 4, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Mar 11, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Mar 11, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Aug 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 22, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 22, 2020", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 22, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Sep 23, 2020", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/160", "issue_status": " Open\n", "issue_list": [{"user_name": "sbernhard", "datetime": "Jan 27, 2019", "body": "HiWould it be possible to add snowboy stt? See I would love to have a offline voice detection only and it looksike snowboy is one of the best.Best regards\nBernhard", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Jan 28, 2019", "body": "Snowboy support was added in 2016 but since then nobody has picked up the ball to support it. Currently the snowboy plugin contains a precompiled binary shared object built for arm on the raspberry pi. The project needs to be updated, and someone needs to also update the documentation so the module can be applied to other platforms. I don't see the plugin listed in the documentation, probably because it simply no longer works. If you are interested in picking up support for this plugin, we would definitely appreciate the help.What are the advantages of snowboy? What is it that makes it \"one of the best?\" I haven't ever used the system myself, so don't really understand their business model or what the benefits are.That being said, have you tried pocketsphinx? It is lightweight, fast, fairly straightforward to configure and good for simple tasks like keyword detection. It is offline, open source, and what I generally use for keyword detection myself.", "type": "commented", "related_issue": null}, {"user_name": "TSRBerry", "datetime": "Oct 15, 2019", "body": "I want to work on this, but it seems like it is already implemented as a stt plugin. What is the issue here?\nIs it outdated and it needs to be updated or is it just about the doc for other OS?Snowboy seems to do a really good job at recording a keyword and then detecting it, so no internet connection is required and the user trains the predownloaded model from  to recognize the new keyword. It should only consume a little amount of the cpu which makes it a great stt for rpi.", "type": "commented", "related_issue": null}, {"user_name": "TSRBerry", "datetime": "Oct 15, 2019", "body": "Okay I read a bit more on snowboy and it seems like the devs are inactive and issues won't get any answers. I think we can still update the current version of it but since this version is already outdated we can't expect any updates or improvements in the future.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Oct 15, 2019", "body": "I think the issue is basically just that the plugin is undocumented. I imagine that is because it doesn't currently work, but I don't know because I've never tried it. I know there is a binary file in there that is pre-compiled for raspberry pi. It would be good to try to locate and link to the source code or official download page for this file so users can use this plugin on other platforms.If you find that the plugin works in its current state, then just update the STT documentation to cover it. If it doesn't currently work, then it would be necessary to fix the issues before creating the documentation.Naomi currently has a few suspiciously old STT plugins that really need to be tested and verified.", "type": "commented", "related_issue": null}, {"user_name": "redragonx", "datetime": "Jan 28, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Jan 29, 2019", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Feb 7, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Oct 4, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Nov 3, 2019", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Aug 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Aug 30, 2020", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/NaomiProject/Naomi/issues/67", "issue_status": " Open\n", "issue_list": [{"user_name": "aaronchantrill", "datetime": "Jul 29, 2018", "body": "It has always bothered me that when the user first types \"./Jasper.py\" it just errors out with a message that the configuration file does not exist. The documentation around creating a configuration file has always been awful, and successfully creating a configuration file at this point requires reading the code itself looking for options. There is a program called \"populate.py'\" in the jasper directory that starts the job of configuring jasper, but it is fairly useless being that it is missing a lot of options and doesn't produce an actual working configuration anyway.I would like to alter the beginning of \"application.py\" to ask if you would like to create a configuration file, then alter \"populate.py\" to at least get you part of the way there.This project also touches on issue 15 (if you select \"pocketsphinx\" as your engine, it should download and configure pocketsphinx automagically, if you select deepspeech as your engine, it should download and install deepspeech automagically), issue 16 (gmail password stored in plain text), and issue 57 (support for email services other than gmail).I will not try to solve these issues in this fix, but will at least try to get it to the point where I can successfully use it to create a configuration file given that I already have pocketsphinx and festival installed. I am also doing a little cleanup in making the language selection the first question that appears so that (given that someone has gone through the trouble to write a translation file) the rest of the configuration can be done in the user's chosen tongue.Eventually, the goal would be to get this all working in such a way that the configuration can be done primarily verbally, with the text interface only being a backup for when you need to enter a complex email password.", "type": "commented", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": "Considering there are 3 options to get this job done: Ask the user in  if he want to create his profile (only if the profile doesn't exist)  and if yes, switch to  Add an argument to the jasper execution  to create your profile using  too.\nSee  and  in the  function Both  and  but  would be to recreate your profile if you missed/want to change somethingI'm currently working on a User Interface with The idea is to maintain your Naomi in your device from this interface only, meaning you'll be able to create, delete, and modify your profile from it (stt engine, tts engine, email adress, weather credentials ...), way more friendly-user than the current wayYou'll also able to see properties of your devices (device cpu type, memory/cpu usages, uptime) system related informations ...Once this UI will be mature enough, we'll be able to remove the program , becoming obsolete and incompleteTheses points are for the I have a lot of ideas for the  ...I'll push it  soonWhen the UI is out with , i'll create a Raspberry Pi image with all the dependencies installed, pocketsphinx included, and with the user interface.Users will just have to boot the raspberry pi with the burned micro sd card, go to the raspberry pi IP adress and setup their profile quickly and be able to use it, not more.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "May 18, 2019", "body": "We need to add the ability to specify passive, active and special mode speech to text engines separately. Also, right now plugin settings are only picked up for SpeechHandler plugins, but need to be expanded to the other speechhandler types (audioengine, stt, tts, vad)", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Aug 27, 2019", "body": "I am having a strange situation right now where somehow on my Buster install I have an incomplete profile.yml that is causing Naomi to crash withso somehow I managed to create a profile without specifying an output device. I think this is because it saves a copy of the profile.yml immediately after setting the language at this point, so exiting the application after starting but not finishing populate.py (say you suddenly realize that you don't have a tts module installed) leads to this unstable situation. I think that adding a \"settings\" check to the main routine that kicks off populate.py if any of the basic fields (first_name, last_name, etc) before initializing the audio devices is the way to handle this.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 7, 2019", "body": "Okay, I figured out what is causing the crash. Basically, each time I am running the pluginstore.detect_plugins() function, if there are any new plugins then I am saving that list to the profile. I can fix this by setting a profile arg for \"save profile\" set to false at the beginning of application.init() and then check it at the end (or at least after populate runs) to see if I need to save it. This is only a problem when the user kills populate before it finishes the first time.I'll put that in my next pull request.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Sep 8, 2019", "body": "By the way, a while back I added some code for playing a test phrase if the user selected a flite voice. I'd love to see that expanded to all voices if anyone has time to work on that.", "type": "commented", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Feb 5, 2021", "body": "In the audio device setup, we start by choosing an output device. Then Naomi plays a tone to the device and asks if we can hear it. In my last few setups, both on Raspberry Pi and VirtualBox, I have had to adjust the output_chunksize parameter to at least 2048 to avoid buffer underrun errors. I'm going to change that to the default, and also the ability for the user to modify output_chunksize, output_padding, and output_pause while getting the beep to work in the validate_output_device() method.", "type": "commented", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 30, 2018", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "TuxSeb", "datetime": "Jul 31, 2018", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#73"}, {"user_name": "aaronchantrill", "datetime": "Sep 17, 2018", "body": [], "type": "pull", "related_issue": "#101"}, {"user_name": "aaronchantrill", "datetime": "Oct 3, 2018", "body": [], "type": "pull", "related_issue": "#107"}, {"user_name": "AustinCasteel", "datetime": "Dec 19, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Dec 20, 2018", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Dec 31, 2018", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "AustinCasteel", "datetime": "Dec 31, 2018", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "aaronchantrill", "datetime": "Apr 6, 2019", "body": [], "type": "pull", "related_issue": "#171"}, {"user_name": "aaronchantrill", "datetime": "Apr 22, 2019", "body": [], "type": "pull", "related_issue": "#183"}, {"user_name": "aaronchantrill", "datetime": "Sep 8, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#230"}, {"user_name": "aaronchantrill", "datetime": "Apr 11, 2020", "body": [], "type": "pull", "related_issue": "#253"}, {"user_name": "AustinCasteel", "datetime": "May 26, 2020", "body": [], "type": "pull", "related_issue": "#274"}, {"user_name": "aaronchantrill", "datetime": "Feb 18, 2021", "body": [], "type": "pull", "related_issue": "#327"}, {"user_name": "aaronchantrill", "datetime": "Aug 18, 2021", "body": [], "type": "pull", "related_issue": "#344"}]},
{"issue_url": "https://github.com/mindslab-ai/voicefilter/issues/30", "issue_status": " Open\n", "issue_list": [{"user_name": "nnbtam99", "datetime": "Jan 27, 2022", "body": "Hello, I have two questions about the implementation.To obtain data from the Librispeech 360h + 100h, I generate the mixed audios for 360h and 100h separately, then add them together in another folder. Is this the right way when I want to use more data to train the voice filter module?Theoretically, I expect the voice filter module will benefit from the embedder trained on more data, but the results got even worse. Can you share how you train this embedder?Thank you in advance!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/639", "issue_status": " Open\n", "issue_list": [{"user_name": "Summerxu86", "datetime": "Jul 11, 2022", "body": "在gen_voice.py最后合成音频时报了这个错误:\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n是单纯的显存不够吗？还是和我在训练生成器时修改了batchsize有关？请问大家应该如何解决？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jul 16, 2022", "body": "训练和推理配置应该无关。", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/537", "issue_status": " Open\n", "issue_list": [{"user_name": "SchweitzerGAO", "datetime": "May 5, 2022", "body": "\n我在实验室的机器上训练，使用的是我自己收集的，并且用aishell3数据集格式标注的数据，总是报这个错误（见截图）\n\nOS: CentOS 7\nPython: Anaconda+python 3.8\npytorch:1.11.0\nCUDA:11.4\n", "type": "commented", "related_issue": null}, {"user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z", "datetime": "May 15, 2022", "body": "Look in the original repo RTVC and search ValueError in issues I saw the same problem being answered before.", "type": "commented", "related_issue": null}, {"user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z", "datetime": "May 15, 2022", "body": "Send your full notebook for better debug if you wish.", "type": "commented", "related_issue": null}, {"user_name": "SchweitzerGAO", "datetime": "May 15, 2022", "body": "Many thanks if that's convenient for you. By the way, could you please give me the link of the 'original repo of RTVC' please?", "type": "commented", "related_issue": null}, {"user_name": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z", "datetime": "May 15, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "SchweitzerGAO", "datetime": "May 15, 2022", "body": "Thanks  but which issue exactly did you refer to? I found many similar issues...", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/691", "issue_status": " Open\n", "issue_list": [{"user_name": "ms903x1", "datetime": "Jul 31, 2022", "body": "\n原文件似乎是为linux环境创建的，没有说明如何在windows下修改地址，请教一下如何在windows下修改ppg2mel.yaml文件里的地址，有好多文件我并没有在预处理后文件夹中找到\n\n环境：windows11，anaconda：python3.9.12数据集文件夹：C:\\test\\test8\\aidatatang_200zh预处理生成文件夹：C:\\test\\test8\\PPGVC\\ppg2mel我只能在预训练生成文件夹里找到原文件4,5,6行的同名的文件，这么修改对不对？剩下的7-14行怎么修改？\n\n直接运行报错\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/459", "issue_status": " Open\n", "issue_list": [{"user_name": "ganggang233", "datetime": "Mar 16, 2022", "body": "In ppg2mel.yaml，there is no details  about （train_fid_list：）which is in second line. hope your reply", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 17, 2022", "body": "you need to replace it with the file in result folder of preprocessing", "type": "commented", "related_issue": null}, {"user_name": "ganggang233", "datetime": "Mar 18, 2022", "body": "  Thanks for your reply, I will try.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/643", "issue_status": " Open\n", "issue_list": [{"user_name": "xiaoguazh", "datetime": "Jul 12, 2022", "body": "1)Use miven's model:\n code: 20212)Got exception\nsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).3)NOT ABLE to have Chinese voice, but only  noise.", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jul 16, 2022", "body": "请仔细看模型的适用代码版本，并搜索issue区解决", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/mindslab-ai/voicefilter/issues/20", "issue_status": " Open\n", "issue_list": [{"user_name": "weedwind", "datetime": "Sep 29, 2019", "body": "Hi, seungwonpark,I was trying to use Google's posted  for LibriSpeech to reproduce their results. But I can not even get their initial mean SDR (10.1 dB in their paper). I got only 1.5 dB. I am wondering have you tried their list and got around 10.1 dB for mean SDR before applying voice filter?Thank you so much.", "type": "commented", "related_issue": null}, {"user_name": "seungwonpark", "datetime": "Sep 29, 2019", "body": "Hi, \nThank you for letting me know! Yes, I was aware of that test list, but haven’t tried to measure the actual performance with that.Considering the followings, I think the experimental result (1.5dB, which turned out to be far worser than Google’s) is not really wrong:Shall we leave this issue open, since this is somewhat critical issue? Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "seungwonpark", "datetime": "Sep 29, 2019", "body": "TL; DR: (to the title of this issue)\nNo, I haven’t tried yet but I don’t think I can.", "type": "commented", "related_issue": null}, {"user_name": "weedwind", "datetime": "Sep 29, 2019", "body": "Hi, Thank you for your reply. I mean the SDR before applying the voice filter, not after. In Table 4 of their paper, this is the mean SDR in the first row, which is 10.1 dB. But I only got 1.5 dB. I used the same bss_eval python function as you did, just feed the function with the clean target utterance and the mixed utterance to compute the SDR before applying the voice filter. Do you have a clue why this SDR is so low?", "type": "commented", "related_issue": null}, {"user_name": "seungwonpark", "datetime": "Sep 29, 2019", "body": "Oh, looks like I had misunderstood your question. Sorry for that.\n10.1dB is relatively high SDR for the mixed audios to have. The authors of VoiceFilter mentioned that the SDR before VoiceFilter got high due to silent part of utterances being sampled and mixed. (Note that fixed length of audio segments are sampled here)\nBut I’m not sure why you’re not getting 10.1dB. Perhaps we should review the preprocessing part and the SDR calculation code in bss_eval.", "type": "commented", "related_issue": null}, {"user_name": "weedwind", "datetime": "Sep 29, 2019", "body": "I noticed that your code used the first 3 sec and threw away the rest. I did not use fixed length. I used the entire length of the target clean signal, and truncate or zero pad the interference signal to the same length. Then I computed the SDR. Did you ever compute the mean SDR for your test set?", "type": "commented", "related_issue": null}, {"user_name": "weedwind", "datetime": "Sep 29, 2019", "body": "Hi, I read your generator again. In your code, both w1 and w2 need to be at least 3 sec long. Then, you take the first 3 sec from them and add. So the resulting target utterance is fully interfered by the other utterance. Since they have the same volume, the SDR should be nearly 0 dB in this case. Why did you get a median SDR of 1.9 dB?", "type": "commented", "related_issue": null}, {"user_name": "seungwonpark", "datetime": "Oct 1, 2019", "body": "Not yet.Actually the value 1.9dB was not calculated from all datasets -- it was from a single dataset. I should fix the table in README accordingly.", "type": "commented", "related_issue": null}, {"user_name": "matnshrn", "datetime": "Mar 15, 2022", "body": " I'm getting the same results as you (1.5dB SDR over the google LibriSpeech test list), have you managed to solve this problem?", "type": "commented", "related_issue": null}, {"user_name": "matnshrn", "datetime": "Mar 16, 2022", "body": [], "type": "issue", "related_issue": "weedwind/CTC-speech-recognition#1"}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/530", "issue_status": " Open\n", "issue_list": [{"user_name": "siao888", "datetime": "May 4, 2022", "body": "\n您好\n在訓練合成器步驟\n\"注意在上一步先下載好ppg2mel.yaml, 修改裡面的地址指向預訓練好的文件夾： python ppg2mel_train.py --config .\\ppg2mel\\saved_models\\ppg2mel.yaml --oneshotvc\"不知道該怎麼修改地址.自己試了好久都失敗.\n\nIf applicable, add screenshots to help\n\n", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "May 4, 2022", "body": "/FakeVC/\n这样的路径应该不符合windows风格， 你要使用 D:\\xxx\\xx 或者 相对路径", "type": "commented", "related_issue": null}, {"user_name": "siao888", "datetime": "May 4, 2022", "body": "謝謝.\n我也試了.好像不行主要是.文件中應該修改第幾行?\n另外命令是\n這樣 python ppg2mel_train.py\n還是 python ppg2mel_train.py --config .\\ppg2mel\\saved_models\\ppg2mel.yaml --oneshotvc\n抱歉.很白癡的問題\n", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "May 4, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "siao888", "datetime": "May 4, 2022", "body": "好.謝謝.我再試試", "type": "commented", "related_issue": null}, {"user_name": "siao888", "datetime": "May 7, 2022", "body": "請問.我用自己的音頻做數據集\n一直出現錯誤訊息.\n我知道是音頻格式的問題.\n但是.轉換了很久.都轉不到他要的格式.\n有什麼方法可以做到呢?", "type": "commented", "related_issue": null}, {"user_name": "siao888", "datetime": "May 9, 2022", "body": "好了.大致上搞定了.\n現在應該是正常訓練了.\n感恩.\n", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "May 11, 2022", "body": "vc这部分我搞了一半觉得模型不是特别好，在用espnet练新的，有兴趣可以邮件我一起搞", "type": "commented", "related_issue": null}, {"user_name": "yxwudi", "datetime": "Jul 1, 2022", "body": " 能给我一个改好的样本截图吗？我也是改了好久没弄好。谢谢！", "type": "commented", "related_issue": null}, {"user_name": "siao888", "datetime": "Jul 5, 2022", "body": "不好意思.一陣子沒用了.那個文件被我刪除了.\n我剛剛找到了一個.不知道對不對.\n您試試看\n", "type": "commented", "related_issue": null}, {"user_name": "yxwudi", "datetime": "Jul 5, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "siao888", "datetime": "Jul 5, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "siao888", "datetime": "Jul 5, 2022", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/440", "issue_status": " Open\n", "issue_list": [{"user_name": "babysor", "datetime": "Mar 7, 2022", "body": "\n\n有足够的样本的前提下我想自己训练出来一个日语模型，不知道行不行\n答：就像把中文句子例如 “你好” 变成 “ni2 hao3”，只需要找到一个tts前端处理一下日语为phenomenon", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 8, 2022", "body": "感觉这样做出来会很强大，还自带翻译功能，会更复杂", "type": "commented", "related_issue": null}, {"user_name": "Emiya0415", "datetime": "Mar 19, 2022", "body": "关于日语方面，我已经找到了对应的tts前端，配合自己写的java脚本对日语处理为phenomenon，可以将一个wav文件夹下所有的语音和文本生成对应的alignment格式。一个是将日语的所有汉字等转换为片假名（类似于中文的拼音），另一个是输入对应的语音和txt文本，输出对应的每个拼音的时间。最后用java脚本生成alignment.txt文件。\ntts前端：链接:  提取码: 6f5s 复制这段内容后打开百度网盘手机App，操作更方便哦\n包括两个文件japankana和segmentation-kit2\n第一个japankana使用比较简单，打开后输入就行了，可以将包括汉字的日语转换为片假名\n\n第二个操作方法请参考里面的readme.txt我已经开始尝试训练了50k步，但日语效果并不是很好。基本只有前面的两个词左右能够识别输出，剩下的都是语音和输入文本对应不上。目前数据集是单独一个人的日语语音10个小时左右，可能是因为数据集太小的原因所以效果不好。", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 20, 2022", "body": "赞", "type": "commented", "related_issue": null}, {"user_name": "584761274", "datetime": "May 15, 2022", "body": "链接：\n提取码：1111\n我不知道gui如何上传音频文件 所以使用百度网盘上传\n这是我训练的效果，我相信相同的步骤可以应用于其他语言\n第一步 修改MockingBird-main\\synthesizer\\utils\\symbols.py 中的参数\n\n这里我是想训练日语tts 所以加入了所有的片假名\n第二步 修改一个日语数据集，使其符合aidatatang_200zh, magicdata, aishell3, data_aishell的格式\n\n第三步用数据集训练合成器（我认为只训练合成器就可以达到较好的效果，如果训练声码器和编码器效果应该会更好？）\n\n（时间原因 我只训练了20k步 更长时间的训练应该能获得更好的效果）", "type": "commented", "related_issue": null}, {"user_name": "Emiya0415", "datetime": "May 16, 2022", "body": "我们用的是同一个数据集，都是英伟达的common voice。你直接用片假名训练的，我则是把片假名再进一步转换为罗马音进行训练。现在是90k step，效果只能说一般，部分文字还是识别不出来。可能是数据集大小还不够", "type": "commented", "related_issue": null}, {"user_name": "584761274", "datetime": "May 16, 2022", "body": "我对日语并不熟悉，但或许片假/平假对tts来说更易拟合？ 我对目前训练的效果还是满意的 毕竟日语的大型数据集太难找了\n我接下来会用jsut 和 jvs混合训练试一试", "type": "commented", "related_issue": null}, {"user_name": "Emiya0415", "datetime": "May 16, 2022", "body": "那我今天根据片假名再重新开始训练好了。日语的数据集还是挺多的比如LaboroTVSpeech和Corpus of Spontaneous Japanese等，都有几百个小时。但前者申请需要国内大学的老师或者日本当地大学的学生、后者一个数据集2000rmb，对于个人兴趣负担还是太大了。方便的话可以分享一下你训练的模型吗，我已经把我训练的上传到issue了。", "type": "commented", "related_issue": null}, {"user_name": "584761274", "datetime": "May 16, 2022", "body": "我训练到90k的时候会分享的，感觉loss还有下降的空间", "type": "commented", "related_issue": null}, {"user_name": "Emiya0415", "datetime": "May 16, 2022", "body": "你训练时有输入音频每个单词对应的时间吗，这部分是怎么处理的。我是用了segment_julius可以同时将片假名转换为罗马音并获取对应单词的时间，但如果只输入片假名如何获取其对应单词的时间呢", "type": "commented", "related_issue": null}, {"user_name": "584761274", "datetime": "May 16, 2022", "body": "\n神经网络似乎可以自己对单词的时间进行划分 所以我没有划分单词时间\n链接：\n提取码：1111", "type": "commented", "related_issue": null}, {"user_name": "Emiya0415", "datetime": "May 16, 2022", "body": "感谢", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "issue", "related_issue": "#356"}, {"user_name": "babysor", "datetime": "Mar 7, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#214"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#588"}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/436", "issue_status": " Open\n", "issue_list": [{"user_name": "babysor", "datetime": "Mar 7, 2022", "body": "将 synthesizer部分换为 tacotron2 详见 @\n我发现合成器在达到一定数量的训练后，对质量影响更大的反而是声码器，hifigan能用更好的效果但始终有电噪音，现在有些新的项目采样lpcnet而非wavernn，据说是复杂度要远低于wavernn，但质量优于wavernn，可以达到类似hifigan的效果而不带电噪音，请问有考虑过引入lpcnet之类的新声码器的计划呢？", "type": "commented", "related_issue": null}, {"user_name": "MaxMax2016", "datetime": "Mar 8, 2022", "body": "声码器可以参考一下：\nSingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation\n标题：SingGan：高保真歌声生成的生成性对抗性网络（声码器中加入F0输入，字节也这样做）\n链接：\n演示：\n作者：Feiyang Chen,Rongjie Huang,Chenye Cui,Yi Ren,Jinglin Liu,Zhou Zhao,Nicholas Yuan,Baoxing Huai\n机构：Zhejiang University, Huawei Cloud\n备注：vocoder, generative adversarial network, singing voice synthesis\n摘要：由于超长的连续发音、高采样率和强的表现力，高保真歌唱语音合成对神经声码器来说是一项挑战。现有的用于文本到语音的神经声码器不能直接应用于歌唱语音合成，因为它们会导致生成的频谱图出现小故障，并且高频重建效果不佳。为了解决歌唱建模的困难，本文提出了一种具有生成对抗网络的歌唱声码器SingGAN。具体来说，\n1）SingGAN使用源激发来缓解谱图中的小故障问题；（字节跳动也这么弄）\n2）SingGAN采用多频带鉴别器，引入频域损耗和子带特征匹配损耗来监督高频重构。\n据我们所知，SingGAN是第一个设计用于高保真多扬声器歌唱语音合成的声码器。实验结果表明，与以前的方法相比，SingGAN合成的人声质量要高得多（0.41MOS增益）。进一步的实验表明，结合FastSpeech~2作为声学模型，SingGAN在歌唱语音合成管道中实现了很高的鲁棒性，并且在语音合成中表现良好。Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus\n标题：Multi-Singer：基于大规模语料的多发音人歌声声码器\n作者：Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao\n代码：\n演示：\n摘要：高保真度多歌手歌唱语音合成由于歌唱语音数据不足、歌手泛化能力有限、计算量大等问题，对神经声码器来说是一个挑战。现有的开放语料库由于规模和质量的不足，无法满足高保真声乐合成的要求。以前的声码器在多歌手建模方面有困难，并且在进行看不见的歌手歌唱的声音生成时出现了明显的退化。为了加快社区对歌唱嗓音的研究，我们发布了一个大规模的、多歌手的中文歌唱嗓音数据集OpenSinger。为了解决隐形歌唱者建模的困难，我们提出了一种基于生成对抗网络的快速多歌唱者声码器Multi-Singer。\n具体来说，\n1)Multi-Singer使用Mulit Band genertor来加速训练和推理过程。\n2) Multi-Singer采用singer条件判别器和条件对抗训练目标，从声学特征(即mell -谱图)中获取并重建歌唱者身份。（字节跳动也是这样做的，必备模块）\n3)为了监督在频域频谱包络中歌唱者身份的重建，我们提出了一种辅助的歌唱者感知损失；联合训练方法是一种有效的多歌唱者语音建模方法。（声纹联合训练）\n实验结果验证了OpenSinger算法的有效性，表明Multi-Singer算法在速度和质量上都比以前的算法得到了提高。进一步的实验证明，Multi-Singer结合FastSpeech 2作为声学模型，在多singer歌唱语音合成流水线中具有较强的鲁棒性。通用声码必备技术：1，F0转换为激励，解决持续发音的断音；\n2，判别器加入speaker embedding;\n3，声纹损失约束；", "type": "commented", "related_issue": null}, {"user_name": "JerryZRF", "datetime": "Mar 9, 2022", "body": "虽然我不太了解深度学习，但是我看到一些相关的文章。\n这个训练的optimizer本来是Adam，不知道换成NAdam或者AdamW或者Adamax有没有什么帮助", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 7, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "issue", "related_issue": "#420"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#588"}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/427", "issue_status": " Open\n", "issue_list": [{"user_name": "AyahaShirane", "datetime": "Mar 6, 2022", "body": "我做了一个利用aliyun tts批量生成数据集的软件，大家可以尝试使用现成的tts制作更多纯净的语音数据集来反哺自己的模型，暂时只有CLI，没有GUI：\n\n希望大家支持，也希望各位大佬帮我继续补全，谢谢", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 7, 2022", "body": "这里的说话人会集中在几个人吗？", "type": "commented", "related_issue": null}, {"user_name": "AyahaShirane", "datetime": "Mar 7, 2022", "body": "将voice参数设置为random可以随机生成28个发音人的内容，再算上语调和语速上的改变，基本上够用", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 7, 2022", "body": "比较适合把vocoder部分训练好一点，其他的会影响模型泛化能力把", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/314", "issue_status": " Open\n", "issue_list": [{"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "\n打开项目文件后，发现源音频部分是黑色不能使用，合成出来的声音也有问题，而且会报错——之前那个兼容性错误，目前用的是新手教程里面给的模型", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "Yes, thank you for your kindness, I ran into some problems while running", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "Excuse me, now I cannot synthesize the sound I want with the loaded model", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "I performed the operation and sound synthesis according to the project's tutorial, and the sound that came out was distorted, not normal sound, only current sound and noise", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "Yes, I checked the dependency package and the version of pytorch according to the requirements before running the project, and proceeded without errors.", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "Let me try to restart my computer", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "It can be seen that the synthesized sound is problematic, and the Mel spectrum image is abnormal.", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "I think it should be the problem of the model I imported. I directly used my own voice, which is not ideal. It should be tested with the given model. My voice needs to be trained with a certain sample set. I think it’s me. I made a mistake", "type": "commented", "related_issue": null}, {"user_name": "Mr-MoNET", "datetime": "Jan 4, 2022", "body": "Thank you for your enthusiastic help, I have found the problem so far", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/388", "issue_status": " Open\n", "issue_list": [{"user_name": "SeedKunY", "datetime": "Feb 14, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "luan78zaoha", "datetime": "Feb 17, 2022", "body": "可以试试这个项目\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/244", "issue_status": " Open\n", "issue_list": [{"user_name": "Rita-ritally", "datetime": "Dec 2, 2021", "body": "由于之前一直关注Real-Time-Voice-Cloning() 这个项目，这次Mocking Bird项目没有使用Tacotron2太令人可惜了。所以自己斗胆将Tacotron2迁到这个系统中，比较粗糙但是可以成功训练和推理。\n模型还在训练中，有效果及时来这里更新！\n代码肯定漏洞百出.....求各位大佬指点！", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Dec 2, 2021", "body": "需要更多协助可以随时联系我", "type": "commented", "related_issue": null}, {"user_name": "Rita-ritally", "datetime": "Dec 2, 2021", "body": "\n 大佬您好，现在一直在这个阶段，loss下降特别慢。。。。\n我的代码在 这个仓库中，希望能得到您的指导～", "type": "commented", "related_issue": null}, {"user_name": "Rita-ritally", "datetime": "Dec 3, 2021", "body": "现在训练了46000步，平均loss在0.74，从测试结果中可以看出已经收敛，且合成的mel谱纹路清晰。\n\n\n但是正式测试的时候发现使用aishell+aidatadang_200zh训练46000步的合成器无法正确合成语音，mel谱很模糊。\n还没有只使用aishell训练合成器8w步合成的效果好\n\n\n现在还在继续训练，不知道是不是训练 步数少的问题。。。。。。", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Dec 26, 2021", "body": "现在效果会好一些了吗？我本来也打算fork个分支试试的，你这边可以fork试一下吗，成为contributor？", "type": "commented", "related_issue": null}, {"user_name": "Rita-ritally", "datetime": "Dec 26, 2021", "body": "在我的实验中使用aishell3数据集训练的效果比aidatatang效果好，由于aishell3男性说话人语料较小，发现无法正确合成男性说话人的声音。所以我将train和test的所有语料整合在一起训练，可以缓解这个问题。在我的仓库中只有synthesizer的code是有变化的，vocoder中加入了melgan和waveglow声码器，但是效果还不是很好。如果可以的话，非常荣幸能成为这个项目的contributor！！！！", "type": "commented", "related_issue": null}, {"user_name": "facenl", "datetime": "Feb 26, 2022", "body": "大佬牛啊！！！", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Feb 27, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/382", "issue_status": " Open\n", "issue_list": [{"user_name": "ZJ-CAI", "datetime": "Feb 11, 2022", "body": "\n            \n          ", "type": "commented", "related_issue": null}, {"user_name": "luan78zaoha", "datetime": "Feb 17, 2022", "body": "可以试试这个项目\n", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Feb 17, 2022", "body": "", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/328", "issue_status": " Open\n", "issue_list": [{"user_name": "absc", "datetime": "Jan 9, 2022", "body": "Traceback (most recent call last):\nFile \"/Users/huyouwangzecai/Documents/项目/个人项目/声音项目/MockingBird-main/toolbox/ui.py\", line 160, in setup_audio_devices\nsd.check_output_settings(device=device_namecheck, samplerate=sample_rate)\nFile \"/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py\", line 691, in check_output_settings\n_check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate))\nFile \"/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py\", line 2736, in _check\nraise PortAudioError(errormsg, err, hosterror_info)\nsounddevice.PortAudioError: \n这是我的报错内容，sounddevice check_output_settings方法运行不成功有了解的么", "type": "commented", "related_issue": null}, {"user_name": "mr-m0nst3r", "datetime": "Jan 12, 2022", "body": "一样遇到这个问题。解决了吗？", "type": "commented", "related_issue": null}, {"user_name": "toimc", "datetime": "Jan 30, 2022", "body": "一样的问题！", "type": "commented", "related_issue": null}, {"user_name": "hxqbeyond", "datetime": "Feb 27, 2022", "body": "一样的问题 求解；打开就闪退", "type": "commented", "related_issue": null}, {"user_name": "hxqbeyond", "datetime": "Feb 28, 2022", "body": "后面解决了吗", "type": "commented", "related_issue": null}, {"user_name": "absc", "datetime": "Apr 1, 2022", "body": "全部找m1专用的arm架构包， 不要用x86架构包，就可以正常运行", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/423", "issue_status": " Open\n", "issue_list": [{"user_name": "yrsn509", "datetime": "Mar 5, 2022", "body": "请问最新的ppg两个模型可以在哪里下载？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 6, 2022", "body": "已补充到readme", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 8, 2022", "body": "好像没有下载链接。。要自己寻找吗", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 8, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 8, 2022", "body": "oh shit，好像一直没有成功，我这里先贴一下：\n链接:   密码: bt9m\n--来自百度网盘超级会员V4的分享", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 8, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 10, 2022", "body": "这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 10, 2022", "body": "不用，大概率是你没选对模型，除了encoder基本上都要替换成新的", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 10, 2022", "body": "这样子吗，有没有demo参照一下呢", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 10, 2022", "body": "readme中文版里面的截图就是一个可行的", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 12, 2022", "body": "请问如果要提高特定人语音的相似度，训练哪一个模型是最好的，Convertor 或者Encoder还是其他的", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 12, 2022", "body": "目前看 convertor 是瓶颈，但我继续用aidatatang数据训练效果收益一般", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 14, 2022", "body": "这个图谱显示了全是电流\n", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 14, 2022", "body": "训练ppg2mel的时候出现了三种错误，用了合成synthesizer时用的数据集，起初显示第一种错误经过我本人手动转采样率后，又出现了第二种错误然后本人调低了n的值，最终出现第三种错误，至今无法解决", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 14, 2022", "body": "你做了什么手动转码？", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 14, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 14, 2022", "body": "感觉你触发了不曾见过的bug，先不转，你试着preprocess一下", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 14, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "xiaoyeye1117", "datetime": "Mar 18, 2022", "body": "应该把作者提供的声码器模型hifigan_24k.pt重命名为g_hifigan_24k.pt~ 再选择~  电流音解决~但感觉效果不咋样捏", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 20, 2022", "body": "确实 而且再播放合成好的声音还会变化成低音", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 21, 2022", "body": "所以为啥训练合成器的时候能去掉时长较短的音频，训练ppg2mel的时候没有这一步？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Mar 29, 2022", "body": "我感觉也不怎么样，怀疑数据集问题，还是相信paper本身的", "type": "commented", "related_issue": null}, {"user_name": "Chopin68", "datetime": "Mar 29, 2022", "body": "有了解过他们的技术吗 看起来已经能和变声器一样实时了，不知道是不是真的", "type": "commented", "related_issue": null}, {"user_name": "Quadcore1010", "datetime": "Jul 7, 2022", "body": "大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jul 10, 2022", "body": "数据集的原因，这个模型对数据集要求高一些", "type": "commented", "related_issue": null}, {"user_name": "Quadcore1010", "datetime": "Jul 10, 2022", "body": "请问具体是哪方面要求呢？有没有适合训练的中文数据集推荐呢？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jul 16, 2022", "body": "开源的很多都不行，最好是有商用的", "type": "commented", "related_issue": null}, {"user_name": "Quadcore1010", "datetime": "Jul 18, 2022", "body": "是采样率的问题吗？还有其他方面会有影响吗？我好按照具体方向去找相应数据集", "type": "commented", "related_issue": null}, {"user_name": "tylqbq", "datetime": "Sep 22, 2022", "body": "求一个最新的 两个ppg模型 百度网盘链接", "type": "commented", "related_issue": null}, {"user_name": "tylqbq", "datetime": "Sep 23, 2022", "body": "求最新的 两个ppg模型 百度网盘链接", "type": "commented", "related_issue": null}, {"user_name": "yrsn509", "datetime": "Mar 14, 2022", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/olivia-ai/olivia/issues/166", "issue_status": " Open\n", "issue_list": [{"user_name": "A-Yamout", "datetime": "Nov 14, 2021", "body": "You can switch between Male or Female, it will default to Female witch is Olivia but you can switch it to male witch it Oscar.", "type": "commented", "related_issue": null}, {"user_name": "hugolgst", "datetime": "Nov 14, 2021", "body": "Is it a question? A suggestion? I do not get it", "type": "commented", "related_issue": null}, {"user_name": "A-Yamout", "datetime": "Nov 16, 2021", "body": " This is a suggestion", "type": "commented", "related_issue": null}, {"user_name": "A-Yamout", "datetime": "Nov 14, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/olivia-ai/olivia/issues/136", "issue_status": " Open\n", "issue_list": [{"user_name": "PiperMp3", "datetime": "Oct 18, 2020", "body": "Hello,I've currently only gotten so far to have her installed, running as CLI and tested Web App a little. I've yet to see the training of the neural network during a new training.json occur. What I've read is when new intents or new datasets are being added Olivia needs a quick nap (restart) . How could we get her to never need a restart and can detect file changes / new datasets.. new intents, during runtime and she will automatically decide to start training. She could still process anything asked during this training that she already knows, but she will announce to the user that she has detected new data and is currently learning. When she's done she will also state the completion and at this point the new information she's learned is available to be requested/answered with.Would such a feature be possible?The other method that would also be interactive is that the user could ask Olivia, \"Why don't you do a little homework?\"(or something) Whereas Olivia remains active, running, but she rewrites her training.json using the files she has at the time.Mighty impressed by your work so far!Side question, as I didn't really see it mentioned, or perhaps I missed it.. 1. Does Olivia have a limit to how many intents / datasets she can handle? If I copied Wikipedia into intents and sets, giving her 800gb of data, how would she handle it?\n2. During runtime,  is Olivia's \"brain\" running in RAM or is she pulling information to make answers from file system on demand?Many thanks, and keep at it. Good work !!", "type": "commented", "related_issue": null}, {"user_name": "hugolgst", "datetime": "Oct 19, 2020", "body": "Hello,It is definitely possible to have a such feature, I only did the current one to save time.\nWe could inject some features and then just re-train the model a bit to include it.Technically she could handle it but I think that it will take a long time, during runtime there is a save of the neural network loaded into the RAM. There is also a cache if you ask multiple times the same question.Thanks a lot for that, I appreciate it.", "type": "commented", "related_issue": null}, {"user_name": "PiperMp3", "datetime": "Oct 19, 2020", "body": "Good morning,Many thanks for coming back to me - I saw mentioned that a Discord server was started?\nI have some further questions and I don't want to spam the GitHub issues page :)I understand Olivia does not store any data, that may come from the Swiss culture.\nWould be cool to have an optional setting where Olivia could track an unique identifier to see whether its \"me\" or someone else, and store data accordingly.An example; I talk to Olivia as ME, I share secrets (perhaps) and then my partner connects as her (different name, perhaps different device ID that Olivia now sees) she will say \"Hi [NAME]!\" and Olivia now knows not to talk about me, but she now uses all data she knows about the other person. Should I ask Olivia to share secrets that my partner has told her, Olivia would refuse etc.How could one implement such option? Ever had a similar thought?\nCurrently I got Olivia via CLI, but depending on how she's evolving and how I can teach her more I would most likely look to eventually talk to her, to extend her speech - How could wouldn't it be that she recognizes different voices? I have one voice, my partner has another and Olivia answers us accordingly.", "type": "commented", "related_issue": null}, {"user_name": "hugolgst", "datetime": "Oct 19, 2020", "body": "I think to do such a feature Olivia would need to share the ID of the other user and make the client replace it in its local storage.\nOf course it can be done but I'm not sure that it is in the spirit of Olivia, you could however implement it in a fork :)", "type": "commented", "related_issue": null}, {"user_name": "A-Yamout", "datetime": "Nov 4, 2020", "body": "I think that is great that we value the privacy of others and keep expanding Olivia.   you have done a great job with this and I would love to help if there is any way that I could help with the new 3.00 release just ask.BTW I am great with logos graphics and art more then go (I am horrible at go) so want a new logo or a new UI for the web app I can help you with that, and thanks for all your hard work.", "type": "commented", "related_issue": null}, {"user_name": "hugolgst", "datetime": "Nov 4, 2020", "body": "Thanks a lot for that! However, it is hard for me to keep maintaining Olivia thus I am pretty busy.\nI am very open to new ideas so you can create issues or work on some PR if you want so!\nHuge thanks! :)", "type": "commented", "related_issue": null}, {"user_name": "A-Yamout", "datetime": "Nov 4, 2020", "body": "No Problem. Thanks for making this open source and open to the world.", "type": "commented", "related_issue": null}, {"user_name": "hugolgst", "datetime": "Oct 19, 2020", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/230", "issue_status": " Open\n", "issue_list": [{"user_name": "mithisha", "datetime": "Nov 23, 2021", "body": "I do want to try this  with a vocoder which has trained for the english speakers.. how can i do that? can i have a little guidance on that?", "type": "commented", "related_issue": null}, {"user_name": "Olivia-Ye", "datetime": "Nov 23, 2021", "body": "This repository is forked from Real-Time-Voice-Cloning which only support English.\nYou'd better see this ", "type": "commented", "related_issue": null}, {"user_name": "Olivia-Ye", "datetime": "Nov 23, 2021", "body": "or maybe you can train synthesizer with English dataset, and keep the vocoder and encoder remain", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Nov 24, 2021", "body": "what vocoder model do you want to use? The one of WaveRNN in this repo is originally trained with English.", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Nov 27, 2021", "body": "For the question you asked in issue title, answer is yes, and it works event better than the wavernn", "type": "commented", "related_issue": null}, {"user_name": "mithisha", "datetime": "Nov 28, 2021", "body": "highly appreciated your reply. I  have used the toolbox provided by the with the . Then I have used your work to change the vocoder into the hifigan. but in the end, there is no difference in the outputs . It gives the same outputs for the bother wave rnn and hifigan vocoder.  I wanted to know whether this scenario is normal to not? ## I have not trained hifigan model, instead of that I have used ur weight files.", "type": "commented", "related_issue": null}, {"user_name": "mithisha", "datetime": "Nov 28, 2021", "body": "yeah I have referred it . thanks", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Nov 28, 2021", "body": "The toolbox from  won't work with our HifiGan in this repo for sure.  Please try the toolbox in this repo and see how different it works with this vocoder.", "type": "commented", "related_issue": null}, {"user_name": "mithisha", "datetime": "Nov 28, 2021", "body": "I want to have the results for the English speakers, can you please give help with that?can I do it by replacing the pretrained.pt file of the synthesizer in this repo with the pretrained.pt  file of the  \"\" repo?sorry for the inconvenience !", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Nov 28, 2021", "body": "This won't work as well. Now these two models has too much difference on states to be compatible.", "type": "commented", "related_issue": null}, {"user_name": "mithisha", "datetime": "Nov 28, 2021", "body": "Then I have to train the synthesizer in this repo using an English speaker data set. Since I don't have enough resources to train a model , cant I use a pre-trained model (trained with English speakers) for the synthesizer? if it is so where can i find a compatible pretrained model ? do you have any guidance for that ?thank you in advance!!!!!!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/10", "issue_status": " Open\n", "issue_list": [{"user_name": "babysor", "datetime": "Aug 16, 2021", "body": "已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)\n需要更多请在这里提建议，并+1投票，将为大家补充支持", "type": "commented", "related_issue": null}, {"user_name": "yfq512", "datetime": "Aug 16, 2021", "body": "朋友，你是怎么跑起来的，我运行python synthesizer_preprocess_audio.py <datasets_root> 就迷惑了，这个datasets_root是指什么呢？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 16, 2021", "body": "假如你下载的 aidatatang_200zh文件放在D盘，train文件路径为  , 你的就是 ", "type": "commented", "related_issue": null}, {"user_name": "hertz-pj", "datetime": "Aug 17, 2021", "body": "推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是而不是", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 17, 2021", "body": "是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。", "type": "commented", "related_issue": null}, {"user_name": "hertz-pj", "datetime": "Aug 17, 2021", "body": "那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 17, 2021", "body": "程序逻辑还没实现 囧 目前我是手动分开跑", "type": "commented", "related_issue": null}, {"user_name": "XiuChen-Liu", "datetime": "Aug 17, 2021", "body": "推薦標貝數據集", "type": "commented", "related_issue": null}, {"user_name": "0we", "datetime": "Aug 23, 2021", "body": "aishell3 和 Mozilla Common Voice 数据集", "type": "commented", "related_issue": null}, {"user_name": "loilih", "datetime": "Aug 27, 2021", "body": "aidatatang_200zh在哪里下载呢", "type": "commented", "related_issue": null}, {"user_name": "XiuChen-Liu", "datetime": "Aug 27, 2021", "body": "這裡 ", "type": "commented", "related_issue": null}, {"user_name": "loilih", "datetime": "Aug 27, 2021", "body": "谢谢", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 28, 2021", "body": " 已支持aishell3，不过训练效果没增强", "type": "commented", "related_issue": null}, {"user_name": "XiuChen-Liu", "datetime": "Aug 28, 2021", "body": "大佬，現在你提供的版本還需要使用原項目的 encoder 和 vocoder 嗎", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 28, 2021", "body": "已经不再需要下载了", "type": "commented", "related_issue": null}, {"user_name": "XiuChen-Liu", "datetime": "Aug 28, 2021", "body": "好的，謝謝大佬的回覆，另外推薦 zhvoice數據集: ", "type": "commented", "related_issue": null}, {"user_name": "FawenYo", "datetime": "Sep 2, 2021", "body": "請問有沒有大佬能提供 Mozilla Common Voice 架構的支援... 原 repo 中有人也發過類似的 issue 可以參考 \n檔案結構大致如圖\n\n其中  資料夾內容包含各項 \n希望日後能支持，謝謝", "type": "commented", "related_issue": null}, {"user_name": "Zhiqing-Xu", "datetime": "Sep 5, 2021", "body": "老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 6, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "Zhiqing-Xu", "datetime": "Sep 6, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 7, 2021", "body": " 这里有新的二维码", "type": "commented", "related_issue": null}, {"user_name": "ShouNichi", "datetime": "Sep 26, 2021", "body": "\n这个看着很厉害的样子\n这边在研究改代码跑跑看\n不过都是mp3的很麻烦", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 26, 2021", "body": "等你好消息，不过我访问不了", "type": "commented", "related_issue": null}, {"user_name": "ShouNichi", "datetime": "Sep 27, 2021", "body": "是指数据集不能访问吗？如果是百度云不能访问的话我这边可以转mega或者GD\n链接:  提取码: dwet-----------------更新---------------------访问不了是链接不知怎的最后多了个z，删掉就行了\n\n写了貌似可以直接用于zhrtvc，同一个分支出来的\n\n突然发现上面已经有人推荐过了...", "type": "commented", "related_issue": null}, {"user_name": "Charlottecuc", "datetime": "Sep 27, 2021", "body": " 二维码过期了，求重发一个～", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 27, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "LxKxC", "datetime": "Oct 7, 2021", "body": "群满了，加不进去，求作者微信号", "type": "commented", "related_issue": null}, {"user_name": "kslz", "datetime": "Oct 11, 2021", "body": "有没有可能提取游戏里的音频素材，或者关闭背景音乐后用软件录制", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Oct 11, 2021", "body": "理论可行，可以联系我讨论", "type": "commented", "related_issue": null}, {"user_name": "kslz", "datetime": "Oct 11, 2021", "body": "你好 已发到你的gmail里  上面的二维码过期了", "type": "commented", "related_issue": null}, {"user_name": "lcp580", "datetime": "Oct 14, 2021", "body": "新增标贝数据支持BZNSYP\nMozillaCommonVoice\n以上两个数据集支持在最新的主版本中没有看到？难道还没能合并进来吗？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Oct 14, 2021", "body": "都在分支上， ", "type": "commented", "related_issue": null}, {"user_name": "woct0rdho", "datetime": "Nov 7, 2021", "body": "给个思路，那些galgame都是几十万字的文本与语音对应的数据集", "type": "commented", "related_issue": null}, {"user_name": "ycMia", "datetime": "Nov 24, 2021", "body": "牵涉版权问题哦 ", "type": "commented", "related_issue": null}, {"user_name": "joshua54321", "datetime": "Dec 23, 2021", "body": "作者您好，请问现在支持自动跑多个数据集了吗？如果手动分开跑，是如何操作的呢？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Dec 26, 2021", "body": "可以，  分开跑比较麻烦，因为混合起来就不太能区分了", "type": "commented", "related_issue": null}, {"user_name": "l-i-p-f", "datetime": "Jan 30, 2022", "body": "请问有粤语数据集吗？", "type": "commented", "related_issue": null}, {"user_name": "ZeroAurora", "datetime": "Aug 17, 2022", "body": "\nTHCHS-30 数据集，体量较小，想拿来练手用", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 10, 2022", "body": "最好确保有100hrs级别的语音", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 27, 2021", "body": [], "type": "pinned this issue", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#7"}, {"user_name": "babysor", "datetime": "Mar 7, 2022", "body": [], "type": "unpinned this issue", "related_issue": null}, {"user_name": "18klove", "datetime": "Jun 19, 2022", "body": [], "type": "issue", "related_issue": "#625"}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/10457", "issue_status": " Open\n", "issue_list": [{"user_name": "NareshDen", "datetime": "Dec 5, 2021", "body": "2.8.0 ( 2.7.0 )3.6LinuxRasa 1.0 form were working, and suddenly it stopped working. The version of rasa was 2.8.2 and was tried on 2.8.0 and even 2.7.0 version also. Code was on production and it worked before November 10th. The temp solution was to remove forms 1.0 definition. The question is why the same code worked before. Here is the log for working and non working forms. ", "type": "commented", "related_issue": null}, {"user_name": "z-bodikova", "datetime": "Dec 6, 2021", "body": "Exalate commented:z-bodikova commented: - this is the issue we talked about in #on-point - I think we should time-box it and was hoping you could give me an idea what an appropriate time box for this investigation (I was thinking maybe 4-6 hrs) would be and when you can spend that time on this - especially since you are not in the firefighting crew this week... Let me know.", "type": "commented", "related_issue": null}, {"user_name": "NareshDen", "datetime": "Dec 5, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "z-bodikova", "datetime": "Dec 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/20", "issue_status": " Open\n", "issue_list": [{"user_name": "j1nx", "datetime": "Jan 30, 2019", "body": "MycroftOS is a voice controlled system, however some settings are way easier to configure via a webbased system. Hence Mycroft A.I. even has exactly this with; home.mycroft.aiI strongly believe MycroftOS should also have some sort of webbased backend where you can change stuff. Not only pairing and skill settings, but also hooks into the new to be released GUI part. So a bullit list of what I think in the end should be available via a browser connecting to MycroftOS.I believe most of the appraoches I have seen and linked above use npm/flask/etc. So believe this should be the way forward to minimise dependencies and extra packages. (Wifi-Wizard also uses it, so npm should be there rather sooner then later)", "type": "commented", "related_issue": null}, {"user_name": "JarbasAl", "datetime": "Nov 18, 2020", "body": "quick note, i stopped maintaining personal-backend when selene was released, some more updated have meanwhile been made at i also package this as a skill, anyone can simply install it and not require backend i feel the settings etc should be essentially handled by the GUI a local frontend might also be served for headless devices if needed, this could happen at/integrate with mock-backendif there is interest i can move mock-backend under OVOS organization", "type": "commented", "related_issue": null}, {"user_name": "j1nx", "datetime": "Jan 30, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "j1nx", "datetime": "Jan 30, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "j1nx", "datetime": "Jan 30, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "j1nx", "datetime": "Jan 30, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "j1nx", "datetime": "Oct 2, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/9952", "issue_status": " Open\n", "issue_list": [{"user_name": "Fares-Ayed", "datetime": "Oct 21, 2021", "body": "Hi, Actually I'm developing a bot based on Rasa. I have integrated it into Telegram. I find a problem that Telegram.py doesn't support the Speech to Text conversion. My main goal is when the user send a voice message via Telegram My bot can understand this messageI suppose that a class can be added into the custom channel (Telegram.py) to convert this audio file to text using Deepspeech models (for examples) then send it to Rasa in Text Form", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Oct 21, 2021", "body": "Exalate commented:rgstephens commented:See  for more details on this request.", "type": "commented", "related_issue": null}, {"user_name": "Fares-Ayed", "datetime": "Oct 21, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/5", "issue_status": " Open\n", "issue_list": [{"user_name": "JarbasAl", "datetime": "Nov 1, 2018", "body": "Hi, i think MycroftOS is a great idea!Since it is aimed at raspi3 for now, it would make sense to support some common mic arrays,Maybe also support some other hats that make sense for mycroftSuggestions:", "type": "commented", "related_issue": null}, {"user_name": "j1nx", "datetime": "Nov 1, 2018", "body": "You are definitely right! Buildroot should take care of all the drivers and OS configuration. The visuals should be handled by a skill I think.I already have a ReSpeaker 4-Mic array at hand, so that one is indeed on  but agree that at this point in time AIY and Matrix voice are the next two important ones.The new ReSpeaker arrays are on this list as well, but I believe they use the exact same driver, just some other OS configuration files. The only \"worry\" I have at the moment is to properly figure out at boot which one the user has. However that are future concerns.", "type": "commented", "related_issue": null}, {"user_name": "j1nx", "datetime": "Nov 3, 2018", "body": "ReSpeaker kernel drivers and OS configuration is merged. At this point we do not have configuration wizards yet, but later on we will have some sort of cornfiguration wizard where you could select it and it will be used. Similar as the picroft, but I do not want that to be done over the cli.In the future I would like to do that webbased. A small local webserver and easy setup of hardware and such.", "type": "commented", "related_issue": null}, {"user_name": "j1nx", "datetime": "Nov 1, 2018", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/babysor/MockingBird/issues/20", "issue_status": " Open\n", "issue_list": [{"user_name": "zhuzaileiting", "datetime": "Aug 19, 2021", "body": "（作者借楼编辑ing\n社区视频教程：\n奶糖 ", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 19, 2021", "body": "再分享 ", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 19, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 19, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 19, 2021", "body": "这。。看起来你都没train起来synthesizer啊", "type": "commented", "related_issue": null}, {"user_name": "zhangji261", "datetime": "Aug 19, 2021", "body": "同求，比如数据集在哪里下载", "type": "commented", "related_issue": null}, {"user_name": "XiuChen-Liu", "datetime": "Aug 19, 2021", "body": " closed裡面有同樣的問題，有放下載連結", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 20, 2021", "body": "E:\\data\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train   数据集解压路径       这一步synthesizer_preprocess_audio.py有问题吗\nC:\\Users\\Administrator\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main>\npython synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh\\aidatatang_200zh\nArguments:\ndatasets_root:   E:\\data\\aidatatang_200zh\\aidatatang_200zh\nout_dir:         E:\\data\\aidatatang_200zh\\aidatatang_200zh\\SV2TTS\\synthesizer\nn_processes:     None\nskip_existing:   False\nhparams:\nno_alignments:   False\ndataset:         aidatatang_200zhUsing data from:\nE:\\data\\aidatatang_200zh\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"synthesizer_preprocess_audio.py\", line 63, in \npreprocess_dataset(**vars(args))\nFile \"C:\\Users\\Administrator\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main\\synthesizer\\preprocess.py\", line 32, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 20, 2021", "body": " 不用多一层", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 20, 2021", "body": "python synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh 不用多一层\n解决了", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 20, 2021", "body": "python synthesizer_preprocess_audio.py E:\\data\\aidatatang_200zh 不用多一层\n@解决了大佬", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 22, 2021", "body": "这步也太慢了。。。\nC:\\Users\\lxd\\Desktop\\Realtime-Voice-Clone-Chinese-main\\Realtime-Voice-Clone-Chinese-main>python synthesizer_train.py mandarin D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\nArguments:\nrun_id:          mandarin\nsyn_dir:         D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\nmodels_dir:      synthesizer/saved_models/\nsave_every:      1000\nbackup_every:    25000\nforce_restart:   False\nhparams:Checkpoint path: synthesizer\\saved_models\\mandarin\\mandarin.pt\nLoading training data from: D:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\train.txt\nUsing model: Tacotron\nUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 30.872MLoading weights at synthesizer\\saved_models\\mandarin\\mandarin.pt\nTacotron weights loaded from step 0\nUsing inputs from:\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\train.txt\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\mels\nD:\\data\\aidatatang_200zh\\SV2TTS\\synthesizer\\embeds\nFound 122482 samples\n+----------------+------------+---------------+------------------+\n| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n+----------------+------------+---------------+------------------+\n|   20k Steps    |     12     |     0.001     |        2         |\n+----------------+------------+---------------+------------------+{| Epoch: 1/2 (500/10207) | Loss: 0.9025 | 0.065 steps/s | Step: 0k | }Input at step 500: wo3 yao4 gei3 wang2 ming2 da3 dian4 hua4~__________________________________________________________\n{| Epoch: 1/2 (1000/10207) | Loss: 0.8266 | 0.071 steps/s | Step: 1k | }Input at step 1000: na4 me wo3 jiu4 chong2 xin1 ren4 shi2 ni3~______________________________________________________________\n{| Epoch: 1/2 (1500/10207) | Loss: 0.7602 | 0.074 steps/s | Step: 1k | }Input at step 1500: mei3 tian1 dou1 na4 me wan3 shui4 jiao4~___________________________________\n{| Epoch: 1/2 (2000/10207) | Loss: 0.7415 | 0.075 steps/s | Step: 2k | }Input at step 2000: da3 dian4 hua4 gei3 deng4 han4 ling2~_________________________________________________________________________\n{| Epoch: 1/2 (2500/10207) | Loss: 0.6921 | 0.068 steps/s | Step: 2k | }Input at step 2500: zhen1 xiang4 yong3 yuan3 zhi3 you3 yi2 ge4~___________________________________\n{| Epoch: 1/2 (3000/10207) | Loss: 0.6741 | 0.072 steps/s | Step: 3k | }Input at step 3000: xia4 men2 wai4 guo2 yu3 xue2 xiao4 chu1 er4 nian2 ji2 chen2 xiao3 qi2 jia1 de zhu4 zhi3~\n{| Epoch: 1/2 (3500/10207) | Loss: 0.6499 | 0.070 steps/s | Step: 3k | }Input at step 3500: ru2 guo3 wo3 he2 ni3 zai4 yi4 qi3~_______________________________________________________\n{| Epoch: 1/2 (4000/10207) | Loss: 0.6679 | 0.073 steps/s | Step: 4k | }Input at step 4000: fu4 jin4 de ping2 an1 yin2 hang2~_________________________________\n{| Epoch: 1/2 (4500/10207) | Loss: 0.6349 | 0.069 steps/s | Step: 4k | }Input at step 4500: ming2 zi4 shi4 hui3 guo4 cheng2 nuo4 shu1~_____________________________________________________\n{| Epoch: 1/2 (5000/10207) | Loss: 0.6392 | 0.073 steps/s | Step: 5k | }Input at step 5000: wo3 shen2 me shi2 hou4 cai2 neng2 chong1 man3 dian4~_______________________\n{| Epoch: 1/2 (5500/10207) | Loss: 0.6293 | 0.073 steps/s | Step: 5k | }Input at step 5500: wo3 da3 ni3 hao3 bu4 hao3 ma ge2 shi4 chong2 fu4~___________\n{| Epoch: 1/2 (6000/10207) | Loss: 0.6715 | 0.077 steps/s | Step: 6k | }Input at step 6000: ci3 ji4 hao3 wu2 liao2 da3 yi1 dian4 ying3 ming2~___________________________________\n{| Epoch: 1/2 (6500/10207) | Loss: 0.6446 | 0.075 steps/s | Step: 6k | }Input at step 6500: wo3 gei3 ni3 fa1 de ni3 shou1 dao4 le ma~___________________________________________________________\n{| Epoch: 1/2 (7000/10207) | Loss: 0.6022 | 0.068 steps/s | Step: 7k | }Input at step 7000: ning4 que1 wu2 lan4 zhi3 wei4 yi3 hou4 de du2 yi1 wu2 er4~________________________\n{| Epoch: 1/2 (7500/10207) | Loss: 0.6178 | 0.067 steps/s | Step: 7k | }Input at step 7500: mei2 you3 wang3 luo4 ni3 hai2 hui4 liao2 tian1 ma~______________________\n{| Epoch: 1/2 (8000/10207) | Loss: 0.6041 | 0.068 steps/s | Step: 8k | }Input at step 8000: wo3 bu4 fa1 le wo3 yao4 shui4 jiao4 le~____________________________________________________________________________________________\n{| Epoch: 1/2 (8500/10207) | Loss: 0.6078 | 0.072 steps/s | Step: 8k | }Input at step 8500: ni3 cai1 lai2 cai1 qu4 ye3 cai1 bu4 ming2 bai2~____________________________________________________\n{| Epoch: 1/2 (9000/10207) | Loss: 0.6055 | 0.072 steps/s | Step: 9k | }Input at step 9000: ni3 wen4 le wo3 tou2 dou1 da4 le~_______________________________________\n{| Epoch: 1/2 (9500/10207) | Loss: 0.5816 | 0.069 steps/s | Step: 9k | }Input at step 9500: xia4 ban1 mei2 you3 mei2 chu1 qu4 guang4~_______________________________________\n{| Epoch: 1/2 (10000/10207) | Loss: 0.5664 | 0.068 steps/s | Step: 10k | }Input at step 10000: ni3 jin1 tian1 bu2 shi4 bu4 shang4 ban1 ma~__________________________________________________\n{| Epoch: 1/2 (10207/10207) | Loss: 0.5879 | 0.071 steps/s | Step: 10k | }\n{| Epoch: 2/2 (293/10207) | Loss: 0.5840 | 0.070 steps/s | Step: 10k | }Input at step 10500: ai4 qing2 xiao3 shuo1 ma2 que4 gao3 ding4 hua1 mei3 nan2~_________________________________\n{| Epoch: 2/2 (322/10207) | Loss: 0.5892 | 0.070 steps/s | Step: 10k | }", "type": "commented", "related_issue": null}, {"user_name": "miven", "datetime": "Aug 23, 2021", "body": " 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 23, 2021", "body": "...GPU，怎么配置显卡全局配置了呀。。", "type": "commented", "related_issue": null}, {"user_name": "dukechain2333", "datetime": "Aug 23, 2021", "body": "Arguments:\ndatasets_root:   D:\\Data\\aidatatang_200zh\nout_dir:         D:\\Data\\aidatatang_200zh\\SV2TTS\\synthesizer\nn_processes:     None\nskip_existing:   False\nhparams:\nno_alignments:   False\ndataset:         aidatatang_200zhUsing data from:\nD:\\Data\\aidatatang_200zh\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"synthesizer_preprocess_audio.py\", line 63, in \npreprocess_dataset(**vars(args))\nFile \"D:\\Realtime-Voice-Clone-Chinese\\synthesizer\\preprocess.py\", line 32, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError", "type": "commented", "related_issue": null}, {"user_name": "dukechain2333", "datetime": "Aug 23, 2021", "body": "路径好像没啥问题啊", "type": "commented", "related_issue": null}, {"user_name": "dukechain2333", "datetime": "Aug 23, 2021", "body": "\ntrian里是长这样的嘛", "type": "commented", "related_issue": null}, {"user_name": "yuzd", "datetime": "Aug 24, 2021", "body": "有群吗 一起交流下怎么跑", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 24, 2021", "body": "\n7天有效", "type": "commented", "related_issue": null}, {"user_name": "hexiosr", "datetime": "Aug 31, 2021", "body": "二维码过期了", "type": "commented", "related_issue": null}, {"user_name": "oceanarium", "datetime": "Aug 31, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "josh-zhu", "datetime": "Sep 8, 2021", "body": "群二维码过期了，求更", "type": "commented", "related_issue": null}, {"user_name": "JackChow6", "datetime": "Sep 12, 2021", "body": "二维码过期了，求更", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 12, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 12, 2021", "body": "见上", "type": "commented", "related_issue": null}, {"user_name": "JackChow6", "datetime": "Sep 12, 2021", "body": "谢谢你", "type": "commented", "related_issue": null}, {"user_name": "chloe5685", "datetime": "Sep 19, 2021", "body": "二维码失效了呜呜呜", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 19, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jan 24, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "pjun463", "datetime": "Feb 4, 2022", "body": "更新一下二维码谢谢", "type": "commented", "related_issue": null}, {"user_name": "pjun463", "datetime": "Feb 4, 2022", "body": "D:\\M\\MockingBird-main>python pre.py D:\\数据集 -d aidatatang_200zh -n 7\nUsing data from:\nD:\\数据集\\aidatatang_200zh\\corpus\\train\nTraceback (most recent call last):\nFile \"D:\\M\\MockingBird-main\\pre.py\", line 74, in \npreprocess_dataset(**vars(args))\nFile \"D:\\M\\MockingBird-main\\synthesizer\\preprocess.py\", line 45, in preprocess_dataset\nassert all(input_dir.exists() for input_dir in input_dirs)\nAssertionError\n有大佬帮帮我吗", "type": "commented", "related_issue": null}, {"user_name": "pjun463", "datetime": "Feb 5, 2022", "body": "更新一下二维码谢谢", "type": "commented", "related_issue": null}, {"user_name": "fengxiangyun", "datetime": "Feb 7, 2022", "body": "+1", "type": "commented", "related_issue": null}, {"user_name": "hentai-hf", "datetime": "Feb 10, 2022", "body": "在输入音频后出现了这个报错……Feel free to add your own. You can still use the toolbox by recording samples yourself.\nTraceback (most recent call last):\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\toolbox__.py\", line 103, in \nfunc = lambda: self.load_from_browser(self.ui.browse_file())\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\toolbox__.py\", line 170, in load_from_browser\nwav = Synthesizer.load_preprocess_wav(fpath)\nFile \"D:\\mokingbird\\MockingBird-main\\MockingBird-main\\synthesizer\\inference.py\", line 146, in load_preprocess_wav\nwav = librosa.load(str(fpath), hparams.sample_rate)[0]\nTypeError: load() takes 1 positional argument but 2 were given", "type": "commented", "related_issue": null}, {"user_name": "hentai-hf", "datetime": "Feb 10, 2022", "body": "已解决\n在命令的命令输入pip install librosa==0.8.1", "type": "commented", "related_issue": null}, {"user_name": "ptilopsisG", "datetime": "Feb 11, 2022", "body": "请问可以更新一下二维码吗，谢谢", "type": "commented", "related_issue": null}, {"user_name": "gdfshzh", "datetime": "Feb 12, 2022", "body": "各位大佬！！帮我看看这是什么错误啊，，我已经把模板放入相应的文件夹里了可是仍然不行额D:\\迅雷下载\\MockingBird-main>python demo_toolbox.py\nArguments:\ndatasets_root: None\nenc_models_dir: encoder\\saved_models\nsyn_models_dir: synthesizer\\saved_models\nvoc_models_dir: vocoder\\saved_models\ncpu: False\nseed: None\nno_mp3_support: FalseWarning: you did not pass a root directory for datasets as argument.\nThe recognized datasets are:\nLibriSpeech/dev-clean\nLibriSpeech/dev-other\nLibriSpeech/test-clean\nLibriSpeech/test-other\nLibriSpeech/train-clean-100\nLibriSpeech/train-clean-360\nLibriSpeech/train-other-500\nLibriTTS/dev-clean\nLibriTTS/dev-other\nLibriTTS/test-clean\nLibriTTS/test-other\nLibriTTS/train-clean-100\nLibriTTS/train-clean-360\nLibriTTS/train-other-500\nLJSpeech-1.1\nVoxCeleb1/wav\nVoxCeleb1/test_wav\nVoxCeleb2/dev/aac\nVoxCeleb2/test/aac\nVCTK-Corpus/wav48\naidatatang_200zh/corpus/dev\naidatatang_200zh/corpus/test\naishell3/test/wav\nmagicdata/train\nFeel free to add your own. You can still use the toolbox by recording samples yourself.", "type": "commented", "related_issue": null}, {"user_name": "SeedKunY", "datetime": "Feb 12, 2022", "body": "二维码需要更新", "type": "commented", "related_issue": null}, {"user_name": "luoyudong", "datetime": "Mar 2, 2022", "body": "求个新的群二维码", "type": "commented", "related_issue": null}, {"user_name": "QuellaMC", "datetime": "Mar 6, 2022", "body": "大佬，我requirements安装没报错，但是运行程序时会报错:\nTraceback (most recent call last):\nFile \"E:\\MockingBird\\demo_toolbox.py\", line 2, in \nfrom toolbox import Toolbox\nFile \"E:\\MockingBird\\toolbox__.py\", line 6, in \nimport ppg_extractor as extractor\nFile \"E:\\MockingBird\\ppg_extractor__.py\", line 6, in \nfrom .frontend import DefaultFrontend\nFile \"E:\\MockingBird\\ppg_extractor\\frontend.py\", line 5, in \nfrom torch_complex.tensor import ComplexTensor\nModuleNotFoundError: No module named 'torch_complex'", "type": "commented", "related_issue": null}, {"user_name": "luoyudong", "datetime": "Mar 7, 2022", "body": "", "type": "", "related_issue": null}, {"user_name": "1135126802", "datetime": "Mar 26, 2022", "body": "大佬们  问个问题  我自己的数据太小了  中途更换别的数据集进行训练  但是出现了这样的错误代码\nwarnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\nTraceback (most recent call last):\nFile \"E:\\数据集制作\\MockingBird-main\\synthesizer_train.py\", line 37, in \ntrain(**vars(args))\nFile \"E:\\数据集制作\\MockingBird-main\\synthesizer\\train.py\", line 208, in train\noptimizer.step()\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 88, in wrapper\nreturn func(*args, **kwargs)\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 28, in decorate_context\nreturn func(*args, **kwargs)\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\", line 133, in step\nF.adam(params_with_grad,\nFile \"C:\\Users\\11351\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim_functional.py\", line 86, in adam\nexp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\nRuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3\n想问一下有大佬遇见过么   这种应该怎么办啊", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Apr 2, 2022", "body": "海外的同学我新建了一个长期tg channel  ", "type": "commented", "related_issue": null}, {"user_name": "MuFannnn", "datetime": "Apr 2, 2022", "body": "请问我训练过程中发现已经符合要求了，怎么保存当前的进度呢？", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Apr 2, 2022", "body": "会按一定步数自动保存得，直接退出就好了", "type": "commented", "related_issue": null}, {"user_name": "MuFannnn", "datetime": "Apr 2, 2022", "body": "好的，谢谢", "type": "commented", "related_issue": null}, {"user_name": "Louis24", "datetime": "May 11, 2022", "body": "can i join the wechat group?", "type": "commented", "related_issue": null}, {"user_name": "feee1ng", "datetime": "May 17, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "skyflym", "datetime": "May 25, 2022", "body": "大佬们 我来求群二维码了 在线等", "type": "commented", "related_issue": null}, {"user_name": "Super-Badmen-Viper", "datetime": "Jun 6, 2022", "body": "大佬们，求群二维码[doge]", "type": "commented", "related_issue": null}, {"user_name": "MichaelChen1989", "datetime": "Jul 12, 2022", "body": "求个群哈，想进行些业务交流", "type": "commented", "related_issue": null}, {"user_name": "babysor", "datetime": "Jul 16, 2022", "body": "email联系把，", "type": "commented", "related_issue": null}, {"user_name": "gwegwegewtg", "datetime": "Aug 15, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "gwegwegewtg", "datetime": "Aug 15, 2022", "body": "群过期了  还有新群吗", "type": "commented", "related_issue": null}, {"user_name": "pzhyyd", "datetime": "Aug 18, 2022", "body": "求更新群二维码", "type": "commented", "related_issue": null}, {"user_name": "TaoTaaaa", "datetime": "Sep 2, 2022", "body": "Initialising Tacotron Model...Traceback (most recent call last):\nFile \"synthesizer_train.py\", line 37, in \ntrain(**vars(args))\nFile \"D:\\mockingbird\\MockingBird-main\\synthesizer\\train.py\", line 74, in train\nloaded_shape = torch.load(str(weights_fpath), map_location=device)[\"model_state\"][\"encoder.embedding.weight\"].shape\nKeyError: 'encoder.embedding.weight'", "type": "commented", "related_issue": null}, {"user_name": "TaoTaaaa", "datetime": "Sep 2, 2022", "body": "求更新", "type": "commented", "related_issue": null}, {"user_name": "zhuzaileiting", "datetime": "Aug 19, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "babysor", "datetime": "Aug 19, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "babysor", "datetime": "Sep 7, 2021", "body": [], "type": "issue", "related_issue": "#10"}, {"user_name": "gwegwegewtg", "datetime": "Aug 15, 2022", "body": [], "type": "issue", "related_issue": "#709"}]},
{"issue_url": "https://github.com/OpenVoiceOS/ovos-buildroot/issues/82", "issue_status": " Open\n", "issue_list": [{"user_name": "limitless-code", "datetime": "Aug 26, 2021", "body": "Hardware: RPi4 2GB, 32GB sd card, HDMI out to TV for sound, PS3 eye USB for mic.\nWake word: Hey JarvisNot sure if this is the correct place (mycroft.ai forum maybe a better place?) or not but didn't have this behaviour before on picroft.Firstly it doesn't respond to the spoken wake word \"hey jarvis\", until text input is processed via mycroft-cli-client or home assistant sends a message via the notification integration for mycroft, but separate to this it also goes into a wake word loop.I enter 'play new retro wave' in the mycroft-cli-client and it finds some good tunes and starts playing, then as below it keeps detecting the wake word even though nothing verbal is being said, just music.I'd say the mic is overly sensitive perhaps, but it doesn't respond to when I say 'hey jarvis', it only \"detects the wake word\" when there is continuous loud sound playing from it's self. It will also detect the wake word when there is silence.", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 26, 2021", "body": "In voice.log it shows below. Could not found find model for hey jarvis on precise. and load pocketsphinx instead which is not as accurate?  i shall change the wake word to 'hey mycroft' and test again but I was having similar issues with that wake word too.", "type": "commented", "related_issue": null}, {"user_name": "ChanceNCounter", "datetime": "Aug 26, 2021", "body": "Indeed, inappropriate PocketSphinx activations are no surprise. Sometimes I drive with PocketSphinx running on my phone, and it goes off when I hit a bump!I think that's probably a config problem, but I don't mess with Precise. ?", "type": "commented", "related_issue": null}, {"user_name": "JarbasAl", "datetime": "Aug 26, 2021", "body": "can you share your config? my guess is that its using the old plugins pre migration to OVOS, the fallback to pocketsphinx is hardcoded from mycroft-core, and only works for \"hey mycroft\" since the phonemes etc are undefined for other words (our plugin should accout for this, but core wont use the plugin in fallback)slightly more concerning and weird is the logs saying STT failed to load before the wake word failures, it def looks like a bad config (prolly my fault with bad docs) can you share your .conf ?", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 26, 2021", "body": "Attached, have also supplied logs. I have redacted some sensitive bits in the logs.\nI did try to swap Hey Jarvis to Hey Mycroft but my sd card got corrupted after  'sudo reboot'\nTried a different sd card but the exact same thing happens after 'sudo reboot'It tries to start then RPI4 green led flashes 4 times and fails to boot. I don't think both sd cards could have a problem, seems too consistent?", "type": "commented", "related_issue": null}, {"user_name": "JarbasAl", "datetime": "Aug 26, 2021", "body": "does it work if you replace \"jarbas_precise_ww_plug\" with \"ovos-ww-plugin-precise\" ?i dont see stt changed anywhere, did you previously select the local backend (it messes with user config)?", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 26, 2021", "body": "I tried the replace suggestion but doesn't improve things. I clicked on mycroft backend and paired via setup screen (with hey jarvis wake word) on first boot after setting up Wifi, nothing else was touched. Still does not respond to \"Hey Jarvis\" but also now does not do TTS. I did have to do 'sudo systemctl stop mycroft' and 'sudo systemctl start mycroft', not sure if this is the correct way, as cannot reboot the pi without it corrupting the sd card currently.Same looping wake word detection happening.It still plays the video and audio of the video though after typing in cli \"play new retrowave\"latest logs and configs attached\n", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 27, 2021", "body": "Attached are logs and configs from another fresh install with the Mycroft Backend but paired with 'Hey Mycroft' instead of 'Hey Jarvis'  for comparison. Hey Mycroft behaves much better. When music/ video is playing wake word detection happens every few minutes compared to Hey Jarvis which was every few seconds. It still doesn't respond to spoken \"hey Mycroft\" as it did on picroft but I suspect that's the Mic settings.re: rpi4 bricking itself after 'sudo reboot' that seems to be OK now after pressing the SD card holder on the RPI4 in more so the all the PINs are in contact with the SD card.", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 27, 2021", "body": "A quick note/ query re: video playback. When playing a video with music with a still image everything works as it should. If it plays a full moving video, the audio and video stutters. It is running via a HDMI port on a big screen. Not sure if VLC (I presume it's using VLC) has hardware acceleration enabled or not. I know in the past the 64 bit kernel (in general and not specific to OVOS) did not have this feature configured/ implemented for VLC. I see the kernel for OVOS is aarch64.", "type": "commented", "related_issue": null}, {"user_name": "limitless-code", "datetime": "Aug 26, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/691", "issue_status": " Open\n", "issue_list": [{"user_name": "colinboice", "datetime": "Nov 11, 2017", "body": "I needs to control the light by voice with GPIO and relay. How can I do ??", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 15, 2017", "body": "Take a look at the lights plugin, that's a good place to get started. You can use  for GPIO control on the pi.", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 24, 2017", "body": "how goes it ?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 24, 2017", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/RasaHQ/rasa/issues/9737", "issue_status": " Open\n", "issue_list": [{"user_name": "rgstephens", "datetime": "Sep 28, 2021", "body": "2.8.23.7LinuxResults generating a model from the same training data can be significantly different when built on a GPU from one build to the next.Details can be seen .Slack conversations regarding this issue can be viewed  and .", "type": "commented", "related_issue": null}, {"user_name": "tttthomasssss", "datetime": "Sep 29, 2021", "body": "Exalate commented:tttthomasssss commented: can you provide some more context for reproducing this issue? I will start an investigation but to speed things up I'd need:", "type": "commented", "related_issue": null}, {"user_name": "tttthomasssss", "datetime": "Sep 29, 2021", "body": "Exalate commented:tttthomasssss commented: there are still a number of unaddressed sources of non-determinism with TF GPU training (see ) - for example with  that we're using in DIET, the Response Selector & TED.The available documentation suggests 2 things to try out (though my reading is, this doesn't get rid of the non-determinism but can make it  ):  As Akela points out in the comment below, Rasa doesn't support TF 2.4 yet, so upgrading is not an option.Its noted that the fix provided with TF 2.4 only works for the dtype , which  mean it works with our codebase (i.e. by default we use  throughout the code as far as I have seen). Additional information is added below.For further context, there is an active  to make TF with GPU training deterministic, but its still an ongoing effort (i.e.  appears to be one of the last unaddressed things). The problem seems to be known for a quite a while now (looks as if  is the original issue). There is currently movement in several different ways in making  deterministic, one way forward was providing a fix for TF 2.4 (and later), subject to the  switch being enabled, that supports  (i.e. should work with our codebase), but not e.g.  (see discussion on a TF PR ).For TF 2.6 onwards the following behaviour is implemented:", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Sep 29, 2021", "body": "Exalate commented:akelad commented:Rasa OSS 2.8 doesn't have 2.4 as a requirement though, as far as I know you can't just upgrade to tf 2.4 just like that - ", "type": "commented", "related_issue": null}, {"user_name": "koernerfelicia", "datetime": "Oct 4, 2021", "body": "Exalate commented:koernerfelicia commented:  we've actually run into this exception being thrown when  is set with TF 2.6 . It turns out that the deterministic implementation (for any data dtype, including float32) is  available for standard TF. If users/customers want determinism, they will have to use the NVIDIA container  onwards.As Thomas noted above, we never had determinism for this particular op. As far as I understand, it was briefly possible for TF 2.4-2.5, but no longer in 2.6. Is this something we want to support? If so, we'd need to look into the NVIDIA container further. I'd create a followup issue and try to get the relevant eyes on it.", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Oct 11, 2021", "body": "Exalate commented:rgstephens commented:GPU determinism issue is an issue with the tensorflow library itself. It’s a bit better in versions 2.4+, unfortunately we haven’t upgraded to a higher version yet due to some performance issues (more info ). Rasa  upgrades to TensorFlow 2.6.Even with the upgrade, it doesn’t sound like tensorflow has fully solved this problem. Aa a result, for the best stability users should train on CPU for now.", "type": "commented", "related_issue": null}, {"user_name": "koernerfelicia", "datetime": "Oct 11, 2021", "body": "Exalate commented:koernerfelicia commented:In fact, in the process of upgrading I discovered there are even more problematic ops, including , and . There's a whole list .", "type": "commented", "related_issue": null}, {"user_name": "JEM-Mosig", "datetime": "Oct 15, 2021", "body": "Exalate commented:JEM-Mosig commented: Are we ok with the workaround (use CPU), or should we invest more time on this? From the TF side there is no near-term solution, so it might be  hard for us to come up with a solution. It seems like the problem here is more that the training data isn't good enough / one would need a validation set, etc.", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Oct 15, 2021", "body": "Exalate commented:rgstephens commented: Can you explain further about the  that would be needed? This  is a well tested production bot which runs cross validation in their ci/cd pipeline as we recommend.", "type": "commented", "related_issue": null}, {"user_name": "b-quachtran", "datetime": "Oct 15, 2021", "body": "Exalate commented:b-quachtran commented: Many customers are training their large bots in a GPU environment, so I don't think recommending CPU training as a workaround is a viable alternative.", "type": "commented", "related_issue": null}, {"user_name": "JEM-Mosig", "datetime": "Oct 18, 2021", "body": "Exalate commented:JEM-Mosig commented: Cross-validation should also do it. But if a model generates wildly different outputs when trained with different random seeds, then either the training data isn't good or it hasn't converged (or both). But that should show up in cross-validation. Or is this mostly about confidence values?", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Oct 18, 2021", "body": "Exalate commented:rgstephens commented:Their random seed is .The customers evaluation details is posted privately .The priority setting for this issue was last discussed  and I think it should stay as .", "type": "commented", "related_issue": null}, {"user_name": "akelad", "datetime": "Nov 1, 2021", "body": "Exalate commented:akelad commented: what's happening with this issue?", "type": "commented", "related_issue": null}, {"user_name": "TyDunn", "datetime": "Nov 1, 2021", "body": "Exalate commented:TyDunn commented: Followed up in Slack ", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Dec 2, 2021", "body": "Exalate commented:rgstephens commented:Any updates on this issue? I may be running into this with another bot.", "type": "commented", "related_issue": null}, {"user_name": "dakshvar22", "datetime": "Feb 7, 2022", "body": "Exalate commented:dakshvar22 commented:Fluctuations in performance of models in terms of F1 score on test set was investigated in this . Results were documented  and in the  as well. (Sneak peek: Extremely small fluctuations observed, the customer dataset was also included in the investigation.)What hasn't been investigated:As this  mentions, the problem might be one level deeper in the values of confidences that are predicted by the models. Since the former issue did not investigate this, we should look into that as part of this issue.One hunch that I have: The config used in the dataset has  included. The featurizer uses batch inference which might result in  since  might be using  operation. So, the real problem might be with the featurizer. If you have time, can you re-run your analysis with two different configs:", "type": "commented", "related_issue": null}, {"user_name": "rgstephens", "datetime": "Sep 28, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "akelad", "datetime": "Sep 29, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "tttthomasssss", "datetime": "Sep 29, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "tttthomasssss", "datetime": "Sep 30, 2021", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 15, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot", "datetime": "Mar 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "rasabot-exalate", "datetime": "Mar 17, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/evancohen/smart-mirror/issues/386", "issue_status": " Open\n", "issue_list": [{"user_name": "decentralgabe", "datetime": "Aug 31, 2016", "body": "Continuing what I was working on in .I have read when you (Evan) have written about TTS in , and agree that having a speech response to most commands would be useless; however, when asking \"what time is it?\" I think it would be a nice feature if the mirror spoke the time at the user, though the \"what time is it\" feature should be fixed to display the time on the mirror too if any other services are open. There are other cases too where TTS could be useful down the line – speaking results from the Wolfram Alpha API, announcing sports scores, timed reminders and so on.I had been looking into the ; however, it appears that Google has dropped support for the API within embedded/dev Chromium environments as of . The problem isn't with the SpeechSynthesis commands itself (they are available within the mirror), but there are no voices available for the API to work with (evident by a call to speechSynthesis.getVoices()). .I had looked into a few other user's projects that make use of the Speech API (they too do not work in the mirror), including , and .What I will attempt next is to work with a few of the Linux-specific TTS solutions outlined .: Might be worth noting that SpeechSynthesisUtterances work when I run the mirror code on my Mac, but does not work on the RPi.", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Sep 6, 2016", "body": "I've been working with getting Festival to speak commands, which work via the terminal, and would work through ; however, ALSA does not allow multiple sound sources (the mirror and festival) to go at the same time.I've been playing around with the .asoundrc file to see if I can get  to work, but I don't understand this stuff too well yet.By the way,  I've been trying to figure out.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Sep 13, 2016", "body": "Sorry for the delay in response here! You've answered your own question, DMIX is the way to go. It would be helpful to know what issues you've been having trying to figure it out.Just taking a stab in the dark here, but if you are using the provided  file then you should just be able to update the  pcm device, for instance:", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Oct 8, 2016", "body": "Working asoundrc...", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Oct 22, 2016", "body": "With the updated speech recognition work that I just completed, my recommendation would be to use Say: Troublesome audio configuration should be a thing of the past (I still have to verify this though).", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Oct 31, 2016", "body": "I had tried say back when I started down this rabbit hole. Just tried it again with the updated mirror code, and it has no output sound, no error messages. I'm guessing there's still a dmix issue.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Nov 1, 2016", "body": "@glcohen have you followed the updated documentation? I tried Say with Sonus a few weeks ago and it worked without a hitch.", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Nov 2, 2016", "body": "Updated asoundrc worked!Should I submit a pull request?", "type": "commented", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Nov 27, 2016", "body": "what's the status on this one... @glcohen were you able to submit a pull request to the dev branch can this and  be closed? also i'm concerned with the asoundrc info above leading people down the wrong troubleshooting path... can you please update the asoundrc file that is working for you?", "type": "commented", "related_issue": null}, {"user_name": "decentralgabe", "datetime": "Nov 27, 2016", "body": "Just submitted pull request .  can be closed.", "type": "commented", "related_issue": null}, {"user_name": "evancohen", "datetime": "Sep 6, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "evancohen", "datetime": "Sep 23, 2016", "body": [], "type": "issue", "related_issue": "#378"}, {"user_name": "decentralgabe", "datetime": "Nov 27, 2016", "body": [], "type": "pull", "related_issue": "#455"}, {"user_name": "justbill2020", "datetime": "Dec 2, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Jan 3, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "justbill2020", "datetime": "Jan 3, 2017", "body": [], "type": "issue", "related_issue": "#280"}, {"user_name": "justbill2020", "datetime": "Mar 17, 2017", "body": [], "type": "issue", "related_issue": "#609"}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/242", "issue_status": " Open\n", "issue_list": [{"user_name": "michelepanegrossi", "datetime": "Jun 19, 2020", "body": "HelloI am experiencing an issue where it is really hard for rhasspy to recognize the word  especially when pronounced by a female voice (sometimes 0% rate over 10 utterances).My sentences.ini file: and  work a lot better even with female voicesMy setup is as follow: rhasspy is running as a service on my raspberry PI4. I have another python3 script which is controlling rhasspy via the HTTP API, receives the transcript and forwards that over to another machine. This other script also runs at startup as a service.I send messages to that script and ask it to query Rhasspy over HTTP.This is the log of one of my requestsWhat can I do to solve this issue?At the moment I am using . I noticed that Rhasspy 2.5 also now supports . Would I get a better result switching to a different recogniser such as Kaldi or Deepspeech?", "type": "commented", "related_issue": null}, {"user_name": "keith721", "datetime": "Jun 28, 2020", "body": "When I previously had word recognition problems in Rhasspy 2.4.x, I switched from pocketSphinx to kaldi, and things got much better. At the time, Rhasspy was having trouble differentiating between 'on' and 'off', quite a bother.", "type": "commented", "related_issue": null}, {"user_name": "michelepanegrossi", "datetime": "Jun 28, 2020", "body": "Yes I had the same experience! I switched to Rhasspy 2.5 too. I started with Deepspeech as I was hoping that would be the answer, but after trying Kaldi I now think that engine is the best solution so far. The issue with 'yes' and 'no' seems solved.However, at the moment I am looking for a way to avoid triggering an intent when a random word is spoken. In my experience Rhasspy tries to force an intent when any word is spoken, not just words that appear in the sentences.ini file. The result is that intents are triggering with random words.Do you know if there is a way of having some kind of 'default' or 'fallback' intent to handle this kind of situation?", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Jul 7, 2020", "body": "See  for some ideas", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/214", "issue_status": " Open\n", "issue_list": [{"user_name": "CoolPiotr", "datetime": "Apr 24, 2020", "body": "If one is using your Pi (for example) as a music player system as well as running Rhasspy for voice control, it would be nice to mute (or substatially lower) the volume on all the other audio sources after wake word detection while listening, and also possibly during text-to-speech playback, much like Alexa or Google Home does.\n(This would require an audio loopback / separate channel for Rhasppy's audio playback of speech or alert noises, I would expect, so one could differentiate its volume from the muted ones.)\nIs anybody working on something like that?", "type": "commented", "related_issue": null}, {"user_name": "litinoveweedle", "datetime": "Apr 25, 2020", "body": "Hello, we discussed this on the forum, it is quite easy to do with external program (muting) if you have information about key word received. For satellite systems this could be done when using Hermes MQTT by subscribing for given topics. I did also asked for possibility to get Rhasspy status, or even better to get programmable hooks (i.e. executing of external command on given Rhasspy action) as a . I can even try to prepare PR for that, the only problem is I am not yet migrated to 2.5, so I would wait for it first.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/199", "issue_status": " Open\n", "issue_list": [{"user_name": "vabene1111", "datetime": "Apr 2, 2020", "body": "I dont know if this is a bug or just some kind of usage error but i have tried both pocketsphyx and prorcupine as wake word providers. Both work and have good recognition rates but the recognition after the wake word is very bad compared to the recognition when clicking \"hold to record\".When using hold to record i feel like the recognition rate is somewhere around 98%, with the wake word and a lot of trial and error for the right pronunciation and timing between waking and speaking its somwhere around 20-30%. I have been trying  and  as commands.I am using v2.4.19 with the docker install and a ReSpeaker 2-Mic board as a microphone (but i dont thing thats the cause since the manual record works great).The settings are all default (except of course that the wake word detection was turned on). I have trained multiple times, restarted, and cleared the training chache and retrained.Any ideas why this difference could occur ?here the settings from the advanced tab (should be only the non default ones if i understand correctly)", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Apr 10, 2020", "body": "There should be almost no difference unless the feedback sounds are bleeding over into the recorded voice command. If you record a command and click the play back button in the web UI, do you hear the beeps?", "type": "commented", "related_issue": null}, {"user_name": "vabene1111", "datetime": "Apr 10, 2020", "body": "so until now i did not have an output device configured. I attatched a headset for testing.When recording, stopping recording or playing the voice command no sounds play. When saying the wake word it does make two sounds, one when starting recording and one when ending (at least that what i think since the icon on the top left changes as well between the beeps).Still the detection is basically useless when using the wake word and almost perfect when triggering the recording manually.", "type": "commented", "related_issue": null}, {"user_name": "markusappel", "datetime": "Apr 15, 2020", "body": "Started playing today and had the same problem. When listening to the last command in the web UI after the wake word (thanks for the hint ), I realized that the first split second of the command was cut off and the first word could not be recognized. Seems like webrtcvad needed some tuning ... adding this to the profile fixed it for me:(Although something was weird about the  setting: somewhere around 2-3 the behaviour jumped from \"cutting the first command word\" to \"keep all the silence between wake word and first command word\")", "type": "commented", "related_issue": null}, {"user_name": "vabene1111", "datetime": "Apr 16, 2020", "body": "Ok that is definitely a huge improvement to how it was before! It feels like everything is a little slower now but that might be something else.", "type": "commented", "related_issue": null}, {"user_name": "Mic92", "datetime": "May 26, 2020", "body": "I can confirm that  is an improvement:Before when I would say:  it would only recognize . Now it recognizes the whole sentence.", "type": "commented", "related_issue": null}, {"user_name": "Mic92", "datetime": "May 26, 2020", "body": "Should I make a PR to change the defaults?", "type": "commented", "related_issue": null}, {"user_name": "Mic92", "datetime": "Jun 7, 2020", "body": "I don't see this problem anymore with rhasspy 2.5 from here: ", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Apr 10, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/193", "issue_status": " Open\n", "issue_list": [{"user_name": "jwillmer", "datetime": "Mar 12, 2020", "body": "It would be most useful if we can train the system to differentiate who said something. Depending on the person we could then start or ignore a command. For instance:", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Mar 28, 2020", "body": "Kaldi apparently supports this through something called \"x-vectors\". I'd be interested to add this, but I haven't had time to look into how to do a basic \"WAV files + labels\" training for classification.BTW, the kids activating Rhasspy are why I can't really use it at home much :/", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "Mar 28, 2020", "body": "I’ve tested Kaldi « i-vectors » for speaker identification but it needs a LOT of training data to approach a satisfactory error rate (a few hundred short WAVs per user is apparently the minimum).The best I got with around 5 samples per user was a 24% error rate following this :\nThe « x-vectors » add some improvements but they still needs like hundreds of samples per user to perform correctly (like 7-8% ER)It would be pretty awesome to achieve speaker identification though ", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Mar 28, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/181", "issue_status": " Open\n", "issue_list": [{"user_name": "basnijholt", "datetime": "Feb 16, 2020", "body": "I am using a HassIO installed Rhasspy 2.4.18 on a RPi 4.I am getting the error consistently:I have removed my entire  and reinstalled the add-on, however, the error returns. All of my relevant config files are .I have found  where  suggested , however, that didn't solve it for me.Another person  the same problem. There,  removed the  folder and the problem was solved. This also didn't help in my case. the same problem. There \"deleting the settings\" is suggested, however, it's unclear to me which settings should be removed.Several others seem to have the problem, judging from .", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Feb 24, 2020", "body": "There must be an error during training then, since the HCLG.fst is not being produced. This could also be caused if you have \"open transcription\" checked in your settings.", "type": "commented", "related_issue": null}, {"user_name": "basnijholt", "datetime": "Feb 27, 2020", "body": "Thanks for your reply,  is turned off.Considering many others experience this problem, I am not sure whether it's due to my sentences.Would you know how to debug this?", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "Feb 27, 2020", "body": "I often had this error when Rhasspy had to generate pronunciations for unknown words when using Kaldi.\nWhat I did to fix it is download the profile files again and re-train.\nIf it doesn't work, try looking at your  file to check if everything is ok (does it have a new line at the end, etc.).\nHope this helps.", "type": "commented", "related_issue": null}, {"user_name": "basnijholt", "datetime": "Feb 27, 2020", "body": ", indeed, thanks a lot. The culprit was my  (see below). I removed them and now it works again., shouldn't there be a better error message that points people in the right direction>?", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "Apr 10, 2020", "body": "The custom words look fine to me. Was there anything in the log from Kaldi about them?", "type": "commented", "related_issue": null}, {"user_name": "basnijholt", "datetime": "Feb 16, 2020", "body": [], "type": "issue", "related_issue": "#172"}, {"user_name": "synesthesiam", "datetime": "Apr 10, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/80", "issue_status": " Open\n", "issue_list": [{"user_name": "russchidy", "datetime": "May 13, 2022", "body": "Hi guys can anyone help with this deploy error in the second stage;Waiting for app to start...\nFAILED\nTIP: use 'cf logs watson-voice-bot-20220513154240582 --recent' for more information\nJob execution ended at Fri May 13 15:53:47 UTC 2022full text is as per below:Preparing to start the job...\nRunning on public worker: au-syd-tok05-backup\nJob execution started at Fri May 13 15:44:33 UTC 2022\nPipeline image: latest\nPreparing the build artifacts...\ncf login -a \"\" -u apikey -p \"****\" -o \"\" -s \"dev\"\nAPI endpoint: \nAuthenticating...\nOKTargeted org Targeted space devAPI endpoint:    (API version: 2.180.0)\nUser:           \nOrg:            \nSpace:          dev\nCreating service instance wvb-watson-assistant in org  / space dev as ...\nOKService wvb-watson-assistant already exists\nCreating service instance wvb-text-to-speech in org  / space dev as ...\nOKService wvb-text-to-speech already exists\nCreating service instance wvb-speech-to-text in org  / space dev as ...\nOKService wvb-speech-to-text already exists\nPushing from manifest to org  / space dev as ...\nUsing manifest file /workspace/4816dbc4-b309-4a7e-a63e-06b97f68932b/manifest.yml\nGetting app info...\nCreating app with these attributes...Creating app watson-voice-bot-20220513154240582...\nMapping routes...\nBinding services...\nComparing local files to remote cache...\nPackaging files to upload...\nUploading files...0 B / 1.60 MiB    0.00%\n192.00 KiB / 1.60 MiB   11.70%\n512.00 KiB / 1.60 MiB   31.20%\n1.12 MiB / 1.60 MiB   70.19%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00%\n1.60 MiB / 1.60 MiB  100.00% 2sWaiting for API to complete processing files...Staging app and tracing logs...\nDownloading python_buildpack...\nDownloaded python_buildpack (4.5M)\nCell 33a42869-b23a-465a-afc6-9f59595e2625 creating container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nCell 33a42869-b23a-465a-afc6-9f59595e2625 successfully created container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nDownloading app package...\nDownloaded app package (1.6M)\n-----> Python Buildpack version 1.7.53\n-----> Supplying Python\n-----> Installing python 3.10.4\nDownload [https://buildpacks.cloudfoundry.org/dependencies/python/python_3.10.4_linux_x64_cflinuxfs3_e053ca78.tgz]\nUsing python's pip module\n-----> Running Pip Install\nCollecting ibm-watson==5.2.3\nDownloading ibm-watson-5.2.3.tar.gz (406 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 406.9/406.9 KB 6.8 MB/s eta 0:00:00\nInstalling build dependencies: started\nInstalling build dependencies: finished with status 'done'\nGetting requirements to build wheel: started\nGetting requirements to build wheel: finished with status 'done'\nPreparing metadata (pyproject.toml): started\nPreparing metadata (pyproject.toml): finished with status 'done'\nCollecting Flask==1.1.1\nDownloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.5/94.5 KB 4.8 MB/s eta 0:00:00\nCollecting python-dotenv==0.10.3\nDownloading python_dotenv-0.10.3-py2.py3-none-any.whl (16 kB)\nCollecting flask-cors==3.0.9\nDownloading Flask_Cors-3.0.9-py2.py3-none-any.whl (14 kB)\nCollecting flask-socketio==4.2.1\nDownloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\nCollecting requests<3.0,>=2.0\nDownloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 KB 301.0 kB/s eta 0:00:00\nCollecting websocket-client==1.1.0\nDownloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.7/68.7 KB 1.8 MB/s eta 0:00:00\nCollecting ibm-cloud-sdk-core==3.*,>=3.3.6\nDownloading ibm-cloud-sdk-core-3.15.1.tar.gz (50 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 KB 667.7 kB/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'done'\nCollecting python-dateutil>=2.5.3\nDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 5.5 MB/s eta 0:00:00\nCollecting Werkzeug>=0.15\nDownloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.9/224.9 KB 5.7 MB/s eta 0:00:00\nCollecting click>=5.1\nDownloading click-8.1.3-py3-none-any.whl (96 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 3.1 MB/s eta 0:00:00\nCollecting Jinja2>=2.10.1\nDownloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 6.1 MB/s eta 0:00:00\nCollecting itsdangerous>=0.24\nDownloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting Six\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting python-socketio>=4.3.0\nDownloading python_socketio-5.6.0-py3-none-any.whl (56 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 KB 3.6 MB/s eta 0:00:00\nCollecting urllib3<2.0.0,>=1.26.0\nDownloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.0/139.0 KB 7.6 MB/s eta 0:00:00\nCollecting PyJWT<3.0.0,>=2.0.1\nDownloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\nCollecting MarkupSafe>=2.0\nDownloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nCollecting bidict>=0.21.0\nDownloading bidict-0.22.0-py3-none-any.whl (36 kB)\nCollecting python-engineio>=4.3.0\nDownloading python_engineio-4.3.2-py3-none-any.whl (52 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 KB 1.7 MB/s eta 0:00:00\nCollecting charset-normalizer~=2.0.0\nDownloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting certifi>=2017.4.17\nDownloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.2/149.2 KB 3.7 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\nDownloading idna-3.3-py3-none-any.whl (61 kB)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 4.4 MB/s eta 0:00:00\nUsing legacy 'setup.py install' for ibm-cloud-sdk-core, since package 'wheel' is not installed.\nBuilding wheels for collected packages: ibm-watson\nBuilding wheel for ibm-watson (pyproject.toml): started\nBuilding wheel for ibm-watson (pyproject.toml): finished with status 'done'\nCreated wheel for ibm-watson: filename=ibm_watson-5.2.3-py3-none-any.whl size=403321 sha256=6293495beb9cb2f15a3c774847508825066fe14cc5bea6220677e49da197392f\nStored in directory: /tmp/cache/final/pip_cache/pip/wheels/56/24/2f/1622dfa6e36d96d4f3df7b335822b80fa26ef2b8e219dab52f\nSuccessfully built ibm-watson\nInstalling collected packages: python-dotenv, certifi, Werkzeug, websocket-client, urllib3, Six, python-engineio, PyJWT, MarkupSafe, itsdangerous, idna, click, charset-normalizer, bidict, requests, python-socketio, python-dateutil, Jinja2, ibm-cloud-sdk-core, Flask, ibm-watson, flask-socketio, flask-cors\nWARNING: The script dotenv is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: The script normalizer is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nRunning setup.py install for ibm-cloud-sdk-core: started\nRunning setup.py install for ibm-cloud-sdk-core: finished with status 'done'\nWARNING: The script flask is installed in '/tmp/contents3222993510/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed Flask-1.1.1 Jinja2-3.1.2 MarkupSafe-2.1.1 PyJWT-2.4.0 Six-1.16.0 Werkzeug-2.1.2 bidict-0.22.0 certifi-2021.10.8 charset-normalizer-2.0.12 click-8.1.3 flask-cors-3.0.9 flask-socketio-4.2.1 ibm-cloud-sdk-core-3.15.1 ibm-watson-5.2.3 idna-3.3 itsdangerous-2.1.2 python-dateutil-2.8.2 python-dotenv-0.10.3 python-engineio-4.3.2 python-socketio-5.6.0 requests-2.27.1 urllib3-1.26.9 websocket-client-1.1.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (65M)\nUploaded droplet (66.8M)\nUploading complete\nCell 33a42869-b23a-465a-afc6-9f59595e2625 stopping instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6\nCell 33a42869-b23a-465a-afc6-9f59595e2625 destroying container for instance 46bf4f8c-ee45-489d-bb5a-3655e58ddba6Waiting for app to start...\nFAILED\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-20220513154240582 --recent' for more information\nJob execution ended at Fri May 13 15:53:47 UTC 2022Finished: FAILED", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/209", "issue_status": " Open\n", "issue_list": [{"user_name": "litinoveweedle", "datetime": "Apr 21, 2020", "body": "When one Rhasspy (client) is using another Rhasspy (server) via HTTP API for STT conversion and no sentence is pronounced after wake word, so webrtcvad will record empty audio, then API error 400 is returned by Rhasspy server (which is probably OK), but it is also TTS by client Rhasspy back to user (which is probably not OK as it is just internal error):It would be nice to handle this and similar exception and for example play just error sounds, when anything else than HTTP 200 is returned by remote STT API. Rhasspy administrator could still see returned error codes in log, if troubleshooting is required.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/211", "issue_status": " Open\n", "issue_list": [{"user_name": "patrickjane", "datetime": "Apr 23, 2020", "body": "Hello,I am on a fresh install of 2.4.20 on a fresh install of raspbian buster, and I've just added credentials and configuration for the wavenet tts. However it always falls back to espeak.This is what I get in the logs:Not sure what could be wrong here?", "type": "commented", "related_issue": null}, {"user_name": "Deca", "datetime": "Apr 25, 2020", "body": "Same error, I too I'm working on 2.4.20, seems that the tts.py can't import google tts client libraries", "type": "commented", "related_issue": null}, {"user_name": "linuxlurak", "datetime": "Apr 25, 2020", "body": "Same here... seems google tts module is missing?", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "Apr 25, 2020", "body": "Well at least when trying to install it by hand it says its already installed:", "type": "commented", "related_issue": null}, {"user_name": "linuxlurak", "datetime": "Apr 25, 2020", "body": "forgot to clarify, i'am on the rhasspy docker image.  if you are running rhasspy in a docker container too: did you log into this container?perhaps  can help? ;)Edit: Ah I see now, your created a python venv", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "Apr 25, 2020", "body": " nope I'm on a venv installation.", "type": "commented", "related_issue": null}, {"user_name": "Romkabouter", "datetime": "Apr 25, 2020", "body": "I have got the same issue, I'll see if I can fix it", "type": "commented", "related_issue": null}, {"user_name": "Romkabouter", "datetime": "Apr 25, 2020", "body": "I had not noticed it, because playing from cache works fine", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 8, 2020", "body": "Seems like there was a missing dependency. After  it still doesnt work, however the error is different in the logs:Continuing to investigate ...", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 8, 2020", "body": "Okay, found the issue. I was giving  as voice (this is shown at the google website), but rhasspy adds an additional  in front of it, making an invalid argument. I've fixed it, and now wavenet TTS works. I didn't follow the docs correctly on this parts.This is the relevant part in profile.json:", "type": "commented", "related_issue": null}, {"user_name": "Romkabouter", "datetime": "May 8, 2020", "body": "Good find, so I think the docker should be updated", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 8, 2020", "body": "I am not using docker, I did the venv installation.", "type": "commented", "related_issue": null}, {"user_name": "Deca", "datetime": "May 8, 2020", "body": "I logged into the docker image, installed pip, the google-cloud-texttospeech library and then restarted rhasspy but I still have the same error\n\nIs there a proper way to install that python libraries in the docker image?", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 8, 2020", "body": "Okay so I remember that two weeks ago I already tried to fix this, and installed some google package, but still it didnt work, so I stopped investigating. Maybe theres a second package missing.Lets compare:", "type": "commented", "related_issue": null}, {"user_name": "Deca", "datetime": "May 8, 2020", "body": "google-api-core           1.17.0\ngoogle-auth               1.14.2\ngoogle-cloud-speech       1.3.2\ngoogle-cloud-texttospeech 1.0.1\ngoogleapis-common-protos  1.51.0Pretty the same for my docker image.\nThe google-cloud-speech library was missing and I manually installed it but isn't relevant with the issue", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 13, 2020", "body": "Okay so I just did a complete new install, and for me it was fixed after manually installing .So after the initial installation I had:In this,  was missing ( ).", "type": "commented", "related_issue": null}, {"user_name": "erikcoin", "datetime": "May 28, 2020", "body": "I am using the hassio addon and have the same problems with wavenet tts.GoogleWaveNetSentenceSpeaker: Falling back to EspeakSentenceSpeaker\n[ERROR:87465816] GoogleWaveNetSentenceSpeaker: speak\nTraceback (most recent call last):\nFile \"/usr/share/rhasspy/rhasspy/tts.py\", line 643, in in_ready\nself.wav_data = self.speak(message.sentence, voice, language_code)\nFile \"/usr/share/rhasspy/rhasspy/tts.py\", line 742, in speak\nfrom google.cloud import texttospeech\nImportError: cannot import name 'texttospeech' from 'google.cloud' (unknown location)Is there a solution for the addon too?", "type": "commented", "related_issue": null}, {"user_name": "Romkabouter", "datetime": "May 29, 2020", "body": "If have found the cause of this issue.\nBack on 7th of december this commit:\nIt sets google from true to false, causing to skip the install of the Google TTS.\nI have created a PR for this: ", "type": "commented", "related_issue": null}, {"user_name": "Romkabouter", "datetime": "Jun 5, 2020", "body": "PR is merged, can you retry? I do not know if the docker image is already released however.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/synesthesiam/rhasspy/issues/217", "issue_status": " Open\n", "issue_list": [{"user_name": "patrickjane", "datetime": "May 1, 2020", "body": "I want to use rhasspy to build a voice assistant which (for now) contains the following functionality:As you can see, not all of those tasks are handled by home assistant. In fact, I have existing implementations (in python) for Snips.ai for all those tasks.\nTo reuse them, I am developing some kind of plugin-based skill server, where I can hook on my existing code from snips (with, of course, slight modifications).Now, regarding rhasspy, I understand that it can have several endpoints for intent handling:From the documentation I understand that rhasspy will HTTP POST any intent which was detected to my server. This works. I can see the intent JSON coming in at my skill server. , it is unclear to me what kind of HTTP response is expected. From the documentation I can see it must be JSON, however I fail to find a detailed description of how this JSON should look like.If I return an empty JSON, rhasspy complains:  (I'll get this error as popup in the rhasspy browser, not in the rhasspy log files)\nThe log looks like:If I return , rhasspy complains:  (I'll get this error as popup in the rhasspy browser, not in the rhasspy log files)\nThe log looks like:From the documentation I can see that in case of outputting speech, this should be given:and in case of forwarding something (what?) to home assistant, this should be given:(and in this case: what is 'rest of input JSON'?)So, , what do I need to send back to rhasspy after my remote server has successfully handled some intent which was detected by rhasspy and send to my remote server?And what is the idea of the \"forward to home assistant\" feature? I mean if my remote server shall handle the intent, why forward anything else to home assistant? Is this meant to be some kind of light-wrapper for the HA-API in order to enable the remote server to easily generate HA events in addition to its very own intent handling?", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "May 1, 2020", "body": "If your intents are already handled via Snips MQTT Hermes protocol then you won’t have to do much using the next version of Rhasspy (2.5) as it is completely compatible with the Hermes protocol already. It is even going to propose Snips NLU.For the last few months, Rhasspy has gone through an intense restructuring of its services for improved modularity based on the MQTT Hermes protocol.I think the official release of the 2.5 version is approaching rapidly.For more info, maybe this can help:\n", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 1, 2020", "body": "Okay so youre saying that the \"remote server\" / HTTP based variant is going to be deprecated soon?\nWhen reading the docs, I rather had the impression that the snips/hermes interface is merely \"something rhasspy also does, but preferred way is home assistant/node red\".", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "May 1, 2020", "body": "I think the remote HTTP handler will go on via a separated Rhasspy service.As the Hermes MQTT protocol will be used as the underlying glue between all services, it might be simpler to interface directly with it instead of relying on an additional service just to forward intents and dialogue handling messages.If your intents are already handling Snips topics then they should be completely compatible with Rhasspy next version (2.5).How did you handle your skills with Snips? We’re you using snips-skill-server?", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 1, 2020", "body": "Okay I see. I wanted to implement both connectors anyway (HTTP + hermes), since its not so much of an effort.Yes I was using the snips-skill-server previously, so basically I am trying to make a replacement for it since snips is dead.In the current version (2.4.x) I can see there is options for MQTT/snips/hermes already. Is there going to be a bigger change regarding this interface in the upcoming 2.5 release?", "type": "commented", "related_issue": null}, {"user_name": "synesthesiam", "datetime": "May 2, 2020", "body": "Hi , the short answer is that the same intent JSON should be returned (in 2.4). My original idea was that an intent handler could alter the intent before it got passed to Home Assistant (maybe add some extra information).Going forward in 2.5, remote HTTP handling is fully supported. Like any other Hermes-compatible service,  listens for intents via MQTT and POSTs them to some HTTP endpoint. It only expects a JSON object back with an optional \"speech\" property (with a \"text\" sub-property).If you're using NodeRED, you have many choices in 2.5 to handle intents: directly via MQTT (Hermes protocol), via WebSocket ( or through the  bridge), or with the \"remote\" HTTP intent handling system. These can all be used simultaneously as well :)", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 2, 2020", "body": "Okay I see. Meanwhile I've switched to MQTT, and I am receiving the intents from rhasspy via . So I've dropped the HTTP approach.I have used node-red before with home assistant to do automations, however at some point I have dropped node-red and decided to just do everything in home assistant.\nI am a programmer for quite a long time, and I would probably always favor coding something fancy instead of clicking things together in node-red. I would probably never try to implement the dark-sky API within node-red. But that might be just me.Also I liked the way snips did it, in that you could pull existing skills from their store and just plug them into your system without much effort. This is why I started working on a skill-server replacement.", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 3, 2020", "body": "So in 2.4 rhasspy does not yet listen on , right? So it currently only pushes intents to  but no further MQTT interaction I guess.\nSo for now I have my existing skills working with said skill server, which might look like this:If you think this project might be useful for rhasspy I'd be happy to put it on github, so far I guess its satisfying my own personal needs for the voice assistant.Idea of the skill server would be:I might add a little cli tool for this to handle skill installation & setup (same as we had with snips).", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 7, 2020", "body": "\nOkay so meanwhile I actually came up with this:\nI'd be happy to work on some kind of skill-platform/marketplace thingy, if you guys be up for it.", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "May 7, 2020", "body": "This is pretty neat I’d be even better if languages outside of python could be used for skills. Like executing a command line and using stdin/stdout to communicate over a simple JSON protocol. A simple JSON/YAML at skill root level with skill properties (name, description, author, intents to handle, command to execute, etc) maybe ?Just thinking ", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 7, 2020", "body": "I think we have that simple JSON protocol already, which is hermes over MQTT. Introducing a similar transport-/language-agnostic protocol on top of it might not be  useful, since you could achieve that with something like node-red already I guess.I think it boils down a little on how the overall workflow of \"skills\" shall be, and if it is meant to be more for like developers and hackers, or more for the average user.\nWhat I mean is, that in Snips/Alexa/.../ one would usually not install the assistant and then start programming. The average user would merely just install the assistant, and pick up existing skills from some kind of skill marketplace. Still, developers/hackers are the ones who can/will provide/develop a plethora of skills, and from my point of view, it would be okay (for a , that is), to keep it in one technology stack.[edit] By the way, when it comes to sharing skills and installing skills from other developers, one half of it right now cannot be easily shared, which is the sentences & slots. Is there any idea/concept for this to enable easy sharing in the future of rhasspy?", "type": "commented", "related_issue": null}, {"user_name": "mathquis", "datetime": "May 7, 2020", "body": "Fair enough ;)It boils down to how the sentences/slots are registered and forwarded to the ASR and NLU services. Maybe  could provide more insights on how Rhasspy 2.5 will handle the dataset.", "type": "commented", "related_issue": null}, {"user_name": "philtweir", "datetime": "May 29, 2020", "body": "Just to feed back,  (and others), I've been experimenting with your  and rhasspy 2.5, and have managed to proof-of-concept (very roughly) a dialogue-based countdown timer skill, and a skill for adding reactions to RocketChat's most recent Gnome notification (happy to share once tidied a little) - for keeping those modular, I find your hss-skill pattern works quite well, and can see a couple of other itches I'll likely to use it to scratch. However, keen to know if you or others have had any more thoughts on trajectory.", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 30, 2020", "body": "I am using the  now for a while with rhasspy 2.4, and indeed it  work, at least as much as to replace my existing snips-based skills.\nI have a couple of skills on my account, which could serve as example implementations (, ,  and ).Yet,  is just a first draft, and I would be happy to augment and improve it further. Two things I would like to implement, but will need help from rhasspy developers:Some thoughts on 2):\nCurrently, this  be possible, since rhasspy's HTTP API contains endpoints for a) updating sentences and b) an endpoint to trigger training. However, I don't feel exactly good about having the  push sentencens into rhasspy like this. It would be any idea, however I'd like to hear rhasspy developer's opinion first on this, maybe there's a smarter way.", "type": "commented", "related_issue": null}, {"user_name": "koenvervloesem", "datetime": "May 30, 2020", "body": "Hi , impressive work! I have been thinking about the same functionality and implementing part of it too.Some remarks:Are you active ? I have discussed about these and other topics here:The result of my thoughts in the first forum post is a helper library for Rhasspy apps, . This is just a wrapper library around  to make it easy as possible to create Rhasspy apps. It's still a proof of concept, but already quite usable.It seems to me that your hss-server and hss-skill are tightly intercoupled. I'm not sure that's the best way to go forward. Ultimately a skill server should be able to install skills developed in various languages, as  . So that's why I'm not too fond of the idea to couple a skill server to an app library.But even with Python alone it would be better to make the architecture more flexible. There are a couple of initiatives to create libraries to develop Rhasspy apps ( is also working on a proof of concept for Rhasspy Hermes apps in AppDaemon) and it would probably better if we could share some parts of the API. Because one of Rhasspy's strong points is its flexibility in which services you can use with it, I think we should try to keep our options open for the creation and distribution of Rhasspy apps, so it's nice that there are various app platform implementations. But it would be good if we could share some resources.Another idea I have created a proof of concept for is running each Rhasspy app in a Docker container. You can see my thoughts about it in the third forum link mentioned above. Coupled with Mosquitto's access control list and a username and password for each app, we can precisely limit what an app is able to do. My goal is to work on this idea further, because I don't like the idea of apps being able to do what they want on my machine or my network. So an alternative \"skill server\" could just install Docker containers this way to add Rhasspy apps.These are just some ideas :-) Don't let this dissuade you from working on hss-server and hss-skill, I think there's still a lot to explore in this domain and having multiple implementations for Rhasspy is good.", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 30, 2020", "body": "Well mainly rhasspy would need to support the following workflow:As I am still on rhasspy 2.4, I have no clue whether or not this already works.Will do.Agreed. All which I have written on this thread should be considered as support for rhasspy, and while I've already implemented a working skillserver, it should be merely a first draft, and subject to change. I would be happy to contribute, and agree to share resources.When I first started using rhasspy, I found that there is no real intent handling in place, other than publishing intents via MQTT, HTTP or to home assistant, all of which were not suitable for me.Nope, didn't even know a forum exists :DWell, I think both approaches have their pros and cons, and as I have stated earlier, it pretty much boils down to how rhasspy is meant to do intent handling. There is the option already to publish intents over MQTT, HTTP and send to home assistant. Lets ignore the HTTP stuff, then MQTT alone is  the language agnostic decoupling, since it would allow anyone to just hook on the MQTT message using their favourite language. It would still work with a running , since the server would just drop unknown intents.When I've started working on  (and the efforts made so far are really trivial, so starting from scratch again is not an issue for me) I had the idea of resembling the , which pretty much acted in the very same way.Some of the reasons for using a server-approach over standalone app runtimes were:So basically what I want to say is, if rhasspy decides to offer intent handling, but at the same time make it possible to use , then it would get a bit hard to bring all this together. As I said, you've got the decoupling via MQTT already, I see no real benefit of  decoupling within the skill-server, only to enable developers to implement skills in other programming languages.Especially when we're talking about installing skills from other developers, skill marketplace/ecosystem, I think all this might get really complicated, when it shall be possible to install skills written in arbitrary programming languages. Just think about all the stuff that might need to be set up, like dependencies, tools, libraries. Right now,  is going to create python venvs for each skill and then install the skill's dependencies upon installation. Similar tasks would be necessary for other programming languages as well. So unless you want to decouple completely - that is, provide no rhasspy tool/script to install skills, and just have the user/developer install their skills manually - this might get overcomplicated.\nI think other voice assistants also don't support arbitrary programming languages for their skill development.Then, you would need some sort of protocol between the skill server and the docker container. And this would essentially mean you're back to zero, as hermes  already. So either your docker containers which contain the skills just need to implement the hermes protocol , or we're talking about a second, non-standard protocol.However maybe the skill server  support two kinds of skills; docker based and python based, and docker based skills would have  with the skill server at all.But even then, the user would need to configure MQTT connection parameters for every docker container, which is pretty much what I wanted to avoid in the first place.No worries, all I want is to contribute to rhasspy's functionality. Although it is named , for me its more like a rhasspy-skill-server.\nI am a professional c++/nodejs/python developer, if you want me to contribute and just tell me what to implement, I'll go for it ;)BTW: you're using the term \"app\" for what I consider \"skill\", so in the above just read \"skill\" as \"app\" :)[edit] So maybe as some kind of rough requirements list for intent handling:(to be continued)", "type": "commented", "related_issue": null}, {"user_name": "koenvervloesem", "datetime": "May 30, 2020", "body": "This should already work on the Rhasspy 2.5 pre-release :-) Have a look at . That's why I was a bit puzzled why you would need a skill server for this.", "type": "commented", "related_issue": null}, {"user_name": "philtweir", "datetime": "May 30, 2020", "body": "I can confirm that this flow seems to work for me with hss-server and Rhasspy Voltron - my understanding (from up the issue) was that that was where you were targetting  ?I'm not sure if this is quite what you mean, but I have added a couple of small tweaks to my local version of hss-server and hss-skill to add a  with same args as  but sends a  instead of the , with an intent-filter of only the current intent. That then comes back into the Skill's  method, and an  switch on the text gives separate flows for original and follow-up commands.Working example:Rhasspy seems to implement that fine, and afaiu the dialogue state handling parameters from the Hermes protocol are implemented (but haven't tried) - I have tested the \"no-matching-intent\" dialogue event too, and that can be picked up, for conversational-response misses. The main downside is that, as it still has to match the intent (even if filtered), the possible responses must be sentences for that intent in Rhasspy, just as the original command is.I do recall seeing suggestion in the forums of modally switching the STT for follow-up, which would be nice, but at least if there was some way of making an intent, or certain sentences that trigger it, only matchable on follow-up dialogue (so words like \"no\" and \"yes\" wouldn't technically be valid opening commands), the next step, of switching speech-to-text from e.g. PocketSphinx to DeepSpeech in follow-up to give greater freedom, would be a bonus.", "type": "commented", "related_issue": null}, {"user_name": "philtweir", "datetime": "May 30, 2020", "body": "On the language-independence as  , I was thinking that too - I can see your reservations  but if  is negotiable, there's one or two RPC options.IMO a simple option from a skill-maker's perspective (which should be an almost drop-in replacement from skill-maker-flow perspective) would be WAMP with Autobahn - I have used this on a number of projects for near-transparent RPC between languages in a Python-native-feeling way (it also has the bonus of supporting event subscription ootb). Happy to PoC that, if it would be a potential option. That said, having MQTT already there, there's maybe an argument for RPC over MQTT, but it those options don't seem nearly as mature as either RPyC or Autobahn.A second benefit of this is that it'll work fine with venv or dockerized processes (Python or otherwise), and not increase the code a skill-creator would write.To  's question about where a skill-server would fit - I think  's point about abstracting MQTT protocol interaction away is important. I probably wouldn't have bothered getting started with those if it wasn't just a case of \"fill in this \" and away you go, only a small papercut, but hss-server (or seemingly Rhasspy Hermes App) does address it.And of course the bullets  mentioned sound like things that, given the modular nature of Rhasspy, it would want to defer to a handler such as  or Rhasspy Hermes App (which I hadn't seen and haven't yet looked at properly!)", "type": "commented", "related_issue": null}, {"user_name": "philtweir", "datetime": "May 30, 2020", "body": "(and a language-independent RPC framework would avoid every language having to have a Hermes implementation as a library for skill-makers)", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 30, 2020", "body": "Yeah, thats exactly the idea. Although I would have named it  :)I havent worked on this since 2.5 is not yet released.Thats what I mean with \"rhasspy needs to support this\". Doing full/plain new intent recognition for a follow up question is probably not more than a workaround I guess.\nMost likely, the follow up responses should also go into sentences.ini, maybe even a separate NLU model could be used for those purposes.See the above. I'm gonna check it out when 2.5 is released.I think my point was not so much about the protocol between skill-server and skill (which  with no issues be language agnostic, e.g. HTTP/JSON), but more about the dependencies and different handling for different languages. For example, node.js based skills would require , which in turn needs to find  on your system. C++ based skills might depend on some c libraries, would you fire off some  upon skill installation?\nUnless you call some generic  upon installation and pretty much leave all those issues to the skill-developers, I can't think of a viable solution that involves arbitrary technologies.So while its perfectly possible and fine to me to use a non-python RPC protocol, you would still have issues when installing the skill via  and later running the skill process.", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 30, 2020", "body": "Maybe we should close this issue, and move the discussion to the forums? I think we have some really good ideas, and we should continue to discuss?", "type": "commented", "related_issue": null}, {"user_name": "philtweir", "datetime": "May 30, 2020", "body": "Fair! Like I say, tidying required :)Indeed - given an intent filter is part of the conversation response API (which Rhasspy implements, afaict, and is a start), it does seems like that approach is not inconsistent with Hermes, at least. However, it would make sense for Rhasspy to do some minimal implementation here - even just to allow marking sentences as ineligible for initial intent matching. Conversely, a potential use-case for full intent recognition (by specifying more than one, or no, intents in the filter) would be to ask a question that could switch path to a different skill.True, but perhaps its a question of level-of-abstraction - if the decision is not made at the protocol level, but potential skill-family helper classes could be made, then language-specific-functionality is not quite so baked in and encapsulated to installation/provisioning functionality (a simple Python install-class for JS might use nodeenv, for instance).Yes, I think this touches on some broader questions that would be great to get input from the Rhasspy architects on (as you'd suggested).Agreed - I think it's safe to say this has turned into solution development rather than issue resolution! If you want to post a link, we can jump over - would be keen to  in that loop - have to say, from my brief look, I like the decorator skill syntax of Rhasspy Hermes App - wondering how hard it might be to use both hss-server and Rhasspy Hermes App together ", "type": "commented", "related_issue": null}, {"user_name": "patrickjane", "datetime": "May 31, 2020", "body": "Okay so I have posted here: Currently I am working on a proper hermes dialog implementation, and I also had some idea for a low-cost marketplace-thingy, I'll see if I can get this up and running until tomorrow.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/78", "issue_status": " Open\n", "issue_list": [{"user_name": "ogbeh", "datetime": "Jan 6, 2022", "body": "I have already copied and paste the api key and url, it still does not work, I have restarted the entire setup and it still throws the same error, what am i doing wrong? Thanks in advance", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/wechaty/wechaty/issues/2343", "issue_status": " Open\n", "issue_list": [{"user_name": "su-chang", "datetime": "Jan 10, 2022", "body": "\nIf the bot has receive  or , I think receive an event notification is better than a message.\nAdd any other context or screenshots about the feature request here.[enhancement]Related issue: ", "type": "commented", "related_issue": null}, {"user_name": "huan", "datetime": "Mar 24, 2022", "body": "I think the  event should have another parameter, for example: , so that we can \"pick it up\".And another design need to be done is that after we have picked up the call, how can we receive the streaming of the voice, and how can we send the streaming of the voice.I have no voice-over IP experience so I have no idea about what they should look like.Any suggestions would be welcome.", "type": "commented", "related_issue": null}, {"user_name": "su-chang", "datetime": "Jan 10, 2022", "body": [], "type": "issue", "related_issue": "wechaty/puppet-whatsapp#23"}, {"user_name": "huan", "datetime": "Jan 11, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/77", "issue_status": " Open\n", "issue_list": [{"user_name": "sfmishra", "datetime": "Dec 21, 2021", "body": "Trying the steps mentioned but getting stuck at the deploying step, everything was fine till build stage but it keep getting failed on Deploy Stage with below logs===================================================================\n4 certifi-2021.10.8 charset-normalizer-2.0.9 click-8.0.3 flask-cors-3.0.8 flask-socketio-4.2.1 ibm-cloud-sdk-core-3.13.2 ibm-watson-5.2.3 idna-3.3 itsdangerous-2.0.1 python-dateutil-2.8.2 python-dotenv-0.10.3 python-engineio-4.3.0 python-socketio-5.5.0 requests-2.26.0 urllib3-1.26.7 websocket-client-1.1.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (59.5M)\nUploaded droplet (61.3M)\nUploading complete\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 stopping instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 destroying container for instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2\nCell e4eb1323-6734-49f0-9d69-c2dbd4a1bc71 successfully destroyed container for instance 6493d65c-4e88-4a76-8b13-8a4da8c94fb2Waiting for app to start...\nFAILED\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-001 --recent' for more information\nJob execution ended at Tue Dec 21 06:07:34 UTC 2021reached out to so many people even posted it issue with Watson group but no one care to respond, can anyone help me resolve this issue, I am very new to Watson so have very minimal knowledge.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/76", "issue_status": " Open\n", "issue_list": [{"user_name": "charlie-2201", "datetime": "Nov 12, 2021", "body": "We want to implement Speech-to-Text and Text-to-Speech functionality over IBM Watson Assistant.\nPlease assist with the process for the same.We visited one of your repositories for the solution\n\nUsing this method, we are facing problems while executing the app.py file\nError: The chatbot speech icon is not functioning and is unable to record any voice input.\nI'm attaching the screenshot of the problem.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/72", "issue_status": " Open\n", "issue_list": [{"user_name": "ather143", "datetime": "Jun 18, 2021", "body": "Preparing to start the job...\nRunning on public worker: jp-tokpublicworker02-2\nPipeline image: latest\nPreparing the build artifacts...\ncf login -a \"\" -u apikey -p \"****\" -o \"\" -s \"dev\"\nAPI endpoint: \nAuthenticating...\nOKTargeted org Targeted space devAPI endpoint:    (API version: 2.164.0)\nUser:           \nOrg:            \nSpace:          dev\nCreating service instance wvb-watson-assistant in org  / space dev as ...\nOKCreating service instance wvb-text-to-speech in org  / space dev as ...\nOKCreating service instance wvb-speech-to-text in org  / space dev as ...\nOKPushing from manifest to org  / space dev as ...\nUsing manifest file /workspace/a8653fd8-66b9-480b-a4e2-75557073a1cb/manifest.yml\nGetting app info...\nCreating app with these attributes...Creating app watson-voice-bot-20210618015210255...\nMapping routes...\nBinding services...\nComparing local files to remote cache...\nPackaging files to upload...\nUploading files...0 B / 1.59 MiB    0.00%\n128.00 KiB / 1.59 MiB    7.84%\n160.00 KiB / 1.59 MiB    9.80%\n576.00 KiB / 1.59 MiB   35.29%\n1.09 MiB / 1.59 MiB   68.62%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00%\n1.59 MiB / 1.59 MiB  100.00% 2sWaiting for API to complete processing files...Staging app and tracing logs...\nDownloading python_buildpack...\nDownloaded python_buildpack\nCell 879700a2-564c-421c-9025-d97173c35f0b creating container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b successfully created container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nDownloading app package...\nDownloaded app package (1.6M)\n-----> Python Buildpack version 1.7.37\n-----> Supplying Python\n-----> Installing python 3.8.9\nDownload [https://buildpacks.cloudfoundry.org/dependencies/python/python_3.8.9_linux_x64_cflinuxfs3_e9cbc67f.tgz]\n-----> Installing pip-pop 0.1.5\nDownload [https://buildpacks.cloudfoundry.org/dependencies/manual-binaries/pip-pop/pip-pop-0.1.5-b32efe86.tar.gz]\n-----> Running Pip Install\nCollecting ibm-watson==4.0.1\nDownloading ibm-watson-4.0.1.tar.gz (297 kB)\nCollecting Flask==1.1.1\nDownloading Flask-1.1.1-py2.py3-none-any.whl (94 kB)\nCollecting python-dotenv==0.10.3\nDownloading python_dotenv-0.10.3-py2.py3-none-any.whl (16 kB)\nCollecting flask-cors==3.0.8\nDownloading Flask_Cors-3.0.8-py2.py3-none-any.whl (14 kB)\nCollecting flask-socketio==4.2.1\nDownloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\nCollecting requests<3.0,>=2.0\nDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\nCollecting python_dateutil>=2.5.3\nDownloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\nCollecting websocket-client==0.48.0\nDownloading websocket_client-0.48.0-py2.py3-none-any.whl (198 kB)\nCollecting ibm_cloud_sdk_core==1.0.0\nDownloading ibm-cloud-sdk-core-1.0.0.tar.gz (19 kB)\nCollecting itsdangerous>=0.24\nDownloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\nCollecting click>=5.1\nDownloading click-8.0.1-py3-none-any.whl (97 kB)\nCollecting Werkzeug>=0.15\nDownloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\nCollecting Jinja2>=2.10.1\nDownloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\nCollecting Six\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting python-socketio>=4.3.0\nDownloading python_socketio-5.3.0-py2.py3-none-any.whl (53 kB)\nCollecting idna<3,>=2.5\nDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\nCollecting certifi>=2017.4.17\nDownloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\nCollecting urllib3<1.27,>=1.21.1\nDownloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\nCollecting chardet<5,>=3.0.2\nDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\nCollecting PyJWT>=1.7.1\nDownloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\nCollecting MarkupSafe>=2.0\nDownloading MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl (30 kB)\nCollecting bidict>=0.21.0\nDownloading bidict-0.21.2-py2.py3-none-any.whl (37 kB)\nCollecting python-engineio>=4.1.0\nDownloading python_engineio-4.2.0-py2.py3-none-any.whl (51 kB)\nUsing legacy 'setup.py install' for ibm-watson, since package 'wheel' is not installed.\nUsing legacy 'setup.py install' for ibm-cloud-sdk-core, since package 'wheel' is not installed.\nInstalling collected packages: idna, certifi, urllib3, chardet, requests, Six, python-dateutil, websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson, itsdangerous, click, Werkzeug, MarkupSafe, Jinja2, Flask, python-dotenv, flask-cors, bidict, python-engineio, python-socketio, flask-socketio\nWARNING: The script chardetect is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nRunning setup.py install for ibm-cloud-sdk-core: started\nRunning setup.py install for ibm-cloud-sdk-core: finished with status 'done'\nRunning setup.py install for ibm-watson: started\nRunning setup.py install for ibm-watson: finished with status 'done'\nWARNING: The script flask is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: The script dotenv is installed in '/tmp/contents018503308/deps/0/python/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed Flask-1.1.1 Jinja2-3.0.1 MarkupSafe-2.0.1 PyJWT-2.1.0 Six-1.16.0 Werkzeug-2.0.1 bidict-0.21.2 certifi-2021.5.30 chardet-4.0.0 click-8.0.1 flask-cors-3.0.8 flask-socketio-4.2.1 ibm-cloud-sdk-core-1.0.0 ibm-watson-4.0.1 idna-2.10 itsdangerous-2.0.1 python-dateutil-2.8.1 python-dotenv-0.10.3 python-engineio-4.2.0 python-socketio-5.3.0 requests-2.25.1 urllib3-1.26.5 websocket-client-0.48.0\nExit status 0\nUploading droplet, build artifacts cache...\nUploading droplet...\nUploading build artifacts cache...\nUploaded build artifacts cache (58.1M)\nUploaded droplet (61.2M)\nUploading complete\nCell 879700a2-564c-421c-9025-d97173c35f0b stopping instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b destroying container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4\nCell 879700a2-564c-421c-9025-d97173c35f0b successfully destroyed container for instance 30eb0400-4a8d-42ce-9433-f2fddb747ec4Waiting for app to start...\nStart unsuccessfulTIP: use 'cf logs watson-voice-bot-20210618015210255 --recent' for more information\nFAILEDFinished: FAILED", "type": "commented", "related_issue": null}, {"user_name": "sfmishra", "datetime": "Dec 21, 2021", "body": "were you able to resolve this, facing same issue", "type": "commented", "related_issue": null}, {"user_name": "russchidy", "datetime": "May 13, 2022", "body": "having same issue...any luck on a solution?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/64", "issue_status": " Open\n", "issue_list": [{"user_name": "markstur", "datetime": "Sep 25, 2020", "body": "There is still a problem with the credentials.  There is a known problem with the assistant and speech-to-text creds roles.  There is a workaround that works for both.Go to the  Cloud Foundry App -> Runtime -> Environment Variables and add the Assistant and STT creds (APIKey and URL) using the same key/values described for use in the local runtime .env.  We'll need to document this if we don't come up with an alternative.Adding a screenshot here for now:", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Sep 25, 2020", "body": "Note:  For assistant make sure you are using the APIKEY from the \"Service\" and not the \"Cloud Foundry Service\".Same for STT.TTS seems to be working fine from the CF provided VCAP_SERVICES runtime variable.  You should not need to add these as shown in the image.  Just ASSISTANT_APIKEY and URL and SPEECH_TO_TEXT_APIKEY and URL", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Jan 12, 2022", "body": "Update:  Seems to work better if you also specify runtime env for TEXT_TO_SPEECH_APIKEY and TEXT_TO_SPEECH_URL.", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Sep 25, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "markstur", "datetime": "Sep 25, 2020", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/wechaty/wechaty/issues/2186", "issue_status": " Open\n", "issue_list": [{"user_name": "ghost", "datetime": "May 7, 2021", "body": "As shown in the error report, my silk voice file is correct and can be sent through other social software, but I can’t send it in Wechaty. What is the specific problem?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/IBM/watson-voice-bot/issues/62", "issue_status": " Open\n", "issue_list": [{"user_name": "Saraswathi24", "datetime": "Sep 17, 2020", "body": "\nI have followed all the steps mentioned  in Readme.nd and tried to deploy the code locally. I haven't installed any web socket yet.\nWith the default one, I am getting output as follows:\nINFO:engineio.server:Server initialized for threading.\nINFO:assistant_setup:Using WORKSPACE_ID=cd761ded-c1a9-476a-89c4-c6de53e00ce2\nWARNING:werkzeug:WebSocket transport not available. Install eventlet or gevent and gevent-websocket for improved performance.The program isn't working when the mic button is clicked it is just enabling the listening mode, after that webpage isn't responding anything. Kindly help me with this issue.", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Sep 17, 2020", "body": " Can you help with this?", "type": "commented", "related_issue": null}, {"user_name": "sanjeevghimire", "datetime": "Sep 25, 2020", "body": " the code is working mostly in firefox and chrome. I tested it on mine and the mic works. Can you make sure the version chrome and firefox you use has support for  and is not deprecated?", "type": "commented", "related_issue": null}, {"user_name": "Saraswathi24", "datetime": "Sep 28, 2020", "body": "  The   is not deprecated in Chrome, even then I find difficulty in recording the audio.It is still not working as expected.", "type": "commented", "related_issue": null}, {"user_name": "SapnaChouta", "datetime": "Oct 12, 2020", "body": " - We have similar issues.  Voice bot shows as \"listening\" when we talk but does not respond back.. We have tried this from multiple Mac laptops and multiple browsers like Firefox and Chrome. navigator.getUserMedia() is not deprecated.", "type": "commented", "related_issue": null}, {"user_name": "edffrench", "datetime": "Jan 7, 2021", "body": "  I'm having the same issue running off a ThinkPad, I've tried chrome and firefox. I've attached the respective DevTools consoles for the browsers.Chrome:\nFirefox:", "type": "commented", "related_issue": null}, {"user_name": "yeshapatel356", "datetime": "Feb 22, 2021", "body": "Having same issue.", "type": "commented", "related_issue": null}, {"user_name": "erum007", "datetime": "Aug 17, 2021", "body": "It worked on my PC but when I did the exact same thing from scratch on laptop, I faced this issue and could not figure it out.", "type": "commented", "related_issue": null}, {"user_name": "erum007", "datetime": "Aug 18, 2021", "body": "OK so I found a solution for those who still need it. First of all, use this repository: [deleted]\nAlso, after you configure it on one device, do not use the same credentials on another device.", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Oct 1, 2021", "body": "I edited the above comment from  suggesting to use another repository because I don't see any updates there.  It is just a copy of this one.  Please clarify if there is an actual fix.  Maybe there is a point-in-time or commit in this repo you are referring to?  Please don't just redirect folks to another repository.", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Oct 1, 2021", "body": "The comment about not using multiple devices is a good tip though.  I think this bot gets confused easily and having multiple connections from wherever probablky doesn't help.", "type": "commented", "related_issue": null}, {"user_name": "erum007", "datetime": "Oct 1, 2021", "body": "I don’t know man, I was just playing around. Found that one and it worked while this didn’t. That one is outdated while this one is relatively updated so there must be a problem with any update made. I didn’t dig deep as to what specifically was the problem.", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Oct 1, 2021", "body": "Right, and thanks for the info.  I think this issue is mostly about browser compatibility, but people are also getting caught in some other problems (e.g. multiple simultaneous connections?) that have not been well reproduced.  With the latest version I've seen some \"works for me\" responses.  I'll put the commit which was used in that \"other repo\" in this comment.  It's from 2018, but if anyone wants to compare an old version w/ the latest I don't want to lose that info. The latest commit used above was:", "type": "commented", "related_issue": null}, {"user_name": "ShuwaiGhz", "datetime": "Feb 21, 2022", "body": "Has anyone fix this issue? I am currently having the same issue. Hope anyone can give me pointer on this. It works fine at first, but now it cannot listen what I speak. I tried using firefox , ie, edge as well. but nothing works", "type": "commented", "related_issue": null}, {"user_name": "markstur", "datetime": "Sep 17, 2020", "body": [], "type": "assigned", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/23733", "issue_status": " Closed\n", "issue_list": [{"user_name": "aspain", "datetime": "May 16, 2022", "body": "2.3.0 (latest released)See recorded screencap - in the task instance pop-up menu, sometimes the top menu options aren't clickable until you move the mouse around a bit and find an area where it will allow you to clickThis only seems to affect the , , and  options - but not , or The entire 'bubble' for the options such as 'XCom' should always be clickable, without having to find a 'sweet spot'I am using Astro Runtime 5.0.0 in a localhost environmentmacOS 11.5.2apache-airflow-providers-amazon==3.3.0\napache-airflow-providers-celery==2.1.4\napache-airflow-providers-cncf-kubernetes==4.0.1\napache-airflow-providers-databricks==2.6.0\napache-airflow-providers-elasticsearch==3.0.3\napache-airflow-providers-ftp==2.1.2\napache-airflow-providers-google==6.8.0\napache-airflow-providers-http==2.1.2\napache-airflow-providers-imap==2.2.3\napache-airflow-providers-microsoft-azure==3.8.0\napache-airflow-providers-postgres==4.1.0\napache-airflow-providers-redis==2.0.4\napache-airflow-providers-slack==4.2.3\napache-airflow-providers-snowflake==2.6.0\napache-airflow-providers-sqlite==2.1.3AstronomerI experience this in an Astro deployment as well (not just localhost) using the same runtime 5.0.0 image", "type": "commented", "related_issue": null}, {"user_name": "aspain", "datetime": "May 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "uranusjr", "datetime": "May 16, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "May 16, 2022", "body": [], "type": "pull", "related_issue": "#23736"}, {"user_name": "bbovenzi", "datetime": "May 17, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/22036", "issue_status": " Closed\n", "issue_list": [{"user_name": "NirmalSharmaRaj", "datetime": "Mar 7, 2022", "body": "2.2.2On the home page, when the window size is reduced, a bottom scrollbar appears. But this scrollbar is unclickable, so I am not able to side-scroll using it. The page navigation buttons seems to be overlapping it.\nI expected bottom scrollbar to be moved using click and hold with mouse, to scroll the page.Reduce the size of the browser window for the bottom scrollbar to appear. Then try to click and hold on the scrollbar.Ubuntu 20 LTSVirtualenv installationBrowsers used: Chrome, Edge, Firefox", "type": "commented", "related_issue": null}, {"user_name": "josh-fell", "datetime": "Mar 7, 2022", "body": " I'm not able to reproduce this behavior on 2.2.2 with both side-scrolling via trackpad nor click and drag of the scrollbar.  Are you able to reproduce and/or is there other information that would be helpful to know if this is a bug? We can always convert to a Discussion too.", "type": "commented", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Mar 7, 2022", "body": "Yeah I am not sure, but the css for the dags page was changed in  So we won't have anything on top of the scrollbar anymore.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 2, 2022", "body": "This issue has been automatically marked as stale because it has been open for 30 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.", "type": "commented", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Jul 5, 2022", "body": "I've checked a few browsers and OSes. This issue no longer exists.", "type": "commented", "related_issue": null}, {"user_name": "NirmalSharmaRaj", "datetime": "Mar 7, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "josh-fell", "datetime": "Mar 7, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "eladkal", "datetime": "Jun 1, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 2, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Jul 5, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/21428", "issue_status": " Closed\n", "issue_list": [{"user_name": "potiuk", "datetime": "Feb 8, 2022", "body": "main (development)There is a weird effect when you click the task in the new Grid view to get to task details. The whole grid view seems to shift a little when the pop-up window is displayed and it has a very disturbing effect for the user because if you did not move your mouse, it looks like you clicked wrong DagRun (the shift in my case is such that it looks like you clicked the \"previous DagRun\" as the mouse remains on top of the previous DagRun.This is pretty disturbing - when I saw it for the first time I literally thought I missed the right box and pressed escape and tried again and again - all the time the same and I thought something is wrong with my mouse. Only after a moment I realized that this is the \"grid\" that shifts a bit.This is best seen with the video:No shift of the grid - mouse should stay on top of the clicked DagRunClick on the DagRunLinux Mint 20.3Not relevantOther in BreezeThis was run in Chrome: Version 98.0.4758.80 (Official Build) (64-bit)", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Feb 8, 2022", "body": "cc:  - > I wonder if this is just me or is it same for everyone :)", "type": "commented", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Feb 8, 2022", "body": "On the main tree view your browser thinks there is vertical/horizontal scrolling, but that isn't the case with the modal overlay. It's especially not good that the scrollbar width is basically the same as the task instance width.I assume this happens in a number of browsers/OSs. So I'll work on a fix.", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Feb 8, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Feb 8, 2022", "body": "Ah yeah. I see it now with the scrollbar :). Great eye!", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Feb 8, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Feb 8, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Feb 8, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Mar 10, 2022", "body": [], "type": "pull", "related_issue": "#22123"}, {"user_name": "bbovenzi", "datetime": "Mar 31, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/15416", "issue_status": " Closed\n", "issue_list": [{"user_name": "pateash", "datetime": "Apr 17, 2021", "body": "Currently, when we run doesn't load local tmux configuration file  and we get default tmux configuration inside the containers.Breeze must load local  in to the containers and developers should be able to use their local configurations.\nYES\nNone", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Apr 19, 2021", "body": "SGTM. Can you submit a PR?", "type": "commented", "related_issue": null}, {"user_name": "uranusjr", "datetime": "Apr 19, 2021", "body": "I wondered about this a while ago (mostly because I don’t want to repeatedly ). The problem is any slightly sophisticated local tmux setup would contain a lot of dependencies and it’s not realistic to pull them all into the container. So maybe a better approach would be have a directory in the repository; if the user puts a (git-ignored)  file in it, the file is mounted to the container’s .", "type": "commented", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 20, 2021", "body": "Yes I will.", "type": "commented", "related_issue": null}, {"user_name": "potiuk", "datetime": "Apr 20, 2021", "body": "We already have similar mechanism for reading user-supplied env variables placed in  folder.\nIt could be done in a very similar way .", "type": "commented", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 20, 2021", "body": "thanks , let me check.", "type": "commented", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 20, 2021", "body": "PR ", "type": "commented", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 17, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 17, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "pateash", "datetime": "Apr 20, 2021", "body": [], "type": "pull", "related_issue": "#15454"}, {"user_name": "potiuk", "datetime": "Apr 21, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/13624", "issue_status": " Closed\n", "issue_list": [{"user_name": "eldernewborn", "datetime": "Jan 12, 2021", "body": ":\n2.0.0 (use ):\nN/A:\nN/A:\nThe UI tooltip is misleading and confuses the user.\nTooltip says \" use this toggle to pause the dag\" which implies that if the toggle is set to  the flow is paused, but in fact it's the reverse of that.\nEither the logic should be reversed so that if the toggle is on, the DAG is paused, or the wording should be changed to explicitly state the actual functionality of the \"on state\" of the toggle.\nsomething like \"When this toggle is ON, the DAG will be executed at scheduled times, turn this toggle off to pause executions of this dag \".:\nUI tooltip should be honest and clear about its function.:\nopen DAGs window of the airflow webserver in a supported browser, hold mouse over the (i) on the second cell from left on the top row.\n", "type": "commented", "related_issue": null}, {"user_name": "mik-laj", "datetime": "Jan 12, 2021", "body": "Are you willing to submit a PR?", "type": "commented", "related_issue": null}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": "That is not entirely mis-leading, as a DAG can be paused/unpaused at creation based on what is set in . just makes it clearer that the toggle is used for pausing and unpausing -- but I still think it was not required", "type": "commented", "related_issue": null}, {"user_name": "eldernewborn", "datetime": "Jan 12, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": [], "type": "pull", "related_issue": "#13642"}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": "kaxil", "datetime": "Jan 12, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "vikramkoka", "datetime": "Jan 18, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "VBhojawala", "datetime": "Jan 19, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "lmaczulajtys", "datetime": "Feb 22, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Sep 17, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Sep 23, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Nov 27, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Mar 10, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Jun 4, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "kosteev", "datetime": "Jul 9, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "leahecole", "datetime": "Aug 27, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/wechaty/wechaty/issues/371", "issue_status": " Open\n", "issue_list": [{"user_name": "lijiarui", "datetime": "Mar 29, 2017", "body": " is a fantastic bot analytics platform, it can provide abundant metrics to help you better monitor your data. You can increase user engagement, acquisition, and monetization through analytics, bot specific metrics, funnel analysis and live transcripts and other functions.Now, I'm trying to communicate with their founder DennisYang and trying to integrate with wechaty.We can send data with generic type, but we want to integrate more.Here is the raw message Json example to see whether we can store more.", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 29, 2017", "body": "Now I'm using dashbot with generic like the following function:", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 29, 2017", "body": "FYI I've chatted with Dennis (as in Tennis :) quite a lot. I had problems with dashbot being blocked from our (china/AWS) servers. One idea is that maybe they were hosted on digitalOcean (I'm not sure though), which sometimes the whole IP block gets censored.Other than that it's a great service!So you don't have any problem now sending events to dashbot?FYI analytics for Bots is a big topic in the US. Another company is:\nbotanalytics.co", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 29, 2017", "body": "Because my bot is hosted on digitalOcean too, so I don't have too many problems when sending events to dashbot. And I just begin to use dashbot these days, so I'm not very sure.By the way, I have several bots, some on aliyun and some on digitalOcean. For now, there isn't any difference between them.And I found  is also good too.", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 29, 2017", "body": "About the userId problem and getting a unique IDinstead of:it looks like Contact class has access to the wechatID - can't we use that?\nis this the real wechat ID, or is it an openID that changes with each session?", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "About the uniqueID, my solution on my bot is to set alias for each contactand use alias(we called remark before, see  ) to find the contact.all of the contact on my phone like this:\nBtw, wechaty do has function  to get contact's wechatID, because sometimes we can get wechatID in contact's rawObj from webwx, but cannot always get some person's wechatID each time for some reason, I haven't found the real reason, so using  is not a good solution.", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 30, 2017", "body": "so alias remains consistent between sessions and devices, since it is stored server side by wechat, correct? ie you set it thru the wechaty/web client but then it shows up on your phone too?the wechat ID is not available about 10% of the time? I think its only for users that joined wechat directly from QQ in the old days, when this was possible.otherwise this is a bit ugly and it makes it hard to know who you are talking to, if a real human was to take over the chat.we could do something likethat would create a user alias  the wechat ID doesn't exist?", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 30, 2017", "body": "in our app at least we often have a live teacher take over from the bot and chat to peoplewe also want to be able to see the userName so we can associate users from the official account into the bot chat.So in this way we would want to keep the original name visible. Maybe you dont need this for a fully automated bot ...", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 30, 2017", "body": "125428 unread messages   \nyou have some catching up to do do you find this slows down wechaty on login?\nI know on native clients it fetches all those messages, even in rooms you have muted.I had a bot and found that the \"get list of rooms\" function was taking a long time - I think because it was syncing a lot of unread messages to the web client even in chromedriver. But not sure...\nDoes the web client only request messages when you want to view them?", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "Yes, alias remains consistent between sessions and devices, even if you reload wechat or relog in wechat on the different phone.And I set alias through wechaty/web client and then it shows up on my phone.The wechat ID is not available all the time, but I don't know the exact number, I guess only the new users can get wechat ID, because I can get my wechat ID by wechaty, but  cannot get his wechat ID by wechaty.", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "I really agree with this, and it confused me a lot when take over. So I suggest whether dashbot can show  both uniqueId and username, then it will easy to recognize.", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "But I do not entirely agree with your solution:because we cannot set contacts's alias by wechaty by default. It should let developer to decide, or if someone uses wechaty and dashbot, and then he saw his contact  has changed to , he will shouted......maybebut I'm not sure it should be obj.id(somethin like ) or obj.name (something like )As you have said, usually names are almost unique.... Although my bot has two contact with the same name....", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "About a great amount of message, it never slows down wechaty on login. But it indeed slow down my android device... Receiving message on server is quite faster then the device, so if I can connect dashbot, I can leave out my device completely.Yes,  does take some time, but not that long. I have more than 500 hundred groups and it won't take me too long time to get all. Actually, server always faster than web or phone.... I thought the time is cost on render front-end show, maybe.", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "Here is the contact rawObj data, maybe you can find something interesting", "type": "commented", "related_issue": null}, {"user_name": "dcsan", "datetime": "Mar 30, 2017", "body": "yes, agreed. that's why I almost think using name is ok... it would have very few collisions, but would be very usable.The code comment saysbut i think this does not  for the same user, right?\nits only when the user has a from QQ type of account? in which case it is always hidden for that user?\nso maybe just in those cases we overwrite the alias with the nickname, and use that.\nIt would mean even less chance that nickname will have a collision (but still not zero of course...)Also using names is risky since the user can change it. We would lose their info, in future this does expose the fact someone could change their name, and impersonate someone to get at content inside our bot app. So, not a good way forward.So the only reliable way is really ugly then...", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 30, 2017", "body": "yes, maybe this is the most important reason why we cannot user name.....\nbut it is better than contactId?So maybe the best implement as follows?And we should add log.warn() if user doesn't set user's alias?", "type": "commented", "related_issue": null}, {"user_name": "lijiarui", "datetime": "Mar 31, 2017", "body": "Here is the rawObj message for Dennis YangGeneral data as follows:", "type": "commented", "related_issue": null}, {"user_name": "huan", "datetime": "Aug 18, 2017", "body": "Is there any progress/milestones update for the dashbot.io integration?", "type": "commented", "related_issue": null}, {"user_name": "sinned", "datetime": "Aug 19, 2017", "body": "Yes! We added conversationId to our Generic implementation now:\nWhen we launch the wechat integration, it will use the same JSON payload as Generic, with platform=wechat in the API call instead.Does that make sense?", "type": "commented", "related_issue": null}, {"user_name": "huan", "datetime": "Aug 19, 2017", "body": "Awesome! Do you have any examples on dashbot website? I'd like to have a try.Cheers!", "type": "commented", "related_issue": null}, {"user_name": "huan", "datetime": "Oct 8, 2017", "body": "See: ", "type": "commented", "related_issue": null}, {"user_name": "huan", "datetime": "Mar 31, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "huan", "datetime": "Jan 2, 2019", "body": [], "type": "pinned this issue", "related_issue": null}, {"user_name": "huan", "datetime": "May 12, 2019", "body": [], "type": "unpinned this issue", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/2660", "issue_status": " Closed\n", "issue_list": [{"user_name": "avivazran", "datetime": "May 5, 2020", "body": "I'm trying to use the python API to enable drone control with keyboard or xbox controller.\nis there a script which implements it? or plans to write one?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 27, 2020", "body": "Have you read thid doc:  ?\nDo you need to enable the rc on runtime?", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 27, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Aug 27, 2020", "body": [], "type": "issue", "related_issue": "#2671"}, {"user_name": "avivazran", "datetime": "Sep 9, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/4281", "issue_status": " Closed\n", "issue_list": [{"user_name": "aib", "datetime": "Jan 12, 2022", "body": "Upon launching LinuxNoEditor/Blocks.sh or LinuxNoEditor/Blocks/Binaries/Linux/Blocks I get a black screen with my mouse cursor visible. From the alt-tab dialog I can surmise that a \"Choose Vehicle\" window and a simulation window have been created. At this point, the desktop the simulation is running at is blocked, I cannot see or visibly switch to any windows on the desktop, including the console that ran the program. If I focus the Choose Vehicle window and close it, the simulation window becomes normal and I am able to use everything normally, including the simulation.Is the Choose Vehicle window a modal dialog over a fullscreen application which takes over the screen?Where is the windowed mode setting?  produces nothing and the documentation only refers to simulation configuration JSON files.No settings. I would play with them if I could find them.Did a search for \"fullscreen\" and converted a 1 to 0 and a True to False and the only change is that I see a quarter-screen window for a split second before I get the working fullscreen simulation window, after fumblingly closing Choose Vehicle.N/AWell, this is my first introduction to this program, or any of its kind. This is a computer/setup that runs hundreds of games through Steam and Proton.", "type": "commented", "related_issue": null}, {"user_name": "aib", "datetime": "Jan 12, 2022", "body": "stdout:(this is where I close Choose Vehicle)stderr:(this is where I close Choose Vehicle)", "type": "commented", "related_issue": null}, {"user_name": "aib", "datetime": "Jan 12, 2022", "body": "Just found out about  thorough an external site. Fixes the problem as expected.", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 31, 2022", "body": "Thanks, , for solving by yourself, and keeping us informed!", "type": "commented", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 31, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jonyMarino", "datetime": "Jan 31, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/219", "issue_status": " Closed\n", "issue_list": [{"user_name": "Murplugg", "datetime": "May 15, 2017", "body": "Proposed concept: Have the PIP views follow the drone while in FPV. The PIPs can look around by using a PS3 / Xbox controller (think typical First Person Shooter mouse-look).Issues: AirSim seems to support just one active camera, it's not possible to control the drone in FPV and still have the PIPs follow the drone (they switch position with the external camera).", "type": "commented", "related_issue": null}, {"user_name": "sytelus", "datetime": "May 16, 2017", "body": "Currently we have kept things simple. You have FPV camera and external follower camera. You can swap them in PIP or main view. This design decision is not the most flexible one, of course, but it makes few things simple, for example, no complications to select which camera is PIP or main.Having said that there are no technical limitations on how many camera you may have and which one is PIP or main. However, as you add more cameras, FPS might start to drop dramatically.In very near future, the plan is this: We allow to place N PIPCamera objects wherever user like. We can then offer following APIs:getCameraCount()\ngetCameraImage(index, imageType)\nsetCameraToPIPView(index)\nsetCameraToMainView(index)Lot of code that existed in Blueprint in now moved to C++ so above is now relatively easier.", "type": "commented", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 16, 2017", "body": "Thanks. So FPV and the PIPs share the same defined camera, the PIPs are just three different render targets? To have the PIPs steerable by a controller basically means adding new dedicated camera and C++ code to take control over orientation of that cam?Do you have a rough idea of when your plan / API can be implemented? (Days, weeks?): Got it working. In short:\nBP_FlyingPawn holds two cameras: LeftPIPCamera and RightPIPCamera. AirSim uses LeftPIPCam by default so I expanded CameraDirector::getCamera() to point to one of BP_FlyingPawn's cameras depending on the index argument (int) (just a switch statement, 0 = TargetPawn.getFpvCamera(), 1 = TargetPawn.getGimbalFpvCamera (a new method)).As seen above, the Scene PIP is on a 2nd camera Actor called GimbalPIPCamera in UE4, placed underneath the drone model, it's controlled from the default CameraDirector instance in AirSim (SimModeWorldMultiRotor::setupVehiclesAndCamera() ). Then PIPCamera::setToPIPView calls: EPIPCameraType pip_state = getCamera(1)->toggleEnableCameraTypes(EPIPCameraType::PIP_CAMERA_TYPE_SCENE);That is, it uses the 2nd cam by calling getCamera(1).A new FRotator variable was made in CameraDirector, similar to \"camera_rotation_manual_\", to hold the 2nd camera's orientation separate from External cam view and so on. Both LeftPIPCamera and our new cam follow the drone model by default, all that's needed for manual control of 2nd cam is (example for negative pitch): In the new pitch down event:\ngimbal_camera_rotation_manual_.Add(-val, 0, 0);\ngetCamera(1)->SetActorRelativeRotation(gimbal_camera_rotation_manual_);", "type": "commented", "related_issue": null}, {"user_name": "sytelus", "datetime": "Feb 21, 2018", "body": "I just added Gimbal APIs in AirSim. .", "type": "commented", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 24, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 26, 2017", "body": [], "type": "issue", "related_issue": "#237"}, {"user_name": "Murplugg", "datetime": "May 26, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 26, 2017", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 29, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Murplugg", "datetime": "May 29, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/138", "issue_status": " Closed\n", "issue_list": [{"user_name": "exploke", "datetime": "Mar 31, 2017", "body": "I have gotten the Airsim environment up and running on linux, but I am unable to make the quadcopter take off. I have followed every step on the Airsim tutorial, and all my software for the Unreal Engine, Airsim, Pixhawk, and Ubunutu is up to date to the tutorial specifications. First, I attempted to use a controller to fly the quadcopter; the controller would be able to arm the quadcopter, but when I thrusted up, nothing would happen. Upon checking the RC inputs on QGroundControl and calibrating the settings multiple times, there was no change in the simulation. With the controller not working, I attempted to use DroneShell to test out different commands, but I ran into the same issues. I was able to arm the quadcopter, but taking off did not work. I either got   or  when I prompted the \"takeoff\" command. Using the \"getimage\" command did save pictures to the Airsim folder (which I verified), so I know the Unreal Simulation is able to connect with the DroneShell. I also observed that using the \"disarm\" command did not make the propellers stop spinning. Does anybody know if I am doing something wrong, and how to fix the errors?", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Apr 3, 2017", "body": "You seem to be experiencing the exact same problems I reported in issue . Just pulled and built the latest AirSim version to test this again, and I can confirm the issues persist.  works fine,  throws the same error you mention. I too wait for a solution ;)", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 4, 2017", "body": "I added some  under  before trying to takeoff, can you check if that is the issue you are running into here?", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 11, 2017", "body": "I am getting the same error as above using the actual PixHawk Hardware. Could it be an issue with how it is set up with QGroundControl? I followed all the steps in the tutorial  and its firmware is up to date so I do not know what could be causing this issue.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 11, 2017", "body": "When you run DroneShell.exe does it get past the \"waiting for GPS location\" step ?  if not I'm guessing your forgot to put your PX4 in HIL mode by selecting \"HIL Quadrocopter X\" in QGroundControl.  Also, please try the latest PX4 firmware (ignore my video that says version 1.4.4).  Use 1.6.0 instead.", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 11, 2017", "body": "I was looking at the qgroundcontrol and it kept saying the accel, mag, and gyro calibration failed. I looked it up online and it's not showing me any solutions, so that may be our problem. I ran unreal and drone shell and saw that it was getting past the step of getting the gps location. Additionally I am running the latest PX4 firmware (1.6.0 px4fmu-v2_default.px4).", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 12, 2017", "body": "Sounds like you need to select the simulation AirFrame as shown below:\n", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 12, 2017", "body": " we already selected the HIL Quadcopter X", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 12, 2017", "body": "Can you paste a screen shot of the Unreal simulator, you should see some text in the top left that looks like this:\n", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 12, 2017", "body": "It says \"taking off Detected\" but the Quad never takes off. We get the same error when using Drone Shell in addition to Drone Shell spitting out (\"drone hasn't came [sic] to expected z of -3.000000 within time 15.000000 sec within error margin or rpclib: function 'takeoff' (called with 1 arg(s)) threw an exception\")", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 12, 2017", "body": "How come the drone looks ghosted and has no shadow? Is the drone under the ice? Mine looks like this:", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 12, 2017", "body": "It is not under the ice, I am using reduced graphics settings so it does not display the shadow of the quad. What should the altitude typically be for this map? I read that this simulator uses the NED frame so should z initially be positive or close to 0?\nWhen I type position on drone shell, it displays Local position: x=0.0410362, y=-0.117092, z=11.0654\nGlobal position: lat=47.6415, lon=-122.14, alt=127.71", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 12, 2017", "body": "The z coordinate is a little high (NED coordinate system has negative z when drone is above ground).  This is what I get :position: x=0.00353828, y=0.0042316, z=1.02402\nGlobal position: lat=47.6415, lon=-122.14, alt=122.276Does your drone do a little freefall before hitting the ground when you hit Play ?  Are you getting no movement at all ?  Can you also try \"movetoposition -z -20\" ?", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 12, 2017", "body": "Yes it does do a free fall after we hit the play button but afterwards the drone does not take off at all (even when I type \"movetoposition -z -20\" . Additionally when I turned the graphics to maximum I see a shadow so I know the quad is not inside the ice. I moved the player start to a different location and when I type \"pos\" on DroneShell it says z = 0.271578 which is more reasonable than the previous position I was getting.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 12, 2017", "body": "Ok, try and lower your PlayerStart actor in the UE editor a little bit so the freefall is minimized.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 13, 2017", "body": "I just checked in a fix that might help with the case where local position 'z' coordinate is unusually high, please pull latest bits and try again.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 13, 2017", "body": "Actually, just found a proper fix, please try again...", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 14, 2017", "body": "\nI updated to most current commit, and I am running the simulator on windows. As you can see from the pictures, it is displayed that drone is armed and taking off, but the drone itself is not going anywhere. Using \"movetoposition -z -20\" seems to be making no difference also. Do you know any more solutions to this issue?", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 14, 2017", "body": "There is a \"CPU\" setting  under Edit/Editor Preferences, search for \"CPU\" and uncheck the box labelled \"Use Less CPU when in Background\".Also, can you attach the PX4 log file mentioned when you ARM the drone, the above shows the path \"/fs/microsd/log/sess021\", you can download the log from PX4 using QGroundControl, or you can use the \"get\" command in MavLinkTest.exe, like this:Then zip the appropriate log and attach it to this issue...", "type": "commented", "related_issue": null}, {"user_name": "vatsagandhi", "datetime": "Apr 14, 2017", "body": "I used QGroundControl to save the log. When I opened QGroundControl though, the display was showing the drone was fluctuating between manual and arm mode before I put in any commands. After attempting to arm the drone, the display on QGroundcontrol was fluctuating between land, takeoff, disarm. The log file I posted is of those occurrences.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 14, 2017", "body": "This is the QGroundControl mavlink log, it is good, but I was looking also for the PX4 log (PX4 has an sdcard with it's own logs) which you can download using QGroundControl using this toolbar button:\n, then click\n\non the right side, to see the logs, pick the most recent one and download it.This QGroundControl mavlink log is interesting it does show the following telling status messages:If this is the problem (you have no RC) then you can tell PX4 to ignore that problem by setting the following PX4 parameters:NAV_RCL_ACT 0You can use QGroundControl to set this parameter, the value zero means \"disabled\":", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 14, 2017", "body": "I changed the NAV_RCL_ACT to 0 or disabled, and tried running the simulation again. This time, when I went to QGroundControl, the drone was not fluctuating between modes, but takeoff was being denied when I tried. I'm attaching the PX4 log you said to download on the comment above of these occurrences.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 14, 2017", "body": "Ok, that is progress, PX4 only denies takeoff if it is not happy with GPS signal being sent by simulator.  So here's some more things to try:Then send me the resulting logs from your d:\\temp\\log folder.", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 14, 2017", "body": "While or after running the simulator, we go into the MavLinkCom\\MavLinkTest\\build\\x64\\Debug folder and attempt to open MavLinkTest, but the window that opens immediately closes.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 14, 2017", "body": "Run it from a DOS command prompt and capture any error message it prints.  I assume here that you are running with ~/Documents/AirSim/settings.json set to \"serial\":true, so the Simulator is connected to PX4 serial port and then the simulator is publishing to 14550 as per:So then MavLinkTest -server:127.0.0.1:14550 will pick up this QGC stream.  Which means if you are running QGC it will not work, which is why I listed \"kill QGroundControl\" above.", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 15, 2017", "body": "Attached is the picture of the error we get when running MavLinkTest. we killed QGroundControl but MavlinkTest still does not work.\n", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 15, 2017", "body": "Note: It is very unusual that anyone would install a git repro into this location:\nThis is the Visual Studio install location.  I normally create a folder at the root of my hard drive named \"git\" then I clone AirSim in there so I end up with d:\\git\\airsim.  Can you move AirSim to a location like this and try again?I'm also guessing you do not have a \"D:\" drive at all, which explains the crash.  Change this argument:-logdir:c:\\tempThis will cause MavLinkTest to write log files to your C:\\temp\\log folder.Then if it still crashes can you run MavLinkTeste.exe with the same command line using Visual Studio and attach the stack trace where it is crashing, thanks.", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 15, 2017", "body": "\nThank you very much for the help, MavLinkTest worked and I am attaching the log files from MavLinkTest in this comment.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 17, 2017", "body": "Thanks for the log, using  I can load the *input.mavlink log and it shows something very weird on the Altitude:Why is the altitude plummeting to -81 meters below ground every 30 seconds or so, very weird.The heading is also drifting a lot, was the drone spinning around by any chance?", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 17, 2017", "body": "The propellers spin in a very peculiar way; they will start spinning back and forth even before I arm the drone. I also notice the 3 bottom camera views sometimes wobble.", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 19, 2017", "body": "I re-downloaded and updated all the files. I tried with a different pixhawk and ran into the same problems.This time when I try to run the simulator, the drone shell is unable to get a GPS location which may be a possible explanation to the weird graphs. I am attaching a screenshot of what our unreal environment looks like along with the drone shell. The FPS is usually at 60 before running the simulation, but drops to 15 when running the simulation, and it drops to 3 when running the drone shell as well.", "type": "commented", "related_issue": null}, {"user_name": "clovett", "datetime": "Apr 19, 2017", "body": "Yep, 3 is not enough to fly properly.  Try mouse click on airsim canvas and type \"0\" to remove the camera views, that should help.", "type": "commented", "related_issue": null}, {"user_name": "clovett", "datetime": "Apr 19, 2017", "body": "I would also move off that bridge, it has weird collision mesh that is up higher than the bridge appears.", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 21, 2017", "body": "I lowered the graphics. moved the drone to the location specified on the github tutorial, and removed the camera views. My FPS went up to about 60 before and during running the simulation. Running the simulation now, the propellers are not fluctuating around anymore; rather, the propellers go slower and faster depending on how much throttle I give from my RC controller. However, the drone is still not going anywhere. Also, the propellers do not stop spinning when I issue a disarm command from my RC controller. I then attempted to use the drone shell, but the FPS dropped to about 3 while I had the program running. The interesting thing I found was that the drone shell kept saying waiting for the GPS location, an issue I was not having before. Is it possible that is why my drone is not going anywhere, because the drone cannot pick up the GPS location?\n", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 21, 2017", "body": "See \"\"", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 21, 2017", "body": "Unchecking the \"Use Less CPU when in Background\" checkbox did have a dramatic FPS rise of 120 while using the RC controller, and 70 FPS while using the DroneShell. Although, the drone is still not moving with RC controller, and the DroneShell is still '\"Waiting for drone to report a valid GPS location...\"", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 21, 2017", "body": "Can you run MavLinkTest.exe -serial:*,115200, then when it is started type \"params c:\\temp\\px4params.log\" and attach the c:\\temp\\px4params.log file ?  I want to try the exact same parameters on my pixhawk and see what happens. Which version of px4 firmware are you using?  1.4.4 or the latest 1.6.0 ?", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 21, 2017", "body": "I am using the latest version of the px4 firmware (1.6.0). Here is the px4params.log file.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 21, 2017", "body": "If you are getting 70fps then flying should be excellent, no problems there.Your params are quite a bit different from mine, but I don't see anything obvious that should affect takeoff.\nYour radio trims are different of course, I assume you calibrated your radio using QGroundControl.I also included the firmware I'm using which I built a couple days ago just to rule out that variable:\n", "type": "commented", "related_issue": null}, {"user_name": "exploke", "datetime": "Apr 22, 2017", "body": "Thank you very much, I used the firmware you provided in the comment earlier and the quadcopter lifts off the ground now. Flight is very stable. I am closing this thread. I really appreciate the help.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Apr 22, 2017", "body": "great, thanks for closing the issue.  Love to know why my firmware works and yours didn't, but we can leave that for another day.  Cheers.", "type": "commented", "related_issue": null}, {"user_name": "udaydlsv", "datetime": "Apr 21, 2017", "body": [], "type": "issue", "related_issue": "#172"}, {"user_name": "exploke", "datetime": "Apr 22, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/116", "issue_status": " Closed\n", "issue_list": [{"user_name": "Maxfashko", "datetime": "Mar 17, 2017", "body": "Hello. I used the sample assembly from this video . The following errors occurred during the build (debug x64):Error LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (client.obj)\nError LNK2038 detected a discrepancy for \"RuntimeLibrary\": the value of \"MD_DynamicRelease\" does not match the value of \"MDd_DynamicDebug\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (client.obj) 1\nError LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (rpc_error.obj) 1\nError LNK2038 detected discrepancy for \"RuntimeLibrary\": MD_DynamicRelease value does not match MDd_DynamicDebug value in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (rpc_error.obj) 1\nError LNK2038 detected a mismatch for \"_ITERATOR_DEBUG_LEVEL\": value \"0\" does not match the value \"2\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (response.obj) 1\nError LNK2038 detected a discrepancy for \"RuntimeLibrary\": the value of \"MD_DynamicRelease\" does not match the value of \"MDd_DynamicDebug\" in main.obj DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ AirLib.lib (response.obj) 1\nWarning LNK4098 default library \"MSVCRT\" is inconsistent with the use of other libraries; Use the / NODEFAULTLIB parameter: library DroneShell C: \\ Users \\ maxfa \\ Code \\ AirSim \\ DroneShell \\ LINK 1\nIf you comment out the lines in build.cmd:REM msbuild /p:Platform=x64 /p:Configuration=Debug AirSim.sln\nREM if ERRORLEVEL 1 goto :buildfailedAssembly is performed without errors.\nAfter completing the steps to create a project, unreal gives an error:The following modules are missing or build with a different engine version:\nUE4Editor-AirSim.dll\nWould you like to rebuild them now?", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 17, 2017", "body": "Hi , have you tried cleaning (Build->Clean Solution) before building in Visual Studio? I sometimes have build errors too when importing newly built versions of AirSim, cleaning beforehand gets rid of those.If not, did you choose to rebuild the UE4-Editor-AirSim.dll?", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 17, 2017", "body": ",No, I have not tried it.Yes, the project is going. In the unreal there is no directory containing the plugin AirSim. When you run the project in unreal, nothing happens", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 17, 2017", "body": "So did cleaning help you build the project or not? I don't understand how you can get to the Unreal project without having built the Visual Studio project. Does this mean you got post the build error?", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 17, 2017", "body": " , Yes, instead of the file I used AirSim.sln in the directory and collected Debug after cleaning.\nBut now nothing is still happening when I press to play in the project. Maybe there are no scene cameras installed?", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 17, 2017", "body": "Ok, so you got the project built and are now in the Unreal environment, check. It's absolutely possible that there is no starting position defined in your current map. You can check this out by typing 'start' in the top right input box, it should show you all the currently defined starting positions. See this cropped screenshot:The  (the link starts at the relevant part) also clearly shows how you should position this starting point for optimal result.Don't forget to set your  to , otherwise the drone won't spawn.", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 17, 2017", "body": ", Yes, I have a starting position. And after clicking on the play, I see:\n\nThe camera does not display segmentation, depth, etc.", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 17, 2017", "body": "So, what's exactly the problem you're having now? Did you check the instructional video and  configuration?", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 17, 2017", "body": "The fact is that I did not find the description of the . I want to achieve the same result as on the video (flight, segmentation cameras, depth, etc)\nIt's normal that I can not find BP_Camera Director?", "type": "commented", "related_issue": null}, {"user_name": "sytelus", "datetime": "Mar 18, 2017", "body": "Hi Folks, this seems to be because you are using 4.14. We just upgraded everything to 4.15 couple of days ago. This required some changes in BP_CameraDirector but unfortunately it turned out that if you modify these in 4.15 then they become invisible (and unusable!) in 4.14. That is, written BPs in 4.15 are no longer compatible to 4.14. Even worse was that there are no error messages and you will basically see nothing happening when you press Play button. This was disappointing however there is no known way to go back and we don't want to get in the hassle of of maintaining two versions. So we suggest that every one upgrade to 4.15. I've also put in some code to display warning message in latest version. Upgrading to 4.15 is easy: .", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 18, 2017", "body": ",  thanks. Really, I have 4.14.3 version.\nI try to reinstall unreal.", "type": "commented", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 18, 2017", "body": ", thanks. Reinstallation helped in my case. Now I got the following:Can I control the drone with a keyboard or mouse? Without specialized tools?", "type": "commented", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 20, 2017", "body": "As far as I know, there is no way to control the drone with mouse or keyboard at this instant. You'll have to either use an RC controller wirelessly connected through a Pixhawk device or directly through USB, or some kind of .It would also be very hard to actually control a drone using a keyboard: the control values would vary from 0 to max, nothing in between. While I do agree it would be a nice-to-have feature to test if your setup is correctly running, it will in no way come close to actually flying a drone using a dedicated controller. You need very accurate control to be able to fly a drone correctly.", "type": "commented", "related_issue": null}, {"user_name": "sytelus", "datetime": "Mar 18, 2017", "body": [], "type": "issue", "related_issue": "#114"}, {"user_name": "Maxfashko", "datetime": "Mar 18, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Maxfashko", "datetime": "Mar 18, 2017", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "Timebutt", "datetime": "Mar 20, 2017", "body": [], "type": "issue", "related_issue": "#121"}, {"user_name": "Maxfashko", "datetime": "Mar 20, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/111", "issue_status": " Closed\n", "issue_list": [{"user_name": "Alejolas", "datetime": "Mar 16, 2017", "body": "Hi all,Thanks a lot for this project. So I just bought a new build to run this properly, Intel I7 6800K, 32GB DDR4 RAM, GTX 1080. Installed everything following the documentation and videos, my problem is: every time I hit play, Unreal Editor crashes. When I debug it, here's what I get:\nAny help would be appreciated.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 16, 2017", "body": "Can you post the full call stack ?  Could be a setup issue.  Can you tell us more about your setup? which OS, Pixhawk hardware versus SITL mode, etc, thanks.", "type": "commented", "related_issue": null}, {"user_name": "Alejolas", "datetime": "Mar 16, 2017", "body": "Thanks for the reply Chris!I still don't have any Pixhawk hardware or drone... I just ordered the stuff from Amazon, I just got the computer, sorry if there some things in Spanish:\n\n\n\n\n", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 16, 2017", "body": "Thanks, looks like it is crashing in the implementation of delete of a MavLinkNode which is not good.  But since you don't have pixhawk hardware, can you change your ~/Documents/AriSim/settings.json so that it has \"Serial\" set to \"false\".  Then I'd expect you would be looking at the  so you can get a flying drone.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 16, 2017", "body": "Note also that the master branch was just changed to switch to Unreal 4.15.  So if you have 4.14 you will need to upgrade.", "type": "commented", "related_issue": null}, {"user_name": "Alejolas", "datetime": "Mar 16, 2017", "body": "Oh, I don't have any controller of any kind, just keyboard and mouse. Should that be the problem?", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 16, 2017", "body": "The  show you how to get a drone that flies.  But we don't yet have support for keyboard/mouse flying.  But you could do some autonomous flying as showing in the .  Depends on what your goals are here.", "type": "commented", "related_issue": null}, {"user_name": "Alejolas", "datetime": "Mar 16, 2017", "body": "Yes, for the moment I just want to do autonomous fly while I get the other hardware, but first let's see if I can get pass through that MavLinkNode error that I'm having and crashing my Unreal Editor. I'm going to try again following all the steps. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 16, 2017", "body": "Ok, let me know how it goes.  if you still run into trouble after following all the steps, then it might also help to enable the debugger to \"break\" on all C++ exception and see what you can find that way.  I'm guessing the delete is crashing because other setup code was skipped.  We have a bug there obviously, but we'll need to do some more debugging to track it down.", "type": "commented", "related_issue": null}, {"user_name": "imoon", "datetime": "Mar 19, 2017", "body": "I'm having this issue also, only I do have HIL but get this stacktrace after running:\nMavLinkTest.exe works fine:I'm at a complete loss. I've stepped through until RpcLibServer.cpp line 116. Executing this crashes Unreal as described above with the stacktrace above.Any assistance would be appreciated. I have followed all instructions and cannot get it to run.Additional Info:I've tried a number of settings.json settings and this is my latest:", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 20, 2017", "body": "I am getting the same issue but with HIL. I have my pixhawk connected to my PC in HIL mode . RC receiver is connected to the pixhawk.  In the SITL mode, I get an error saying \" Computer load temporarily too high for real-time simulation\". My Hardware looks something like this. 16gb RAM, Nvidia GTX 1060- 6GB GPU Memory, 128 GB SSD,. I am running the latest version of AirSim on Unreal 4.15 . Do you think 16GB RAM is the problem ?", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 22, 2017", "body": "I am able to run smaller worlds with 16gb no problem.  What is the CPU ?  I've seen this message in SITL also \"Computer load temporarily too high for real-time simulation\" but I'm still able to fly, so I doubt this is the root issue.One thing with HIL mode is you must set this Airframe using QGroundControl then click \"Apply and Restart\"", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 22, 2017", "body": "And for HIL mode your settings should look like this:", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 22, 2017", "body": "Thanks a lot for the reply. I am using Intel i7 6th gen processor. Also, pixhawk is on HIL mode. I haven't loaded any environment into unreal engine as I thought CPU load might get too high and my settings are the same as yours. And although I connect my Taranis Fr Sky to PC through USB. When I launch the project I am getting RC Controller not detected on USB", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 22, 2017", "body": "Ok, great, i7 is fine.  That message is coming from AFlyingPawn::detectUsbRc.  Perhaps there are addition steps required to get your RC Controller to appear as a device that Unreal is happy with. There are some remote control instructions at the .I believe we currently only support \"xinput\" devices (as per SimJoyStick.cpp) and there may be a missing step to convert your USB joystick into something that emulates xinput.  For example  might be what you need there.I also added .", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 23, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 23, 2017", "body": "hang on, why can't you connect your RC to your Pixhawk then ?  You do not need to use RC through USB to PC.  AirSim can get the RC controls via pixhawk as I show in .", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 23, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 23, 2017", "body": "Ah, I see now, so the message about RC and USB is something you have to ignore in your case.  I agree it is confusing.  I will try and fix that so you don't get the confusing message.But now I realize you haven't actually yet specified on this thread what your problem actually is.  Can you describe the symptoms you are seeing and the goal you are trying to reach ?", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 23, 2017", "body": "I am getting the same error as seen in the snapshots of the 1st post of this thread. The GUI crashes after reading settings.json from the documents. The debugger says the crash occurs at MavLinkNodeImpl.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 23, 2017", "body": "I see thanks for clarifying.  Ok, I just made this  and followed every step there and it works fine, so this is indeed a mystery.  It would help me if you could paste the text of each C++ exception call stack you see before the crash while running the game in Visual Studio debugger.  Is that possible?Also, to ensure I never have any \"stale bits\" involved in my unreal projects I run this little script:then I update AirSim bits, and then I \"regenerate the visual studio project\", load it in VS and hit F5.", "type": "commented", "related_issue": null}, {"user_name": "lkumar93", "datetime": "Mar 23, 2017", "body": "Surprisingly it seems to be working right now. Previously I had launched the project file manually from unreal engine and that didn't work. This time I opened the solution file and put it in debug mode on Visual studio and then hit F5 , which launched the engine and then I was able to run it. However the drone is not flying as I increase the throttle after arming it. The gui says takeoff detected but It doesn't really move. The propellers are spinning though. Btw , the pixhawk is on altitude mode.", "type": "commented", "related_issue": null}, {"user_name": "lovettchris", "datetime": "Mar 23, 2017", "body": "Great, that's progress then.  Better to take off without Altitude mode - Altitude mode tries to hold the current altitude which is probably fighting the takeoff.  You also need to ensure Pixhawk is in HIL mode as I showed in the QGroundControl screen shot earlier in this thread.  Sometimes \"rebooting\" the pixhawk is necessary to reset any residual state it had from previous crashes or weird behavior.", "type": "commented", "related_issue": null}, {"user_name": "CaoYongshengcys", "datetime": "Jan 12, 2019", "body": "I came across the same problem,  takeoff detected but It doesn't really move. How do you fix it", "type": "commented", "related_issue": null}, {"user_name": "sytelus", "datetime": "Mar 16, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "imoon", "datetime": "Mar 22, 2017", "body": [], "type": "issue", "related_issue": "#126"}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/1300", "issue_status": " Closed\n", "issue_list": [{"user_name": "matt-gardner", "datetime": "May 29, 2018", "body": "I'm pretty sure you used to get span highlighting on mouse-over for all of the constituents, but that's not currently happening.  Not sure what happened.More generally, it'd be really nice if we had some tests of our frontend JS code.  I have no idea how to write or set those up, though.  If we had a frontend engineer...", "type": "commented", "related_issue": null}, {"user_name": "DeNeutoy", "datetime": "May 29, 2018", "body": "Yeah, I knew about this already:\nIt's pretty tricky to get it to line up correctly, and it's less important for a constituency parse where the tree is more explicit.I'll probably get around to fixing this eventually when i'm bored, so leave it open.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "May 29, 2018", "body": "Oh, I thought the earlier demo I saw did this, which is why I opened the issue.  I didn't realize you already knew about this.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 14, 2018", "body": "Moving to allenai/allennlp-demo.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Jun 22, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 14, 2018", "body": [], "type": "issue", "related_issue": "allenai/allennlp-demo#58"}, {"user_name": "matt-gardner", "datetime": "Sep 14, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5685", "issue_status": " Closed\n", "issue_list": [{"user_name": "astopo", "datetime": "Nov 11, 2021", "body": "\nIn Studio, when adding a second slot, the first slot on the line above is duplicated. Afterwards, any kind of input (mouse click, key down) triggers more duplicated words on the lines above.\nSteps to reproduce the behavior:\nSuccessfully adding a second slot.\nScreen recording available here: ", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Nov 11, 2021", "body": "What  \nThanks for reporting the issue. We'll prioritize this quickly ! has this been fixed in next branch already ?", "type": "commented", "related_issue": null}, {"user_name": "franklevasseur", "datetime": "Nov 11, 2021", "body": "Hey , thx for the report!Can you check if this fixes your issue: Feel free to do some QA and try to break it", "type": "commented", "related_issue": null}, {"user_name": "justis18", "datetime": "Nov 29, 2021", "body": "Hi, is the problem fixed?", "type": "commented", "related_issue": null}, {"user_name": "franklevasseur", "datetime": "Nov 29, 2021", "body": "Hi , it's not as botpress still uses studio It will be fixed once, botpress upgrades to studio v.0.0.43", "type": "commented", "related_issue": null}, {"user_name": "Gordon-BP", "datetime": "Dec 2, 2021", "body": "Fixed in ", "type": "commented", "related_issue": null}, {"user_name": "astopo", "datetime": "Nov 11, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Nov 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Nov 21, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "Gordon-BP", "datetime": "Dec 2, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Gordon-BP", "datetime": "Dec 2, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "EFF", "datetime": "Dec 2, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4469", "issue_status": " Closed\n", "issue_list": [{"user_name": "hacheybj", "datetime": "Feb 3, 2021", "body": "\nThe list of content and transitions associated with a node in the left side panel are quite unfriendly to use. One must first hover over the content for action buttons to appear. This causes losing track of the row we are on, miss clicks, and a lot of back and forth with the mouse.\nFor the actions buttons to be available at all times on each row.\nN/A\nN/A", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Dec 7, 2021", "body": "Not relevant as we have specs for BP 13 that fixes this and more", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 4, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "sebburon", "datetime": "Mar 8, 2021", "body": [], "type": "pull", "related_issue": "#4631"}, {"user_name": "hacheybj", "datetime": "Mar 9, 2021", "body": [], "type": "pull", "related_issue": "#4620"}, {"user_name": "EFF", "datetime": "Dec 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/3334", "issue_status": " Closed\n", "issue_list": [{"user_name": "LuizCesarLeite", "datetime": "May 17, 2020", "body": "I notice that when we have a flow that is bigger than the screen area, duplicated it force the flow view to fit every node in the same view, turning the nodes pretty small.Steps to reproduce the behavior:I'm on Linux 18.04 LTS, using BP 12.9.1", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": "This is intended, the first time a flow is loaded, it will be displayed in its entirety, then you can zoom in to focus on a specific part", "type": "commented", "related_issue": null}, {"user_name": "LuizCesarLeite", "datetime": "Oct 28, 2020", "body": "Thanks for your answer .But when trying to zoom in/out (at least to me) it's necessary roll the mouse wheel A LOT of times.More than that, it's a bad UX approuch: the user, not the application, must decide the view size.", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 30, 2020", "body": " Good point. We will soon bring back a small change on the flow so you can quickly change to a set of preconfigured zoom levels:Keeping the issue open until then", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 20, 2021", "body": "Hi - fixed, we will close this, please reopen if need be.", "type": "commented", "related_issue": null}, {"user_name": "LuizCesarLeite", "datetime": "May 17, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "LuizCesarLeite", "datetime": "May 17, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "slvnperron", "datetime": "Oct 22, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 28, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "allardy", "datetime": "Oct 30, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "stale", "datetime": "Oct 30, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "J-FMartin", "datetime": "Sep 20, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/AirSim/issues/28", "issue_status": " Closed\n", "issue_list": [{"user_name": "morrisonbrett", "datetime": "Feb 20, 2017", "body": "Is it possible to use AirSim without an external controller? Keyboard / Mouse / Even XBox Controller.Or, must you have an external device?Is there a way to have a \"software emulator\" version of what's setup in the settings.json file?", "type": "commented", "related_issue": null}, {"user_name": "morrisonbrett", "datetime": "Feb 20, 2017", "body": "I see that this has indeed been addressed in the \"alternatives\" doc.Closing this issue.", "type": "commented", "related_issue": null}, {"user_name": "morrisonbrett", "datetime": "Feb 20, 2017", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/1782", "issue_status": " Closed\n", "issue_list": [{"user_name": "nitishgupta", "datetime": "Sep 18, 2018", "body": "I have Tensor T = [B, T, D]  of contextual word embeddings for a given piece of text. Alongside, I have a tensor S = [B, M, 2] of M-spans in this text and their representations, i.e. a tensor R = [B, M, D]. For any given span (i, j), I want to add it's representation to all the tokens in span [i : j].Currently the solution I have is to loop over dim-1 in S (and R), and for each span, repeat the span representation to the length of the span, and add it to the corresponding slice in T.I was wondering if somebody has a better solution in mind.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 18, 2018", "body": "Just making sure I understand the problem:Let's say I have the sentence \"The cat ate the mouse\", and I have four spans: \"the cat\", \"the cat ate\", \"the mouse\", and \"the cat ate the mouse\".  So each word will get either two or three span representations added to it.  Yes?  Something like this:Right?  What this suggests is that you want to construct a  shape binary tensor, which you can then multiply by your  tensor to get something of shape  that you can add to your token representations.  You should be able to construct that  tensor using a range vector (for the token indices) and some greater than / less than operations.Does this make sense?  (I'll add that doing this seems a  bit odd to me, but hey, maybe the model can segregate the span features into one part of the feature space, and have them be additive...)", "type": "commented", "related_issue": null}, {"user_name": "nitishgupta", "datetime": "Sep 18, 2018", "body": "Yes, you understood the problem correctly and the solution seems correct. Thanks!Sidenote: The actual modeling in mind is different from this, but I simplified it for the sake of easy explanation. Still, is having additive features a bad idea?", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 18, 2018", "body": "I'm glad it helped!  I'm closing this issue now.And additive features aren't necessarily bad - the model can just partition the feature space so that each feature gets its own bucket, and things can work out.  I've been surprised at how well it works in a number of different occasions (e.g., positional embeddings).  I wouldn't trust my intuitions on this point very much - much easier to just try something and see empirically if it's a good idea.", "type": "commented", "related_issue": null}, {"user_name": "nitishgupta", "datetime": "Sep 18, 2018", "body": "I implemented a test version of this and it works. Thanks again for the idea.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 18, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/1727", "issue_status": " Closed\n", "issue_list": [{"user_name": "aleSuglia", "datetime": "Sep 7, 2018", "body": "Hi there,I was wondering if you have some recommendation in terms of how to implement a copy mechanism for a seq2seq model following AllenNLP best practices. I was thinking to modify the DataIterator in order to return, for each batch, an extended vocabulary mapping the OOV tokens to the original tokens in the sequence. This change will help me to both compute the loss function and the predictions generated by the model. Do you have any suggestions?Thank you in advance for your reply!Alessandro", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 7, 2018", "body": "This is a good question, and I had to think a bit to come up with some ideas for how to do this.  Here's my initial take; there are probably ways to improve it.As you say, there are two places where a copy mechanism needs special handling: in computing the loss function (should I have copied this word?) and when showing the output of decoding to a person.  These happen in very different places in the code, and should be handled separately.First, for decoding, we typically  about what tokens were in the input, so that it can be displayed nicely in a demo.  You should be able to do the same thing during decoding in a copy model - you just keep track of whether you sampled from the vocab or from the copy mechanism during decoding, and display the result to the user accordingly, using that metadata field.  Doing it this way, instead of fudging with the vocabulary or input data, means that you can still use an arbitrary encoder to do your input embedding (including, e.g., ELMo), without resorting to any crazy hackery.Second, the harder part, is how you train a model with a copy mechanism.  I  the right thing to do here is to add a new  that tells you token overlap on the target outputs.  For example, say I have the input sentence \"the dog ate the food .\" and the target sentence \"the cat ate the mouse .\".  What I need during training is to know which target tokens overlap with which input tokens, so my loss function can include the correct copy probabilities.  This field would output a tensor like this for those two inputs:where  is used for padding here.Then, in my loss computation, I'd do something like this:Does this make sense?  Any thoughts?  , what do you think?", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 7, 2018", "body": "Also, we would  to have a nice implementation of a seq2seq + copy model in AllenNLP.  If you get this working, please consider contributing back.", "type": "commented", "related_issue": null}, {"user_name": "aleSuglia", "datetime": "Sep 12, 2018", "body": ": thanks for your feedback. At the moment I was checking the official implementation of See et al. () and the copy mechanism implementation in OpenMT-py (). Apparently over there the idea is to treat the attention scores as a sort of probability score associated to the UNK tokens. Ultimately, they minimise the negative log-likelihood associated to the token indexes that compose the sequence plus additional special tokens associated to the unknowns tokens in the sentence.I'm going to work on this from now on. I'll let you know when I'll have a basic implementation. Are you thinking to integrate it in the current seq2seq implementation?", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Sep 12, 2018", "body": "Abigail's implementation makes one larger probability distribution from the vocab distribution and the attention distribution, seen here: This accomplishes the same math as what I suggested, but requires a single distribution of size  for each timestep.  Actually constructing this distribution is unnecessary, as you really just need to compute the loss.The part that will be very tricky to make work with our data code is this line: Abigail's code has a separate input that includes token ids for the inputs with no OOVs computed.  You can do that with our data code - just have a separate  for the input that uses the same vocabulary namespace as your target .  The trouble is we compute the vocabulary once, so if you ever want to run this on data that wasn't seen before (like in a demo), this could very easily break, because the tokens you want to copy aren't in the vocab.So, these are the two options, I think:It feels like there are a lot more details that are easy to get wrong in the second way, especially as you think about putting up a demo for the model or using a trained model on new test data.As far as integrating something with our current seq2seq implementation - that implementation was just an example, seeing if we could get any kind of seq2seq model implemented before starting some much more complex semantic parsing stuff.  I wouldn't suggest going out of your way to merge what you have with that, if you are starting from something else.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Oct 21, 2018", "body": ",  just mentioned in  that he has a copynet implementation available here: .  He says it's only almost implemented, and I don't want to overload him with a support burden he didn't ask for, but that code may be a good starting place for you.", "type": "commented", "related_issue": null}, {"user_name": "aleSuglia", "datetime": "Oct 25, 2018", "body": ": thank you for mentioning it. I'll have a look asap :)I think we can close this issue!", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Oct 15, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Oct 21, 2018", "body": [], "type": "pull", "related_issue": "#1928"}, {"user_name": "aleSuglia", "datetime": "Oct 25, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5462", "issue_status": " Closed\n", "issue_list": [{"user_name": "AlanQuille", "datetime": "Nov 9, 2021", "body": "Hi all,\nIs it possible to train AllenNLP's coreference resolver with custom data? What format does the custom data have to be in? And finally, if it is possible how do I accomplish it? Thank you very much.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 12, 2021", "body": "Yes, you can train coref with custom data. The training instructions are at . The hard part is that the original data is not freely available, so it's hard to look at.You probably don't need all of that stuff. It'll be easier to modify the dataset reader for coref to read some other format that you have available. Dataset readers are quite easy.", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 14, 2021", "body": "Thank you very much. I assume that allennlp can train custom data for relation extraction as well?", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 15, 2021", "body": "Hi, I'm facing an issue with training. I am running this command:I get the following error:I installed allennlp-2.8.0 allennlp-models-2.8.0 and my coref_spanbert_large.jsonnet file is the same as the link provided except I change these three lines:I'm using v4_gold_conll files (which are basically the same as Conll-2012 files), is that acceptable? I am using these because neuralcoref uses these as well.Also, why do we need ? Is that for accessing online files? Thank you very much.", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 17, 2021", "body": "Hi, I have nearly successfully trained the coreference resolver. I did this on a Linux machine. I ran this command in coref/training_config in allennlp-models (I installed both allennlp and allennlp-models from source):I have this file (train.demo.v4_gold_conll) in train/, test/ and dev/ folders (just for a test train) in the same folder:I get this error:Do you know where I might be going wrong? Could it be #begin document (demo); part 000, should it be #begin document (bc/cctv/00/cctv_0000); part 000?This is the version of Linux I am running:", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 18, 2021", "body": "We use that to read environment variables. You can say  in your shell, and then read out the value in the config with .", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 18, 2021", "body": "There is likely something wrong with your input file, maybe a special character that's not encoded correctly? Either way, it sounds like you got past that problem?", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 18, 2021", "body": "I don't know exactly what's going wrong with your input file, but I have two observations:", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 19, 2021", "body": "I used the Inception tool to create the ConLL-2012 files (), but the file that you showed above is slightly different from what I got using the Inception tool. I do not get the same columns with the asterisk. Do you think adding 2-3 columns using a script with  an asterisk can solve the issue? Thank you very much.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 19, 2021", "body": "I don't know what the asterisk is used for in the reader. Probably for constructing parse trees, which you don't need for coref. I think it might be better to make a copy of the reader and modify it. You could get rid of a lot of code in there and only keep the bits you need for coref.", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 22, 2021", "body": "I tried to add 2 more columns with an asterisk, I ran into the same error as before:The conll file I am using is as follows (note I change it to a .conll file not a .txt file for training):I then tried to train the sample file you gave, except with #end document at the end (it is as follows):I got this error:What do you think is causing the error? It looks like your code needs the entire Ontonotes dataset to do custom training but that makes custom training very difficult. Can you recommend a course of action? Thank you very much", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 23, 2021", "body": "AllenNLP does not need the entire Ontonotes dataset, just a dataset in the right format. But the Ontonotes format is complicated, because it contains a lot of stuff that's unnecessary for training a coref model. I recommend writing your own  that produces data in the right format for the model, but reads a different input format.", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 24, 2021", "body": "I will attempt to write my own DatasetReader. However, in order to do so I need to start with data which trains successfully as a base for my code otherwise I cannot proceed. I humbly request a source for this sample data as the sample data you provided me is not working. If that is not possible, could you guide me for references for finding the data in the right format for the model. Thank you very much.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 2, 2021", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 2, 2021", "body": "Sorry, we are legally not allowed to give out the source data. It's stupid, but that's what it is. It came from . I think you can go to that website, sign up, and then you get a link to download.This information is at .", "type": "commented", "related_issue": null}, {"user_name": "AlanQuille", "datetime": "Nov 9, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 2, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 2, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2579", "issue_status": " Closed\n", "issue_list": [{"user_name": "Namec", "datetime": "Nov 8, 2019", "body": "Hello all,I am new to botpress and trying To achieve a task that can make me send data through\nwindow.botpressWebChat.init method. And then storing this additional data as temp variable.Is there any solution to fetch the url custom paramsBest", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Nov 14, 2019", "body": "Hi  , what are you trying to achieve exactly? You can trigger an event sent by the user by sending  , then capture the event on Botpress.", "type": "commented", "related_issue": null}, {"user_name": "Namec", "datetime": "Nov 14, 2019", "body": "Hello  yes this is exactly what i a m trying to achieve, but i need that process trigered at the begining of the chat session to route the flow the right was.Last but not the least, how To get Back this data with my flow.Best", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Nov 19, 2019", "body": "  Please have a look at the proactive example in the documentation: It triggers once the webchat is loaded, then you can send the event with your custom payload.You can create a hook to fetch the data. Ex:", "type": "commented", "related_issue": null}, {"user_name": "Namec", "datetime": "Nov 20, 2019", "body": " thank you for you help, this is exactly what i am looking for. you'r the bestand this can make me really add any process i want. i just need to master the syntax and now all the variables.in my hook i am trying  to store the value of event.payload.text, on a temp variable,  and the get it back in the flow to show it to the user", "type": "commented", "related_issue": null}, {"user_name": "Namec", "datetime": "Nov 20, 2019", "body": "i have succeded getting and parsing a custom payload each time webchatOpened ! and that is what i am looking for.but now i 'am trying to store that parsed values, in temp variable, but i dosen't work !! or i don't to now how to write it down. (assigning a value to a temp variable and then getting that value back.", "type": "commented", "related_issue": null}, {"user_name": "allardy", "datetime": "Nov 21, 2019", "body": " When using the code editor, you have access to the typings, which auto-completes variables for you. There is a small difference between actions and hooks. In actions, we added a shortcut to access  variables.You can see the difference in signatures below:Basically, you just need to use  instead of using directly  when using the hookDon't hesitate to move your mouse over squiggly lines, it will tell you why there's an errorBTW, if you skip the dialog engine, the event will not trigger any flow, and when it ends, the temp variable is cleared. You can set the flag FORCE_PERSIST_STATE, and I recommend you use either  or  instead of temp (user is kept indefinitely for the user, while session is kept until the session timeouts, by default 30 mins)", "type": "commented", "related_issue": null}, {"user_name": "epaminond", "datetime": "Apr 25, 2020", "body": " , I assume you were able to succeed with your task so closing this for now.", "type": "commented", "related_issue": null}, {"user_name": "Namec", "datetime": "Nov 8, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Nov 8, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "epaminond", "datetime": "Apr 25, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/2280", "issue_status": " Closed\n", "issue_list": [{"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": "This is so strange I am not sure if this may be somethign on my end specifically...?! Anyway, symptoms:If I launch the Studio and go into editing a bot, in the lower right of the screen are buttons to open the Emulator, to see notifications, etc. These buttons are non-responsive. I cannot click them using my mouse. I tried on a production server and also on a dev enviroment. I tried on 2 different computers. I also tried a brand new install of V12.1.1, and the same.HOWEVER, if I open the developer console in my browser (press F12), THEN these buttons become active. If I close the debug console, they become inactive again.Same happens on a different computer as well. One computer is a Windows 10, the other is a Windows Server 2012.In both cases I tried the latest Google Chrome browser and also Microsoft Edge. The same happens.", "type": "commented", "related_issue": null}, {"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": "Here is a screen video of the above:\n", "type": "commented", "related_issue": null}, {"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": "Honestly, have no idea why and how - as you saw we tried different installations of Botpress on different computers. And now on same two computers and same bot instances: it magically works again.", "type": "commented", "related_issue": null}, {"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "lampyon-canada", "datetime": "Aug 22, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/4666", "issue_status": " Closed\n", "issue_list": [{"user_name": "JoyMazumder", "datetime": "Sep 20, 2021", "body": "Hello everyone, I am experimenting with xodr file and tried to load it into carla. I run the following command  and got the output something like this. The load are not aligned properlyI followed this tutorial to install Carla  (Debian installation). OS linux 18.04, UE ", "type": "commented", "related_issue": null}, {"user_name": "JoyMazumder", "datetime": "Sep 20, 2021", "body": "Running  works fine\n", "type": "commented", "related_issue": null}, {"user_name": "hitabm", "datetime": "Sep 25, 2021", "body": "After loading xodr map into CARLA, you can move camera via holding mouse click and WASD keys on keyboard.\nYou see roads as invisible because the camera is viewing them from bottom then just move camera upward and roads will show up correctly.", "type": "commented", "related_issue": null}, {"user_name": "CaffreyXu", "datetime": "Apr 13, 2022", "body": "hello, I have the same question as you. The lane line can't be seen. Do you solve the question? Thank you so much", "type": "commented", "related_issue": null}, {"user_name": "CaffreyXu", "datetime": "Apr 13, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": "hitabm", "datetime": "Apr 16, 2022", "body": "Hi, you can draw string on each lane. It will show red signs for each lane in server side window. Use this code:\n\n", "type": "commented", "related_issue": null}, {"user_name": "CaffreyXu", "datetime": "Apr 18, 2022", "body": "  Thank you so much! I saw many useful function in \"debug\" library, I will try to draw the line i want.", "type": "commented", "related_issue": null}, {"user_name": "JoyMazumder", "datetime": "Sep 20, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/3616", "issue_status": " Closed\n", "issue_list": [{"user_name": "GustavoSilvera", "datetime": "Nov 21, 2020", "body": "Hello Carla team!I am working with the python clients for  and was wondering if there is a good way to move the perspective of the user (or camera) around with the mouse. Specifically a mechanism like the simulator's  view but on the client pygame instance would be ideal!I tried something with destroying and reinitializing the camera sensor but this is very slow. Similarly flooding the server with  requests for  is also very slow. I'm not sure of a better way to do this.Any help is appreciated!", "type": "commented", "related_issue": null}, {"user_name": "Axel1092", "datetime": "Mar 4, 2021", "body": "Hi , you could use pygame to detect position and drag of the mouse to change the camera orientation accordingly  with the  function you mention but there is no better way to do this.", "type": "commented", "related_issue": null}, {"user_name": "GustavoSilvera", "datetime": "Mar 4, 2021", "body": "Hi , thanks for your suggestion. Unfortunately I already tried something similar to this and found that it does not suit my purposes well as it is very laggy with a moving vehicle and I was looking into VR support.In this regard I've transitioned to using the UE4 game itself (simulator 'server') as the main viewpoint since UE4 has native VR support and so far this approach has been successful.", "type": "commented", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Mar 4, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Mar 4, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "GustavoSilvera", "datetime": "Mar 4, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/3578", "issue_status": " Closed\n", "issue_list": [{"user_name": "flacomalone", "datetime": "Nov 12, 2020", "body": "Hi,When running CARLA server, if I try to use the mouse to drag and change the orientation of the camera (not the position), it will, but so much I have no control on it. Is there any way to change mouse sensibility? I don't remember having this issue in previous versions of CARLA", "type": "commented", "related_issue": null}, {"user_name": "flacomalone", "datetime": "Nov 12, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/3080", "issue_status": " Closed\n", "issue_list": [{"user_name": "RansML", "datetime": "Jul 18, 2020", "body": "I have been running carla_release_0.9.5 on Ubuntu 18.04 (with cuda 10.0) without having any trouble. I just downloaded the latest Carla binary (carla_release_0.9.9).Now  gives the following error.Running  shows nothing after printing the first two lines:It looks like the GPU is trying to run something.", "type": "commented", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jul 20, 2020", "body": " could it be that your driver doesn't support vulkan or that you haven't installed the vulkan dependencies? It seems that it is working perfectly using opengl (yes, that is what those 2 lines mean...everything is working fine). Could you check your driver, dependencies, etc?", "type": "commented", "related_issue": null}, {"user_name": "RansML", "datetime": "Jul 20, 2020", "body": "Just to summarize,I also tried on a different laptop on Ubuntu 20.04. I get exactly the same error.", "type": "commented", "related_issue": null}, {"user_name": "Axel1092", "datetime": "Jul 20, 2020", "body": "Hi ,\nUsing the  is intended to disable rendering (no window will appear) and it only works with the  option. After running  have you tried to connect to the server? Try running the  script in the PythonAPI/util folder.", "type": "commented", "related_issue": null}, {"user_name": "RansML", "datetime": "Jul 20, 2020", "body": " When I remove  it loads the CarlaUE4 window and I can interact with it using my mouse. However, nothing shows on the CLI after, fails with the messageWhen I run v.0.9., lots of print messages are typically displayed on the CLI before I can actually connect successfully.", "type": "commented", "related_issue": null}, {"user_name": "wendersonj", "datetime": "Aug 24, 2020", "body": "Using  worked for me.\nGPU: GeForce MX130 (2GB)Cheers", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Oct 24, 2020", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jul 20, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jul 20, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Oct 24, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Nov 8, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/2879", "issue_status": " Closed\n", "issue_list": [{"user_name": "NicoSchoenfisch", "datetime": "May 22, 2020", "body": "Hey everyone,so I have a problem right now that might be pretty basic, but I didn't find a solution in the docs for it.I followed the instructions to build Carla on Windows step by step from here: \nand am now stuck at the very last step.'make launch' and 'make PythonAPI' both finished successfully, and the project is also opened in UnrealEngine and looks like this:citing from the instructions, my steps after 'make launch' should be this:\"make launch compiles the server simulator and launches Unreal Engine. Press Play to start the spectator view and close the editor window to exit. Camera can be moved with WASD keys and rotated by clicking the scene while moving the mouse around.\"Now under the \"Play\" Tab in Unreal Engine, I can choose a mode. If I choose anything other than \"Standalone Game\", I get this message:If I choose to play anyway, this is what it looks like:On the other hand, if I choose \"Standalone Game\" as the mode, I don't get the warning or an error message, but I still end up with the black screen and this grey stick-thingy in the middle, like this:From what I understand from reading the instructions and watching some videos, by pressing \"Play\" at this point after the build, I should be able to start a simulation of the current map (Town03 as standard) in which I can move the camera with WASD, instead of having this black screen. It's definitely possible I missed something I have to do before it works, but I have no idea what it is.Could someone maybe help me out with this?Thanks in advance!", "type": "commented", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jun 26, 2020", "body": " you are probably the right person for this task.", "type": "commented", "related_issue": null}, {"user_name": "bernatx", "datetime": "Jul 6, 2020", "body": "Hi  did you download all the content assets? It seems that you don't have it.\nRefer to this link with instructions: ", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 5, 2020", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jun 26, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "germanros1987", "datetime": "Jun 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 5, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 12, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/2763", "issue_status": " Closed\n", "issue_list": [{"user_name": "ZhengZhongyuan11", "datetime": "Apr 18, 2020", "body": "I just want to know is there any way that we can enable mouse in carla and use the mouse to interact with the carla world? Just like click on a point in the map and get the location information of that point?", "type": "commented", "related_issue": null}, {"user_name": "yaknostoyok", "datetime": "Apr 23, 2020", "body": "Hi,  !Unfortunately, this kind of feature is not implemented in CARLA.\nHowever, should I guess that you are working with a CARLA package? If so, maybe you could consider moving to a build from source. That would allow you to open CARLA with the Unreal Editor, and maybe provide you with the information you need, depending on the exact intentions.\nHere is a link to the docs, where the build process for Linux and Windows are explained: ", "type": "commented", "related_issue": null}, {"user_name": "ZhengZhongyuan11", "datetime": "May 1, 2020", "body": "OK I will try it, thanks!", "type": "commented", "related_issue": null}, {"user_name": "yaknostoyok", "datetime": "Apr 23, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": null, "datetime": "Apr 29, 2020", "body": [], "type": "", "related_issue": "#2800"}, {"user_name": "ZhengZhongyuan11", "datetime": "May 1, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/2525", "issue_status": " Closed\n", "issue_list": [{"user_name": "anna-srl", "datetime": "Feb 27, 2020", "body": "Hi everyone and thanks for your amazing work!I'm a CARLA newbie and I recently installed CARLA 9.5 in windows.I was able to spawn npcs, run manual control and follow several initial steps by the available python modules.However, in the town window I have not been able to zoom in or out, either by the mouse wheel or the keyboard buttons. I am aware that there is an issue with my server (not the required GPU) and I will resolve this issue soon. However, since the ASDW buttons work,  I was wondering if the issue is something different..Any ideas?", "type": "commented", "related_issue": null}, {"user_name": "amstrudy", "datetime": "Mar 4, 2020", "body": "The \"zooming in and out\" is done through the WASDQE keys, where W is forwards, A is left, S is backwards, D is right, Q is down, and E is up in relation to your player frame of reference. You can only use the mouse to move around when using the editor. (Are you running via  or ? If it's the latter then you don't use the mouse to navigate.)", "type": "commented", "related_issue": null}, {"user_name": "anna-srl", "datetime": "Mar 4, 2020", "body": "Yes, that was it! I am using  , that's why the mouse was not working. Thank you very much for your help!", "type": "commented", "related_issue": null}, {"user_name": "amstrudy", "datetime": "Mar 4, 2020", "body": "Yes, no problem! If you don't have any more questions, please close the issue :)", "type": "commented", "related_issue": null}, {"user_name": "anna-srl", "datetime": "Mar 4, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/2186", "issue_status": " Closed\n", "issue_list": [{"user_name": "RonGros", "datetime": "Oct 25, 2019", "body": "Hi,\nI have a laptop (Dell 7490) with Ubuntu 19.X. I tried running Carla and it froze the entire computer.\nMeaning - if I run it, I can sometimes actually use the AWSD keys to navigate the demo, but if I try anything else (say CTRL+TAB) the entire OS freezes and I have to reboot the machine.\nI tried upgrading from 19.04 to 19.10 because I know some changes were made, but the same occurs there.\nNow - I don't know if it is an OS issue (was it ever tested on Ubuntu 19.X?) or a hardware issue (it has intel graphics and not nVidia).\nIs there something I can do? are there any logs I can raise and send for someone to have a look?Thanks!\nRon", "type": "commented", "related_issue": null}, {"user_name": "cassfalg", "datetime": "Oct 31, 2019", "body": "Out of memory? Is the disk active when it freezes? What specs does it have?", "type": "commented", "related_issue": null}, {"user_name": "RonGros", "datetime": "Oct 31, 2019", "body": "Hi,\nI don't know if the disk is active because the computer totally freezes.... if you can help me in how to find the logs that can help.\nThe computer has 16gb of RAM so I doubt memory is the issue here....I really wanted to run the carla on windows (I have dual boot) so that I can try and understand if it is a hardware issue or an OS thing, but I could not find any pre-built binaries and we don't have visual studio license here... (we build on linux...)", "type": "commented", "related_issue": null}, {"user_name": "cassfalg", "datetime": "Oct 31, 2019", "body": "I'm sorry, but this is not really a good place to ask for general Linux support. This is the carla issue tracker. It tracks issues within Carla. And while Carla might trigger those issues, it's not likely to be the issue itself. A lot of information can be found online with a search for \"ubuntu froze what do i do\" or similar.Like you don't think that memory is the issue. Have you checked? Googled for \"linux memory usage monitor\" or similar, tried a tool, started carla, checked if there's still free memory while it runs? 16 gig is not much. I'm running out of memory on my 16 gig machine using carla with my use case for example.", "type": "commented", "related_issue": null}, {"user_name": "RonGros", "datetime": "Oct 31, 2019", "body": "Let me make it clear,\nThis is NOT a linux issue. My ubuntu runs flawlessly no matter what I do, and as a programmer I do run some memory hogging programs and never encountered any issue,\nNevertheless once I run Carla, even when it is the only thing running on my computer, the computer freezes completely.\nI suspect it is either an OS API that went bad or something similar\nI didn’t even load anything specific to Carla...", "type": "commented", "related_issue": null}, {"user_name": "cassfalg", "datetime": "Oct 31, 2019", "body": "Well, if you could monitor memory consumption while you start carla we could exclude the scenario that the machine is simply running out of memory and swapping heavily? That can make a system so unresponsive that it feels frozen. Because from your description I am not sure if your system is frozen or just really really slow. A lot of PCs/Notebooks have indicator lights somewhere to indicate hard disk activity, that's what my question about hard disk activity was aiming at.", "type": "commented", "related_issue": null}, {"user_name": "RonGros", "datetime": "Nov 6, 2019", "body": "Hi, so sorry for the late response but I had to create a presentation and was on my windows for the last few days.\nNow I am back to Ubuntu and tried it again. so first time I tried to run Carla it actually ran pretty smoothly (except for the fact that I didn't know how to release the mouse from it)\nThen I tried to change the window size and then it froze again. from the moment, even after I rebooted it started freezing the entire computer again\nI did run it with the system monitor, the memory was at around 40% all along, regardless of whether it worked a bit or not.\nI then thought maybe it was because I was using an external display, so I disconnected my laptop from the docking station and tried - but it was actually worse, it didn't even render a single frame before freezing the entire computer\nI also tried taking a video but because it freezes everything the video file wasn't written to disk...Any suggestion on how to debug it next?!", "type": "commented", "related_issue": null}, {"user_name": "cassfalg", "datetime": "Nov 6, 2019", "body": "You can Alt + Tab to release the mouse.How did you change the screen resolution? And of what? The  server windows, or some client?Did you disconnect the docking station while the simulator was running?", "type": "commented", "related_issue": null}, {"user_name": "RonGros", "datetime": "Nov 7, 2019", "body": "Okay, so some answers and updates....I did not change the screen resolution, I changed the size of the window (it was not on full screen) I changed the size of the CarleUE4 windowOf course not. I made sure all windows are closed, then disconnected and then tried itNow for the update:So it only happens without the -opengl (although I was sure I tested it before), and as a standalone window", "type": "commented", "related_issue": null}, {"user_name": "kbarbora", "datetime": "Nov 8, 2019", "body": "is all about the GPU that you computer has. If you are running it with the CPU graphics, it will freeze.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 7, 2020", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 7, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 14, 2020", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1844", "issue_status": " Closed\n", "issue_list": [{"user_name": "benkmoore", "datetime": "Jul 4, 2019", "body": "CARLA 0.9.5\nUbuntu 16.04When I launch the CARLA sim the mouse becomes the change heading view control and I have to force quit the sim with tty6.Is there a way I change the controls so that the mouse is not used by default?Thanks", "type": "commented", "related_issue": null}, {"user_name": "benkmoore", "datetime": "Jul 8, 2019", "body": "Crtl+Alt+T to open a terminal and the mouse control comes back", "type": "commented", "related_issue": null}, {"user_name": "benkmoore", "datetime": "Jul 8, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1630", "issue_status": " Closed\n", "issue_list": [{"user_name": "morningsky", "datetime": "May 10, 2019", "body": "According by , we can get collisions  info by , and it will print info about collisions (I use jupyter notebook), but how can I get the collision image or video, such as\n", "type": "commented", "related_issue": null}, {"user_name": "bernatx", "datetime": "May 13, 2019", "body": "Following the tutorial the steps are:That is how it was done in the tutorial.", "type": "commented", "related_issue": null}, {"user_name": "morningsky", "datetime": "May 13, 2019", "body": "  Thanks！ But... How can I start the replayer at that exact moment \"? Which API can do this or command? I mean how to let it run in simulation env", "type": "commented", "related_issue": null}, {"user_name": "bernatx", "datetime": "May 13, 2019", "body": "The command you wrote from the tutorial (\"client.replay_file(\"col2.log\", 13, 0, 122)\") means to start replaying the file 'col2.log' at time 13 (seconds), until the end of the replay, and following the vehicle with id.122. So, the second parameter is the time to start replaying.", "type": "commented", "related_issue": null}, {"user_name": "morningsky", "datetime": "May 15, 2019", "body": " Thanks very much! I know the mean of these params, but how can I get the images/vews when cilent.repaly_file()? when I run the code client.replay_file, it just prints info on terminal, and the pygame windows keep no change...", "type": "commented", "related_issue": null}, {"user_name": "bernatx", "datetime": "May 15, 2019", "body": "Which info do you get from terminal?\nIt should say it is replaying the file, and then the server starts replaying the simulation.", "type": "commented", "related_issue": null}, {"user_name": "morningsky", "datetime": "May 15, 2019", "body": "  Thanks, I solved it. When I run CarlaUE4 it start a window 1,  then I run manual_control.py it start  window 2. when I run the code  client.replay_file , the replaying video running in the window 1.... I just focus on windows 2.\nAnd I have another problem, when I run CarlaUE4, my mouse blocked in the window 1, I have to get the mouse point by use Win+D, it's not convenient， Do you have any advice？\n", "type": "commented", "related_issue": null}, {"user_name": "bernatx", "datetime": "May 13, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "morningsky", "datetime": "May 17, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1533", "issue_status": " Closed\n", "issue_list": [{"user_name": "klemen666", "datetime": "Apr 12, 2019", "body": "I open Carla server on one computer and i open python client on another (py manual_control.py) which opens PyGame. In the HUD it says both server and client are running with 60fps, but the rendering in client is bad - seems like 15 fps. What could be the reason? Network between computers is 1Gbit. I'm using Carla 0.9.4 on Windows.", "type": "commented", "related_issue": null}, {"user_name": "tanghui0102", "datetime": "Apr 19, 2019", "body": "\nMaybe I have the same problem with U.And I haven't dealt it yet.\nDo U run the map of carla own  or other maps?There is one tip is that when the carla server is runnning,never let the server in the background.That's to say,when the server running,the mouse and keyboard can only move the map's perspective and location of camera,cannot operate other programs.Otherwise,the fps is low.", "type": "commented", "related_issue": null}, {"user_name": "Xiaoyu006", "datetime": "May 17, 2019", "body": "when I ran manual_control.py using carla 0.9.5,the fps of server is about 15 fps.\nthat of client is about 60fps, which is good enough.But when i ran manual_control_steeringwheel.py on carla 0.9.5.\nthe fps of server is about 15 fps.\nthat of client is about 10fps, which is bad enough.Not fixed now.", "type": "commented", "related_issue": null}, {"user_name": "tanghui0102", "datetime": "Jun 1, 2019", "body": "Now I see.In carla,we can set it to run in the background without lowering the cpu.Open 'Edit'-->'edit performance'-->'performance'-->'use less cpu when in background'.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 31, 2019", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jul 31, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Aug 7, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1487", "issue_status": " Closed\n", "issue_list": [{"user_name": "Narendarselva", "datetime": "Apr 3, 2019", "body": "Hi Team,\nI m working on a project which requires creation of vehicle on timely interval on a desired location on a desired lane . Is it possible using crala simulator ??\nI gone through the PythonAPI documents there are lot of options which can use random spawn_points which choices randomly . And i dont know exactly how the lane based APIs are used .\nKindly help me on this .\nIs there any specific examples for this is also welcome .\nThanks\nNaren", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Apr 4, 2019", "body": "Hi ,You can spawn vehicles at any location/rotation in the map, the only problem is that the autopilot won't work correctly everywhere, that's why we have recommended spawn points.Alternatively, you can use waypoints to project locations to the road", "type": "commented", "related_issue": null}, {"user_name": "Narendarselva", "datetime": "Apr 5, 2019", "body": "Thanks a lot.", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Apr 5, 2019", "body": "", "type": "commented", "related_issue": null}, {"user_name": "Narendarselva", "datetime": "Apr 8, 2019", "body": "Thanks a lot.\nIs the transform_to_geolocation is provided as PythonAPI ??\nHow i can make my route in a MAP ??\nI could see predefined routes and spaw_points inside all maps , But what if i want to create my own route in MAP in UE4 editor?", "type": "commented", "related_issue": null}, {"user_name": "DongChen06", "datetime": "Apr 19, 2019", "body": "Hi  , I have a similar problem as you,  I want to use the multi-lane senario of the Town3. So I want to put some car models along with a self-driving car on the particular position, Have you figure out how to implement this by a simple way?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 18, 2019", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "talsperre", "datetime": "Jun 26, 2019", "body": "Hi @Derekabc, could you find any way of positioning car models in other lanes near the ego vehicle?", "type": "commented", "related_issue": null}, {"user_name": "DongChen06", "datetime": "Jun 26, 2019", "body": "you can use the code like this :to inform the waypoint of the lane next to the ego car and then spawn a car on that point.", "type": "commented", "related_issue": null}, {"user_name": "talsperre", "datetime": "Jun 26, 2019", "body": "@Derekabc, thanks a lot. This works perfectly for my use case. Although, I had to generate the waypoints approximately 5-10m ahead in order to avoid collision.", "type": "commented", "related_issue": null}, {"user_name": "lichunly", "datetime": "May 22, 2020", "body": "@Derekabc hi, is the function that getting the location on map by mouse-pick possible now(carla 0.9.9)?", "type": "commented", "related_issue": null}, {"user_name": "fty0724", "datetime": "Apr 22, 2019", "body": [], "type": "issue", "related_issue": "#1561"}, {"user_name": "stale", "datetime": "Jun 18, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 25, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1390", "issue_status": " Closed\n", "issue_list": [{"user_name": "HeYDwane3", "datetime": "Mar 15, 2019", "body": "Ubuntu 18.04\nCarla 0.9.4I created a map in roadrunner, but when I exported it and imported into the Carla, I can only see the part within -300<x<300, -300<y<300. Is there any way I can see the full map? I am not sure if it is the problem from Roadrunner or the Carla.Anyone can help me?", "type": "commented", "related_issue": null}, {"user_name": "XGodina", "datetime": "Mar 18, 2019", "body": "Hi, The problem is from RoadRunner, if you would create a big map, you need to change the world settings. Wich version of RoadRunner do you use?", "type": "commented", "related_issue": null}, {"user_name": "eloy96vg", "datetime": "Apr 25, 2019", "body": "Hi I have the same problem to add the full map. I'm using version 2019.0.4 of RoadRunner. What do i need to change in the world settings to get the full map? Would i solve this problem by following the steps indicated with this new way (Docs/generate_map_from_fbx.md)?Thanks in advance.", "type": "commented", "related_issue": null}, {"user_name": "XGodina", "datetime": "Apr 26, 2019", "body": "Hi If you create a huge map you need to change the world setting.  Move the blue frame until the map stays inside. ( put the mouse above of frame and move it) After that confirm the changes in \"Apply World Changes\".Every version RoadRunner change a bit, but the philosophy is the same.", "type": "commented", "related_issue": null}, {"user_name": "DongChen06", "datetime": "Jul 3, 2019", "body": " Where can I found the Roadrunner files of Town 02 and Town01, so I can make some modification to create a new map.", "type": "commented", "related_issue": null}, {"user_name": "XGodina", "datetime": "Jul 4, 2019", "body": "Hi, @Derekabc well, if you download our .xodr files you can load in RoadRunner. RoadRunner should generate the map with OpenDrive information.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 2, 2019", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Mar 18, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Mar 18, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "XGodina", "datetime": "Mar 18, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "carla-admin", "datetime": "Mar 30, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 2, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Sep 9, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1319", "issue_status": " Closed\n", "issue_list": [{"user_name": "micmarty", "datetime": "Feb 28, 2019", "body": "Remote client command: \nInfo that probably doesn't matter (but who knows ):pygame opens a window for a very short time with text , then it crashes:Problem first noticed here:\n", "type": "commented", "related_issue": null}, {"user_name": "micmarty", "datetime": "Feb 28, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "carla-admin", "datetime": "Mar 30, 2019", "body": [], "type": "added this to", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/312", "issue_status": " Closed\n", "issue_list": [{"user_name": "GokulNC", "datetime": "Mar 27, 2018", "body": "When I run  from my VNC, I get the following error::\nOS: Ubuntu 16.04\nCARLA: Release version 0.7.0\nNvidia drivers version: 390.42\nOpenGL version: 4.6Also, if you notice, why does it have  's path? ;)", "type": "commented", "related_issue": null}, {"user_name": "felipecode", "datetime": "Mar 27, 2018", "body": "Hey \nThanks for the issue.Can you tell me exactly which commands you executed ?\nAre you using VGL as well ?\nAre using our tutorial ? \nIt seems your vnc didn't work, the display is not available.Cheers.", "type": "commented", "related_issue": null}, {"user_name": "GokulNC", "datetime": "Mar 28, 2018", "body": "Thanks  for you quick reply.I solved the problem.\nHere are the steps I took:So, I'm closing this issue :)I think apart from the docs, this is also another way to run CARLA on a headless server.\nDo you think it can be added to the docs (since it's actually very simple steps, and guaranteed to work)?Also, just out of curiosity, why did my error logs have nsubiron's path? ;)", "type": "commented", "related_issue": null}, {"user_name": "chankim", "datetime": "Nov 3, 2018", "body": "Hello, I exactly followed the doc : \nbut when I connect using RealVNC from windows 10 to my machine xx.yy.cc.dd:8, I get just the purple screen. seems like there is no windowmanager running.  I understand the last command \"DISPLAY=:8 vglrun -d :7.<gpu_number> $CARLA_PATH/CarlaUE4/Binaries/Linux/CarlaUE4\" should be run on the vnc window. Am I correct? (I mean, after this window manager problem is solved)\nand I didn't run \"sudo service lightdm stop\" before I ran \"sudo nohup Xorg :7 &\".\nIs the document above correct? shouldn't I modify the xstartup file for TurboVNC?\nUpdate : I tried \"/opt/TurboVNC/bin/vncserver -3dwm\", and now I can see folders on the background and can use right mouse click to make pull down menu appear, but it disappears and blocks constantly (with period 1,2 secons) so I can't use it. So my guess the window manager is not started was correct. Any adivce appreicated.", "type": "commented", "related_issue": null}, {"user_name": "nuomizai", "datetime": "Jun 18, 2020", "body": "Hey  , the doc link  does't work anymore. Does carla still support running from headless server?", "type": "commented", "related_issue": null}, {"user_name": "marcgpuig", "datetime": "Mar 27, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Mar 27, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "felipecode", "datetime": "Mar 27, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "felipecode", "datetime": "Mar 27, 2018", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "GokulNC", "datetime": "Mar 28, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/46", "issue_status": " Closed\n", "issue_list": [{"user_name": "farzaa", "datetime": "Nov 21, 2017", "body": "Hi all!If I wanted to create my own driving dataset by driving in Carla manually myself (ex. for an hour), would I be able to grab the exact steering angle? Right now,  script obviously would let me do this. But, it seems the steering angle is set to either -1 or 1 depending on if I'm holding down A or D. But in reality, steering angles change smoothly between -1 and 1 and don't jump because a person needs to actually turn the wheel.The code from the script:\nSo, this would mean I would lose all those angles in between, correct? So my ground truth for steering angles would be just a bunch of -1's and 1's. Is this intended?", "type": "commented", "related_issue": null}, {"user_name": "felipecode", "datetime": "Nov 22, 2017", "body": "Hey farzaa.\nThat is correct !\nWell, this happens because of limitations of using a keyboard ( That keys are either pressed or not pressed)\nIf you have a different joystick, with sensitive buttons, you could access a continuous value and also use pygame.\nAlso consider using the in game AI for data collection , please check  .", "type": "commented", "related_issue": null}, {"user_name": "Tak-Au", "datetime": "Nov 23, 2017", "body": "I modify the manual control file to take mouse input so you can steer left or right with the mouse.  This will give you continuous steering angle value.", "type": "commented", "related_issue": null}, {"user_name": "adiveloper", "datetime": "Oct 22, 2018", "body": "How do I record data any idea ?", "type": "commented", "related_issue": null}, {"user_name": "adiveloper", "datetime": "Oct 22, 2018", "body": "I am new to carla. can someone please explain me how to start recording data(camera and steer,brake) train the data and then run my model based on training?\nA very very brief overview will be highly appreciated....", "type": "commented", "related_issue": null}, {"user_name": "Fangwq", "datetime": "Jan 29, 2019", "body": "I'm new to Carla. I want to gather the data during each driving simulation and use it to train my own model. How to do it ?", "type": "commented", "related_issue": null}, {"user_name": "haoliHuan9", "datetime": "Jul 21, 2022", "body": "hi i am just wondering if you solve this problem to record your own data like steer angle, brake.. and i will be very appreciate if you can share your solution. Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 22, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 30, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "tanglrHello", "datetime": "Jan 8, 2019", "body": [], "type": "issue", "related_issue": "#1101"}, {"user_name": "atusi-nakajima", "datetime": "Sep 5, 2020", "body": [], "type": "issue", "related_issue": "#3240"}]},
{"issue_url": "https://github.com/huggingface/datasets/issues/2708", "issue_status": " Closed\n", "issue_list": [{"user_name": "danyaljj", "datetime": "Jul 22, 2021", "body": "The training instances are not loaded properly.For test and validation, we can see the examples in the output (which is good!):However, only a few instances are loaded for the training split, which is not correct.", "type": "commented", "related_issue": null}, {"user_name": "albertvillanova", "datetime": "Jul 23, 2021", "body": "Hi , thanks for reporting.Unfortunately, I have not been able to reproduce your problem. My train split has 8134 examples:and the content of the last 5 examples is:Could you please load again your dataset and print its shape, like this:and confirm which is your output?", "type": "commented", "related_issue": null}, {"user_name": "danyaljj", "datetime": "Jul 23, 2021", "body": "Hmm .... it must have been a mistake on my side. Sorry for the hassle!", "type": "commented", "related_issue": null}, {"user_name": "danyaljj", "datetime": "Jul 22, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "danyaljj", "datetime": "Jul 23, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1296", "issue_status": " Closed\n", "issue_list": [{"user_name": "liqi198786", "datetime": "Feb 25, 2019", "body": "hello,i'm making a self-driving test platform with CARLA.\ni'm now working with CARLA 0.9.2. i just use the manualcontrol.py included in the realse version. i made some changes in the script that i added a lidar and attached it to the vehicle. the lidar has been added successfully and i could get the pointcloud from it. but in the game scene, i could not see the lidar being rendered.\nis there any method i could see the sensors added to the vehicle?thanks", "type": "commented", "related_issue": null}, {"user_name": "YashBansod", "datetime": "Feb 26, 2019", "body": " in the  the following docstirng is provided:Which basically tells that you can change the sensor visualizations on the hud using the keys [1-9]. However, this only creates a new sensor using the camera manager (it creates lidar too) and visualizes that. To visualize the sensor you have added yourself, you can modify the code in the HUD to plot the point cloud generated by your lidar. (Similar to how its already implemented for lidar created using camera manager).", "type": "commented", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Feb 26, 2019", "body": "hi ,really thank you for your reply. in fact,i didn't mean the visualization for pointcloud data from the sensor, i mean the sensor itsself like a lidar or a camera. i'd like to know where i have placed it on the vehicle.\n\njust as what shown in the picture above.", "type": "commented", "related_issue": null}, {"user_name": "YashBansod", "datetime": "Feb 26, 2019", "body": " I do not think that the sensors have any physical body in the simulator. I think this question would then be better handled by CARLA's members.", "type": "commented", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Feb 26, 2019", "body": ",thanks, so i would not be caught up in the issue. just wait. maybe they will add it in future versions.", "type": "commented", "related_issue": null}, {"user_name": "jagdishbhanushali", "datetime": "Mar 6, 2019", "body": "Hi  , Sensors do not have physical body as you mentioned in your image. But you can see where sensor is placed in carla simulator. For that please follow below steps.", "type": "commented", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Mar 7, 2019", "body": "hi,, thanks for your reply. the situation for me now is that i once compiled the 0.9.0 version(so i could launch the UE4editor of v0.9.0), but now i'm working on v0.9.2 with the released binary, and also have made some changes in the mannualcontrol.py script of v0.9.2.\ni just tried to start the mannualcontrol.py of v0.9.2 to connect the carla server launched from v0.9.0 UE4editor. unfortunately, it failed with the message 'RuntimeError: rpc::rpc_error during call in function get_episode_info'.\ni'd like to know if there is any way to edit the uncompiled(myself) released binary map level(like v0.9.2) in the once compiled UE4editor(v0.9.0 for my case) just through changing the folder content or something?", "type": "commented", "related_issue": null}, {"user_name": "jagdishbhanushali", "datetime": "Mar 7, 2019", "body": "You can't run manual_control.py of v0.9.2 with 0.9.0 server. However you can run carla v0.9.0 in Unreal editor, with manual_control.py of v0.9.0.  You should be able to do your task.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 6, 2019", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "ShanshanYOU", "datetime": "May 16, 2019", "body": "Hi ,\nThanks for sharing all those steps. I've tried to follow it but just for the second step it was stoped. Python scripts were failed to launch because fo an runtime error : \"RuntimeError: rpc::rpc_error during call in function version \". Any further information will be really welcomed !Cheers !", "type": "commented", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Feb 25, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Feb 26, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "liqi198786", "datetime": "Mar 7, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "carla-admin", "datetime": "Mar 30, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "stale", "datetime": "May 6, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 13, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "ShanshanYOU", "datetime": "May 16, 2019", "body": [], "type": "issue", "related_issue": "#1015"}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/1006", "issue_status": " Closed\n", "issue_list": [{"user_name": "anshulpaigwar", "datetime": "Nov 30, 2018", "body": "Carla 0.91 has loads of useful features that can be used for research, thanks to the carla team.\nBut unlike in Carla 0.8x versions, my viewpoint in the map does not move with the player vehicle. We have to manually move it with arrow keys and mouse.I want to be able to see the environment around my player vehicle and what it is doing. I understand that this is not the case in Carla 0.9x due to multi-client support and we would have to do this using something like pygame. But it would be nice if we could directly shift viewpoints over different player vehicles in Carla window as in 0.8x version.\nIt would  be helpful if I could get the sample pygame code for viewing the camera outputs as in your video of Carla 9.0", "type": "commented", "related_issue": null}, {"user_name": "M-Hammod", "datetime": "Nov 30, 2018", "body": "you easily can do that  by piloting the camera attached to the vehicle, after spawning the vehicle from python, in the unreal editor-->world outliner-->search for 'cam'-->you will see the a camera actor attached to the vehicle that you spawned-->rightclick on it and choose pilot.I had a proplem that I had to eject the player for the pilot option to get activatedhope this helps", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 30, 2018", "body": "The  provides a view similar to that in Python with pygame.Alternatively, you can also move the \"spectator\" actor from Python to move the simulator view.e.g.,  uses this trick.", "type": "commented", "related_issue": null}, {"user_name": "anshulpaigwar", "datetime": "Nov 30, 2018", "body": "Thanks for the reply it helped to solve my problem.", "type": "commented", "related_issue": null}, {"user_name": "anshulpaigwar", "datetime": "Dec 4, 2018", "body": "How to attach spectator to the vehicle like a camera ?? I tried this simplyThis kind of works but there are two problems:", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Dec 4, 2018", "body": "Unfortunately, the transformation matrices are not yet implemented in the new API, but you can get them in Python from the old API at .The best way for synchronizing the client with simulator updates is using the wait for tick function", "type": "commented", "related_issue": null}, {"user_name": "Sakrust", "datetime": "Dec 5, 2018", "body": "Just use a little trick by placing a dummy actor in the scene:camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\ncamera_transform = carla.Transform(carla.Location(x=10, z=10))\ncamera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)\nspectator.set_transform(camera.get_transform())", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Dec 5, 2018", "body": "That's a nice workaround :D but I would use instead a cheaper sensor, collision sensor for instance, cameras make your FPS drop significantly.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 3, 2019", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "ChrisHRZ", "datetime": "Apr 17, 2019", "body": "How can i chose to view this camera?", "type": "commented", "related_issue": null}, {"user_name": "Sakrust", "datetime": "Apr 17, 2019", "body": "You don't. Your simulator window view will be updated.", "type": "commented", "related_issue": null}, {"user_name": "gunki-geek", "datetime": "Jan 25, 2022", "body": "u can write two scripts master.py and slave.py and synchronize between the two like in the attached scripts\n", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 30, 2018", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 30, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 3, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "Feb 11, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/26", "issue_status": " Closed\n", "issue_list": [{"user_name": "WenchaoDing", "datetime": "Nov 16, 2017", "body": "Hi,\nI am writing a ros wrapper for carla client, almost finished... but I get confused about the format of depth image received from the client.In your code, you do the post-processing like this for visualization purpose:andwe can see that you actually use the three channels of the received depth image. What is the physical meaning of the three channels? Why you normalize the three channels use different coefficients?How can reconstruct the ground truth depth from your format (I mean for example 1.2m 3.3m etc for every pixel)?", "type": "commented", "related_issue": null}, {"user_name": "WenchaoDing", "datetime": "Nov 16, 2017", "body": "what about the intrinsic parameter of the capturing camera? so I can reproject the depth to 3d point cloud", "type": "commented", "related_issue": null}, {"user_name": "marcgpuig", "datetime": "Nov 16, 2017", "body": "Hi ,\nThis is due our depth codification, we use the 3 different channels to accomplish the maximum precision that we can get from a image format of that size.24 bit floating precision point codified in the 3 channels of the RGB.\nThe order from less to more significant bytes is R -> G -> B.Our max render distance (far) is 1km.In this case Unreal Engine gives us the images in  (Blue, Green, Red, Alpha), so first we select only the  with  and then we reverse it to  with .Here you have a matlab implementation in case any of you need to use it. Thanks to your issue we noticed that we left this  on the client code, so the correct depth decodification is:We are gonna fix this soon.", "type": "commented", "related_issue": null}, {"user_name": "WenchaoDing", "datetime": "Nov 16, 2017", "body": "Hi  ,Thanks for your answer. Based on your answer, I get the true depth now. :)BTW: we are actually reversing the channels (with [:,:,::-1]) so that blue is the MSB, instead of choosing the blue channel, aren't we?", "type": "commented", "related_issue": null}, {"user_name": "marcgpuig", "datetime": "Nov 16, 2017", "body": "Hey !Sorry, I misunderstood the code before. I edited the comment with the right answer.\nThe thing is that we get the image from Unreal Engine in  format.The camera parameters (FOV, position, and so on) are set by you in settings, please take a look at the example in the Carla documentation . With these information you can build the intrinsic matrix.\nWe are working on the documentation of these topics. ;)Thanks!", "type": "commented", "related_issue": null}, {"user_name": "dosovits", "datetime": "Nov 16, 2017", "body": "Hi  , btw, would be great if you can share your ROS integration code when it's done, I'm sure many would be happy to use it ( see e.g. issue  )", "type": "commented", "related_issue": null}, {"user_name": "WenchaoDing", "datetime": "Nov 17, 2017", "body": "Hi I wrote a ros wrapper for my quick development. So I didn't plan to write a very official, well-shaped one and submit a pull request for this. Anyway, you can find my simple  here if someone is interested.", "type": "commented", "related_issue": null}, {"user_name": "dosovits", "datetime": "Nov 17, 2017", "body": " , thanks! Well, hopefully one day it will grow and become better-shaped :)", "type": "commented", "related_issue": null}, {"user_name": "s7ev3n", "datetime": "Nov 21, 2017", "body": " Hi, thanks for your ros implementation. I have the same question about the camera intrinsic parameter, do you have any idea how to calculate it ? Thanks!", "type": "commented", "related_issue": null}, {"user_name": "WenchaoDing", "datetime": "Nov 29, 2017", "body": "HiBased on FOV and image size you can compute it. Assume that the image is (800 * 600, FOV 100), you can simply get cx = 400, cy = 300, (assume fx = fy =f),   tan(FOV/2) = (1000/2) /f ==> f = 500/tan50 = 419.6...", "type": "commented", "related_issue": null}, {"user_name": "WenchaoDing", "datetime": "Nov 29, 2017", "body": "Hi ,I found the z axis box extent for all the vehicles is strange. The z extent is always 0.32m, for all vehicles, no matter what type of the vehicle is. And the actually z size should be 0.32m * 2 = 0.64m? which is still too small.", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 29, 2017", "body": "Hi ,Yes, the Z axis of vehicle's bounding boxes is always the same. The reason why is because we didn't care about the Z axis when we set up these boxes as they were meant to be used as 2D boxes for measuring the percentage of the car on other lane and off-road. Now that they are exposed to the client perhaps is worth to fit the height too, we will consider adding this.", "type": "commented", "related_issue": null}, {"user_name": "marcgpuig", "datetime": "Nov 29, 2017", "body": "Hi ,Regarding the intrinsic matrix, look at my comment .", "type": "commented", "related_issue": null}, {"user_name": "nsubiron", "datetime": "Nov 16, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dosovits", "datetime": "Nov 17, 2017", "body": [], "type": "issue", "related_issue": "#12"}, {"user_name": "nsubiron", "datetime": "Dec 1, 2017", "body": [], "type": "issue", "related_issue": "#64"}, {"user_name": "nsubiron", "datetime": "Dec 13, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "RehabHsn", "datetime": "Feb 3, 2022", "body": [], "type": "issue", "related_issue": "#4995"}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/1156", "issue_status": " Closed\n", "issue_list": [{"user_name": "hemaAI", "datetime": "Jan 24, 2020", "body": "Hi,I have trained my own ssd model with pictures in the voc structure. The training passes and I have a params file. But when I try to use the demo script it still plots me the pretrained voc classes. How can I get rid of this and only see my own classes?I have trained my own model like this: python train_ssd.py --num-workers=0 --epochs=10Code:`\"\"\"Train SSD\"\"\"\nimport argparse\nimport os\nimport logging\nimport warnings\nimport time\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import nd\nfrom mxnet import gluon\nfrom mxnet import autograd\nimport gluoncv as gcv\ngcv.utils.check_version('0.6.0')\nfrom gluoncv import data as gdata\nfrom gluoncv import utils as gutils\nfrom gluoncv.model_zoo import get_model\nfrom gluoncv.data.batchify import Tuple, Stack, Pad\nfrom gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform\nfrom gluoncv.data.transforms.presets.ssd import SSDDefaultValTransform\nfrom gluoncv.data.transforms.presets.ssd import SSDDALIPipeline\nfrom gluoncv.data import VOCDetectionfrom gluoncv.utils.metrics.voc_detection import VOC07MApMetric\nfrom gluoncv.utils.metrics.coco_detection import COCODetectionMetric\nfrom gluoncv.utils.metrics.accuracy import Accuracyfrom mxnet.contrib import amptry:\nimport horovod.mxnet as hvd\nexcept ImportError:\nhvd = Nonetry:\nfrom nvidia.dali.plugin.mxnet import DALIGenericIterator\ndali_found = True\nexcept ImportError:\ndali_found = Falsedef parse_args():\nparser = argparse.ArgumentParser(description='Train SSD networks.')\nparser.add_argument('--network', type=str, default='vgg16_atrous',\nhelp=\"Base network name which serves as feature extraction base.\")\nparser.add_argument('--data-shape', type=int, default=300,\nhelp=\"Input data shape, use 300, 512.\")\nparser.add_argument('--batch-size', type=int, default=32,\nhelp='Training mini-batch size')\nparser.add_argument('--dataset', type=str, default='voc',\nhelp='Training dataset. Now support voc.')\nparser.add_argument('--dataset-root', type=str, default='~/.mxnet/datasets/',\nhelp='Path of the directory where the dataset is located.')\nparser.add_argument('--num-workers', '-j', dest='num_workers', type=int,\ndefault=4, help='Number of data workers, you can use larger '\n'number to accelerate data loading, if you CPU and GPUs are powerful.')\nparser.add_argument('--gpus', type=str, default='0',\nhelp='Training with GPUs, you can specify 1,3 for example.')\nparser.add_argument('--epochs', type=int, default=240,\nhelp='Training epochs.')\nparser.add_argument('--resume', type=str, default='',\nhelp='Resume from previously saved parameters if not None. '\n'For example, you can resume from ./ssd_xxx_0123.params')\nparser.add_argument('--start-epoch', type=int, default=0,\nhelp='Starting epoch for resuming, default is 0 for new training.'\n'You can specify it to 100 for example to start from 100 epoch.')\nparser.add_argument('--lr', type=float, default=0.001,\nhelp='Learning rate, default is 0.001')\nparser.add_argument('--lr-decay', type=float, default=0.1,\nhelp='decay rate of learning rate. default is 0.1.')\nparser.add_argument('--lr-decay-epoch', type=str, default='160,200',\nhelp='epochs at which learning rate decays. default is 160,200.')\nparser.add_argument('--momentum', type=float, default=0.9,\nhelp='SGD momentum, default is 0.9')\nparser.add_argument('--wd', type=float, default=0.0005,\nhelp='Weight decay, default is 5e-4')\nparser.add_argument('--log-interval', type=int, default=100,\nhelp='Logging mini-batch interval. Default is 100.')\nparser.add_argument('--save-prefix', type=str, default='',\nhelp='Saving parameter prefix')\nparser.add_argument('--save-interval', type=int, default=10,\nhelp='Saving parameters epoch interval, best model will always be saved.')\nparser.add_argument('--val-interval', type=int, default=1,\nhelp='Epoch interval for validation, increase the number will reduce the '\n'training time if validation is slow.')\nparser.add_argument('--seed', type=int, default=233,\nhelp='Random seed to be fixed.')\nparser.add_argument('--syncbn', action='store_true',\nhelp='Use synchronize BN across devices.')\nparser.add_argument('--dali', action='store_true',\nhelp='Use DALI for data loading and data preprocessing in training. '\n'Currently supports only COCO.')\nparser.add_argument('--amp', action='store_true',\nhelp='Use MXNet AMP for mixed precision training.')\nparser.add_argument('--horovod', action='store_true',\nhelp='Use MXNet Horovod for distributed training. Must be run with OpenMPI. '\n'--gpus is ignored when using --horovod.')classes = ['person', 'cat']class VOCLike(VOCDetection):\nCLASSES = classesdef get_dataset(dataset, args):\nif dataset.lower() == 'voc':\n#train_dataset = gdata.VOCDetection(splits=[(2007, 'trainval'), (2012, 'trainval')])\ntrain_dataset =VOCLike(root='/Users/henrik/VOCdevkit', splits=[(2019, 'train')])\nval_dataset =VOCLike(root='/Users/henrik/VOCdevkit', splits=[(2019, 'val')])\n#val_dataset = gdata.VOCDetection(splits=[(2007, 'test')])\nval_metric = VOC07MApMetric(iou_thresh=0.5, class_names=val_dataset.classes)\nprint(val_dataset.classes)\nsys.exit()\nelif dataset.lower() == 'coco':\ntrain_dataset = gdata.COCODetection(root=args.dataset_root + \"/coco\", splits='instances_train2017')\nval_dataset = gdata.COCODetection(root=args.dataset_root + \"/coco\", splits='instances_val2017', skip_empty=False)\nval_metric = COCODetectionMetric(\nval_dataset, args.save_prefix + '_eval', cleanup=True,\ndata_shape=(args.data_shape, args.data_shape))\n# coco validation is slow, consider increase the validation interval\nif args.val_interval == 1:\nargs.val_interval = 10\nelse:\nraise NotImplementedError('Dataset: {} not implemented.'.format(dataset))\nreturn train_dataset, val_dataset, val_metricdef get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers, ctx):\n\"\"\"Get dataloader.\"\"\"\nwidth, height = data_shape, data_shape\n# use fake data to generate fixed anchors for target generation\nwith autograd.train_mode():\n_, _, anchors = net(mx.nd.zeros((1, 3, height, width), ctx))\nanchors = anchors.as_in_context(mx.cpu())\nbatchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\ntrain_loader = gluon.data.DataLoader(\ntrain_dataset.transform(SSDDefaultTrainTransform(width, height, anchors)),\nbatch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\nval_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\nval_loader = gluon.data.DataLoader(\nval_dataset.transform(SSDDefaultValTransform(width, height)),\nbatch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=num_workers)\nreturn train_loader, val_loaderdef get_dali_dataset(dataset_name, devices, args):\nif dataset_name.lower() == \"coco\":\n# training\nexpanded_file_root = os.path.expanduser(args.dataset_root)\ncoco_root = os.path.join(expanded_file_root,\n'coco',\n'train2017')\ncoco_annotations = os.path.join(expanded_file_root,\n'coco',\n'annotations',\n'instances_train2017.json')\nif args.horovod:\ntrain_dataset = [gdata.COCODetectionDALI(num_shards=hvd.size(), shard_id=hvd.rank(), file_root=coco_root,\nannotations_file=coco_annotations, device_id=hvd.local_rank())]\nelse:\ntrain_dataset = [gdata.COCODetectionDALI(num_shards= len(devices), shard_id=i, file_root=coco_root,\nannotations_file=coco_annotations, device_id=i) for i, _ in enumerate(devices)]def get_dali_dataloader(net, train_dataset, val_dataset, data_shape, global_batch_size, num_workers, devices, ctx, horovod):\nwidth, height = data_shape, data_shape\nwith autograd.train_mode():\n_, _, anchors = net(mx.nd.zeros((1, 3, height, width), ctx=ctx))\nanchors = anchors.as_in_context(mx.cpu())def save_params(net, best_map, current_map, epoch, save_interval, prefix):\ncurrent_map = float(current_map)\nif current_map > best_map[0]:\nbest_map[0] = current_map\nnet.save_params('{:s}{:.4f}.params'.format(prefix, epoch, current_map))def validate(net, val_data, ctx, eval_metric):\n\"\"\"Test on validation dataset.\"\"\"\neval_metric.reset()\n# set nms threshold and topk constraint\nnet.set_nms(nms_thresh=0.45, nms_topk=400)\nnet.hybridize(static_alloc=True, static_shape=True)\nfor batch in val_data:\ndata = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\nlabel = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\ndet_bboxes = []\ndet_ids = []\ndet_scores = []\ngt_bboxes = []\ngt_ids = []\ngt_difficults = []\nfor x, y in zip(data, label):\n# get prediction results\nids, scores, bboxes = net(x)\ndet_ids.append(ids)\ndet_scores.append(scores)\n# clip to image size\ndet_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n# split ground truths\ngt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\ngt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\ngt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)def train(net, train_data, val_data, eval_metric, ctx, args):\n\"\"\"Training pipeline\"\"\"\nnet.collect_params().reset_ctx(ctx)if  == '':\nargs = parse_args()Demo script:`from gluoncv import model_zoo, data, utils\nfrom matplotlib import pyplot as pltnet = model_zoo.get_model('ssd_300_vgg16_atrous_voc', pretrained=True)\nnet.load_parameters('/Users/henrik/Downloads/ssd_300_vgg16_atrous_voc_best.params')\nnet.hybridize()im_fname = utils.download('' +\n'gluoncv/detection/street_small.jpg?raw=true',\npath='street_small.jpg')\nx, img = data.transforms.presets.ssd.load_test(im_fname, short=512)\nprint('Shape of pre-processed image:', x.shape)class_IDs, scores, bounding_boxes = net(x)ax = utils.viz.plot_bbox(img, bounding_boxes[0], scores[0],\nclass_IDs[0], class_names=net.classes)\nplt.show()\n`", "type": "commented", "related_issue": null}, {"user_name": "lgov", "datetime": "Jan 25, 2020", "body": "Hi,the names of the classes are not stored in the params file or used in the neural net. If you print the class_ID's in your demo script you'll see that the net will just return a number, like 1. How you map that number to a class name is your decision.You are using the classes list from the net:But since your net is just created from the model zoo, and as mentioned before the .params file doesn't contain the class names, it is using the original VOC classes to map the class_ID numbers to names.I suppose you can just use your CLASSES list directly, as in:hth,\nLieven", "type": "commented", "related_issue": null}, {"user_name": "hemaAI", "datetime": "Jan 25, 2020", "body": "Hi lgov,thanks for you advise. To make it easier:\nWhat if I would train two classes the are not contained inside the model. E.g. [\"mouse\", \"snake\"]If I would then do:Would it find the classes? Or should I rather (some how) reset the classes of the model so that it only contains mouse and snake?Maybe like this?\n", "type": "commented", "related_issue": null}, {"user_name": "lgov", "datetime": "Jan 25, 2020", "body": "Okay, so you build a model that knows to return class_id=1 when a bounding box in an image contains a mouse, and 2 if a bounding box contains a snake.Suppose that your net returns 5 bounding boxes, two mice and 3 snakes. The class_IDs[0] list you have there will then be a list like this: [1,2,1,2,2].With \"would it find the classes\" you mean then: can the utils.viz.plot_bbox function plot the correct label \"mouse\" for class id 1 and \"snake\" for class id 2 right?\nIn that case, like I said, you can just pass your classes list to the plot_bbox function as you show in your last example code.Note that the classes member variable of the net can be used to store your actual class names, that can be convenient but is not needed. It will by default be initiated with the VOC classes.The reset_class function serves a different purpose. Suppose that you want to build a object detection algo that can detect two classes already in scope of a pretrained model, e.g. cat and person. There is already a pretrained model which recognizes all the VOC classes, so we can start from that instead of retraining:The reset_class here does the following:Done, you now have a network that can classify only cats and persons.Not sure if this answers your question though...L.", "type": "commented", "related_issue": null}, {"user_name": "hemaAI", "datetime": "Jan 27, 2020", "body": "Thanks for the explanation. However it seems like something still goes wrong. I trained 20 images of mice and 20 images of snakes. 75% train, 25% val. Then I tried to plot my prediction like this:But the result looks like this: \nDo you have an idea whats going wrong?", "type": "commented", "related_issue": null}, {"user_name": "hemaAI", "datetime": "Jan 28, 2020", "body": " any idea?", "type": "commented", "related_issue": null}, {"user_name": "lgov", "datetime": "Jan 29, 2020", "body": "When I train a classification network I prepare to gather up to 1000 images per class. This is for finegrained classification, so more complex than mouse versus snake classification.So my suggestion is to gather much more images for both classes.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 30, 2021", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 30, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 7, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/dmlc/gluon-cv/issues/598", "issue_status": " Closed\n", "issue_list": [{"user_name": "yuanCnD", "datetime": "Jan 30, 2019", "body": "hi,here is training log:\nloading annotations into memory...\nDone (t=16.61s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.46s)\ncreating index...\nindex created!\nINFO:root:Namespace(batch_size=64, data_shape=416, dataset='coco', epochs=280, gpus='0,1,2,3,4,5,6,7', label_smooth=True, log_interval=100, lr=0.001, lr_decay=0.1, lr_decay_epoch='220,250', lr_decay_period=0, lr_mode='step', mixup=True, momentum=0.9, network='darknet53', no_mixup_epochs=20, no_random_shape=False, no_wd=True, num_samples=117266, num_workers=32, resume='', save_interval=10, save_prefix='yolo3_darknet53_coco', seed=233, start_epoch=0, syncbn=True, val_interval=1, warmup_epochs=2, warmup_lr=0.0, wd=0.0005)\nINFO:root:Start training from [Epoch 0]\nINFO:root:[Epoch 0][Batch 99], LR: 2.70E-05, Speed: 33.324 samples/sec, ObjLoss=878.333, BoxCenterLoss=17.802, BoxScaleLoss=14.941, ClassLoss=339.257\nINFO:root:[Epoch 0][Batch 199], LR: 5.43E-05, Speed: 27.818 samples/sec, ObjLoss=464.404, BoxCenterLoss=17.508, BoxScaleLoss=13.389, ClassLoss=237.332\nINFO:root:[Epoch 0][Batch 299], LR: 8.16E-05, Speed: 56.125 samples/sec, ObjLoss=321.394, BoxCenterLoss=16.973, BoxScaleLoss=11.971, ClassLoss=174.369\nINFO:root:[Epoch 0][Batch 399], LR: 1.09E-04, Speed: 30.589 samples/sec, ObjLoss=249.321, BoxCenterLoss=16.702, BoxScaleLoss=11.015, ClassLoss=140.578\nINFO:root:[Epoch 0][Batch 499], LR: 1.36E-04, Speed: 38.707 samples/sec, ObjLoss=205.859, BoxCenterLoss=16.567, BoxScaleLoss=10.342, ClassLoss=119.724\nINFO:root:[Epoch 0][Batch 599], LR: 1.63E-04, Speed: 37.456 samples/sec, ObjLoss=176.808, BoxCenterLoss=16.499, BoxScaleLoss=9.844, ClassLoss=105.734\nINFO:root:[Epoch 0][Batch 699], LR: 1.91E-04, Speed: 34.759 samples/sec, ObjLoss=155.749, BoxCenterLoss=16.326, BoxScaleLoss=9.392, ClassLoss=95.448\nINFO:root:[Epoch 0][Batch 799], LR: 2.18E-04, Speed: 34.887 samples/sec, ObjLoss=140.052, BoxCenterLoss=16.265, BoxScaleLoss=9.074, ClassLoss=87.846\nINFO:root:[Epoch 0][Batch 899], LR: 2.45E-04, Speed: 21.149 samples/sec, ObjLoss=127.875, BoxCenterLoss=16.238, BoxScaleLoss=8.806, ClassLoss=81.887\nINFO:root:[Epoch 0][Batch 999], LR: 2.73E-04, Speed: 39.372 samples/sec, ObjLoss=117.959, BoxCenterLoss=16.182, BoxScaleLoss=8.577, ClassLoss=77.059\nINFO:root:[Epoch 0][Batch 1099], LR: 3.00E-04, Speed: 23.731 samples/sec, ObjLoss=109.935, BoxCenterLoss=16.154, BoxScaleLoss=8.364, ClassLoss=73.143\nINFO:root:[Epoch 0][Batch 1199], LR: 3.27E-04, Speed: 32.060 samples/sec, ObjLoss=103.141, BoxCenterLoss=16.092, BoxScaleLoss=8.195, ClassLoss=69.817\nINFO:root:[Epoch 0][Batch 1299], LR: 3.55E-04, Speed: 55.695 samples/sec, ObjLoss=97.359, BoxCenterLoss=16.040, BoxScaleLoss=8.067, ClassLoss=67.010\nINFO:root:[Epoch 0][Batch 1399], LR: 3.82E-04, Speed: 29.831 samples/sec, ObjLoss=92.345, BoxCenterLoss=15.977, BoxScaleLoss=7.947, ClassLoss=64.564\nINFO:root:[Epoch 0][Batch 1499], LR: 4.09E-04, Speed: 47.742 samples/sec, ObjLoss=87.972, BoxCenterLoss=15.919, BoxScaleLoss=7.830, ClassLoss=62.440\nINFO:root:[Epoch 0][Batch 1599], LR: 4.36E-04, Speed: 34.914 samples/sec, ObjLoss=84.214, BoxCenterLoss=15.883, BoxScaleLoss=7.729, ClassLoss=60.608\nINFO:root:[Epoch 0][Batch 1699], LR: 4.64E-04, Speed: 36.860 samples/sec, ObjLoss=80.848, BoxCenterLoss=15.821, BoxScaleLoss=7.624, ClassLoss=58.938\nINFO:root:[Epoch 0][Batch 1799], LR: 4.91E-04, Speed: 46.941 samples/sec, ObjLoss=77.929, BoxCenterLoss=15.795, BoxScaleLoss=7.533, ClassLoss=57.506\nINFO:root:[Epoch 0] Training cost: 4479.540, ObjLoss=77.019, BoxCenterLoss=15.767, BoxScaleLoss=7.509, ClassLoss=57.045\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type \nDONE (t=21.98s).\nAccumulating evaluation results...\nDONE (t=1.79s).\nINFO:root:[Epoch 0] Validation:", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Feb 7, 2019", "body": "Are you following the same arguments as in the model zoo script?", "type": "commented", "related_issue": null}, {"user_name": "xupengcoding", "datetime": "Mar 28, 2019", "body": "I also met this problem. When I was training, the validation mAP is always zero. My script is \"python3 -u train_yolo3.py --network darknet53 --dataset coco --lr 0.0005 --gpus 4,5,6,7 --batch-size 32 -j 32 --log-interval 100 --lr-decay-epoch 220,250 --epochs 280 --syncbn --warmup-epochs 2 --mixup --no-mixup-epochs 20 --label-smooth --no-wd 1>train_yolo_dark53.log\".  , Could you please help me!", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Mar 28, 2019", "body": "  try this first to make sure you have all set up correctly:If this still doesn't work, then there might be some dataset problem", "type": "commented", "related_issue": null}, {"user_name": "xupengcoding", "datetime": "Mar 29, 2019", "body": "I found the ans. The score_thresh in COCODetectionMetric is 0.5, but the predict scores always less than 0.5 in several starting epochs. I think this does not matter, and the issue can be closed.", "type": "commented", "related_issue": null}, {"user_name": "yuzhms", "datetime": "Apr 25, 2019", "body": "I meet this problem when I use scripts likeLosses seemed ok, dropped about 30x. I check and found the score is all below 0.5 even after training 150 epochs.\nBut, when I turn off label smoothing, mixup and no-wd, It work fine, I can get none zero map after 2 epochs.\nIs this normal?\nWhat should I do if I want to use label smoothing, mixup and no-wd?\n", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Apr 25, 2019", "body": "please update to master, there has been a small bug causing smooth to be incorrect", "type": "commented", "related_issue": null}, {"user_name": "zhreshold", "datetime": "Mar 29, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/980", "issue_status": " Closed\n", "issue_list": [{"user_name": "brimoor", "datetime": "Apr 24, 2021", "body": "It would be natural if clicking on an already-opened graphs tab would collapse the graphs section. This would present a more convenient alternative to mousing way over to the righthand side to click hide.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Apr 24, 2021", "body": [], "type": "issue", "related_issue": null}, {"user_name": "benjaminpkane", "datetime": "May 7, 2021", "body": [], "type": "pull", "related_issue": "#1005"}, {"user_name": "benjaminpkane", "datetime": "May 10, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/891", "issue_status": " Closed\n", "issue_list": [{"user_name": "brimoor", "datetime": "Feb 25, 2021", "body": "The  view stage supports creating a view that only contains certain objects, and, as shown in the example code in the docs, it works very naturally with , which provides the necessary info automatically from a user's mouse clicks.However, it also may be useful for a user to select objects using only their , without having to worry about what field or sample the object is in. One way to accomplish this is via the pattern below:The following ideas come to mind:\n(a) add optional support for passing a simple list of IDs to , in which case the stage will internally use the pattern above to construct the view\n(b) deprecate the more expressive but clunky current syntax and  support the above\n(c) add a new stage that allows for selection via the above syntaxPerhaps (a) is the best best for now?An important observation is that the pattern shown above will be quite slow for datasets with many  fields.Usage of the existing  stage:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Feb 25, 2021", "body": "Another option could be to add a  method that uses the above construction to locate the objects and return information about them in the format expected by . The benefit here would be that you run  (slow) only once, and then use  (fast) in your subsequent operations", "type": "commented", "related_issue": null}, {"user_name": "benjaminpkane", "datetime": "Feb 25, 2021", "body": "(a) would probably be an improvement, especially now that it seems like ids being used by other parts of FiftyOne.I also think it would be nice to simplify our approach to , , and s for users so they never have to think about it...", "type": "commented", "related_issue": null}, {"user_name": "ehofesmann", "datetime": "Feb 25, 2021", "body": "I agree with Ben, I think  should support both the current input and just a list of object ids. It should probably support different versions of the current input as well, for example, a dict with just object ids and associated label fields, or just sample ids and object ids without label fields. The more information you add the faster the method becomes.Though most importantly, I really we should get rid of the need to use . Just doing this through  on the  of objects was the most natural/first thing that I wanted to do.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Feb 25, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "brimoor", "datetime": "Feb 26, 2021", "body": [], "type": "pull", "related_issue": "#892"}, {"user_name": "brimoor", "datetime": "Mar 5, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/390", "issue_status": " Closed\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 13, 2020", "body": "It would be extremely useful to be able to zoom in/out and pan around the viewer in expanded sample mode. Typical use case is to inspect specific regions of large and/or complicated imagesIn terms of implementation, the obvious best practice is to follow the Google Maps-style interface:", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 13, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "benjaminpkane", "datetime": "Nov 16, 2020", "body": [], "type": "issue", "related_issue": "#135"}, {"user_name": "benjaminpkane", "datetime": "Nov 16, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "brimoor", "datetime": "Apr 28, 2021", "body": [], "type": "issue", "related_issue": "#991"}, {"user_name": "benjaminpkane", "datetime": "May 18, 2021", "body": [], "type": "pull", "related_issue": "#1041"}, {"user_name": "benjaminpkane", "datetime": "Aug 9, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Aug 9, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/1196", "issue_status": " Closed\n", "issue_list": [{"user_name": "brimoor", "datetime": "Aug 16, 2021", "body": "It would be helpful to see confidence thresholds (if possible, see below) in the tooltips of PR/ROC curves plotable viaIn the first two,  and  are used to compute the curves and they return thresholds, so it'll be trivial to get the data.For detection evaluation, there are some nuances. See  for details.In terms of adding the threshold data to the plot tooltips, you can see in  how to customize the tooltip. A similar thing should be possible to add for the other PR/ROC curve plotting methods for the  backend.Tooltips aren't supported by the  backend, so this would be a feature only supported by the  backend. But that's okay, there is already precedent for backend-specific arguments to plotting functions, e.g., . Also, we would like to remove the  backend altogether in the future.", "type": "commented", "related_issue": null}, {"user_name": "pradkrish", "datetime": "Sep 30, 2021", "body": "Hello, a newcomer to this project looking for good first issues. is someone working on this? Otherwise, I can give it a go.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Sep 30, 2021", "body": "Hi   No one is working on this yet so please do give it a go! Happy to assist if needed", "type": "commented", "related_issue": null}, {"user_name": "pradkrish", "datetime": "Oct 1, 2021", "body": "I looked briefly at the implementation and also looked around for any available tests in the directory that uses  and  and found none, to my surprise. do you think it's a good idea to test those functions and if yes, can we make it part of this issue?", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Oct 1, 2021", "body": "Yeah we don't have good test coverage for graphical things like plots or the App (difficult to ensure plots are rendering as expected). Though I would certainly be onboard with adding basic unit tests to ensure the code runs without error (excluding ).In terms of testing you work, the primary existing way is just to verify that the examples in the docs in places like the following work as expected: ", "type": "commented", "related_issue": null}, {"user_name": "pradkrish", "datetime": "Oct 1, 2021", "body": "Will it be equivalent to making sure all the tests in  pass after changes?", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Oct 1, 2021", "body": "yes, good point. I forgot about those because the tests in  are not automatically included in our CI workflow. But, yes, running those tests manually (and adding new ones as necessary) is a good way to verify plotting-related functionality is working as expected", "type": "commented", "related_issue": null}, {"user_name": "pradkrish", "datetime": "Oct 2, 2021", "body": "I have never worked with Plotly before and I have a basic question. This task is about adding a tooltip, does it mean  when I move the mouse cursor over the PR or ROC curve, I should be able to see a box with values for precision, recall and the corresponding threshold? In that case, it looks like I need to create a dataframe with columns for precision, recall and threshold and pass it into plot function. is that the right approach? Thanks.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Oct 2, 2021", "body": "yes exactly. The plotly functions have something called  that you can use to store extra data (confidences, in this case) that you can use to define a custom tooltip that shows the confidence value when you hover over a point.This snippet shows an example of doing that elsewhere in our codebase:\nIf you get stuck, forget about FiftyOne for a second and just try to make a plotly plot directly that does what you want. Then you can get it integrated into the FiftyOne codebase.", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Aug 16, 2021", "body": [], "type": "issue", "related_issue": null}, {"user_name": "brimoor", "datetime": "Dec 27, 2021", "body": [], "type": "pull", "related_issue": "#1490"}, {"user_name": "brimoor", "datetime": "Dec 30, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4173", "issue_status": " Closed\n", "issue_list": [{"user_name": "dvkruchinin", "datetime": "Jan 14, 2022", "body": "An error occurs when the movement of the mouse cursor after removing the shape during its movement.The error does not occur.The error occurs in the Firefox browser when interacting with shapes that can be moved (rectangle, points, ellipse, cuboid).Fix a bug.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "dvkruchinin", "datetime": "Jan 14, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 14, 2022", "body": [], "type": "pull", "related_issue": "#4175"}, {"user_name": "bsekachev", "datetime": "Jan 14, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4785", "issue_status": " Closed\n", "issue_list": [{"user_name": "mandharsh38", "datetime": "Aug 1, 2022", "body": "Read/searched \nSearched Should annotate automaticallyServer Error\nError: Request failed with status code 500. \"500 Server Error: Internal Server Error for url: \".1 run ./serverless/deploy_cpu.sh ./serverless/pytorch/ultralytics/yolov5/\n2 Create a task and try to auto-annotate using the detector yolo-v5Wanted to automatic annotate my custom dataset using yolov5 detectorYou may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "dschoerk", "datetime": "Aug 3, 2022", "body": "looks like the body of the http request isn't delivering the image correctly. are you using google chrome? the docs state that it's the only supported browser.i just tried with the develop branch on  and spun up an instance within WSL with the CPU version of YOLOv5 ... and it worked fine.", "type": "commented", "related_issue": null}, {"user_name": "dschoerk", "datetime": "Aug 3, 2022", "body": "logs for comparisoni'm not getting \"[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\" although that seems to be just a warning.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 4, 2022", "body": " , thanks for sharing. We did a number of fixes for serverless functions in  repo. Don't hestitate to update the nuclio version: . It was updated as well.", "type": "commented", "related_issue": null}, {"user_name": "mandharsh38", "datetime": "Aug 13, 2022", "body": "Works now..updated nuctl and running: docker bridge fixed it. Thanks for the help..  ", "type": "commented", "related_issue": null}, {"user_name": "matiassalriv1998", "datetime": "Sep 6, 2022", "body": "Hello  , I'm having problems to auto annotate my own model with Yolo and the tutorial only shows detectron, could you explain about the changes you made at the function.yaml (the default one, to adapt at your own model) also main.py. I'm having problems with that.", "type": "commented", "related_issue": null}, {"user_name": "mandharsh38", "datetime": "Aug 13, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3652", "issue_status": " Closed\n", "issue_list": [{"user_name": "kgloveyou", "datetime": "Sep 7, 2021", "body": "the annotation page can interact correctlyyou can zoom, but you cannot select the bouding box and change to next picture1.Click on the bouding box without releasing the mouse\n2.press delete key\n3.the annotation page will be stuck(you can zoom, but you cannot select the bouding box and change to next picture)You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 23, 2021", "body": "Hi Thank you for the report.", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "hixio-mh/cvat#77"}, {"user_name": "bsekachev", "datetime": "Sep 23, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 23, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 12, 2022", "body": [], "type": "pull", "related_issue": "#4151"}, {"user_name": "bsekachev", "datetime": "Jan 13, 2022", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Jan 13, 2022", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4503", "issue_status": " Closed\n", "issue_list": [{"user_name": "admer7", "datetime": "Mar 23, 2022", "body": "We are using the \"Standard\" Workspace and a handful of labels and one tag with attributes for our annotation jobs. We often need to switch between placing a tag and placing a label.Current workaround is to select by mouse another label/tag from the side-bar, place it, delete it again and create the original label.No errors or warnings gets printed.Same problem with , CVAT v2.0.0-alpha and v2.0.0 release version.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 23, 2022", "body": "Thank you for the report, it is a bug.P.S. For tags I would recommend to look at \"Tag annotation\" mode. If you do not have lots of classes it probably would be more effective.", "type": "commented", "related_issue": null}, {"user_name": "admer7", "datetime": "Mar 24, 2022", "body": "Thanks for the quick response!We have it the other way round. A lot of classes and usually only one or two tags. I assume for this scenario the \"Standard\" mode is the logical one.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 25, 2022", "body": "Hi Could you please check if the fix above works for you?", "type": "commented", "related_issue": null}, {"user_name": "admer7", "datetime": "Mar 27, 2022", "body": "Problem still remains. After pressing \"n\", the rectangle button in the side-bar is locked.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 27, 2022", "body": "Are u sure you updated CVAT? What steps did you do to update? I tried this patch and actually looks like it works.", "type": "commented", "related_issue": null}, {"user_name": "admer7", "datetime": "Mar 28, 2022", "body": "Ok, it is fixed after fresh cloning and building from source :)But we are using 2.0.0 release and only coping  is not enough to fix this bug... Any suggestions?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 28, 2022", "body": "Maybe because you fetched server release from docker hub instead of buidling from source first?  file must be included when build.", "type": "commented", "related_issue": null}, {"user_name": "admer7", "datetime": "Mar 28, 2022", "body": "I used the git repo with the , added the modified  file and build via .", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 23, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 23, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "klakhov", "datetime": "Mar 25, 2022", "body": [], "type": "pull", "related_issue": "#4517"}, {"user_name": "bsekachev", "datetime": "Mar 25, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 25, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4341", "issue_status": " Closed\n", "issue_list": [{"user_name": "yaoguang97", "datetime": "Feb 14, 2022", "body": "Environment：ubuntu20.04，nuctl1.5.16\n\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 15, 2022", "body": " , thanks for the report. I can reproduce the issue on our side. I will try to fix before 2.0.0 release.", "type": "commented", "related_issue": null}, {"user_name": "yaoguang97", "datetime": "Feb 17, 2022", "body": ", I pulled the latest code, rebuilt the YOLOv5 model, and the same error occurred.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 17, 2022", "body": " , could you please send logs for the function? ", "type": "commented", "related_issue": null}, {"user_name": "yaoguang97", "datetime": "Feb 18, 2022", "body": " ，Hello, this is the log about the container.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 21, 2022", "body": " ,From your logs I see that  leads to an exception. It tries to download the model locally and cannot. Please check your network settings.", "type": "commented", "related_issue": null}, {"user_name": "maecky", "datetime": "Jun 25, 2022", "body": "Any progress on this? For me, it fails as well.  Network connection is OK.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 15, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 15, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 15, 2022", "body": [], "type": "pull", "related_issue": "#4344"}, {"user_name": "nmanovic", "datetime": "Feb 15, 2022", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Feb 15, 2022", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3582", "issue_status": " Closed\n", "issue_list": [{"user_name": "TOsmanov", "datetime": "Aug 23, 2021", "body": "Related You can change the restrictive threshold without triggers the block.When you change the limiting threshold intelligent scissors () the block is also triggered.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 23, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 23, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "klakhov", "datetime": "Oct 25, 2021", "body": [], "type": "pull", "related_issue": "#3825"}, {"user_name": "bsekachev", "datetime": "Oct 26, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3361", "issue_status": " Closed\n", "issue_list": [{"user_name": "davodogster", "datetime": "Jun 23, 2021", "body": "Hi Guys,I successfully updated CVAT and deployed IOG on my ubuntu machine (not windows anymore), remote server IP address, but it still gives the same error from months ago:\n\n\nI also tried adding port 32001 (from the below issue) to the yaml and it gives the same error.\nAre you able to help?", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Jun 24, 2021", "body": "Hi, please attach logs from cvat and nuclio containers:  ", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jun 24, 2021", "body": "Hi :2021-06-23 23:00:10,912 DEBG 'rqworker_low' stderr output:\nINFO - 2021-06-23 23:00:10,912 - worker - Cleaning registries for queue: low2021-06-23 23:01:06,754 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:06.753887 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57528] [2021-06-23 23:01:06,753] WARNING django.request: Not Found: /analytics/app/kibana2021-06-23 23:01:06,754 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:06.753975 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57528] WARNING - 2021-06-23 23:01:06,753 - log - Not Found: /analytics/app/kibana2021-06-23 23:01:21,081 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:21.081261 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57592] ERROR - 2021-06-23 23:01:21,081 - views - {\"system\":\"Windows 10\",\"client\":\"chrome\",\"time\":\"2021-06-23T23:01:20.991000Z\",\"client_id\":266469,\"message\":\"Cannot read property 'toString' of undefined\",\"filename\":\": Cannot read property 'toString' of undefined\\n    at Vl.renderParameters (    at Vl.render (    at $i (    at qi (    at Ss (    at ml (    at fl (    at ol (    at     at t.unstable_runWithPriority ( exception\"}2021-06-23 23:01:25,784 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:25.784606 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57658] [2021-06-23 23:01:25,784] INFO cvat.server.task_140: get repository request\n[Wed Jun 23 23:01:25.784673 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57658] INFO - 2021-06-23 23:01:25,784 - views - get repository request2021-06-23 23:01:55,521 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:55.521658 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57742] [2021-06-23 23:01:55,521] WARNING django.request: Unauthorized: /api/v1/users/self2021-06-23 23:01:55,521 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:55.521748 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57742] WARNING - 2021-06-23 23:01:55,521 - log - Unauthorized: /api/v1/users/self2021-06-23 23:01:55,604 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:55.604343 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57746] [2021-06-23 23:01:55,604] WARNING django.request: Unauthorized: /api/v1/auth/password/change2021-06-23 23:01:55,604 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:55.604691 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57746] WARNING - 2021-06-23 23:01:55,604 - log - Unauthorized: /api/v1/auth/password/change2021-06-23 23:01:57,865 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:57.865350 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57760] [2021-06-23 23:01:57,865] WARNING django.request: Unauthorized: /api/v1/users/self2021-06-23 23:01:57,865 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:57.865659 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57760] WARNING - 2021-06-23 23:01:57,865 - log - Unauthorized: /api/v1/users/self2021-06-23 23:01:57,948 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:57.948715 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57764] [2021-06-23 23:01:57,948] WARNING django.request: Unauthorized: /api/v1/auth/password/change2021-06-23 23:01:57,949 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:57.948856 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57764] WARNING - 2021-06-23 23:01:57,948 - log - Unauthorized: /api/v1/auth/password/change2021-06-23 23:01:59,964 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:59.964480 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57778] [2021-06-23 23:01:59,964] WARNING django.request: Unauthorized: /api/v1/users/self2021-06-23 23:01:59,965 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:01:59.964785 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57778] WARNING - 2021-06-23 23:01:59,964 - log - Unauthorized: /api/v1/users/self2021-06-23 23:02:00,048 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:00.048532 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57782] [2021-06-23 23:02:00,048] WARNING django.request: Unauthorized: /api/v1/auth/password/change2021-06-23 23:02:00,049 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:00.048830 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:57782] WARNING - 2021-06-23 23:02:00,048 - log - Unauthorized: /api/v1/auth/password/change2021-06-23 23:02:02,940 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:02.940565 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57798] [2021-06-23 23:02:02,940] WARNING django.request: Unauthorized: /api/v1/users/self2021-06-23 23:02:02,941 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:02.940898 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:57798] WARNING - 2021-06-23 23:02:02,940 - log - Unauthorized: /api/v1/users/self2021-06-23 23:02:03,051 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:03.051685 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57802] [2021-06-23 23:02:03,051] WARNING django.request: Unauthorized: /api/v1/auth/password/change2021-06-23 23:02:03,052 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:02:03.052022 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:57802] WARNING - 2021-06-23 23:02:03,051 - log - Unauthorized: /api/v1/auth/password/change2021-06-23 23:05:09,275 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:09.275347 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:58184] [2021-06-23 23:05:09,275] INFO cvat.server.task_133: get repository request2021-06-23 23:05:09,276 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:09.275828 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:58184] INFO - 2021-06-23 23:05:09,275 - views - get repository request2021-06-23 23:05:16,937 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:16.937629 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58228] [2021-06-23 23:05:16,937] INFO cvat.server.task_125: get repository request2021-06-23 23:05:16,937 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:16.937723 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58228] INFO - 2021-06-23 23:05:16,937 - views - get repository request2021-06-23 23:05:18,449 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:18.448989 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:58246] INFO - 2021-06-23 23:05:18,448 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:05:18.408000Z\",\"payload\":{\"working_time\":131387},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:05:29,689 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:29.689166 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58288] [2021-06-23 23:05:29,689] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:29,689 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:29.689242 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58288] ERROR - 2021-06-23 23:05:29,689 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:32,074 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:32.074364 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58308] [2021-06-23 23:05:32,074] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:32,074 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:32.074441 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58308] ERROR - 2021-06-23 23:05:32,074 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:35,326 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:35.326429 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58328] [2021-06-23 23:05:35,326] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:35,327 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:35.326745 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58328] ERROR - 2021-06-23 23:05:35,326 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:36,439 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:36.439051 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58344] [2021-06-23 23:05:36,438] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:36,439 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:36.439206 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:58344] ERROR - 2021-06-23 23:05:36,438 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:05:43,975 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:43.975792 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58372] INFO - 2021-06-23 23:05:43,971 - views - {\"job_id\":108,\"task_id\":125,\"client_id\":284839,\"name\":\"Load job\",\"time\":\"2021-06-23T23:05:18.408000Z\",\"payload\":{\"duration\":1969,\"frame count\":129,\"track count\":814,\"object count\":814,\"box count\":814,\"polygon count\":0,\"polyline count\":0,\"points count\":0,\"cuboids count\":0,\"tag count\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:05:43,976 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:43.975913 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58372] INFO - 2021-06-23 23:05:43,975 - views - {\"job_id\":108,\"task_id\":125,\"client_id\":284839,\"name\":\"Fit image\",\"time\":\"2021-06-23T23:05:20.415000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}\n[Wed Jun 23 23:05:43.976001 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58372] INFO - 2021-06-23 23:05:43,975 - views - {\"job_id\":108,\"task_id\":125,\"client_id\":284839,\"name\":\"Zoom image\",\"time\":\"2021-06-23T23:05:22.104000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:05:43,976 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:05:43.976101 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:58372] INFO - 2021-06-23 23:05:43,976 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:05:43.923000Z\",\"payload\":{\"working_time\":20732},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:06:55,957 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:06:55,956 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:06:55,957 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:06:55,956 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:06:56,057 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:06:56,057 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:13:40,979 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:13:40,979 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:13:40,979 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:13:40,979 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:13:41,080 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:13:41,079 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:20:26,073 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:20:26,072 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:20:26,073 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:20:26,072 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:20:26,173 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:20:26,173 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:23:39,786 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:23:39.785984 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60306] [2021-06-23 23:23:39,784] INFO cvat.server.task_140: get repository request2021-06-23 23:23:39,786 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:23:39.786141 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60306] INFO - 2021-06-23 23:23:39,784 - views - get repository request2021-06-23 23:23:40,883 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:23:40.883380 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:60324] INFO - 2021-06-23 23:23:40,883 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:23:40.837000Z\",\"payload\":{\"working_time\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:23:58,897 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:23:58.897616 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60364] [2021-06-23 23:23:58,897] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:23:58,897 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:23:58.897730 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60364] ERROR - 2021-06-23 23:23:58,897 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:24:06,962 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:24:06.962204 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:60394] [2021-06-23 23:24:06,962] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:24:06,962 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:24:06.962288 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:60394] ERROR - 2021-06-23 23:24:06,962 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:24:12,237 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:24:12.237653 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60428] INFO - 2021-06-23 23:24:12,237 - views - {\"job_id\":119,\"task_id\":140,\"client_id\":284839,\"name\":\"Load job\",\"time\":\"2021-06-23T23:23:40.837000Z\",\"payload\":{\"duration\":2762,\"frame count\":2,\"track count\":5,\"object count\":5,\"box count\":0,\"polygon count\":5,\"polyline count\":0,\"points count\":0,\"cuboids count\":0,\"tag count\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:24:12,237 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:24:12.237737 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60428] INFO - 2021-06-23 23:24:12,237 - views - {\"job_id\":119,\"task_id\":140,\"client_id\":284839,\"name\":\"Fit image\",\"time\":\"2021-06-23T23:23:44.447000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}\n[Wed Jun 23 23:24:12.237796 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:60428] INFO - 2021-06-23 23:24:12,237 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:24:12.188000Z\",\"payload\":{\"working_time\":11814},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:27:11,142 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:27:11,140 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:27:11,142 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:27:11,140 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.\nINFO - 2021-06-23 23:27:11,141 - worker - Cleaning registries for queue: low2021-06-23 23:27:11,143 DEBG 'rqworker_default_0' stderr output:\nINFO - 2021-06-23 23:27:11,141 - worker - Cleaning registries for queue: default2021-06-23 23:27:11,240 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:27:11,240 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:33:56,241 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:33:56,240 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:33:56,241 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:33:56,240 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:33:56,339 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:33:56,338 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:34:08,087 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:08.087036 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34074] [2021-06-23 23:34:08,086] INFO cvat.server.task_133: get repository request2021-06-23 23:34:08,092 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:08.092674 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34074] INFO - 2021-06-23 23:34:08,086 - views - get repository request2021-06-23 23:34:09,336 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:09.336107 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34094] INFO - 2021-06-23 23:34:09,335 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:34:09.264000Z\",\"payload\":{\"working_time\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:34:30,213 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:30.213556 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34186] [2021-06-23 23:34:30,213] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:30,214 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:30.213875 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34186] ERROR - 2021-06-23 23:34:30,213 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:32,006 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:32.005893 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34210] [2021-06-23 23:34:32,005] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:32,006 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:32.006247 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34210] ERROR - 2021-06-23 23:34:32,005 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:33,437 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:33.437583 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34232] [2021-06-23 23:34:33,437] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:33,438 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:33.437708 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34232] ERROR - 2021-06-23 23:34:33,437 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:34:36,137 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:36.137761 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34260] INFO - 2021-06-23 23:34:36,137 - views - {\"job_id\":116,\"task_id\":133,\"client_id\":284839,\"name\":\"Load job\",\"time\":\"2021-06-23T23:34:09.264000Z\",\"payload\":{\"duration\":833,\"frame count\":12,\"track count\":105,\"object count\":105,\"box count\":0,\"polygon count\":105,\"polyline count\":0,\"points count\":0,\"cuboids count\":0,\"tag count\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:34:36,138 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:36.137843 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34260] INFO - 2021-06-23 23:34:36,137 - views - {\"job_id\":116,\"task_id\":133,\"client_id\":284839,\"name\":\"Fit image\",\"time\":\"2021-06-23T23:34:10.139000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}\n[Wed Jun 23 23:34:36.137901 2021] [wsgi:error] [pid 438:tid 140564161627904] [remote 172.21.0.7:34260] INFO - 2021-06-23 23:34:36,137 - views - {\"client_id\":284839,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:34:36.086000Z\",\"payload\":{\"working_time\":23713},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:34:44,396 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:44.396496 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34322] [2021-06-23 23:34:44,396] WARNING django.request: Not Found: /analytics/app/kibana2021-06-23 23:34:44,397 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:44.396831 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34322] WARNING - 2021-06-23 23:34:44,396 - log - Not Found: /analytics/app/kibana2021-06-23 23:34:48,860 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:48.860028 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34364] ERROR - 2021-06-23 23:34:48,859 - views - {\"system\":\"Windows 10\",\"client\":\"chrome\",\"time\":\"2021-06-23T23:34:48.808000Z\",\"client_id\":284091,\"message\":\"Cannot read property 'toString' of undefined\",\"filename\":\": Cannot read property 'toString' of undefined\\n    at Vl.renderParameters (    at Vl.render (    at $i (    at qi (    at Ss (    at ml (    at fl (    at ol (    at     at t.unstable_runWithPriority ( exception\"}2021-06-23 23:34:57,363 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:57.363221 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34452] [2021-06-23 23:34:57,362] INFO cvat.server.task_130: get repository request2021-06-23 23:34:57,364 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:57.363686 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34452] INFO - 2021-06-23 23:34:57,362 - views - get repository request2021-06-23 23:34:59,443 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:34:59.443628 2021] [wsgi:error] [pid 438:tid 140564170020608] [remote 172.21.0.7:34474] INFO - 2021-06-23 23:34:59,443 - views - {\"client_id\":294787,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:34:59.387000Z\",\"payload\":{\"working_time\":2496},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:35:08,814 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:08.814735 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34532] [2021-06-23 23:35:08,814] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:35:08,815 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:08.814857 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34532] ERROR - 2021-06-23 23:35:08,814 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:35:10,055 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:10.055407 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:34552] [2021-06-23 23:35:10,055] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:35:10,056 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:10.055704 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:34552] ERROR - 2021-06-23 23:35:10,055 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:35:13,050 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:13.050817 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34572] [2021-06-23 23:35:13,050] ERROR django.request: Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:35:13,051 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:35:13.050897 2021] [wsgi:error] [pid 438:tid 140564144842496] [remote 172.21.0.7:34572] ERROR - 2021-06-23 23:35:13,050 - log - Internal Server Error: /api/v1/lambda/functions/pth.shiyinzhang.iog2021-06-23 23:37:05,001 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:37:05.001058 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:35000] INFO - 2021-06-23 23:37:05,000 - views - {\"job_id\":113,\"task_id\":130,\"client_id\":294787,\"name\":\"Load job\",\"time\":\"2021-06-23T23:34:59.387000Z\",\"payload\":{\"duration\":1648,\"frame count\":14,\"track count\":359,\"object count\":359,\"box count\":0,\"polygon count\":359,\"polyline count\":0,\"points count\":0,\"cuboids count\":0,\"tag count\":0},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:37:05,001 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:37:05.001187 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:35000] INFO - 2021-06-23 23:37:05,001 - views - {\"job_id\":113,\"task_id\":130,\"client_id\":294787,\"name\":\"Fit image\",\"time\":\"2021-06-23T23:35:01.062000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}\n[Wed Jun 23 23:37:05.001285 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:35000] INFO - 2021-06-23 23:37:05,001 - views - {\"job_id\":113,\"task_id\":130,\"client_id\":294787,\"name\":\"Zoom image\",\"time\":\"2021-06-23T23:35:02.638000Z\",\"payload\":{},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:37:05,001 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:37:05.001362 2021] [wsgi:error] [pid 438:tid 140564153235200] [remote 172.21.0.7:35000] INFO - 2021-06-23 23:37:05,001 - views - {\"client_id\":294787,\"name\":\"Send user activity\",\"time\":\"2021-06-23T23:37:04.922000Z\",\"payload\":{\"working_time\":10304},\"is_active\":true,\"username\":\"Davidsoscvat\"}2021-06-23 23:40:41,283 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:40:41,282 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:40:41,283 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:40:41,282 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:40:41,383 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:40:41,382 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:47:26,315 DEBG 'rqworker_default_0' stderr output:\nDEBUG - 2021-06-23 23:47:26,315 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:47:26,315 DEBG 'rqworker_low' stderr output:\nDEBUG - 2021-06-23 23:47:26,315 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:47:26,416 DEBG 'rqworker_default_1' stderr output:\nDEBUG - 2021-06-23 23:47:26,415 - worker - Sent heartbeat to prevent worker timeout. Next one should arrive within 480 seconds.2021-06-23 23:47:41,570 DEBG 'runserver' stderr output:\n[Wed Jun 23 23:47:41.570027 2021] [wsgi:error] [pid 438:tid 140564136449792] [remote 172.21.0.7:37268] [2021-06-23 23:47:41,569] INFO cvat.server.task_133: get repository request", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jun 25, 2021", "body": " the Nuclio.log is 800MB of letters", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Jun 29, 2021", "body": " thanks, I'll try to reproduce", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 6, 2021", "body": " , IOG function has non-intuitive interface for now. In the future you should draw a bounding box + a point inside. For now, AI tools doesn't allow to specify a bounding box for  serverless functions. Thus you have to specify two negative points using right mouse click and one positive using left mouse click.Also I have found a problem, that the function doesn't support 4 channel images for now. , could you please improve  interfaces? The function has  in function.yaml. For such functions we need to draw a bounding box at the beginning of the annotation process.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 7, 2021", "body": " Sure.", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jul 9, 2021", "body": " great so all along it has been working, but we didn't know how to use it properly.\nIt's working for me now that I draw the box first - but that is super slow to chop and change from box to interactor for every single instance..Thanks ", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 12, 2021", "body": "Could you please explain in details? What do you mean?", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jul 12, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 12, 2021", "body": "Why do you need both segmentation and bounding box? You can get bounding boxes from segmentation masks easily.", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jul 12, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 12, 2021", "body": " , you need to put 2 negative points which are top left and bottom right corners of the bounding box + a point inside the object.In the future when you use IOTG it will ask you to draw a bounding box first + a point inside the object. It will be the interface of the function.Now IOG doesn't look at the bounding box at all. It uses only several negative points and a positive point which you provide.", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jul 12, 2021", "body": "Ok currently IOG is working for me but the only way is to draw a bbox first and then click the points. The point I'm trying to make is that annotation speed is slow and it's inefficient to change between bbox and points for every single instance ?? Because that is the only way the tool works ?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 13, 2021", "body": " , it looks like there is a problem with your CVAT instance. It should work without the bounding box. I have checked that on my side and if set 2 negative points + 1 positive point, IOG works as expected.\nCould you please record a small video clip to demonstrate the problem if you don't draw a bounding box?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 14, 2021", "body": "Now it works this way. The PR will be opened soon.", "type": "commented", "related_issue": null}, {"user_name": "davodogster", "datetime": "Jul 14, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 15, 2021", "body": "No, there is not suchthe feature, but you can submit an issue", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 6, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 6, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 6, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 6, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 14, 2021", "body": [], "type": "pull", "related_issue": "#3417"}, {"user_name": "snyk-bot", "datetime": "Jul 14, 2021", "body": [], "type": "pull", "related_issue": "hixio-mh/cvat#42"}, {"user_name": "nmanovic", "datetime": "Jul 20, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Jul 20, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2991", "issue_status": " Closed\n", "issue_list": [{"user_name": "nstolyarov", "datetime": "Mar 22, 2021", "body": "You can copy and paste polygons in CVAT. This helps us in cases when we mark one place with different classes. We need to be able to paste the copied polygon in exactly the same place. But when you insert a polygon, it snaps to the mouse cursor and moves away from its original position.After pressing ctrl-v, the polygon is inserted exactly in the same place as the original.After pressing ctrl-v, the polygon snaps to the mouse cursor and moves.Insert a polygon in exactly the same place by default, if necessary, drag it with the mouse from its original position.When working with complex regions of segmentation, it can be useful to duplicate the polygon at the same place.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 22, 2021", "body": "Hi, What is usecase when you need the same polygon in exactly the same place?", "type": "commented", "related_issue": null}, {"user_name": "nstolyarov", "datetime": "Mar 22, 2021", "body": "Hi, Here is the example below\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 15, 2021", "body": " , As far as I know the automatic border solved the issue. Could you please confirm and close the issue in the case?", "type": "commented", "related_issue": null}, {"user_name": "nstolyarov", "datetime": "Apr 15, 2021", "body": "Hi, !You are right, the automatic bordering solved this issue. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 22, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 22, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 15, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 15, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nstolyarov", "datetime": "Apr 15, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Apr 15, 2021", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2899", "issue_status": " Closed\n", "issue_list": [{"user_name": "bsekachev", "datetime": "Mar 3, 2021", "body": "If two issue tags created at the same place it was impossible to access on of them ().\nTo fix the behavior we've added the feature of scrolling these tags by mouse wheel () as shown on the gif below.\nNeed to hover cursor under an issue and use wheel your mouse. We should add a piece of information about it to our user guide", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": " , could you please check that it is covered by our documentation? If it is not, please add corresponding information. I will close the issue.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 3, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Mar 8, 2021", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "TOsmanov", "datetime": "Nov 24, 2021", "body": [], "type": "pull", "related_issue": "#3944"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2650", "issue_status": " Closed\n", "issue_list": [{"user_name": "Smirkey", "datetime": "Jan 8, 2021", "body": "Hello, when I draw a boundingBox (2 points) in track mode it's totally possible to drag it on the image. But sadly when I try the same thing with a polygon the whole image moves instead of moving the polygon on the image.I couldn't find anything in the docs or in previous issues related to this problem.Thanks in advance for your response :)Best", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Jan 13, 2021", "body": " could you please take a look?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 14, 2021", "body": "Hi, thanks for your concern.\nPolygons are default \"pinned\" and cannot be moved (in the most of task you do not need to move an entire polygon).\nTo do them \"unpinned\" press the icon button (shown on the screenshot below)\n", "type": "commented", "related_issue": null}, {"user_name": "Smirkey", "datetime": "Jan 14, 2021", "body": "Thank you :) !", "type": "commented", "related_issue": null}, {"user_name": "mohammadreza-sheykhmousa", "datetime": "Dec 7, 2021", "body": "Hi  there actually are many instances that suffer from mismatching thus it is crucial to move ans also sometimes to rotate multiple polygons at once. I know about the unpinning, however I was wondering is there any other convenient way of moving (single or multiple at once) polygons? How about rotating? if not is there any plan to add such capabilities -- which seems crucial to many geospatial data labeling-- to CVAT? I appreciate the help and thank you :)", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 8, 2021", "body": "Hi We do not have these features in our current road map. Since our team has limited resources we can't satisfy all the community requests. But CVAT is an open source tool which everyone can contribute into, and we would like to review community PRs, provide feedbacks, and give advice if necessary", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Jan 13, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 14, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2587", "issue_status": " Closed\n", "issue_list": [{"user_name": "ITBOX-ITBOY", "datetime": "Dec 17, 2020", "body": "Model deployment statement：nuctl deploy --project-name cvat \n--path $SCRIPT_DIR/tensorflow/faster_rcnn_inception_v2_coco/nuclio \n--platform localThe configuration file is as follows：\nfunctions.yamlmetadata:\nname: tf-faster-rcnn-inception-v2-coco\nnamespace: cvat\nannotations:\nname: Faster RCNN via Tensorflow\ntype: detector\nframework: tensorflow\nspec: |\n[\n{ \"id\": 1, \"name\": \"person\" },\n{ \"id\": 2, \"name\": \"bicycle\" },\n{ \"id\": 3, \"name\": \"car\" },\n{ \"id\": 4, \"name\": \"motorcycle\" },\n{ \"id\": 5, \"name\": \"airplane\" },\n{ \"id\": 6, \"name\": \"bus\" },\n{ \"id\": 7, \"name\": \"train\" },\n{ \"id\": 8, \"name\": \"truck\" },\n{ \"id\": 9, \"name\": \"boat\" },\n{ \"id\":10, \"name\": \"traffic_light\" },\n{ \"id\":11, \"name\": \"fire_hydrant\" },\n{ \"id\":13, \"name\": \"stop_sign\" },\n{ \"id\":14, \"name\": \"parking_meter\" },\n{ \"id\":15, \"name\": \"bench\" },\n{ \"id\":16, \"name\": \"bird\" },\n{ \"id\":17, \"name\": \"cat\" },\n{ \"id\":18, \"name\": \"dog\" },\n{ \"id\":19, \"name\": \"horse\" },\n{ \"id\":20, \"name\": \"sheep\" },\n{ \"id\":21, \"name\": \"cow\" },\n{ \"id\":22, \"name\": \"elephant\" },\n{ \"id\":23, \"name\": \"bear\" },\n{ \"id\":24, \"name\": \"zebra\" },\n{ \"id\":25, \"name\": \"giraffe\" },\n{ \"id\":27, \"name\": \"backpack\" },\n{ \"id\":28, \"name\": \"umbrella\" },\n{ \"id\":31, \"name\": \"handbag\" },\n{ \"id\":32, \"name\": \"tie\" },\n{ \"id\":33, \"name\": \"suitcase\" },\n{ \"id\":34, \"name\": \"frisbee\" },\n{ \"id\":35, \"name\": \"skis\" },\n{ \"id\":36, \"name\": \"snowboard\" },\n{ \"id\":37, \"name\": \"sports_ball\" },\n{ \"id\":38, \"name\": \"kite\" },\n{ \"id\":39, \"name\": \"baseball_bat\" },\n{ \"id\":40, \"name\": \"baseball_glove\" },\n{ \"id\":41, \"name\": \"skateboard\" },\n{ \"id\":42, \"name\": \"surfboard\" },\n{ \"id\":43, \"name\": \"tennis_racket\" },\n{ \"id\":44, \"name\": \"bottle\" },\n{ \"id\":46, \"name\": \"wine_glass\" },\n{ \"id\":47, \"name\": \"cup\" },\n{ \"id\":48, \"name\": \"fork\" },\n{ \"id\":49, \"name\": \"knife\" },\n{ \"id\":50, \"name\": \"spoon\" },\n{ \"id\":51, \"name\": \"bowl\" },\n{ \"id\":52, \"name\": \"banana\" },\n{ \"id\":53, \"name\": \"apple\" },\n{ \"id\":54, \"name\": \"sandwich\" },\n{ \"id\":55, \"name\": \"orange\" },\n{ \"id\":56, \"name\": \"broccoli\" },\n{ \"id\":57, \"name\": \"carrot\" },\n{ \"id\":58, \"name\": \"hot_dog\" },\n{ \"id\":59, \"name\": \"pizza\" },\n{ \"id\":60, \"name\": \"donut\" },\n{ \"id\":61, \"name\": \"cake\" },\n{ \"id\":62, \"name\": \"chair\" },\n{ \"id\":63, \"name\": \"couch\" },\n{ \"id\":64, \"name\": \"potted_plant\" },\n{ \"id\":65, \"name\": \"bed\" },\n{ \"id\":67, \"name\": \"dining_table\" },\n{ \"id\":70, \"name\": \"toilet\" },\n{ \"id\":72, \"name\": \"tv\" },\n{ \"id\":73, \"name\": \"laptop\" },\n{ \"id\":74, \"name\": \"mouse\" },\n{ \"id\":75, \"name\": \"remote\" },\n{ \"id\":76, \"name\": \"keyboard\" },\n{ \"id\":77, \"name\": \"cell_phone\" },\n{ \"id\":78, \"name\": \"microwave\" },\n{ \"id\":79, \"name\": \"oven\" },\n{ \"id\":80, \"name\": \"toaster\" },\n{ \"id\":81, \"name\": \"sink\" },\n{ \"id\":83, \"name\": \"refrigerator\" },\n{ \"id\":84, \"name\": \"book\" },\n{ \"id\":85, \"name\": \"clock\" },\n{ \"id\":86, \"name\": \"vase\" },\n{ \"id\":87, \"name\": \"scissors\" },\n{ \"id\":88, \"name\": \"teddy_bear\" },\n{ \"id\":89, \"name\": \"hair_drier\" },\n{ \"id\":90, \"name\": \"toothbrush\" }\n]spec:\ndescription: Faster RCNN from Tensorflow Object Detection API\nruntime: \"python:3.6\"\nhandler: main:handler\neventTimeout: 30s\nimage: 47.104.169.133:8888/cvat/tf.faster_rcnn_inception_v2_coco:latest\nimagePullPolicy: Never\nserviceType: NodePort\nminReplicas: 1\nmaxReplicas: 3\ntriggers:\nmyHttpTrigger:\nmaxWorkers: 2\nkind: \"http\"\nworkerAvailabilityTimeoutMilliseconds: 10000\nattributes:\nmaxRequestBodySize: 33554432 # 32MBplatform:\nattributes:\nrestartPolicy:\nname: always\nmaximumRetryCount: 3Partial logs after execution：[root@master serverless]# nuctl deploy --project-name cvat --path $SCRIPT_DIR/tensorflow/faster_rcnn_inception_v2_coco/nuclio --namespace nuclio --kubeconfig /etc/kubernetes/admin.conf --platform kube\n20.12.18 10:17:38.202                     nuctl (I) Deploying function {\"name\": \"\"}\n20.12.18 10:17:38.203                     nuctl (I) Building {\"versionInfo\": \"Label: 1.5.8, Git commit: c12022750a6003a309d4f20c72a98addacd3e95d, OS: linux, Arch: amd64, Go version: go1.14.3\", \"name\": \"\"}\n20.12.18 10:17:38.284                     nuctl (I) Staging files and preparing base images\n20.12.18 10:17:38.284                     nuctl (I) Building processor image {\"imageName\": \"nuclio/processor-tf-faster-rcnn-inception-v2-coco:latest\"}\n20.12.18 10:17:38.284     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/handler-builder-python-onbuild:1.5.8-amd64\"}Spec. image I have specified my own custom image, why not use a custom image instead of nuclio/ processor-TF-faster - rCNN-inception -v2-coco:latestAfter I execute the following command, image uses custom：nuctl deploy --project-name cvat \n--path $SCRIPT_DIR/tensorflow/faster_rcnn_inception_v2_coco/nuclio \n--image 47.104.169.133:8888/cvat/tf.faster_rcnn_inception_v2_coco:latest\n--platform localPartial logs after execution：\n[root@master serverless]# nuctl deploy --project-name cvat --path $SCRIPT_DIR/tensorflow/faster_rcnn_inception_v2_coco/nuclio --namespace nuclio --kubeconfig /etc/kubernetes/admin.conf --platform kube\n20.12.18 10:17:38.202                     nuctl (I) Deploying function {\"name\": \"\"}\n20.12.18 10:17:38.203                     nuctl (I) Building {\"versionInfo\": \"Label: 1.5.8, Git commit: c12022750a6003a309d4f20c72a98addacd3e95d, OS: linux, Arch: amd64, Go version: go1.14.3\", \"name\": \"\"}\n20.12.18 10:17:38.284                     nuctl (I) Staging files and preparing base images\n20.12.18 10:17:38.284                     nuctl (I) Building processor image {\"imageName\": \"47.104.169.133:8888/cvat/tf.faster_rcnn_inception_v2_coco:latest\"}\n20.12.18 10:17:38.284     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/handler-builder-python-onbuild:1.5.8-amd64\"}Is there any difference between using image in functions.YAMl definition and using image in nuctl execution command", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Dec 24, 2020", "body": " Please ask this question nuclio developers", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Dec 24, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 28, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2807", "issue_status": " Closed\n", "issue_list": [{"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": "", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#2712"}, {"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 18, 2021", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Feb 18, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "dvkruchinin", "datetime": "Feb 19, 2021", "body": [], "type": "pull", "related_issue": "#2833"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2776", "issue_status": " Closed\n", "issue_list": [{"user_name": "xs818818", "datetime": "Feb 6, 2021", "body": "metadata:\nname: darknet-yolov4\nnamespace: cvat\nannotations:\nname: yolov4\ntype: detector\nspec: |\n[\n{ \"id\": 1, \"name\": \"person\" },\n{ \"id\": 2, \"name\": \"bicycle\" },\n{ \"id\": 3, \"name\": \"car\" },\n{ \"id\": 4, \"name\": \"motorbike\" },\n{ \"id\": 5, \"name\": \"aeroplane\" },\n{ \"id\": 6, \"name\": \"bus\" },\n{ \"id\": 8, \"name\": \"train\" },\n{ \"id\": 9, \"name\": \"truck\" },\n{ \"id\": 10, \"name\": \"boat\" },\n{ \"id\": 11, \"name\": \"traffic light\" },\n{ \"id\": 12, \"name\": \"fire hydrant\" },\n{ \"id\": 13, \"name\": \"stop sign\" },\n{ \"id\": 14, \"name\": \"parking meter\" },\n{ \"id\": 15, \"name\": \"bird\" },\n{ \"id\": 16, \"name\": \"cat\" },\n{ \"id\": 17, \"name\": \"dog\" },\n{ \"id\": 18, \"name\": \"horse\" },\n{ \"id\": 19, \"name\": \"sheep\" },\n{ \"id\": 20, \"name\": \"cow\" },\n{ \"id\": 21, \"name\": \"elephant\" },\n{ \"id\": 22, \"name\": \"bear\" },\n{ \"id\": 23, \"name\": \"zebra\" },\n{ \"id\": 24, \"name\": \"giraffe\" },\n{ \"id\": 25, \"name\": \"backpack\" },\n{ \"id\": 26, \"name\": \"umbrella\" },\n{ \"id\": 27, \"name\": \"handbag\" },\n{ \"id\": 28, \"name\": \"tie\" },\n{ \"id\": 29, \"name\": \"suitcase\" },\n{ \"id\": 30, \"name\": \"frisbee\" },\n{ \"id\": 31, \"name\": \"skis\" },\n{ \"id\": 32, \"name\": \"snowboard\" },\n{ \"id\": 33, \"name\": \"sports ball\" },\n{ \"id\": 34, \"name\": \"kite\" },\n{ \"id\": 35, \"name\": \"baseball bat\" },\n{ \"id\": 36, \"name\": \"baseball glove\" },\n{ \"id\": 37, \"name\": \"skateboard\" },\n{ \"id\": 38, \"name\": \"surfboard\" },\n{ \"id\": 39, \"name\": \"tennis racket\" },\n{ \"id\": 40, \"name\": \"bottle\" },\n{ \"id\": 41, \"name\": \"wine glass\" },\n{ \"id\": 42, \"name\": \"cup\" },\n{ \"id\": 43, \"name\": \"fork\" },\n{ \"id\": 44, \"name\": \"knife\" },\n{ \"id\": 45, \"name\": \"spoon\" },\n{ \"id\": 46, \"name\": \"bowl\" },\n{ \"id\": 47, \"name\": \"banana\" },\n{ \"id\": 48, \"name\": \"apple\" },\n{ \"id\": 49, \"name\": \"sandwich\" },\n{ \"id\": 50, \"name\": \"orange\" },\n{ \"id\": 51, \"name\": \"broccoli\" },\n{ \"id\": 52, \"name\": \"carrot\" },\n{ \"id\": 53, \"name\": \"hot dog\" },\n{ \"id\": 54, \"name\": \"pizza\" },\n{ \"id\": 55, \"name\": \"donut\" },\n{ \"id\": 56, \"name\": \"cake\" },\n{ \"id\": 57, \"name\": \"chair\" },\n{ \"id\": 58, \"name\": \"sofa\" },\n{ \"id\": 59, \"name\": \"pottedplant\" },\n{ \"id\": 60, \"name\": \"bed\" },\n{ \"id\": 61, \"name\": \"diningtable\" },\n{ \"id\": 62, \"name\": \"toilet\" },\n{ \"id\": 63, \"name\": \"tvmonitor\" },\n{ \"id\": 64, \"name\": \"laptop\" },\n{ \"id\": 65, \"name\": \"mouse\" },\n{ \"id\": 66, \"name\": \"remote\" },\n{ \"id\": 67, \"name\": \"keyboard\" },\n{ \"id\": 68, \"name\": \"cell phone\" },\n{ \"id\": 69, \"name\": \"microwave\" },\n{ \"id\": 70, \"name\": \"oven\" },\n{ \"id\": 71, \"name\": \"toaster\" },\n{ \"id\": 72, \"name\": \"sink\" },\n{ \"id\": 73, \"name\": \"refrigerator\" },\n{ \"id\": 74, \"name\": \"book\" },\n{ \"id\": 75, \"name\": \"clock\" },\n{ \"id\": 76, \"name\": \"vase\" },\n{ \"id\": 77, \"name\": \"scissors\" },\n{ \"id\": 78, \"name\": \"teddy bear\" },\n{ \"id\": 79, \"name\": \"hair drier\" },\n{ \"id\": 80, \"name\": \"toothbrush\" },\n]\nframework: darknetspec:\ndescription: yolov4 from darknet Object Detection API\nruntime: 'python:3.6'\nhandler: main:handler\neventTimeout: 30sbuild:\nimage: cvat/darknet.yolov4\nbaseImage: baoxin/darknettriggers:\nmyHttpTrigger:\nmaxWorkers: 2\nkind: 'http'\nworkerAvailabilityTimeoutMilliseconds: 10000\nattributes:\nmaxRequestBodySize: 33554432 # 32MBplatform:\nattributes:\nrestartPolicy:\nname: always\nmaximumRetryCount: 3\nThis is my function.yaml\nnuclio :8070/projects/cvat/functions is running\nbut cvat Could not get models from the server\nOpen the Browser Console to get details", "type": "commented", "related_issue": null}, {"user_name": "xs818818", "datetime": "Feb 6, 2021", "body": "If i only use siammask i will success", "type": "commented", "related_issue": null}, {"user_name": "spatra007", "datetime": "May 11, 2021", "body": "Hi  , were you able to run this? I am stuck with this in my project and I want to deploy my custom yolov4 darknet model in Nuclio for CVAT auto annotations. Can you share the function and yaml file or atleast a way so that I can refer how to make it work?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "May 11, 2021", "body": " , please start to read the tutorial. I hope it can help you. ", "type": "commented", "related_issue": null}, {"user_name": "gautami-yara", "datetime": "Sep 15, 2021", "body": " Hi, the tutorial at  is not very helpful as it does tell us how we can change the function.yaml file to upload our own custom Yolov3 models from local machine. Can you please help us?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 15, 2021", "body": " , could you please describe your issue? The tutorial describes how to create a serverless function from scratch. Thus it should answer how to update function.yaml file. I will be happy to answer on a specific question.", "type": "commented", "related_issue": null}, {"user_name": "gautami-yara", "datetime": "Sep 15, 2021", "body": " Thank you for replying so promptly. I have built a custom YoloV3 model in my local machine. I am not sure what parameters are supposed to be added in the 'Directives' section in function.yaml file to deploy the model on nuclio. Currently, the tutorial (specifically retinanet) only tells how to use opensource models from model zoos.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 15, 2021", "body": " section should contain steps to re-create your local environment inside a docker container. For example, locally you can have  script to run inference of your model. The script depends on a number of python packages. Thus you have to install them. The script can rely on Python3. Thus it has to be available inside the docker container.", "type": "commented", "related_issue": null}, {"user_name": "gautami-yara", "datetime": "Sep 16, 2021", "body": " I understand now. Does this also mean that we can deploy models in every format (.h5, .onnx, .pb, etc) as long as we install the correct python packages? Thank you!", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 16, 2021", "body": " , basically you can do that ever you want as soon as your serverless function will accept and return expected json sctructures.", "type": "commented", "related_issue": null}, {"user_name": "xs818818", "datetime": "Feb 6, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2692", "issue_status": " Closed\n", "issue_list": [{"user_name": "Smirkey", "datetime": "Jan 20, 2021", "body": "Hi :)When I try to use the split function on a polygon/boundingbox nothing appens. My cursors changes and when I drag the mouse over a bbx/poly the object becomes blue, but when I click nothing appens. Am i missing something here?My goal is to be able to cut a polygon in two by drawing a cutting line.Thanks !", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 20, 2021", "body": "Hi The function of split doesn't cut a polygon into several polygons by cutting line (CVAT doesn't have such a function like you described).\nPlease, refer to user guide.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 21, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2713", "issue_status": " Closed\n", "issue_list": [{"user_name": "valeriyvan", "datetime": "Jan 24, 2021", "body": "Currently if label selected in dropdown is inside of bounds of popup, popup stays open allowing annotator to choose labelling method pressing [Shape] or [Track] buttons:But if selected label is beyond bounds of popup, it hides. Annotator has to open popup once again to choose labelling method by pressing [Shape] or [Track] buttons.:In both cases popup should not hide allowing annotator press [Shape] or [Track] buttons.Popup hides unexpectedly. Annotator has to open popup again.Possible solution: if selected labels is inside bounds of popup, behaviour stays the same - popup hides. If selected label is beyond bounds of popup, it stays open until user presses [Shape] or [Track] buttons or until user moves mouse so that mouse pointer goes beyond right edge of popup (mouse y coordinate become greater then popup's greatest y coordinate).Look gifs attached.n/a", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 25, 2021", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "valeriyvan", "datetime": "Jan 25, 2021", "body": "Yes, but  is about different popups.", "type": "commented", "related_issue": null}, {"user_name": "valeriyvan", "datetime": "Jan 25, 2021", "body": "Any plans to fix?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 26, 2021", "body": "Yes, we have plans to fix it.", "type": "commented", "related_issue": null}, {"user_name": "valeriyvan", "datetime": "Jan 26, 2021", "body": "Create! Does it fit next release?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 26, 2021", "body": "We will try to implement it earlier than in next release. It will be available in develop branch", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 25, 2021", "body": [], "type": "marked this as    a duplicate of", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 26, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 26, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 15, 2021", "body": [], "type": "pull", "related_issue": "#2809"}, {"user_name": "bsekachev", "datetime": "Feb 20, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2820", "issue_status": " Closed\n", "issue_list": [{"user_name": "miseonkang", "datetime": "Feb 17, 2021", "body": "I am currently testing the work on the CVAT site(cvat.org), and I tried to find cars and people using the automatic detector function using Yolov3 of AI TOOL.\nHowever, the following message appears.Is there no workaround?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 17, 2021", "body": "Hi,Try to re-deploy the model.\nProbably related ", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 18, 2021", "body": "my problem is not related . i am testing the work on the CVAT site(cvat.org). not localhost:8080 site.  I haven't installed cvat on my machine. Only logging into the site and testing.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 18, 2021", "body": " ,We have fixed the issue on CVAT.org. Could you please specify a task where it doesn't work? I will double check.", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 18, 2021", "body": "When I run it on the site now, the auto anotation function is running. Thanks for fixing it.May I ask you one more question,I am installing cvat on local. While installing the semi-automation function, the nuctl-linux-1.5.16 version file in git bash is not installed due to the following problems.chmod +x nuctl-linux-1.5.16Command does not apply. How can this be solved?./nuctl-1.5.16-linux-amd64File properties are not changed to executable files. The windows version file is executed, but the semi-automation function cannot be installed because this file cannot be executed.And, among the semi-automation functions in cvat, there is a video that automatically performs segmentation by spotting it. Is this using ai tool or which tool is used?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 18, 2021", "body": "I see you try to add executable flag and run different files", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 22, 2021", "body": "Is there any different file??\nI downloaded it from the site here()\nAre there any other download sites?my OS : Windows 10, docker, git-bash window...", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 22, 2021", "body": "You apply  to file \"nuctl-linux-1.5.16\". Then you try to run \"nuctl-1.5.16-linux-amd64\". They have different names, don't they?", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 22, 2021", "body": "i tried to chmod +x nuctl-linux-1.5.16.. that doesn't work.chmod: cannot access 'nuctl-linux-1.5.16': No such file or directory", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 23, 2021", "body": "so i install ubuntu 16.04  cvat..\nsuccess install nuctl..but i have error message like this  (when install   =>   nuctl deploy --project-name cvat \n--path serverless/openvino/dextr/nuclio \n--volume /serverless/openvino/common:/opt/nuclio/common \n--platform local)21.02.23 07:54:17.485 cessor.healthcheck.server (I) Listening {\"listenAddress\": \":8082\"}\n21.02.23 07:54:17.485            processor.http (D) Creating worker pool {\"num\": 2}\n21.02.23 07:54:17.485 sor.http.w1.python.logger (D) Creating listener socket {\"path\": \"/tmp/nuclio-rpc-c0qb9aflufhca7t2blug.sock\"}\n21.02.23 07:54:17.485 sor.http.w0.python.logger (D) Creating listener socket {\"path\": \"/tmp/nuclio-rpc-c0qb9aflufhca7t2blv0.sock\"}\n21.02.23 07:54:17.485 sor.http.w1.python.logger (D) Using Python wrapper script path {\"path\": \"/opt/nuclio/_nuclio_wrapper.py\"}\n21.02.23 07:54:17.485 sor.http.w1.python.logger (D) Using Python handler {\"handler\": \"main:handler\"}\n21.02.23 07:54:17.486 sor.http.w1.python.logger (E) Can't find Python exe {\"error\": \"exec: \"/opt/nuclio/common/openvino/python3\": stat /opt/nuclio/common/openvino/python3: no such file or directory\"}\n21.02.23 07:54:17.486 sor.http.w0.python.logger (D) Using Python wrapper script path {\"path\": \"/opt/nuclio/_nuclio_wrapper.py\"}\n21.02.23 07:54:17.486 sor.http.w0.python.logger (D) Using Python handler {\"handler\": \"main:handler\"}\n21.02.23 07:54:17.486 sor.http.w0.python.logger (E) Can't find Python exe {\"error\": \"exec: \"/opt/nuclio/common/openvino/python3\": stat /opt/nuclio/common/openvino/python3: no such file or directory\"}Error - exec: \"/opt/nuclio/common/openvino/python3\": stat /opt/nuclio/common/openvino/python3: no such file or directory\n...//nuclio/pkg/processor/runtime/rpc/abstract.go:231how can i solve???", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 23, 2021", "body": "I have version 1.5.16 but still the problemMaybe I didn't write the correct path?\nVersion:", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 24, 2021", "body": "So, what version can be installed to use the Semi-automatic and Automatic Annotation tool?Can I install a lower version?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 24, 2021", "body": " ,", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 24, 2021", "body": "Then a very long log output. The end of the output looks like this:", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 24, 2021", "body": "If you find the command in documentation, need to update it. The path to the volume wasn't correct. Please use the command below:Also it is better to use scripts inside serverless directory. They should work out of the box: ", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 24, 2021", "body": "Thank you very much, i did as you said above and it worked", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 25, 2021", "body": "i run nuctl version`user01@kmsDeepServer01:~/cvat/serverless$ ./deploy_cpu.sh openvino/dextr/\nDeploying openvino/dextr function...\n21.02.25 09:02:14.011                     nuctl (I) Deploying function {\"name\": \"\"}\n21.02.25 09:02:14.011                     nuctl (I) Building {\"versionInfo\": \"Label: 1.5.16, Git commit: ae4                                                                      3a6a560c2bec42d7ccfdf6e8e11a1e3cc3774, OS: linux, Arch: amd64, Go version: go1.14.3\", \"name\": \"\"}\n21.02.25 09:02:14.353                     nuctl (I) Cleaning up before deployment {\"functionName\": \"openvino                                                                      -dextr\"}\n21.02.25 09:02:14.447                     nuctl (I) Function already exists, deleting function containers {\"                                                                      functionName\": \"openvino-dextr\"}\n21.02.25 09:02:14.569                     nuctl (I) Staging files and preparing base images\n21.02.25 09:02:14.570                     nuctl (I) Building processor image {\"imageName\": \"cvat/openvino.de                                                                      xtr:latest\"}\n21.02.25 09:02:14.570     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/handler-buil                                                                      der-python-onbuild:1.5.16-amd64\"}\n21.02.25 09:02:18.798     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/uhttpc:0.0.1                                                                      -amd64\"}\n21.02.25 09:02:24.332            nuctl.platform (I) Building docker image {\"image\": \"cvat/openvino.dextr:lat                                                                      est\"}\n21.02.25 09:02:24.655            nuctl.platform (I) Pushing docker image into registry {\"image\": \"cvat/openv                                                                      ino.dextr:latest\", \"registry\": \"\"}\n21.02.25 09:02:24.655            nuctl.platform (I) Docker image was successfully built and pushed into dock                                                                      er registry {\"image\": \"cvat/openvino.dextr:latest\"}\n21.02.25 09:02:24.655                     nuctl (I) Build complete {\"result\": {\"Image\":\"cvat/openvino.dextr:                                                                      latest\",\"UpdatedFunctionConfig\":{\"metadata\":{\"name\":\"openvino-dextr\",\"namespace\":\"nuclio\",\"labels\":{\"nuclio.                                                                      io/project-name\":\"cvat\"},\"annotations\":{\"framework\":\"openvino\",\"min_pos_points\":\"4\",\"name\":\"DEXTR\",\"spec\":\"\"                                                                      ,\"type\":\"interactor\"}},\"spec\":{\"description\":\"Deep Extreme Cut\",\"handler\":\"main:handler\",\"runtime\":\"python:3                                                                      .6\",\"env\":[{\"name\":\"NUCLIO_PYTHON_EXE_PATH\",\"value\":\"/opt/nuclio/common/openvino/python3\"}],\"resources\":{},\"                                                                      image\":\"cvat/openvino.dextr:latest\",\"targetCPU\":75,\"triggers\":{\"myHttpTrigger\":{\"class\":\"\",\"kind\":\"http\",\"na                                                                      me\":\"myHttpTrigger\",\"maxWorkers\":2,\"workerAvailabilityTimeoutMilliseconds\":10000,\"attributes\":{\"maxRequestBo                                                                      dySize\":33554432}}},\"volumes\":[{\"volume\":{\"name\":\"volume-1\",\"hostPath\":{\"path\":\"/home/user01/cvat/serverless                                                                      /common\"}},\"volumeMount\":{\"name\":\"volume-1\",\"mountPath\":\"/opt/nuclio/common\"}}],\"build\":{\"image\":\"cvat/openv                                                                      ino.dextr\",\"baseImage\":\"openvino/ubuntu18_runtime:2020.2\",\"directives\":{\"postCopy\":[{\"kind\":\"RUN\",\"value\":\"c                                                                      url -O https://download.01.org/openvinotoolkit/models_contrib/cvat/dextr_model_v1.zip\"},{\"kind\":\"RUN\",\"value                                                                      \":\"unzip dextr_model_v1.zip\"},{\"kind\":\"RUN\",\"value\":\"pip3 install Pillow\"}],\"preCopy\":[{\"kind\":\"USER\",\"value                                                                      \":\"root\"},{\"kind\":\"WORKDIR\",\"value\":\"/opt/nuclio\"},{\"kind\":\"RUN\",\"value\":\"ln -s /usr/bin/pip3 /usr/bin/pip\"}                                                                      ]},\"codeEntryType\":\"image\"},\"platform\":{\"attributes\":{\"mountMode\":\"volume\",\"restartPolicy\":{\"maximumRetryCou                                                                      nt\":3,\"name\":\"always\"}}},\"readinessTimeoutSeconds\":60,\"securityContext\":{},\"eventTimeout\":\"30s\"}}}}\n21.02.25 09:02:26.239            nuctl.platform (I) Waiting for function to be ready {\"timeout\": 60}\n21.02.25 09:02:27.542                     nuctl (I) Function deploy complete {\"functionName\": \"openvino-dext                                                                      r\", \"httpPort\": 53441}\nNAMESPACE |              NAME              | PROJECT |  STATE   | NODE PORT | REPLICAS\nnuclio    | openvino-dextr                 | cvat    | ready    |     53441 | 1/1\nnuclio    | openvino-omz-public-yolo-v3-tf | cvat    | building |         0 | 1/1wow!! thank you!!!!\nopenvino-dextr  state ready!!!", "type": "commented", "related_issue": null}, {"user_name": "miseonkang", "datetime": "Feb 25, 2021", "body": "Installation was successful.However, if you do DEXTR in task task at localhost:8080, the following message appears.How to fix this error? ??", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 25, 2021", "body": "I have same problem. I think the difference is in the requests tnat the CVAT send.\nOn CVAT.org request for fbrs looks like:\nBut on localhost it is different:\n\nand i get 500 error\nWhy does the CVAT send requests with another fields?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 25, 2021", "body": "Looks like your localhost wasn't updated.\nSend us a version: user dropdown at top right corner -> About", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 25, 2021", "body": "You can also try to update page with enabled checkbox \"Disable cache\". JS page you use, theoretically might be cached by a browser (with old version)", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 25, 2021", "body": "like the latest version\n\"Disable cache\" doesn't work too", "type": "commented", "related_issue": null}, {"user_name": "AlexandrMoruk", "datetime": "Feb 25, 2021", "body": "Yesterday the fbrs on cvat.org also did not work, maybe something was fixed there?", "type": "commented", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Feb 25, 2021", "body": " If you use develop branch you should build images with  command. Only release images v1.2.0 are available on the Dockerhub that dont yet support this unreleased functionality.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Mar 8, 2021", "body": " ,  , I will close the issue. Please don't hesitate to reopen it, if you still can reproduce it on the latest develop branch.P.S. I have checked and FBRS works on CVAT.org now.", "type": "commented", "related_issue": null}, {"user_name": "Ashish-25-11", "datetime": "Jul 11, 2022", "body": "nuctl deploy --project-name cvat \n--path serverless/openvino/dextr/nuclio \n--volume /serverless/common:/opt/nuclio/common \n--platform local\nI am running the above command and getting the error which is shown belowError - exec: \"/opt/nuclio/common/openvino/python3\": stat /opt/nuclio/common/openvino/python3: no such file or directory\n...//nuclio/pkg/processor/runtime/rpc/abstract.go:239Any soltuion", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 25, 2021", "body": [], "type": "issue", "related_issue": "#2844"}, {"user_name": "nmanovic", "datetime": "Mar 8, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1585", "issue_status": " Closed\n", "issue_list": [{"user_name": "LukeAI", "datetime": "May 22, 2020", "body": "Feature Request:\nAn Appearance tick box that shows all labels on an image.Motivation:\nWhen reviewing annotations and checking for errors I can see this:\nI cannot easily, at a glance, assert that every annotation is of the correct class, I have to hover the mouse over each annotation in turn.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 22, 2020", "body": "Hi, please look here:\n", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 22, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "snyk-bot", "datetime": "Oct 9, 2020", "body": [], "type": "pull", "related_issue": "#2287"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2176", "issue_status": " Closed\n", "issue_list": [{"user_name": "Mylszd", "datetime": "Sep 15, 2020", "body": "nuctl deploydocker-compose up on the yml in master branchYou may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "Mylszd", "datetime": "Sep 15, 2020", "body": "Use nuctl-1.4.17-linux-amd64 instead of the latest nuctl-1.1.37-linux-amd64.", "type": "commented", "related_issue": null}, {"user_name": "Mylszd", "datetime": "Sep 15, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1825", "issue_status": " Closed\n", "issue_list": [{"user_name": "nmanovic", "datetime": "Jun 30, 2020", "body": "A tooltip should be hidden after mouse is out of an element which generates the tooltip.Even I move the mouse cursor out of an element, the tooltip is still visible. It is really annoying problem which doesn't allow in come cases modify/press other UI elements.I have the problem each time when I try to demonstrate CVAT to internal/external customers. It is a really critical usability issue.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 30, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 30, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Jul 28, 2020", "body": [], "type": "pull", "related_issue": "#1955"}, {"user_name": "bsekachev", "datetime": "Jul 29, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Jul 29, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "dvkruchinin", "datetime": "Sep 3, 2020", "body": [], "type": "pull", "related_issue": "#2124"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2329", "issue_status": " Closed\n", "issue_list": [{"user_name": "arasharchor", "datetime": "Oct 17, 2020", "body": "If a point is selected and mouse gets away from it, its size shall not change similar to the early versions.When a point is select and mouse gets away from it, it becomes significantly bigger which hinders to see the area near that point.\n\n\n\n\n\nReverting back to the previous settings?!It does not allow fine annotation because it blocks to see area near that point.Origin/develop branch - 19th October 2020\nLinux", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 18, 2020", "body": "Please, provide exact version (not ) and git hash.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 18, 2020", "body": "I would say we have fixed similar issue in ", "type": "commented", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 20, 2020", "body": "Oops!\n\nI used the \"origin/develop\" branch which has the commit dating until October 9th. 2020. I did not use any specific git tags as I wanted to clone the latest develop version.\nI see the commit below as the latest commit by calling \"git log\"commit \nAuthor: tdowgiel \nDate:   Fri Oct 9 21:13:47 2020 +0200Thanks. I see issue in  dates back to 8 days ago. I will pull the merge and try again. I will update this issue afterwards.", "type": "commented", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 20, 2020", "body": "I just pulled the repo by \"git pull\" and compiled it again\n\"docker-compose -f docker-compose.yml -f components/serverless/docker-compose.serverless.yml -f components/analytics/docker-compose.analytics.yml  up -d\"\nStill points appear to be bigger when selected and released.Is there any way to define the point size when released?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 20, 2020", "body": " doesn't rebuild image.\nYou need to use ", "type": "commented", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 21, 2020", "body": "\nOh I missed it. Now it is being built, but it takes long to finish.\nIn this way, even by a tiny modification in the code, the whole repo has to be built again.\nI am wondering whether there is any alternatives to compile the code faster and see the changes.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 21, 2020", "body": "@smajidaInternally we use docker deployment only for production. For development & debugging we install the whole system on a host operating system. The process is described in CONTRIBUTING.md", "type": "commented", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 22, 2020", "body": "\nThanks for the explanation.\nI can confirm that the issue has been solved. I will then close the issue.", "type": "commented", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 17, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 21, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "arasharchor", "datetime": "Oct 22, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2317", "issue_status": " Closed\n", "issue_list": [{"user_name": "cpudney", "datetime": "Oct 13, 2020", "body": "It would be useful if keyboard shortcuts existed for the labels used during an annotation task.During the annotation process it is often necessary to change to a different label from the current one. This involves using the mouse to select the label in the annotation widgets on the left-hand side of the GUI. Providing this action via a shortcut would make annotation more facile and quicker.The shortcuts could be keys: 0 ... 9 and would correspond to the (first 10) labels (e.g. alphabetically ordered).", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 13, 2020", "body": "Hi,One question here. Where a label should be changed? In all drawing popups? Or on the latest used? Or only for N shortcut? How do you see the feature?Additionally I can say that 0 ... 9 isn't the best choice, because these keys used in attribute annotation mode. We can use them only with an additional modifier (like ctrl or shift). In long term view these shortcuts could be setup by a user.", "type": "commented", "related_issue": null}, {"user_name": "cpudney", "datetime": "Oct 14, 2020", "body": "Thanks, Boris for your quick response.Ideally, apply to all drawing popups but if that's not feasible, the current drawing mode (N shortcut). So for exampleUnderstood - I think Ctrl+ (or Alt?) might be better as Shift+ produces different characters on various keyboard layouts.", "type": "commented", "related_issue": null}, {"user_name": "xyc2690", "datetime": "Dec 14, 2020", "body": "Any update? How could I help to do it?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 28, 2020", "body": "Hi, The core team isn't working on the feature. If somebody from the community could propose a PR, it would be great", "type": "commented", "related_issue": null}, {"user_name": "xyc2690", "datetime": "Dec 28, 2020", "body": "Thanks for your kind notification.\nI am willing to work on the feature, however, I am not familiar with TS.\nDoes there any examples or similar functions which I can take them as references?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 28, 2020", "body": "TS is just a superset of JS, nothing really difficult if you familiar with JS. You can look how other shortcuts are implemented I suppose. I would suggest to make a search using key  in the project. We use the library  to maintain shortcuts.", "type": "commented", "related_issue": null}, {"user_name": "xyc2690", "datetime": "Dec 28, 2020", "body": "Thanks a lot, I will try to make it！", "type": "commented", "related_issue": null}, {"user_name": "lisc199", "datetime": "Feb 2, 2021", "body": " Hi~ I have the same requirement. Have you achieved this function?", "type": "commented", "related_issue": null}, {"user_name": "JimEverest", "datetime": "Mar 11, 2021", "body": "Same requirement. It's a very useful shortcut for improving work efficiency.", "type": "commented", "related_issue": null}, {"user_name": "Serakov", "datetime": "Mar 11, 2021", "body": "Same requirement. It's a very useful shortcut.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 13, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 13, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Oct 13, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "vnishukov", "datetime": "Jan 18, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Mar 11, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Apr 6, 2021", "body": [], "type": "pull", "related_issue": "#3070"}, {"user_name": "bsekachev", "datetime": "Apr 7, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2127", "issue_status": " Closed\n", "issue_list": [{"user_name": "cognitiveRobot", "datetime": "Sep 4, 2020", "body": "I was expecting to see a model that is currently running.After deploying the TensorFlow model, when I go to  I see the error message as below.Still searching.Please help me to find a solution. :)", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 4, 2020", "body": " , could you please attach server logs? ().\nAlso please attach logs from the serverless function ()", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 4, 2020", "body": "Thank  . Here they are.\ndocker logs cvat:docker logs nuclio-nuclio-tf.faster_rcnn_inception_v2_coco", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 7, 2020", "body": ", any update on this? Thanks.", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 8, 2020", "body": "To resolve this, I installed a fresh ubuntu 18.04. Now, I am running into a new problem. I can't deploy the model.\nEnv:When I run the deploying commad:But looks like docker image has been created.\n$ docker imagesBut no container:\n$docker psAny help would be highly appreciated.", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 8, 2020", "body": "Working with the latest release of nuclio.\n", "type": "commented", "related_issue": null}, {"user_name": "varchanaiyer", "datetime": "Sep 8, 2020", "body": "Hello,I am also facing a similar issue. I am using the 1.4.17 version of nuclio, but when I try to perform automatic sengmentation using the openvino yolo model, then I get an error message:Here are the logs:Thank you", "type": "commented", "related_issue": null}, {"user_name": "WeiChihChern", "datetime": "Sep 9, 2020", "body": "\nSame issue, and fixed by using latest nuclio suggested by .  Please update the link for nuclio in the guide, that one is an older version which will cause problems for serverless functions.", "type": "commented", "related_issue": null}, {"user_name": "veer5551", "datetime": "Sep 12, 2020", "body": "Hello,I am getting these errors while executing the first command itself  .\nTried multiple versions and got different errors.Error on nuctl version -  1.4.17Error on nuctl version -  1.3.17 and 1.1.37Running on Windows 10.\nLet me know If I am missing something here!Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 12, 2020", "body": ", after trying to perform automatic segmentation, immediately check  (make sure the image name is correct in this command) and see what's the error message. Hope you will find the solution. If not then share your docker logs share, we will try.", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 12, 2020", "body": ", go to the location where you downloaded  and check the filename. In my case, it's , so I run .", "type": "commented", "related_issue": null}, {"user_name": "veer5551", "datetime": "Sep 13, 2020", "body": "Hello ,I renamed the downloaded files and ran accordingly. Still getting the same Errors.Thanks!", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 13, 2020", "body": "well,  means there is something wrong in your execution. It could be either you are not going to the directory where the  is or you are not executing the command properly. Hope these give you some directions.", "type": "commented", "related_issue": null}, {"user_name": "veer5551", "datetime": "Sep 13, 2020", "body": "Followed the same procedure on a Linux system and everything was smooth. Models are deployed correctly and I am able to use them as well.Need to check if this is an issue of Nuclio + WSL in Windows 10 combination problem.\nMaybe I'll report the issue there.\nIf anyone here has got it working, please share the procedure/guide.Alternatively, is it possible to deploy the models via the Nuclio UI Dashboard by loading the appropriate YAML and model handler? Tried doing it but got some function invocation error.Thanks a lot!", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Sep 14, 2020", "body": " , could you please check if basic examples from nuclio are working on your Windows machine and report problems to nuclio github if it doesn't work?", "type": "commented", "related_issue": null}, {"user_name": "veer5551", "datetime": "Sep 14, 2020", "body": "Reported the Issue on Nuclio github:\nReference Link: Thanks!", "type": "commented", "related_issue": null}, {"user_name": "beep-love", "datetime": "Dec 4, 2020", "body": "I was having an error while executing:\n\npwd\nError - Container wasn't healthy in time\n.../nuclio/nuclio/pkg/dockerclient/shell.go:395Call stack:\nContainer wasn't healthy in time\n.../nuclio/nuclio/pkg/dockerclient/shell.go:395\nFunction wasn't ready in time. Logs:`20.12.04 08:40:45.720 sor.http.w0.python.logger (E) Unexpected termination of child process {\"error\": null, \"status\": \"exit status 1\"}\npanic: Wrapper process for worker 0 exited unexpectedly with: exit status 1goroutine 22 [running]:\ngithub.com/nuclio/nuclio/pkg/processor/runtime/rpc.(*AbstractRuntime).watchWrapperProcess(0xc4205160b0)\n/go/src/github.com/nuclio/nuclio/pkg/processor/runtime/rpc/abstract.go:453 +0x5bb\ncreated by github.com/nuclio/nuclio/pkg/processor/runtime/rpc.(*AbstractRuntime).startWrapper\n/go/src/github.com/nuclio/nuclio/pkg/processor/runtime/rpc/abstract.go:232 +0x1c8Failed to deploy function\n.../nuclio/pkg/platform/abstract/platform.go:172\n`\nI checked the GitHub repo and found that the URL does not exist but the file is rather with URL  instead of github.com/nuclio/nuclio/pkg/ ......Can you please help on this issue   ?", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Dec 9, 2020", "body": " looks like you are trying to use an older version of nuclio. I did the same mistake, then when I used  this, it worked.", "type": "commented", "related_issue": null}, {"user_name": "beep-love", "datetime": "Dec 9, 2020", "body": "Yeah. It was the same issue. Mine worked with v 1.5.7.", "type": "commented", "related_issue": null}, {"user_name": "cognitiveRobot", "datetime": "Sep 8, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "veer5551", "datetime": "Sep 14, 2020", "body": [], "type": "issue", "related_issue": "nuclio/nuclio#1821"}, {"user_name": "Loc-Vo", "datetime": "Oct 7, 2020", "body": [], "type": "issue", "related_issue": "#2259"}, {"user_name": "veer5551", "datetime": "Oct 9, 2020", "body": [], "type": "issue", "related_issue": "nuclio/nuclio#1850"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1886", "issue_status": " Closed\n", "issue_list": [{"user_name": "miso-ramen", "datetime": "Jul 12, 2020", "body": "This makes labeling polygons in my videos very difficult and frustrating. Sometimes very jagged points are added to my polygons, which takes a lot of time to correct.\n", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 12, 2020", "body": "Looks like you have several points with the same coordinates and move one of them. Could you please check it?", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "Thank you, bsekachev. In the instances above, the polygons are only 4 pointed. As soon as a point is touched to move it, a new point seems to be created.If I think I'm creating a 4-pointed polygon, but somehow the program is creating additional points, how would I go about checking that? Would I need to do an annotation dump and view the annotation file for that frame?I specifically added only a 4-point polygon, and confirmed that only 4 points were used. So long as the directional arrow is located  points, the editing of points moves as expected. However, for some unknown reason, the directional arrow will locate itself  a point. When this happens (the arrow on top of a point), trying to move a point only results in creating new points. It's almost like it goes a little wild. You can see in the videos above this behavior in action. Note the directional arrow is on a point in both videos, and note how at that time, a new point is generated instead of being moved.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": "Actually code of dragging points even doesn't assume ability to create new points. Issue is probably in something else.By the way, speaking about orientation arrow. It always directed from the latest point to the first point, so if you see this arrow between two points, everything is alright. If you see it over a point, it means, that the latest point and the first point have the same coordinates, as I wrote in the first message.I believe, need to understand why some points are duplicated for you. Maybe every click of your mouse is double? At least I am unable to reproduce the issue from provided steps", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": "By the way, git says that hash you provided  is not in the working tree of our  branch.\nAre you sure it is correct?", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "Thank you for your time on this. The hash is correct. I had installed probably the master branch. Would this hash match up? Then, a few days ago, there was a bug in annotation.py that you all fixed (problem with dumping annotations). I (perhaps incorrectly) changed the files affected, then rebuilt cvat. Should I clone the development branch and rebuild instead? I'm sorry if I did this incorrectly.", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "I was able to screen capture labeling a new polygon from the beginning (a brand new task that hadn't yet been labeled) that shows all steps, when things are working as expected, then when things go awry. I hope this helps you in recreating the problem. Thank you.\nI start at frame 80, add a 4 pointed polygon (with track). I create a 4 pointed polygon, jump ahead by 10 frames at a time, unpin object, then move points, jump 10 frames, move points, move entire polygon; this is just as expected. Note: the orientation arrow in these frames is between points.Then I jump backward to beginning frame (sorry...mistake), then move forward back to where polygon was created. Note: the orientation arrow is now on a point. Now, if I try to move a point, an additional point is created. If try to move the point at the location of the orientation arrow, the entire image moves. This is unexpected behavior.This is reproducible by me each time. I'm hoping that by repeating these events on your system, you'll also be able to recreate them. Again, thank you.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": "I suspicious that it is related with \nAnyway let's assign  label, I will look into this issue", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "Much appreciated. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": "Please, check if  resolves the issue. Let me know if it does not", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "I built the development branch and made the changes you outlined, then built it again. I tried to recreate the issue like I did from scratch in the video above. I was unable to. However, I imported the task my labeler was working on and asked them to see if the issue was gone. The issue still exists in the imported task.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": "Actually it is expected because wrong annotation is probably have been saved for some frames (with duplicated points).", "type": "commented", "related_issue": null}, {"user_name": "miso-ramen", "datetime": "Jul 13, 2020", "body": "I'll keep you posted on this. I had my labeler only create new polygons, and  continue edited existing polygons. I'm told that on the new polygons, things seem to be working well. We'll continue with only new polygons from this point forward. If we see problems again, we'll let you know. Many thanks to you for your time and work on this!", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jul 13, 2020", "body": [], "type": "pull", "related_issue": "#1892"}, {"user_name": "bsekachev", "datetime": "Jul 15, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Jul 15, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "dvkruchinin", "datetime": "Aug 24, 2020", "body": [], "type": "pull", "related_issue": "#2075"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1540", "issue_status": " Closed\n", "issue_list": [{"user_name": "mkostiv", "datetime": "May 14, 2020", "body": "Steps to reproduce:TypeError\nCannot read property 'removeClass' of undefinedTypeError: Cannot read property 'removeClass' of undefined\nat Mu.deactivateShape ()\nat Mu.deactivate ()\nat Mu.activate ()\nat Mu.notify ()\nat Cc.notify ()\nat Cc.activate ()\nat Lu.activate ()\nat od.componentDidUpdate ()\nat yl ()\nat t.unstable_runWithPriority ()\n", "type": "commented", "related_issue": null}, {"user_name": "mkostiv", "datetime": "May 14, 2020", "body": " 0.2.0-706-g6566e4a", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 14, 2020", "body": "Also found the issue. It is enough just to deactivate a tag. could you please look into this?", "type": "commented", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 14, 2020", "body": ", maybe it's good idea to add activated shape id checking in cvat-canvas in activation method, is it?", "type": "commented", "related_issue": null}, {"user_name": "mkostiv", "datetime": "May 14, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 14, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 14, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 14, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 14, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "May 14, 2020", "body": [], "type": "pull", "related_issue": "#1541"}, {"user_name": "bsekachev", "datetime": "May 15, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "May 15, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 15, 2020", "body": [], "type": "issue", "related_issue": "#1546"}, {"user_name": "dvkruchinin", "datetime": "Aug 28, 2020", "body": [], "type": "pull", "related_issue": "#2096"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1505", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Apr 8, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up is clicked.\n\"A privacy reminder from Google\" is reviewed and agreed and the yellow language pop-up is closed. (if applicable)Doorhanger doesn't have display issues.There is a scrollbar present that shows a partial white border of the black background.Reproduced on Mac 10.14 with Firefox Nightly 77.0a1 (2020-04-08).", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": "  I'd like to work on this.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 8, 2020", "body": "Go for it ", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": "  I've made some CSS changes and tested on Local Machine.\nPlease have a look and confirm.\n\nOS: Windows 10x64\nFirefox Nightly: Version 75.0a1\nNode: v12.16.1", "type": "commented", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 25, 2020", "body": "  If everything is working fine then please close this issue. Let me know in case I've to make any other changes?", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 27, 2020", "body": "Looks good. Closing.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 5, 2020", "body": "Verified on Mac 10.14 and Windows 10x64 with Firefox Nightly 78.0a1 (2020-05-04).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Apr 8, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "awallin", "datetime": "Apr 8, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "vandnakapoor19", "datetime": "Apr 8, 2020", "body": [], "type": "pull", "related_issue": "#1513"}, {"user_name": "awallin", "datetime": "Apr 27, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "May 5, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1977", "issue_status": " Closed\n", "issue_list": [{"user_name": "creativesiva", "datetime": "Aug 3, 2020", "body": "I am getting error while trying to deploy openvino and pytorch models to nuclio (serverless functions), I have used  'bash serverless/deploy.sh to deploy models. Below is the log error for semantic-segmentation-adas-0001Error - Container wasn't healthy in time\n/nuclio/pkg/dockerclient/shell.go:429Call stack:\nContainer wasn't healthy in time\n/nuclio/pkg/dockerclient/shell.go:429\nFunction wasn't ready in time. Logs:Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265As a end results, it is installing i am able to deploy tensorflow models, below is the error message for rest of the models.\nNAMESPACE |                             NAME                              | PROJECT | STATE | NODE PORT | REPLICAS\nnuclio    | openvino.dextr                                                | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.intel.person-reidentification-retail-0300        | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.intel.text-detection-0004                        | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.faster_rcnn_inception_v2_coco             | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.mask_rcnn_inception_resnet_v2_atrous_coco | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.yolo-v3-tf                                | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.semantic-segmentation-adas-0001                  | cvat    | error |         0 | 1/1\nnuclio    | tf.faster_rcnn_inception_v2_coco                              | cvat    | ready |     39731 | 1/1\nnuclio    | tf.matterport.mask_rcnn                                       | cvat    | ready |     37237 | 1/1\nCONTAINER ID        IMAGE                                                                       COMMAND                  CREATED             STATUS                          PORTS                                                 NAMES\nce454f5ed371        cvat/tf.faster_rcnn_inception_v2_coco:latest                                \"processor\"              44 minutes ago      Up 44 minutes (healthy)         0.0.0.0:39731->8080/tcp                               nuclio-nuclio-tf.faster_rcnn_inception_v2_coco\ndde7ad667e3b        cvat/tf.matterport.mask_rcnn:latest                                         \"processor\"              45 minutes ago      Up 45 minutes (healthy)         0.0.0.0:37237->8080/tcp                               nuclio-nuclio-tf.matterport.mask_rcnn\n2a9b80907c35        cvat/openvino.dextr:latest                                                  \"processor\"              46 minutes ago      Restarting (1) 44 seconds ago                                                         nuclio-nuclio-openvino.dextr\n21170f4a7916        cvat/openvino.omz.intel.person-reidentification-retail-0300:latest          \"processor\"              48 minutes ago      Restarting (1) 3 seconds ago                                                          nuclio-nuclio-openvino.omz.intel.person-reidentification-retail-0300\n13b9d8996523        cvat/openvino.omz.intel.semantic-segmentation-adas-0001:latest              \"processor\"              49 minutes ago      Restarting (1) 28 seconds ago                                                         nuclio-nuclio-openvino.omz.semantic-segmentation-adas-0001\n81c6d6abd728        cvat/openvino.omz.intel.text-detection-0004:latest                          \"processor\"              50 minutes ago      Restarting (1) 57 seconds ago                                                         nuclio-nuclio-openvino.omz.intel.text-detection-0004\n13f30e51675d        cvat/openvino.omz.public.yolo-v3-tf:latest                                  \"processor\"              52 minutes ago      Restarting (1) 20 seconds ago                                                         nuclio-nuclio-openvino.omz.public.yolo-v3-tf\n4e454024a67b        cvat/openvino.omz.public.mask_rcnn_inception_resnet_v2_atrous_coco:latest   \"processor\"              53 minutes ago      Restarting (1) 41 seconds ago                                                         nuclio-nuclio-openvino.omz.public.mask_rcnn_inception_resnet_v2_atrous_coco\n86900a751357        cvat/openvino.omz.public.faster_rcnn_inception_v2_coco:latest               \"processor\"              55 minutes ago      Restarting (1) 6 seconds ago                                                          nuclio-nuclio-openvino.omz.public.faster_rcnn_inception_v2_coco\nc096e1d34a9f        alpine:3.11                                                                 \"/bin/sh -c '/bin/sl…\"   3 hours ago         Up 3 hours                                                                            nuclio-local-storage-reader\n10761a37f44d        nginx:stable-alpine                                                         \"/docker-entrypoint.…\"   9 hours ago         Up 9 hours                      0.0.0.0:8080->80/tcp                                  cvat_proxy\n888326330e63        cvat/server                                                                 \"/usr/bin/supervisord\"   9 hours ago         Up 9 hours                      8080/tcp, 8443/tcp                                    cvat\na82bd68fdccd        cvat_logstash                                                               \"/usr/local/bin/dock…\"   9 hours ago         Up 9 hours                      5000/tcp, 5044/tcp, 9600/tcp                          cvat_logstash\na89ce4f4ad30        cvat_kibana                                                                 \"/usr/local/bin/kiba…\"   9 hours ago         Up 9 hours                      5601/tcp                                              cvat_kibana\n1f55c7d21943        cvat_elasticsearch                                                          \"/usr/local/bin/dock…\"   9 hours ago         Up 9 hours                      9200/tcp, 9300/tcp                                    cvat_elasticsearch\n4117c788c958        cvat/ui                                                                     \"/docker-entrypoint.…\"   9 hours ago         Up 9 hours                      80/tcp                                                cvat_ui\n0c50074a51c5        quay.io/nuclio/dashboard:1.4.8-amd64                                        \"sh -c ./runner.sh\"      9 hours ago         Up 9 hours                      80/tcp, 0.0.0.0:8070->8070/tcp                        nuclio\nnuctl-1.4.14-linux-amd64", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 3, 2020", "body": " , could you please provide the full deployment log for functions? Something like I did below:", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 3, 2020", "body": " , Below is the deployment log.20.08.03 22:26:11.277            nuctl.platform (I) Waiting for function to be ready {\"timeout\": 60}\n20.08.03 22:26:12.568                     nuctl (I) Function deploy complete {\"functionName\": \"tf.matterport.mask_rcnn\", \"httpPort\": 37237}\n20.08.03 22:26:13.531                     nuctl (I) Deploying function {\"name\": \"\"}\n20.08.03 22:26:13.532                     nuctl (I) Building {\"versionInfo\": \"Label: 1.4.14, Git commit: e10e9fb123caafbe4f95310a0e2ccfc71368ba87, OS: linux, Arch: amd64, Go version: go1.14.3\", \"name\": \"\"}\n20.08.03 22:26:13.767                     nuctl (I) Cleaning up before deployment\n20.08.03 22:26:13.880                     nuctl (I) Function already exists, deleting\n20.08.03 22:26:15.712                     nuctl (I) Staging files and preparing base images\n20.08.03 22:26:15.714                     nuctl (I) Building processor image {\"imageName\": \"cvat/tf.faster_rcnn_inception_v2_coco:latest\"}\n20.08.03 22:26:15.714     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/handler-builder-python-onbuild:1.4.14-amd64\"}\n20.08.03 22:26:22.932     nuctl.platform.docker (I) Pulling image {\"imageName\": \"quay.io/nuclio/uhttpc:0.0.1-amd64\"}\n20.08.03 22:26:34.942            nuctl.platform (I) Building docker image {\"image\": \"cvat/tf.faster_rcnn_inception_v2_coco:latest\"}\n20.08.03 22:26:35.301            nuctl.platform (I) Pushing docker image into registry {\"image\": \"cvat/tf.faster_rcnn_inception_v2_coco:latest\", \"registry\": \"\"}\n20.08.03 22:26:35.301            nuctl.platform (I) Docker image was successfully built and pushed into docker registry {\"image\": \"cvat/tf.faster_rcnn_inception_v2_coco:latest\"}\n20.08.03 22:26:35.301                     nuctl (I) Build complete {\"result\": {\"Image\":\"cvat/tf.faster_rcnn_inception_v2_coco:latest\",\"UpdatedFunctionConfig\":{\"metadata\":{\"name\":\"tf.faster_rcnn_inception_v2_coco\",\"namespace\":\"nuclio\",\"labels\":{\"nuclio.io/project-name\":\"cvat\"},\"annotations\":{\"framework\":\"tensorflow\",\"name\":\"Faster RCNN via Tensorflow\",\"spec\":\"[\\n  { \"id\": 1, \"name\": \"person\" },\\n  { \"id\": 2, \"name\": \"bicycle\" },\\n  { \"id\": 3, \"name\": \"car\" },\\n  { \"id\": 4, \"name\": \"motorcycle\" },\\n  { \"id\": 5, \"name\": \"airplane\" },\\n  { \"id\": 6, \"name\": \"bus\" },\\n  { \"id\": 7, \"name\": \"train\" },\\n  { \"id\": 8, \"name\": \"truck\" },\\n  { \"id\": 9, \"name\": \"boat\" },\\n  { \"id\":10, \"name\": \"traffic_light\" },\\n  { \"id\":11, \"name\": \"fire_hydrant\" },\\n  { \"id\":13, \"name\": \"stop_sign\" },\\n  { \"id\":14, \"name\": \"parking_meter\" },\\n  { \"id\":15, \"name\": \"bench\" },\\n  { \"id\":16, \"name\": \"bird\" },\\n  { \"id\":17, \"name\": \"cat\" },\\n  { \"id\":18, \"name\": \"dog\" },\\n  { \"id\":19, \"name\": \"horse\" },\\n  { \"id\":20, \"name\": \"sheep\" },\\n  { \"id\":21, \"name\": \"cow\" },\\n  { \"id\":22, \"name\": \"elephant\" },\\n  { \"id\":23, \"name\": \"bear\" },\\n  { \"id\":24, \"name\": \"zebra\" },\\n  { \"id\":25, \"name\": \"giraffe\" },\\n  { \"id\":27, \"name\": \"backpack\" },\\n  { \"id\":28, \"name\": \"umbrella\" },\\n  { \"id\":31, \"name\": \"handbag\" },\\n  { \"id\":32, \"name\": \"tie\" },\\n  { \"id\":33, \"name\": \"suitcase\" },\\n  { \"id\":34, \"name\": \"frisbee\" },\\n  { \"id\":35, \"name\": \"skis\" },\\n  { \"id\":36, \"name\": \"snowboard\" },\\n  { \"id\":37, \"name\": \"sports_ball\" },\\n  { \"id\":38, \"name\": \"kite\" },\\n  { \"id\":39, \"name\": \"baseball_bat\" },\\n  { \"id\":40, \"name\": \"baseball_glove\" },\\n  { \"id\":41, \"name\": \"skateboard\" },\\n  { \"id\":42, \"name\": \"surfboard\" },\\n  { \"id\":43, \"name\": \"tennis_racket\" },\\n  { \"id\":44, \"name\": \"bottle\" },\\n  { \"id\":46, \"name\": \"wine_glass\" },\\n  { \"id\":47, \"name\": \"cup\" },\\n  { \"id\":48, \"name\": \"fork\" },\\n  { \"id\":49, \"name\": \"knife\" },\\n  { \"id\":50, \"name\": \"spoon\" },\\n  { \"id\":51, \"name\": \"bowl\" },\\n  { \"id\":52, \"name\": \"banana\" },\\n  { \"id\":53, \"name\": \"apple\" },\\n  { \"id\":54, \"name\": \"sandwich\" },\\n  { \"id\":55, \"name\": \"orange\" },\\n  { \"id\":56, \"name\": \"broccoli\" },\\n  { \"id\":57, \"name\": \"carrot\" },\\n  { \"id\":58, \"name\": \"hot_dog\" },\\n  { \"id\":59, \"name\": \"pizza\" },\\n  { \"id\":60, \"name\": \"donut\" },\\n  { \"id\":61, \"name\": \"cake\" },\\n  { \"id\":62, \"name\": \"chair\" },\\n  { \"id\":63, \"name\": \"couch\" },\\n  { \"id\":64, \"name\": \"potted_plant\" },\\n  { \"id\":65, \"name\": \"bed\" },\\n  { \"id\":67, \"name\": \"dining_table\" },\\n  { \"id\":70, \"name\": \"toilet\" },\\n  { \"id\":72, \"name\": \"tv\" },\\n  { \"id\":73, \"name\": \"laptop\" },\\n  { \"id\":74, \"name\": \"mouse\" },\\n  { \"id\":75, \"name\": \"remote\" },\\n  { \"id\":76, \"name\": \"keyboard\" },\\n  { \"id\":77, \"name\": \"cell_phone\" },\\n  { \"id\":78, \"name\": \"microwave\" },\\n  { \"id\":79, \"name\": \"oven\" },\\n  { \"id\":80, \"name\": \"toaster\" },\\n  { \"id\":81, \"name\": \"sink\" },\\n  { \"id\":83, \"name\": \"refrigerator\" },\\n  { \"id\":84, \"name\": \"book\" },\\n  { \"id\":85, \"name\": \"clock\" },\\n  { \"id\":86, \"name\": \"vase\" },\\n  { \"id\":87, \"name\": \"scissors\" },\\n  { \"id\":88, \"name\": \"teddy_bear\" },\\n  { \"id\":89, \"name\": \"hair_drier\" },\\n  { \"id\":90, \"name\": \"toothbrush\" }\\n]\\n\",\"type\":\"detector\"}},\"spec\":{\"description\":\"Faster RCNN from Tensorflow Object Detection API\",\"handler\":\"main:handler\",\"runtime\":\"python:3.6\",\"resources\":{},\"image\":\"cvat/tf.faster_rcnn_inception_v2_coco:latest\",\"targetCPU\":75,\"triggers\":{\"myHttpTrigger\":{\"class\":\"\",\"kind\":\"http\",\"name\":\"\",\"maxWorkers\":2,\"workerAvailabilityTimeoutMilliseconds\":10000,\"attributes\":{\"maxRequestBodySize\":33554432}}},\"build\":{\"image\":\"cvat/tf.faster_rcnn_inception_v2_coco\",\"baseImage\":\"tensorflow/tensorflow:2.1.1\",\"directives\":{\"postCopy\":[{\"kind\":\"RUN\",\"value\":\"curl -O http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\"},{\"kind\":\"RUN\",\"value\":\"tar -xzf faster_rcnn_inception_v2_coco_2018_01_28.tar.gz \\u0026\\u0026 rm faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\"},{\"kind\":\"RUN\",\"value\":\"ln -s faster_rcnn_inception_v2_coco_2018_01_28 faster_rcnn\"},{\"kind\":\"RUN\",\"value\":\"pip install pillow pyyaml\"}],\"preCopy\":[{\"kind\":\"RUN\",\"value\":\"apt install curl\"},{\"kind\":\"WORKDIR\",\"value\":\"/opt/nuclio\"}]},\"codeEntryType\":\"image\"},\"platform\":{\"attributes\":{\"restartPolicy\":{\"maximumRetryCount\":3,\"name\":\"always\"}}},\"readinessTimeoutSeconds\":60,\"eventTimeout\":\"30s\"}}}}\n20.08.03 22:26:37.812            nuctl.platform (I) Waiting for function to be ready {\"timeout\": 60}\n20.08.03 22:26:39.205                     nuctl (I) Function deploy complete {\"functionName\": \"tf.faster_rcnn_inception_v2_coco\", \"httpPort\": 39731}\nNAMESPACE |                             NAME                              | PROJECT | STATE | NODE PORT | REPLICAS\nnuclio    | openvino.dextr                                                | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.intel.person-reidentification-retail-0300        | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.intel.text-detection-0004                        | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.faster_rcnn_inception_v2_coco             | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.mask_rcnn_inception_resnet_v2_atrous_coco | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.public.yolo-v3-tf                                | cvat    | error |         0 | 1/1\nnuclio    | openvino.omz.semantic-segmentation-adas-0001                  | cvat    | error |         0 | 1/1\nnuclio    | tf.faster_rcnn_inception_v2_coco                              | cvat    | ready |     39731 | 1/1\nnuclio    | tf.matterport.mask_rcnn                                       | cvat    | ready |     37237 | 1/1", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 4, 2020", "body": " , I have updated the documentation. Could you please add  command line argument when you deploy a function (deploy.sh already has the additional argument)? Please notify me if it solves the problem.For further investigation please add --verbose flag to a  command. It will provide additional information.Also it looks like need more information about your system. Which OS do you use? Please provide as much details about your system as possible. For example, docker version.", "type": "commented", "related_issue": null}, {"user_name": "AJ-RR", "datetime": "Aug 4, 2020", "body": "I have the same issue. I am using Ubuntu 18.04.1 and docker version 19.03.6. Adding --platform local command line argument does not solve the issue.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 4, 2020", "body": " , could you please also publish your deployment log?", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 4, 2020", "body": "I have tried with --platfom local, Hard Luck.\nOS:Ubuntu 18.04.4, Docker-version: 19.03.12", "type": "commented", "related_issue": null}, {"user_name": "AJ-RR", "datetime": "Aug 4, 2020", "body": "\n, Here it is\nError - Container wasn't healthy in time\n/nuclio/pkg/dockerclient/shell.go:429Call stack:\nContainer wasn't healthy in time\n/nuclio/pkg/dockerclient/shell.go:429\nFunction wasn't ready in time. Logs:Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Error - open /etc/nuclio/config/processor/processor.yaml: permission denied\n/nuclio/cmd/processor/app/processor.go:265Call stack:\nFailed to open configuration file\n/nuclio/cmd/processor/app/processor.go:265Failed to deploy function\n...//nuclio/pkg/platform/abstract/platform.go:171", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 5, 2020", "body": " , we are investigating the issue. I cannot reproduce it on my Linux and Mac machines. But an internal team also reported the problem. I know that nuclio guys also are investigating the issue. I hope to find a solution soon. If you can help and investigate on your end, it will be perfect.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 6, 2020", "body": " , could you please try a workaround in PR ?", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 6, 2020", "body": " , Please suggest how can i check this!!, I am not sure, what to do!", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 6, 2020", "body": "I understand that you have removed       \"- kind: USER value: openvino\" from function.yml and did the same in my local repository. Now it is showing different error.\n\nsudo ./nuctl deploy --project-name cvat --path serverless/openvino/omz/intel/semantic-segmentation-adas-0001/nuclio     --volume serverless/openvino/common:/opt/nuclio/common --platform local\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": " , the mentioned deploy command isn't correct. The error message is clear here: . For volumes you have to use absolute paths:  Also you don't need to use  if your docker setup is correct (you are in docker group).", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 7, 2020", "body": ", Thanks. The model deployment is successful now. (I have not made changes to text-detection function, that is why error, rest all functions are modified as per your suggestion)Thanks a lot.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": " , could you please help me to test a fix in nuclio? Below I have attached nuctl for linux with a fix which just adds necessary permissions to /tmp/processor-config-* file (). Please revert all changes inside CVAT and try the binary to deploy functions. Does it work? I cannot check on my end unfortunately.", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 7, 2020", "body": " , i have reverted function.yml changes and used nuctl-latest-linux-amd64 to run the deploy.sh. It is throwing error.\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": " , thanks for the info. I have merged the PR. Now it should work. I hope nuclio team will be able to reproduce the issue and fix in future releases.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": " , are you sure that you used the attached binary? At least Git commit   in your logs looks strange.", "type": "commented", "related_issue": null}, {"user_name": "creativesiva", "datetime": "Aug 10, 2020", "body": " Sorry i did not noticed your question.\nYes, i have used (nuctl-latest-linux-amd64.zip)", "type": "commented", "related_issue": null}, {"user_name": "thangvip4321", "datetime": "Aug 12, 2020", "body": " i have updated my code, but the error still persist.\nHere's the log:I'm running on Ubuntu 18.04, docker version : 19.03.11 , nuctl: 1.4.16.\n", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 12, 2020", "body": " , indeed it is another issue: ", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 12, 2020", "body": " , let me check with nuctl 1.4.16 if I can reproduce the issue.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 12, 2020", "body": " , I cannot reproduce the problem. Need your help.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 3, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 3, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 3, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 3, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 5, 2020", "body": [], "type": "pull", "related_issue": "#1988"}, {"user_name": "nmanovic", "datetime": "Aug 5, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 6, 2020", "body": [], "type": "pull", "related_issue": "#1996"}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "issue", "related_issue": "nuclio/nuclio#1782"}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "pull", "related_issue": "#1996"}, {"user_name": "nmanovic", "datetime": "Aug 7, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": null, "datetime": "Aug 7, 2020", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "thangvip4321", "datetime": "Aug 12, 2020", "body": [], "type": "issue", "related_issue": "#2024"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1329", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nThe \"Preferences\" page is opened, by using the shortcut or clicking on the mic icon from the browser toolbar and selecting the \"Settings\" icon from the bottom left side of the doorhanger.\"Routines\" page doesn't have discrepancies compared to the mock-up from .There are some discrepancies between the mock-up and the actual display.Reproduced on Mac 10.14 and Win10x64 with Firefox Nightly 76.0a1 (2020-03-16).\nThe discrepancies between the mock-up and the actual display are as followed:Some of these can change, depending on the desired user experience. One example is the position of the \"+ New Routine\" button.Mock-up:\n", "type": "commented", "related_issue": null}, {"user_name": "Shulammite-Aso", "datetime": "Mar 18, 2020", "body": "Hi  I think Issue    Is a part of this,  or are they different?\nIf they are the same and you intend to close the other, then I would like to work on this.", "type": "commented", "related_issue": null}, {"user_name": "Shulammite-Aso", "datetime": "Mar 18, 2020", "body": "    I can start working on this after you give your thought on it.", "type": "commented", "related_issue": null}, {"user_name": "shreyaa-s-zz", "datetime": "Mar 18, 2020", "body": "Hey, can I work on this one?  \nThanks!", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 19, 2020", "body": "  has a more general approach for the \"Routines\" page display (the page itself seems a bit zoomed in compared to other pages), while this report looks at more specific changes that need to be done. Hope that clarifies things.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "May 20, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 23, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "danielamormocea", "datetime": "Jun 30, 2020", "body": [], "type": "pull", "related_issue": "#1787"}, {"user_name": "ianb", "datetime": "Jun 30, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ianb", "datetime": "Jun 30, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "Simpcyclassy", "datetime": "Jul 15, 2020", "body": [], "type": "issue", "related_issue": "#1328"}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/1327", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA Google Docs tab is opened.\nThe tab that the user wants to copy the image from is made active.Image is copied in the Google Doc successfully.Other information is copied or nothing happens.Reproduced on Mac 10.14 and Win10x64 with Firefox Nightly 76.0a1 (2020-03-18).\nIf the user right clicks with the mouse on the image and selects \"Copy Image\", then it is possible to copy the picture in the desired tab, e.g. Google Docs, Google Slides, Google Sheets, . (\"Paste\" command works for calmlywriter or Google Sheets. For Google Docs and Google Slides, the paste keyboard command should be used)", "type": "commented", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 18, 2020", "body": "I was able to reproduce the bug. May I try to fix it?\nUpdate: fixed", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 31, 2020", "body": "Verified on Mac 10.14 with Firefox Nightly 76.0a1 (2020-03-31).\n\"Paste\" commands are not working in Google Docs and Google Slides, like in , but image is copied with the paste keyboard shortcut.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 18, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 19, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "fleur101", "datetime": "Mar 22, 2020", "body": [], "type": "pull", "related_issue": "#1335"}, {"user_name": "ianb", "datetime": "Mar 30, 2020", "body": [], "type": "pull", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 30, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Mar 31, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/976", "issue_status": " Closed\n", "issue_list": [{"user_name": "Sav22999", "datetime": "Feb 7, 2020", "body": "I think it's a good idea add a voice command to open an extension just with the voice.\nFor example. I installed uBlock. If I want to open it, i should click the icon on address bar.\nSo, why don't add \"Open uBlock extension\" voice command to do the same thing?", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": "This is going to be hard on a couple levels – invoking another extension isn't clear, but it probably means click the accompanying toolbar button. That's not something that we easily have access to. But then once you've done that you have a popup, and an extension popup is impossible for us to control at all, so you would be forced to continue interacting with your mouse and not your voice.Because of this, we don't think this will be a useful experience for someone.Note that we are very open to allowing other extensions to be controlled by voice, if those extensions want to add code to their own extension to run those commands.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Feb 19, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/704", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 10, 2019", "body": "Mic permission are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA command is made beforehand.Button changes color when hovered.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": "Also the frowny face is not centered", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 10, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": "#719"}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 22, 2020", "body": [], "type": "unassigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/689", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": "Mic permissions are enabled.\n\"Allow\" or \"Don't Allow\" button from “Allow Firefox Voice to Collect Voice Transcriptions” pop-up needs to be clicked.\nA command is made beforehand.Emoticons change to blue and change size.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": "The colors do change, but it's so subtle and the lines are so thin that it's hard to tell. to give guidance on colors.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 11, 2019", "body": " from the mock-up, the emoticons are filled with blue and increased in size after being clicked and I assume this happens also on hover. But  can clarify exactly, if changes also apply when icons are hovered, or only when they are clicked, or both.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Dec 11, 2019", "body": "We'll want a slightly different visual for hover than clicked to ensure there change between the two states is obvious.Attached are SVGs with a hover state for the two icons using a background with a slight opacity.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit). Not sure if there are changes when emoticons are clicked though.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 10, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": "#730"}, {"user_name": "ianb", "datetime": "Dec 11, 2019", "body": [], "type": "pull", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 12, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Jul 22, 2020", "body": [], "type": "unassigned", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/678", "issue_status": " Closed\n", "issue_list": [{"user_name": "alexandra-martin", "datetime": "Dec 6, 2019", "body": "Mic permissions are enabled.\"Settings\" page is displayed without issues.Nothing happens.Reproduced on Mac 10.14.6 and Win10x64 with Firefox Nightly 72.0a1 (64-bit).\nReproduced also for the \"Feedback?\" link when the \"Doorhanger\" is first opened.\nNot reproduced in other \"Doorhanger\" instances, like \"Type Input\", \"In progress\", \"Sorry, there was an issue\", \"Thanks for the feedback\".\n\"Settings\" button and \"Feedback?\" link work, when the \"How was your last experience?\" is displayed in the \"Doorhanger\".\nOn Mac the \"Feedback?\" link can be accessed if the mouse is place on the text underline. (2nd gif)", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": "Myself, , and  haven't been able to reproduce this. Do you have any other thoughts  ?", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Dec 6, 2019", "body": "Unable to reproduce.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 9, 2019", "body": "Sorry, it seems something was not right from my part, although I don't know what that was. After several attempts where I made new profiles on both Win and Mac, it was reproducing every time. When I tried it again today, the link and button work.", "type": "commented", "related_issue": null}, {"user_name": "jenniferharmon", "datetime": "Dec 13, 2019", "body": "The issue with some buttons not being clickable was because of a hidden text input field in the view. Even though it wasn't visible it was overlaying some things and preventing clicks. This has been resolved.", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 16, 2019", "body": "Verified on Mac 10.14.6 and Win10x64 with Firefox Nightly 73.0a1 (64-bit).", "type": "commented", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 6, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 6, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 9, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "alexandra-martin", "datetime": "Dec 13, 2019", "body": [], "type": "issue", "related_issue": "#745"}, {"user_name": "alexandra-martin", "datetime": "Dec 16, 2019", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/514", "issue_status": " Closed\n", "issue_list": [{"user_name": "awallin", "datetime": "Oct 30, 2019", "body": "When Google returns a sidebar card for a search we should display in the doorhanger (and preference over the main column snippit).Examples utterances:", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 31, 2019", "body": "Should be fixed by ", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Nov 4, 2019", "body": "Cards are not/no longer being displayed for sidebar cards.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Oct 30, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 31, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "awallin", "datetime": "Nov 1, 2019", "body": [], "type": "issue", "related_issue": "#520"}, {"user_name": "awallin", "datetime": "Nov 4, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ianb", "datetime": "Nov 4, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/mozilla-extensions/firefox-voice/issues/486", "issue_status": " Closed\n", "issue_list": [{"user_name": "dholbert", "datetime": "Oct 24, 2019", "body": "STR:\n(1) Click the mic icon on your toolbar.\n(2) Watch the gear icon (and try to click it, pretending you're a bit slow with your mouse)ACTUAL RESULTS:\nThe gear moves between three different positions, due to the dialog resizing as its content changes.  (There are three phases: \"Listening\", \"One Second\", and then \"Sorry, there was an issue\".  Each phase has a different dialog height and a different position of the gear.)EXPECTED RESULTS:\nThe gear should be at a consistent position so that it is easy to click.", "type": "commented", "related_issue": null}, {"user_name": "xlisachan", "datetime": "Mar 8, 2020", "body": " I am an Outreachy applicant. Can I be assigned to this issue?", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 9, 2020", "body": "Unless  feels differently, I think there's nothing left to actually fix on this ticket. It does move around a little bit when the examples open up, but otherwise it's OK. Though if  wanted to move it to a more consistent location (e.g., the top of the popup) then we could reopen.", "type": "commented", "related_issue": null}, {"user_name": "awallin", "datetime": "Mar 9, 2020", "body": "Yea, it's ok to close.", "type": "commented", "related_issue": null}, {"user_name": "ianb", "datetime": "Oct 25, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "ianb", "datetime": "Dec 3, 2019", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "ianb", "datetime": "Mar 9, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/527", "issue_status": " Closed\n", "issue_list": [{"user_name": "Sanket307", "datetime": "May 1, 2020", "body": "Hello,when i try to run customobjectdetection on google colaboratory with gpu getting  below error.Please help me to figure it out.", "type": "commented", "related_issue": null}, {"user_name": "Terizian", "datetime": "May 18, 2020", "body": "I think custom_objects takes a CustomObjects object, not a dictionary as you have there.I would suggest commenting the two lines and writing this: ", "type": "commented", "related_issue": null}, {"user_name": "vinaykumarprabhu", "datetime": "May 30, 2020", "body": " ,the parameter that needs to be passed should to custom object should have the values mentioned for all the output categories supported by imageai, which in ur case is not i.e. u have specified only {'person':'valid'}\nInstead u should have done {'person': 'valid', 'bicycle': 'invalid', 'car': 'invalid', 'motorcycle': 'invalid', 'airplane': 'invalid', 'bus': 'invalid', 'train': 'invalid', 'truck': 'invalid', 'boat': 'invalid', 'traffic light': 'invalid', 'fire hydrant': 'invalid', 'stop sign': 'invalid', 'parking meter': 'invalid', 'bench': 'invalid', 'bird': 'invalid', 'cat': 'invalid', 'dog': 'invalid', 'horse': 'invalid', 'sheep': 'invalid', 'cow': 'invalid', 'elephant': 'invalid', 'bear': 'invalid', 'zebra': 'invalid', 'giraffe': 'invalid', 'backpack': 'invalid', 'umbrella': 'invalid', 'handbag': 'invalid', 'tie': 'invalid', 'suitcase': 'invalid', 'frisbee': 'invalid', 'skis': 'invalid', 'snowboard': 'invalid', 'sports ball': 'invalid', 'kite': 'invalid', 'baseball bat': 'invalid', 'baseball glove': 'invalid', 'skateboard': 'invalid', 'surfboard': 'invalid', 'tennis racket': 'invalid', 'bottle': 'invalid', 'wine glass': 'invalid', 'cup': 'invalid', 'fork': 'invalid', 'knife': 'invalid', 'spoon': 'invalid', 'bowl': 'invalid', 'banana': 'invalid', 'apple': 'invalid', 'sandwich': 'invalid', 'orange': 'invalid', 'broccoli': 'invalid', 'carrot': 'invalid', 'hot dog': 'invalid', 'pizza': 'invalid', 'donut': 'invalid', 'cake': 'invalid', 'chair': 'invalid', 'couch': 'invalid', 'potted plant': 'invalid', 'bed': 'invalid', 'dining table': 'invalid', 'toilet': 'invalid', 'tv': 'invalid', 'laptop': 'invalid', 'mouse': 'invalid', 'remote': 'invalid', 'keyboard': 'invalid', 'cell phone': 'invalid', 'microwave': 'invalid', 'oven': 'invalid', 'toaster': 'invalid', 'sink': 'invalid', 'refrigerator': 'invalid', 'book': 'invalid', 'clock': 'invalid', 'vase': 'invalid', 'scissors': 'invalid', 'teddy bear': 'invalid', 'hair dryer': 'invalid', 'toothbrush': 'invalid'}\nand this would have worked.And also as mentioned  if u specify detector.CustomObjects(truck=True) this method actually return a dictionary as mention\n\ned above.Hope this _helps.*\n", "type": "commented", "related_issue": null}, {"user_name": "Sanket307", "datetime": "Jun 9, 2020", "body": "Thanks for your comments.Issue solved, As i am replacing original script but it is not replaced in colab so it generate error.", "type": "commented", "related_issue": null}, {"user_name": "Sanket307", "datetime": "Jun 9, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/OlafenwaMoses/ImageAI/issues/391", "issue_status": " Closed\n", "issue_list": [{"user_name": "rEufrazio", "datetime": "Oct 29, 2019", "body": "The code I am trying to run is the one below, and it is almost identical to the one available at the documentation for \"Video and Live-Feed Detection and Analysis\":The error is as follows:I have already installed the latest version, v.2.1.5, and tried the YOLOv3 and TinyYOLOv3 models, both returning the same error. Also tried 2 different cameras.At first I thought it might be the threading problem, as I was using the multithreaded solution from imutils, but it turns out that the same thing is happening with the cv2.VideoCapture.Could you please help me out? I have no idea what is causing this problem.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": "Could someone please help me here? I'm two weeks in and still have the same problem.I recently discovered that this just happens when I use the  in any way, even if I assign it to another variable.Already updated and downgraded the library and dependencies to every possible option, and tried using the other \"for\" methods, with no success.If anyone could help me in any way I would be very grateful.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": "I managed to solve the problem. It looks like the function will always display this message when anything wrong happens inside the  call. The error was being caused simply because I was calling the  function incorrectly and not calling  inside the method.", "type": "commented", "related_issue": null}, {"user_name": "rEufrazio", "datetime": "Nov 6, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/2920", "issue_status": " Closed\n", "issue_list": [{"user_name": "gianturcom", "datetime": "Mar 14, 2018", "body": "\nOMNIBUSF7V2\n\n1.9.2\n NO_Go to CLI, execute  command copy its output to  and provide a link to a paste here.", "type": "commented", "related_issue": null}, {"user_name": "gianturcom", "datetime": "Mar 14, 2018", "body": "Zapped LED strip, not a software issue.", "type": "commented", "related_issue": null}, {"user_name": "gianturcom", "datetime": "Mar 14, 2018", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/216", "issue_status": " Closed\n", "issue_list": [{"user_name": "digitalentity", "datetime": "May 17, 2016", "body": "I'm thinking about supporting a cheap OpFlow ADNS3080 mouse sensor, but it should be connected to SPI bus. We have a few options:Ideas?", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "May 17, 2016", "body": "Hmm... can we make our own FC board with 5 UARTs please?\nBut seriously, I'v been thinking about similar problem and the best solution I've found is MSP hub/bridge/proxy with USB port.\nIt allows to connect multiple MSP devices, with different speeds and passes messages to FC and then passes answer to device that asked for info. If messages would be queued then response can be presented to proper external device. But building smth like this is as probable as FC with 5 UARTs", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "May 17, 2016", "body": "And from sensor docs: \"Distance from lens reference plane to surface: 2.4mm (typical)\". So what would be this thing useful for?", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": "If equipped with a lens it can be used up to several meters distance. ", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "May 17, 2016", "body": "And now we are talking! How do you think, what kinds of refresh rates would be required here? 10Hz?", "type": "commented", "related_issue": null}, {"user_name": "theArchLadder", "datetime": "May 17, 2016", "body": "Searching youtube for \"optical flow sensor\" gave some interesting videos.\n\nHard to tell but it does not look more accurate than a ublox M8N gps, but i guess that could be up to bad tuning. At least it allows indoor poshold...", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": " the more the better - we don't have an absolute position measurement with this sensor - only relative motion (velocity) in arbitrary units (if we have sonar sensor - in proper cm/s). I think at least 10Hz is absolutely required. 50Hz should give better results.\nThat's likely optical flow without any assistance. We will benefit from inertial position esitmation and quite possibly have much better results. One caveat - we need working sonar to correctly process optical flow data. On miniquads sonar is almost useless - noise from propellers overloads it at altitudes >50-70 cm.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": "Ok, got ADNS3080 working with Arduino. It really needs a good light, but it works. A first step to have optical flow in INAV.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": "Actually, in well-lit environments it's rather stable. I moved the sensor over my desk a few dozen times and accumulated the flow values - error is minimal. Now a big task of attaching it to test copter and make it fly!", "type": "commented", "related_issue": null}, {"user_name": "theArchLadder", "datetime": "May 17, 2016", "body": "I'm guessing it requires some clever code to compensate for when the copter tilts over and it look like the ground is moving, but the copter actually just changes attitude...?", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": "Yes, it certainly does.\nThe most challenging part is making it work with our position estimator and provide readings in meters. Also ADNS3080 will likely return invalid data when copter is yawing.", "type": "commented", "related_issue": null}, {"user_name": "martinbudden", "datetime": "May 20, 2016", "body": "This looks very interesting.The new sonar code allows easy implementation of other rangefinders, for example the  infrared rangefinder or the PulsedLight LIDAR-Lite rangefinder (although these may no longer be available now that Garmin has bought PulsedLight). These rangefinders avoid the problems sonar has with propeller noise.Anyway, it's on my todo list to implement drivers for these rangefinders, but in the medium rather than short term future (I have to get hold of one first).", "type": "commented", "related_issue": null}, {"user_name": "johnsilvester", "datetime": "Oct 19, 2016", "body": "How's the development on this going? Any luck with the ADNS3080 Optical Flow Sensor?\nExcited to hear!", "type": "commented", "related_issue": null}, {"user_name": "Jivefunk", "datetime": "Nov 4, 2016", "body": "Just created an account to follow this thread. I also need this implementation for my quad.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Nov 4, 2016", "body": "This is a very slo-mo task, sorry. Eventually we'll see opflow support, but probably not very soon.", "type": "commented", "related_issue": null}, {"user_name": "kitchung", "datetime": "Nov 4, 2016", "body": "I'd love to see this implemented too!", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Nov 4, 2016", "body": "Work is being slowly done here ", "type": "commented", "related_issue": null}, {"user_name": "kitchung", "datetime": "Nov 4, 2016", "body": " Thanks for the info, subscribing there now.", "type": "commented", "related_issue": null}, {"user_name": "bk79", "datetime": "May 5, 2017", "body": " playing with a cheap defective mouse from banggood I've found that inside it was using an a flow sensor marked KA8\n\nlooks like it's uses a serial protocol probably is worth trying using I2C protocol to read it", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 9, 2017", "body": " protocol of that sensor seems to be half-duplex SPI and it's possible to connect it to a dedicated SPI interface. Worth trying.", "type": "commented", "related_issue": null}, {"user_name": "bk79", "datetime": "May 11, 2017", "body": "  it's a two wire communication, spi half duplex shouldn't be at least with three?", "type": "commented", "related_issue": null}, {"user_name": "10ishq", "datetime": "Nov 17, 2017", "body": " , a separate board with open source hardware with 3080 optical flow sensor which could calculate horizontal movements as well as compensates for tilt using a gyroscope+magnetometer+accelerometer, with a separate processor, cortex M0 seems a good idea, just like ardupilot folks did with PX4Flow but cheaper. your thoughts?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 13, 2018", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 27, 2018", "body": "Automatically closing as inactive.", "type": "commented", "related_issue": null}, {"user_name": "markandkymward", "datetime": "Mar 11, 2019", "body": "Anyone interested in PX4FLOW sensor?", "type": "commented", "related_issue": null}, {"user_name": "vc", "datetime": "Mar 11, 2019", "body": "very important for me", "type": "commented", "related_issue": null}, {"user_name": "0120shrey", "datetime": "Apr 28, 2019", "body": "Hi! I'm here after reading about the experimental OPFLOW mode in the 2.0.0 release notes. After reading through a lot of threads on github, I now know that it needs an ADNS3080 sensor, and a sonar (I don't know which). But I couldn't find anything on how to actually connect and use it. I have a quad running on Omnibus F4 clone, which i'm willing to crash! Any support would be great!", "type": "commented", "related_issue": null}, {"user_name": "georgekucher", "datetime": "Apr 28, 2019", "body": "+1 I have matek f722 and us100 sonar and desire to buy opt flow sensor. Which one?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 27, 2019", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "whoim2", "datetime": "Aug 31, 2019", "body": "Тоже хочется поэкспериментировать с простыми и недорогими датчиками оптическими. Лазерный дальномер скоро приедет, буду пробовать использовать по i2c.\nОптический датчик хотелось бы подключить по spi либо тоже i2c, поскольку все uart заняты. Но это не критично, для помещения можно отключать gps и перенастраивать inav.\nГотов погрузится в компиляцию прошивки под свои omnibusf4prov3.\nЕсли могу чем то помочь - обращайтесь.", "type": "commented", "related_issue": null}, {"user_name": "whoim2", "datetime": "Aug 27, 2020", "body": "Where i may read about msp2 messages, opflow, sonar data?", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "May 17, 2016", "body": [], "type": "added", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jun 15, 2016", "body": [], "type": "pull", "related_issue": "#283"}, {"user_name": "martinbudden", "datetime": "Sep 8, 2016", "body": [], "type": "issue", "related_issue": "cleanflight/cleanflight#529"}, {"user_name": "stale", "datetime": "May 13, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 27, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "shellixyz", "datetime": "Mar 11, 2019", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "shellixyz", "datetime": "Mar 11, 2019", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "stale", "datetime": "Jun 27, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jun 27, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/14401", "issue_status": " Closed\n", "issue_list": [{"user_name": "GrzegorzKrug", "datetime": "Jan 26, 2021", "body": "In simple words I got class that loads params and weights. So everytime I recreate model, and load weights with method . But sometimes or maybe every 2 runs, error pops up for no reason.Get 2 questions about this. This most likely corelated with . Reason of error can be cause by model instantly saved after creating? or Ctrl+C is break some keras savings maybe?It does not matter if checkpoint file exists or not. Error still happens.\nAnd I will mention that I am not using any TF checkpoint features, it all happend within kerasWindows 10 x64", "type": "commented", "related_issue": null}, {"user_name": "akloss", "datetime": "Jul 22, 2021", "body": "I had a similar issue, however I'm working with a custom training loop and checkpoint.read/write instead of using the keras functions.\nMy problem was that the optimizer weights are only created when apply_gradients is called for the first time. So creating a checkpoint object (where the saved weights are then restored to) before that will result in a checkpoint that does not contain the optimizer variables and thus runs into an error when restoring from a file that does.A workaround that seems to work for me is manually calling optimizer._create_all_weights(var_list) before restoring the checkpoint, where var_list is the trainable_weights of the model.Maybe adding a call of optimizer._create_all_weights() in the compile or in save_weights function of keras Models would ensure that the created checkpoint is always complete with respect to the optimizer?", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Oct 26, 2021", "body": " Sorry for the late response. Is this still an issue for you?Can you please share a simple standalone code to reproduce the issue? The code you mentioned above is throwing errors. If possible please share a colab gist or jupyter notebnook. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Nov 2, 2021", "body": "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "GrzegorzKrug", "datetime": "Nov 3, 2021", "body": " sorry, not really, thats why I pust so broad description, it just points to specific state in github. I guess there is some problem with dependencies, did you used package versions from post above  maybe?", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Nov 3, 2021", "body": " I tried to run your code with recent TF/keras versions.  library is not getting imported.  is a gist for reference. Thanks!Can you please run your code with recent TF/keras versions?", "type": "commented", "related_issue": null}, {"user_name": "GrzegorzKrug", "datetime": "Nov 4, 2021", "body": "Check my github link, its my module with parameters named ", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Nov 20, 2021", "body": "I ran your code (including ) and I don't see any error as your are facing.  is a gist for reference. Thanks!Following is the outputIf you still have any question, please share a simple standalone code. It is hard to debug long code. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Nov 27, 2021", "body": "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Dec 4, 2021", "body": "Closing as stale. Please reopen if you'd like to work on this further.", "type": "commented", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Jan 26, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "saikumarchalla", "datetime": "Feb 4, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Oct 26, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Oct 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Nov 2, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Nov 3, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Nov 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Nov 27, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "google-ml-butler", "datetime": "Dec 4, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1797", "issue_status": " Closed\n", "issue_list": [{"user_name": "EliasRas", "datetime": "Nov 30, 2021", "body": "\nIf you zoom in when labeling an image, the movement of the cursor and the crosshair do not match. I haven't tested but the movement of the crosshair seems to be multiplied by the amount of zoom (e.g. 2x distance with 200% zoom).\nThe crosshair and the cursor would move together.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 3, 2021", "body": " Thank you for your report, I've created a JIRA ticket for this, hope it will be fixed soon.", "type": "commented", "related_issue": null}, {"user_name": "TrueWodzu", "datetime": "May 25, 2022", "body": "A new Label Studio user here. How can I enable a cross hair? I've searched everywhere but can't find it.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "May 27, 2022", "body": "  ---  is an image attribute.<Image crosshair=\"true\" ... >", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "May 27, 2022", "body": "btw this issue has been fixed, most likely in 1.4.1. And 100% in the  branch.", "type": "commented", "related_issue": null}, {"user_name": "TrueWodzu", "datetime": "May 27, 2022", "body": " Thank you, works like a charm.", "type": "commented", "related_issue": null}, {"user_name": "EliasRas", "datetime": "Nov 30, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 3, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "May 27, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/12969", "issue_status": " Closed\n", "issue_list": [{"user_name": "ZeroMaxinumXZ", "datetime": "Jun 16, 2019", "body": "Okay, so... I'm trying to implement a custom loss function in tf-keras.My tf version is 1.13.1, my keras version is 2.2.4, my OS is Linux Mint Ubuntu 16.04, and my python version is Anaconda Python 3.6.8Basically, this loss function is a Keras dense siamese neural network that I'm trying to get to learn a loss between y_true and y_pred. However I receive the above error every time the model above this loss func. compiles. The input_shape param is (1, 1920), and the code is as follows:Siamese Net Code:Error + Traceback:", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Jun 18, 2019", "body": " Please provide a standalone code to reproduce the issue. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "ZeroMaxinumXZ", "datetime": "Jun 20, 2019", "body": ", I fixed this issue by adding a tensorflow Session, and using the backend. Closing this.", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Jun 18, 2019", "body": [], "type": "added", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Jun 18, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ZeroMaxinumXZ", "datetime": "Jun 20, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/12042", "issue_status": " Closed\n", "issue_list": [{"user_name": "flyingduck92", "datetime": "Jan 15, 2019", "body": "Hello guys, I am trying to make a face recognition with VGG16 pretrained model in Keras.However the training did not increase the accuracy.\nI only have 46 data training, 26 validation, and 12 classesHow to increase the accuracy ? Thanks.This is my actual code:On the 7 epoch the accuracy still the same:", "type": "commented", "related_issue": null}, {"user_name": "flyingduck92", "datetime": "Jan 15, 2019", "body": "I try to change the batch size to 3. The loss become nan on epoch 30", "type": "commented", "related_issue": null}, {"user_name": "abtExp", "datetime": "Jan 16, 2019", "body": "Use softmax activation.", "type": "commented", "related_issue": null}, {"user_name": "flyingduck92", "datetime": "Jan 16, 2019", "body": " It is works. But the accuracy did not increase:", "type": "commented", "related_issue": null}, {"user_name": "colt18", "datetime": "Jan 16, 2019", "body": "You need to elaborate the \"face recognition\" that you are dealing with. Is it 12 classes with a total of 46 training and 26 validation images or are there a total of 72 images for each class. In either way your sample size is too small. In addition to this, CNN implementations that you use are better suited for large inter- class variation problems. For instance cat, dog and mouse classification would be more successful then classifying 3 persons with different traits. In the end faces will look a like and your model may not found meaningful features to classify. BTW your val accuracy is just random guess. Try pre-deep learning methods for face recognition such as Eigenfaces, LBP etc.", "type": "commented", "related_issue": null}, {"user_name": "flyingduck92", "datetime": "Jan 19, 2019", "body": " hi thanks for the response. Sir now i am trying to change it using dlib library for face recognition", "type": "commented", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 22, 2019", "body": "Thank you  . This issue is now closed. Thanks.", "type": "commented", "related_issue": null}, {"user_name": "qiaohong-li", "datetime": "May 13, 2019", "body": "This is because you do not call the preprocess_input method coming with VGG net.\nYou scale the input by divide it by 224 to the range 0-1.\nThis is not keras VGG preprocessing way.", "type": "commented", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 15, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 17, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 17, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 22, 2019", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "msymp", "datetime": "Jan 22, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1358", "issue_status": " Closed\n", "issue_list": [{"user_name": "Morgana025", "datetime": "Aug 25, 2021", "body": "\nWhen labeling an audio track, the audio pointer does not move where my mouse pointer is but always starts from the beginning. Therefore, it becomes impossible to reproduce only an already labeled region or to listen to a part of the waveform in the middle.\nSteps to reproduce the behavior:\nThe audio playing pointer moves to the position of my mouse pointer.\nThis problem does not exist in the ’interface preview‘ in the ‘Setup’, but it will appear when labeling.This issue was also mentioned here:  and ", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 26, 2021", "body": "\nMay I also ask a question about the audio tracks displaying settings? Is there a method to display only one channel for stereo audio?\nThe strangest thing is that now when I try to label the mono-waveforms, everything works as expected while the problem persists if I work with the stereo audios.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 29, 2021", "body": " Do you use Local Storages? If yes, try this PR please: ", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 29, 2021", "body": "unfortunately, there is no such method. You can simply convert your audios before the import using ffmpeg.", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 30, 2021", "body": " Thanks for the answers.\nI tried both with Local Storage and \"direct\" import. In the end, what seems to work for me, is to use direct import and mono-waveforms.", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 30, 2021", "body": "Today, I see this issue too:\"A page or script is accessing at least one of navigator.userAgent, navigator.appVersion, and navigator.platform. In a future version of Chrome, the amount of information available in the User Agent string will be reduced.\nTo fix this issue, replace the usage of navigator.userAgent, navigator.appVersion, and navigator.platform with feature detection, progressive enhancement, or migrate to navigator.userAgentData.\nNote that for performance reasons, only the first access to one of the properties is shown.\"I'm not very lucky!", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 30, 2021", "body": "Little update: The solution for my last problem was to change the browser from Chrome to Firefox", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 30, 2021", "body": "Yes, it looks like a browser specific thing.", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 25, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 30, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1263", "issue_status": " Closed\n", "issue_list": [{"user_name": "royschwartz2", "datetime": "Aug 9, 2021", "body": "\nThere is a cool feature that highlights the region on which the mouse cursor hovers. However, when zooming into an image, the highlight is thrown off target and shown in the wrong location.\nSteps to reproduce the behavior:\nAnnotate, zoom in, hover over the relevant annotation with the mouse, observe image.\nThe highlight should be in the correct place.\nThe highlighted annotations are for the right-most and left-most purple-annotated lesions in this medical image. Due to the zoom they appear below the originals and in different scale.\nAdd any other context about the problem here.", "type": "commented", "related_issue": null}, {"user_name": "Gondragos", "datetime": "Aug 26, 2021", "body": "We have made some fixes for that in LS 1.2, it might help.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 12, 2021", "body": " Does the latest version 1.3 fix this issues?", "type": "commented", "related_issue": null}, {"user_name": "royschwartz2", "datetime": "Oct 20, 2021", "body": "Yes it does", "type": "commented", "related_issue": null}, {"user_name": "royschwartz2", "datetime": "Aug 9, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "heartexlabs/label-studio-frontend#228"}, {"user_name": "makseq", "datetime": "Oct 12, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "royschwartz2", "datetime": "Oct 20, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/11481", "issue_status": " Closed\n", "issue_list": [{"user_name": "brunoalano", "datetime": "Oct 24, 2018", "body": "I'm trying to build a multiple-input single-output network using Keras (functional API), but I'm getting NaN loss for every epoch (validation loss too).I already tried with other optimizers like Adam and Nadam. I also tested using Tensorback, directly in the TF Session, and got the same issue.Info:", "type": "commented", "related_issue": null}, {"user_name": "brunoalano", "datetime": "Oct 24, 2018", "body": "I also tried to change it to a , and encode as one_hot of 2 classes (true / false). It also gives NaN as loss.", "type": "commented", "related_issue": null}, {"user_name": "kevinluvian", "datetime": "Mar 27, 2019", "body": "any updates regarding this issue?", "type": "commented", "related_issue": null}, {"user_name": "gabrieldemarmiesse", "datetime": "Oct 24, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 13, 2018", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "Harshini-Gadige", "datetime": "Nov 13, 2018", "body": [], "type": "added", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 16, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "fchollet", "datetime": "Jun 24, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/2653", "issue_status": " Closed\n", "issue_list": [{"user_name": "moon5648", "datetime": "May 7, 2016", "body": "From this page: It says that I can read labels not as integers, but as meaningful names by reading the batches.meta file in the dataset.But I just can't seem to find a way to read the file.I tried by modifying the keras/datasets/cifar10.py at , like below:But I get an error message like below:KeyError                                  Traceback (most recent call last)\n in ()\n24     return data, labels, label_names\n25\n---> 26 data, labels, label_names = load_data()\n27\n28 print(labels[10], label_names[7]) in load_data()\n20         bpath = os.path.join(path, 'batches.meta')\n21         data, labels = load_batch(fpath)\n---> 22         label_names = load_batch(bpath)\n23\n24     return data, labels, label_names/home/robotics/anaconda3/lib/python3.5/site-packages/keras/datasets/cifar.py in load_batch(fpath, label_key)\n17             d[k.decode(\"utf8\")] = v\n18     f.close()\n---> 19     data = d[\"data\"]\n20     labels = d[label_key]\n21KeyError: 'data'It doesn't look like I'm on the right track, so could anyone help me with this please??In addition, I'm trying to display how my model does via displaying the target image and my model prediction with meaningful names.How can I do those also??Thank you very much in advance.", "type": "commented", "related_issue": null}, {"user_name": "fchollet", "datetime": "May 7, 2016", "body": "The indices of each labels can be read on this page:0 is airplane, 1 is automobile, etc.", "type": "commented", "related_issue": null}, {"user_name": "sachin1066", "datetime": "Jun 19, 2017", "body": "using this code <<     >>\nfor cifar10 for python\ngot this error", "type": "commented", "related_issue": null}, {"user_name": "pGit1", "datetime": "Feb 15, 2018", "body": "How about for CIFAR 100? I need the Coarse and Fine labels", "type": "commented", "related_issue": null}, {"user_name": "dqii", "datetime": "May 13, 2018", "body": "It would be nice to have the coarse labels easily available as well.", "type": "commented", "related_issue": null}, {"user_name": "angpapadi", "datetime": "Sep 17, 2018", "body": "is it the same for cifar100? are they ordered too in the dataset page? e.g. ids 0-5 are aquatic mammals etc?", "type": "commented", "related_issue": null}, {"user_name": "itai12312", "datetime": "Dec 27, 2018", "body": "No, it doesn't look like that. Here is the list:\ncoarse_label = [\n'apple', # id 0\n'aquarium_fish',\n'baby',\n'bear',\n'beaver',\n'bed',\n'bee',\n'beetle',\n'bicycle',\n'bottle',\n'bowl',\n'boy',\n'bridge',\n'bus',\n'butterfly',\n'camel',\n'can',\n'castle',\n'caterpillar',\n'cattle',\n'chair',\n'chimpanzee',\n'clock',\n'cloud',\n'cockroach',\n'couch',\n'crab',\n'crocodile',\n'cup',\n'dinosaur',\n'dolphin',\n'elephant',\n'flatfish',\n'forest',\n'fox',\n'girl',\n'hamster',\n'house',\n'kangaroo',\n'computer_keyboard',\n'lamp',\n'lawn_mower',\n'leopard',\n'lion',\n'lizard',\n'lobster',\n'man',\n'maple_tree',\n'motorcycle',\n'mountain',\n'mouse',\n'mushroom',\n'oak_tree',\n'orange',\n'orchid',\n'otter',\n'palm_tree',\n'pear',\n'pickup_truck',\n'pine_tree',\n'plain',\n'plate',\n'poppy',\n'porcupine',\n'possum',\n'rabbit',\n'raccoon',\n'ray',\n'road',\n'rocket',\n'rose',\n'sea',\n'seal',\n'shark',\n'shrew',\n'skunk',\n'skyscraper',\n'snail',\n'snake',\n'spider',\n'squirrel',\n'streetcar',\n'sunflower',\n'sweet_pepper',\n'table',\n'tank',\n'telephone',\n'television',\n'tiger',\n'tractor',\n'train',\n'trout',\n'tulip',\n'turtle',\n'wardrobe',\n'whale',\n'willow_tree',\n'wolf',\n'woman',\n'worm',\n]mapping = {\n'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n'household electrical device': ['clock', 'computer_keyboard', 'lamp', 'telephone', 'television'],\n'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n'large man-made outdoor things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor'],\n}", "type": "commented", "related_issue": null}, {"user_name": "adam-dziedzic", "datetime": "Nov 19, 2020", "body": "I extended the mappings between fine and coarse labels by  to other useful mappings between fine (classes) and coarse labels (super classes) for CIFAR100: An example of a mapping from id of a fine label to id of a coarse label:", "type": "commented", "related_issue": null}, {"user_name": "Sehaba95", "datetime": "Oct 9, 2021", "body": "Here is a CIFAR-100 labels, since, I was facing problems to find them, I will share them in here:You can get the label of a given class with this line of code:I wish that will help.", "type": "commented", "related_issue": null}, {"user_name": "fchollet", "datetime": "May 7, 2016", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/keras-team/keras/issues/11819", "issue_status": " Closed\n", "issue_list": [{"user_name": "marc88", "datetime": "Dec 7, 2018", "body": "I get the following error after running an embedding layer as;Each datapoint here is a number(index). Upon checking indices[6,4] I found the followingar_train_data is an array of shape (162896, 5) where each value is  between [0, 23624).\nThe training stops towards the end of the first epoch with the error above.Am amazed! 5088 is no where out of range for [0, 23624).  Can anyone suggest what could be the issue here?\nPlease suggest if additional code snippets are required for clarity.\nThe model roughly goes below as:Full error trace:Keras version - 2.2.4\ntensorflow version: 1.5.0Regards", "type": "commented", "related_issue": null}, {"user_name": "SriRangaTarun", "datetime": "Dec 8, 2018", "body": "You can try increasing vocab_size in the Embedding layer from 23624 to 23625.", "type": "commented", "related_issue": null}, {"user_name": "Surojit88", "datetime": "Dec 9, 2018", "body": "  It's a common practice to have the embeddings as vocab_size+1 and I've just done that by having my embeddings at 23624 with a vocab size of 23623. So, the problem doesn't lie there. : Is there anyway you can help? Apologies but, I couldn't understand the meaning of the tag added. Is this a bug that has something to do with these versions of Keras/TF?", "type": "commented", "related_issue": null}, {"user_name": "gabrieldemarmiesse", "datetime": "Dec 9, 2018", "body": "There is a description for each tag that you can see if you put your mouse on it.This means that this may be a bug, but I'm not sure. It could be a user error. Someone needs to investigate to go to the bottom of it. (not necessarily me since I don't have the time to investigate in depth each issue in keras).", "type": "commented", "related_issue": null}, {"user_name": "mkaze", "datetime": "Dec 12, 2018", "body": " It seems there is (are) an element(s) with index  in the training data: . Use  to find out where they are: .Have you used an ?", "type": "commented", "related_issue": null}, {"user_name": "marc88", "datetime": "Dec 12, 2018", "body": "  As suggested above, that's not the case", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Mar 13, 2019", "body": "\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Could you mention which TF version you are using? Thanks!", "type": "commented", "related_issue": null}, {"user_name": "completelyboofyblitzed", "datetime": "Mar 14, 2019", "body": "I'm having the same issue:increasing the vocabulary length doesn't helpWhat could be the issue in this case?", "type": "commented", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 7, 2019", "body": "It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from . We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.", "type": "commented", "related_issue": null}, {"user_name": "gabrieldemarmiesse", "datetime": "Dec 8, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Mar 13, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Mar 13, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "Mar 13, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "jvishnuvardhan", "datetime": "May 7, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1241", "issue_status": " Closed\n", "issue_list": [{"user_name": "royschwartz2", "datetime": "Aug 5, 2021", "body": "\nWhen making a brush stroke, part of the brush stroke disappears while working on the stroke, only to appear again when releasing the mouse click.\nSteps to reproduce the behavior:\nCreate a brush stroke of more than just a click.\nThe brush stroke (mask) should appear throughout the annotation process. Otherwise, this is quite confusing.\nIf applicable, add screenshots to help explain your problem.\nAdd any other context about the problem here.", "type": "commented", "related_issue": null}, {"user_name": "nicholasrq", "datetime": "Aug 26, 2021", "body": "successfully reproduced it. please, expect a fix in the next release", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Oct 14, 2021", "body": "Hi,  !\nCan you check PR  ? The problem was in drafts saving in the middle of drawing process, so now I changed the way it triggers and stores, so it should not interfere with drawing no more. Will be merged soon after checks.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 27, 2021", "body": " The fix is in the master branch. Feel free to reopen this issue if the problem persists.", "type": "commented", "related_issue": null}, {"user_name": "royschwartz2", "datetime": "Aug 5, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Oct 14, 2021", "body": [], "type": "pull", "related_issue": "#1609"}, {"user_name": "makseq", "datetime": "Oct 15, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 16, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 27, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1096", "issue_status": " Closed\n", "issue_list": [{"user_name": "fanweiya", "datetime": "Jun 24, 2021", "body": "thanks label-studio,very good label annotation tools,please add slide the mouse over the picture and zoom in and out of the picture.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 23, 2021", "body": " it's already there. Please add zoom=\"true\" and zoomControl=\"true\" to  tag.\nHope it solves your issue.For more info:\n", "type": "commented", "related_issue": null}, {"user_name": "fanweiya", "datetime": "Jun 24, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 23, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/959", "issue_status": " Closed\n", "issue_list": [{"user_name": "danielspicar", "datetime": "May 21, 2021", "body": "\nIn v. 1.0.1 I could draw a rectangle (bounding box) on top of another bounding box. In v 1.0.2 this sopped working. Using the same procedure I cannot draw a new bounding box.\nWhen drawing the second rectangle on top of the first while pressing the CTRL key, I should be able to draw the second rectangle just like the first one and it should be placed \"on top\" of the first rectangle.\nI think this hotkey was undocumented. However is is extremely important for my use case to be able to draw bounding boxes on top or inside of each other.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "May 22, 2021", "body": " Thank you for your report, I've reproduced this bug too.\n", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 1, 2021", "body": "This bug was fixed in . Feel free to reopen if you have this problem again.", "type": "commented", "related_issue": null}, {"user_name": "danielspicar", "datetime": "May 21, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "May 22, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "May 22, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "makseq", "datetime": "Jun 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 1, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/862", "issue_status": " Closed\n", "issue_list": [{"user_name": "Nuccy90", "datetime": "Apr 29, 2021", "body": "I'm trying to build from source because I want to get a fix for an issue with the mouse pointer in audio playback. I get the following error:I also tried to run LS afterwards and I get this error:This happens on Ubuntu 20.04 and I am installing in a blank environment.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 29, 2021", "body": "Try this one:", "type": "commented", "related_issue": null}, {"user_name": "niklub", "datetime": "May 7, 2021", "body": " please reopen if Max's suggestion won't work for you", "type": "commented", "related_issue": null}, {"user_name": "arianpasquali", "datetime": "May 13, 2021", "body": "Same error here on Ubuntu 20.04.I run pipdeptree to check conflicts and this is what I get", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "May 13, 2021", "body": " It looks strange, because we don't use Jinja2.. Are you on a fresh python env? Could you check that LS was installed from scratch into the new env?", "type": "commented", "related_issue": null}, {"user_name": "Nuccy90", "datetime": "Apr 29, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "niklub", "datetime": "May 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/757", "issue_status": " Closed\n", "issue_list": [{"user_name": "suvojit-0x55aa", "datetime": "Apr 6, 2021", "body": "Having to hold and drag the mouse during bounding box annotation can be a bit jarring specially if zoom level is not set correctly.An option to use two click bounding box drawing where user will click on two diagonal points  of the bounding box.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 1, 2021", "body": "This bug was fixed in . Feel free to reopen if you have this problem again.", "type": "commented", "related_issue": null}, {"user_name": "suvojit-0x55aa", "datetime": "Apr 6, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 6, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "removed this from", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nicholasrq", "datetime": "May 31, 2021", "body": [], "type": "pull", "related_issue": "heartexlabs/label-studio-frontend#199"}, {"user_name": "makseq", "datetime": "Jun 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 1, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "niklub", "datetime": "Mar 9, 2022", "body": [], "type": "removed this from", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/424", "issue_status": " Closed\n", "issue_list": [{"user_name": "p-sodmann", "datetime": "Sep 17, 2020", "body": "\nEspecially when modifying existing roi polygons, e.g. prelabeling it is not possible (?) to  add or remove points.\nadd ctrl click on polygon points to remove points and alt click to add new points when clicking on a line.\nalso please add different cursors: ", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Sep 17, 2020", "body": "Hi! You can always add a point on any edge of polygon — there is green ghost point appears under the cursor when you mouse over the edge. Do you have it?\nBut currently you can not remove point from polygon, this is important lack of functionality, we'll fix it soon. Our plan to delete point by double click on it — pretty common behaviour. Does it fit?", "type": "commented", "related_issue": null}, {"user_name": "p-sodmann", "datetime": "Sep 17, 2020", "body": "That would work as well.", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Jun 7, 2021", "body": "fixed by  some time ago\nnow you can delete polygon point with double click", "type": "commented", "related_issue": null}, {"user_name": "p-sodmann", "datetime": "Sep 17, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Oct 5, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "niklub", "datetime": "Mar 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Jun 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/423", "issue_status": " Closed\n", "issue_list": [{"user_name": "tlaud", "datetime": "Sep 14, 2020", "body": "\nOne of the great features of Label Studio is the possibility of keyboard-only labeling by relying on hotkeys. Another great feature is the machine learning backend for label suggestions. In a use case, in which not only a bounding box is labeled but also classes are annotated, it would be nice to toggle the bounding box - which might occlude image parts relevant to the classes labeling - on/off with a hotkey\nA hotkey which toggles the visibility of all bounding boxes for an image.Thanks :)", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Sep 14, 2020", "body": "Hi! That's good and simple idea, thank you! First of all we have to add button to hide all regions and then hotkey can be assigned to it)\nWe'll do this soon, I'll update you here.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 8, 2021", "body": "Please, try alt + h.", "type": "commented", "related_issue": null}, {"user_name": "tlaud", "datetime": "Sep 14, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Oct 5, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "niklub", "datetime": "Mar 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 5, 2021", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 7, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 8, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/825", "issue_status": " Closed\n", "issue_list": [{"user_name": "SN4KEBYTE", "datetime": "Apr 20, 2021", "body": "\nIn the audio task, when I click the audio waveform, the audio playing pointer does not move to the position of my mouse pointer, resulting in that every time I want to listen to a part in the middle, I always listen from the beginning.\nSteps to reproduce the behavior:\nThe audio playing pointer move to the position of my mouse pointer.\nIf applicable, add screenshots to help explain your problem.\nThis problem does not exist in the ’interface preview‘ in the ‘Setup’, but it will appear when labeling.This issue was also mentioned here: ", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 20, 2021", "body": "Please, try the latest commits from the master branch. I hope it's fixed there.", "type": "commented", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 20, 2021", "body": "Thank you for response, I'll try it!", "type": "commented", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 21, 2021", "body": "I've tried the latest commits, but the bug is still active :(", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 23, 2021", "body": " Could you send your  page? (or )", "type": "commented", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 27, 2021", "body": "\n{\n\"package\": {\n\"version\": \"1.0.1\",\n\"short_version\": \"1.0\",\n\"latest_version_from_pypi\": \"1.0.1\",\n\"latest_version_upload_time\": \"2021-04-05T19:13:41\",\n\"current_version_is_outdated\": false\n},\n\"backend\": {\n\"commit\": \"71278b\",\n\"date\": \"2021-04-05 11:30:54 -0700\",\n\"branch\": \"master\",\n\"version\": \"1.0.0+87.g71278b0b\"\n},\n\"label-studio-frontend\": {\n\"commit\": \"5164462ced2fe8a0bbdd7cd9c4a5bec3772577ab\",\n\"branch\": \"master\",\n\"date\": \"2021-03-31T10:57:00Z\"\n},\n\"dm2\": {\n\"commit\": \"7751a996682f145d651123af27286f4f392c293c\",\n\"branch\": \"master\",\n\"date\": \"2021-04-02T13:15:22Z\"\n}\n}", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 27, 2021", "body": "I see you are using LS backend from 05 April, but we fixed it about 14 days ago: I recommend you to use master branch to solve this trouble.", "type": "commented", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 28, 2021", "body": "Thank you, this problem was solved!\nI've tested it on WSL2 Ubuntu 20.04, but on Windows 10 I had an error while creating project.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 28, 2021", "body": " What problem did you have on Windows?", "type": "commented", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 25, 2021", "body": "Hi, I'm getting the same error :(\nMy environment is:I tried to downgrade to version 1.1.0 but the issue is still there. Do you have any suggestions?\nThanks", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 25, 2021", "body": " Coud you record a video with opened browser console and logs?", "type": "commented", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 20, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 28, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "Morgana025", "datetime": "Aug 25, 2021", "body": [], "type": "issue", "related_issue": "#1358"}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/409", "issue_status": " Closed\n", "issue_list": [{"user_name": "p-sodmann", "datetime": "Aug 28, 2020", "body": "When using the ROI Polygon Tool, it is frustrating to have no visual feedback weather you will drag the whole polygon or change a single point. This often results in me dragging the whole polygon instead of a single point as intended.Change the cursor icon depending on the action that will happen when I drag at the current position.\nFor example curser:grab if I would move the whole polygon and cursor:move when I would move a single polygon corner.\nMaybe also change the cursor to cursor:crosshair while drawing new points / a new polygon.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Aug 28, 2020", "body": " I think you can implement this very quickly.", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Aug 28, 2020", "body": "Yes, sounds very easy and helpful, just to change  in one place. Will deploy soon, thanks!", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 21, 2020", "body": "It's fixed after 0.8.1 release.", "type": "commented", "related_issue": null}, {"user_name": "p-sodmann", "datetime": "Aug 28, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "p-sodmann", "datetime": "Sep 17, 2020", "body": [], "type": "issue", "related_issue": "#424"}, {"user_name": "makseq", "datetime": "Dec 21, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/306", "issue_status": " Closed\n", "issue_list": [{"user_name": "RenJinan", "datetime": "Jun 2, 2020", "body": "\nIn the audio task, when I click the audio waveform, the audio playing pointer does not move to the position of my mouse pointer, resulting in that every time I want to listen to a part in the middle, I always listen from the beginning.\nthe audio playing pointer move to  the position of my mouse pointer.\nThis problem does not exist in the ’interface preview‘ in the ‘Setup’, but it will appear when labeling.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jun 2, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Jun 5, 2020", "body": "Hey, ! It sounds strange, especially if it works in preview. Could you please provide your config? How long is your audio? And what the error do you see? Screenshots or browser console output would be very helpful.", "type": "commented", "related_issue": null}, {"user_name": "RenJinan", "datetime": "Jun 6, 2020", "body": "Thank you very much for your reply! My configuration file is the same as the official sample, but the problem still exists. I made a GIF to show this problem and look forward to your help.\n", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jun 8, 2020", "body": " have you any ideas?", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Jun 9, 2020", "body": "No ideas sadly :(\n do you have errors in browser console?\nI checked this config in LSF, it works well. I'll check in LS soon.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 8, 2020", "body": " have you tried to use AudioPlus tag?", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Sep 28, 2020", "body": "I hope it's fixed after 0.7.3 release.", "type": "commented", "related_issue": null}, {"user_name": "RenJinan", "datetime": "Jun 2, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "RenJinan", "datetime": "Jun 2, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Jun 5, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Sep 28, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "SN4KEBYTE", "datetime": "Apr 20, 2021", "body": [], "type": "issue", "related_issue": "#825"}, {"user_name": "Morgana025", "datetime": "Aug 25, 2021", "body": [], "type": "issue", "related_issue": "#1358"}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/265", "issue_status": " Closed\n", "issue_list": [{"user_name": "johann-petrak", "datetime": "Apr 19, 2020", "body": "Create text classification project that shows text to the annotator. The text also includes URLs or terms that the annotator may want to look up.\nHowever normal copying the string to look up is not possible, presumably because the code that normally handles text selection in order create a sequence annotation (like for NER) is active.\n\"Normal copying\" would be that the  user marks the text to copy with the mouse, releases the mouse button (the marked text span stays highligthed) then presses Ctrl-C or right-clicks and chooses \"Copy\". This is not possible because the highlighted text immediately gets de-selected when the mouse button is released.A workaround is to press Ctrl-C while the mouse button for selecting the text is still being held, but this is unintuitive and hard to discover.", "type": "commented", "related_issue": null}, {"user_name": "johann-petrak", "datetime": "Apr 19, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "johann-petrak", "datetime": "Apr 19, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "hlomzik", "datetime": "May 20, 2020", "body": [], "type": "pull", "related_issue": "heartexlabs/label-studio-frontend#84"}, {"user_name": "makseq", "datetime": "Jun 1, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/105", "issue_status": " Closed\n", "issue_list": [{"user_name": "vijaygill", "datetime": "Oct 22, 2019", "body": "\nMany times I end up dragging whole polygon where I just want to move a point. In some cases it is very difficult to make it fit back in on the image.\nAllowing user to have a flag in config.json / config.xml to allow dragging of polygon when only CTRL key is pressed. This way I will be moving stuff when I am sure I want it to move.I am using only polygons for now so my problem may seem to be related to those only. But you can evaluate it for other shapes too and decide.\nNone TBH..\nNone", "type": "commented", "related_issue": null}, {"user_name": "shevchenkonik", "datetime": "Oct 25, 2019", "body": "Yes, your point is scalable for other labeling types (for example bounding boxes) and I think that we implement this one on the next versions of LS. Currently, we are focused on features and bugs of polygons, bounding boxes, key points, image segmentation.If you can describe your workflow with polygons and Image zoom, then we'll implement more correct UI.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 25, 2019", "body": " \nI think this issue can have something in common with my list I wrote in :I think we should make here some conclusions what would be the best. I am planning to implement some improvements to  next month.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 24, 2020", "body": "Polygons are strongly reworked and now it's much more comfortable.", "type": "commented", "related_issue": null}, {"user_name": "vijaygill", "datetime": "Oct 22, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "vijaygill", "datetime": "Oct 22, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 24, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/apache/airflow/issues/25861", "issue_status": " Open\n", "issue_list": [{"user_name": "hwooson12", "datetime": "Aug 22, 2022", "body": "2.3.3On DAG Dependencies, all nodes don’t have links even though they seem clickable.The mouse pointer turns clickable(sorry, I couldn't capture the mouse pointer) and it activates the node. However it cannot be actually clicked.Each node has its reference so that having a link to move to a page that belongs to it could be better for users in terms of UX.Linux, but it's not depending on OSOtherDeployed with customized helm charts", "type": "commented", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Aug 22, 2022", "body": "Yes, I've wanted to add links and make this page more integrated into the app.That's what I've done over in . Right now, we're only showing dataset dependencies there. But we may expand it to be a more dynamic replacement of tis view.", "type": "commented", "related_issue": null}, {"user_name": "hwooson12", "datetime": "Aug 22, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "uranusjr", "datetime": "Aug 22, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "bbovenzi", "datetime": "Aug 22, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/27", "issue_status": " Closed\n", "issue_list": [{"user_name": "VictorAtPL", "datetime": "Aug 17, 2019", "body": "Hi,When can we expect the guide how to extend annotations types in your app?I need to annotate the part of pictures, in general:\nimage; many polygons; each polygon should have label like text/logo.I also need an annotation like image to text (for OCR fine-tuning).I am able to try writing the code myself, although I need your guidance. Could you tell me what files should I modify to add more annotation types?", "type": "commented", "related_issue": null}, {"user_name": "deppp", "datetime": "Aug 19, 2019", "body": "Hey Yep, we're working on updating the documentation as well as releasing a \"Getting Started\" guide. It will be covering both embedding and extending the functionality with additional data types.There is an initial implementation of the polygons, try out this config:For an image to text, you can use a config like this:To add more annotation types, I'd first check how other types are implemented  for example the one for .Let me know right away if you get stuck somewhere. We're glad to help!", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Aug 19, 2019", "body": ",\nThank you for your response. I will test it till Wednesday and get back here to presumably ask more questions.If it's ok for you, please do not close this issue for now.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Aug 25, 2019", "body": " \nSorry guys, I didn't have time to look at it, but will in next week for sure.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 11, 2019", "body": "I made some little tests and it looks like the polygons are now not usable. I will make a list of changes I need to introduce to make it usable in my use case and I will try to address them.Could you tell me what was the purpose of  in ? Except making Polygon labels I need also to write some comment what is inside that (it varies every label, because it's text). Should I use  or develop another type in. e. ?", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 13, 2019", "body": " \nMy question above is still up to date (about the ).I am now struggling with another problem. I try to run \"frontend\" (written in React) live, so not the compiled version run by backend. I try to do it by running the backend, and then executing  with . The frontend starts, but it looks like it's not in the production, because I can not see \"go to task list\" link.If you have some time, please answer me to these two questions.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 13, 2019", "body": "Well, so how can I debug the frontend? If I need to compile it every time and run it via backend to see the effect, it would take ages to progress with the code.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 14, 2019", "body": "I do not understand the way, how files with task and completions for it are created.When I place one task in  file and make a submit for it in frontend, it creates a file with completions, but without ,  and  keys. File such as this cannot be loaded later via frontend for in. e. update annotations.What's wrong with it? The task file should be created even before making completion, or the first completion should be saved with the fields containing information about task?It seems like there is some bug, or I use the app not how I should be.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 15, 2019", "body": "I hope  fixes the problem described above.Unfortunately I will have to look at the , because it looks like they are not loaded in the task view mode. It has the highest priority on my  list.", "type": "commented", "related_issue": null}, {"user_name": "deppp", "datetime": "Sep 16, 2019", "body": "Yep, normalization is a bit misleading name right now, as it's just a way to add a piece of extra information to the labeled part of data (entity). Alternatively, based on your use case, you can create a new tag that subclasses  and adds an input for that additional info you need.When you're saying that it's not usable, can you describe what's going wrong when you're using it?", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Sep 28, 2019", "body": "\nAs you can see the main problem is that the annotations doesn't load after save.Here are more code which could help in debugging the problem:\n", "type": "commented", "related_issue": null}, {"user_name": "deppp", "datetime": "Oct 4, 2019", "body": " we've recently released an updated version for the Polygons, as well as the website, here some links you may find useful:Please let me know if the updated version works well for you.", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 4, 2019", "body": "\nI've pulled your code and tested it deeply.I think a couple of things doesn't work at all, a couple of them are not intuitive and a couple of things are lack.Here is detailed list:Overall, your app is the best open-source annotation tool in the market, but for my case I have to work a little bit on details of polygon labels.The bugs are presented in following video:\nI will take care of the improvements & new features in the end of this month, or next month. But for the sake of your users use-experience, it would be nice if you fixed the bugs.", "type": "commented", "related_issue": null}, {"user_name": "deppp", "datetime": "Oct 7, 2019", "body": " Thanks for such a thorough testing and bug reporting! That's really helpful!We'll be looking into the bugs you've listed this week. Can I please ask you to add them as separate issues so that we have a more natural way to track what gets fixed and when. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 10, 2019", "body": "\nSure, but next week. I am quite busy nowadays. I hope you can wait.", "type": "commented", "related_issue": null}, {"user_name": "shevchenkonik", "datetime": "Oct 18, 2019", "body": "\nThank you, your bug report is very helpful. Every item of your report will be a new issue. But I will write on this issue if the bug or feature will be ready.\nWill be fixed in patch release 0.2.1-4: ", "type": "commented", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 21, 2019", "body": "\nI've added three bugs listed above as new issues. The rest (2 bugs) have already corresponding issues and as you stated, one of them is already fixed.I won't create issues for improvements & new features, because I suppose you've got a plenty of work even without it. I will try to do it on my own at the end of the month or next month.I consider this issue as resolved, so I am closing this issue. Thank you.", "type": "commented", "related_issue": null}, {"user_name": "shevchenkonik", "datetime": "Aug 22, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 21, 2019", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "VictorAtPL", "datetime": "Oct 25, 2019", "body": [], "type": "issue", "related_issue": "#105"}, {"user_name": "ReDeiPirati", "datetime": "Dec 6, 2019", "body": [], "type": "issue", "related_issue": "#142"}, {"user_name": "snyk-bot", "datetime": "Apr 27, 2022", "body": [], "type": "pull", "related_issue": "gamerXI1/label-studio#12"}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4464", "issue_status": " Open\n", "issue_list": [{"user_name": "hacheybj", "datetime": "Feb 3, 2021", "body": "\nWhen working with large flows, the flow-editor gives us a \"birds-eye view\" of the flow, which tends to make the nodes very small and unreadable.Also, the current mouse scroll-in implementation isn't super friendly.\nI've thought of a few ideas, but I'm definitely open to more.\nAlternatively, and probably the better solution would be to fix the scroll in implementation\n", "type": "commented", "related_issue": null}, {"user_name": "hacheybj", "datetime": "Feb 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Feb 3, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/3579", "issue_status": " Open\n", "issue_list": [{"user_name": "flacomalone", "datetime": "Nov 12, 2020", "body": "Hi,When running CARLA server, if I try to use the mouse to drag and change the orientation of the camera (not the position), it will, but so much I have no control on it. Is there any way to change mouse sensibility? I don't remember having this issue in previous versions of CARLA", "type": "commented", "related_issue": null}, {"user_name": "Axel1092", "datetime": "Mar 3, 2021", "body": "Hi @davidtena98, sorry for the late response. We don't have means to change the sensitivity of the server camera. The rotation will be larger the greater the fps you get at any given point.", "type": "commented", "related_issue": null}, {"user_name": "uebian", "datetime": "Jul 18, 2021", "body": "Is there any options to set the sensitivity of the server camera?", "type": "commented", "related_issue": null}, {"user_name": "satyamjay-iitd", "datetime": "Feb 4, 2022", "body": "I have the same issue. I am running on ubuntu 18.04 with -vulkan. Mouse sensitivity is too high to navigate around the world. W/A/S/D works fine though. Was anyone able to fix this?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "Axel1092", "datetime": "Mar 3, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "corkyw10", "datetime": "Mar 3, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5584", "issue_status": " Open\n", "issue_list": [{"user_name": "bassamtantawi-botpress", "datetime": "Oct 19, 2021", "body": "\nI followed the below link to use an upload control:\nIt worked successfully but I found couple of problems:\n", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Oct 26, 2021", "body": "The worst problem with the document upload currently in use is that you have no way of easily linking the uploaded document to the uploader.I think either the naming should be very clear or the document should be stored against unique user id in the database for easier querying.", "type": "commented", "related_issue": null}, {"user_name": "codeilm", "datetime": "Jun 21, 2022", "body": "Hi,\nMay I know, how to use the uploaded image by user?Means, I want to send the uploaded file to server using api.", "type": "commented", "related_issue": null}, {"user_name": "bassamtantawi-botpress", "datetime": "Oct 19, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Oct 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "EFF", "datetime": "Nov 1, 2021", "body": [], "type": "pull", "related_issue": "#5647"}]},
{"issue_url": "https://github.com/botpress/botpress/issues/5108", "issue_status": " Open\n", "issue_list": [{"user_name": "hacheybj", "datetime": "Jun 15, 2021", "body": "\nThe auto-complete in a nodes Edit Action is too small with nested scroll bars.Additional note: the nested scrollbar cannot be clicked for scroll as it makes the \"dropdown\" disappear. Only use of the mouse scroll is possible.\nWider value fields, and potentially a taller popup window.\n", "type": "commented", "related_issue": null}, {"user_name": "EFF", "datetime": "Jun 19, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Jun 29, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/botpress/botpress/issues/4742", "issue_status": " Open\n", "issue_list": [{"user_name": "dvncan", "datetime": "Mar 23, 2021", "body": "\nThe code editor's file view does not change when toggling between a local & global file of the same name. The bug causes confusion when copying a global file to a local bot or vise versa. As the tab which has the file name does not denote if it is a global or local file, you must close the tab and be sure to open the correct file before editing.\nEither two tabs open, one for each copy of the file. A denotation will have to accompany the file title.\nor\nThe file tab toggles between the two files being selected, and the title of the tab does not change.\nLocal file (clicking both)\n\nGlobal file (clicking both)\n\nDemo deployment", "type": "commented", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 26, 2021", "body": "With the new tab support in code editor, maybe opening it in a seperate tab then appending the file paths to the title to differentiate them.", "type": "commented", "related_issue": null}, {"user_name": "dvncan", "datetime": "Mar 26, 2021", "body": "that would work, something like \"global/hitlnext.json\" right next to \"local/hitlnext.json\".. You only really need to differentiate the second tab, but right now it makes me confused as to which file I am editing.", "type": "commented", "related_issue": null}, {"user_name": "dvncan", "datetime": "Mar 23, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "Michael-N-M", "datetime": "Mar 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "charlescatta", "datetime": "Oct 6, 2021", "body": [], "type": "issue", "related_issue": "#5467"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#5632"}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5362", "issue_status": " Open\n", "issue_list": [{"user_name": "Chup123", "datetime": "Apr 13, 2022", "body": "CARLA version: 0.9.12\nPlatform/OS: ubuntu 18.04\nProblem you have experienced: When I move the mouse in any direction, the camera will always do the same thing. It will look at the ground and spin to the right. Even when I try to move the mouse somewhere else.\nWhat you expected to happen: When i point the mouse up, I except the camera to move to the position I put my mouse, just like in a video game\nSteps to reproduce: Move the mouse in any map. It will always move in this direction\nOther information (documentation you consulted, workarounds you tried): I tried to lower to mouse sensitivity in the DefaultInput.ini file. This made it much more clear that the mouse always moves in this direction", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/carla-simulator/carla/issues/5100", "issue_status": " Open\n", "issue_list": [{"user_name": "kanji95", "datetime": "Jan 21, 2022", "body": "I am having trouble getting the 3D world coordinates in CARLA. Basically, I use mouse click to select the destination point (pixel coordinate) in the front-view RGB camera image and get the depth of that point through a depth sensor. Now, I get the 3D coordinate using the following,\n\nHere, K is the intrinsic camera matrix and T_cam is Camera extrinsic transformation matrix. However, the final point I get after the above transformation is not correct, it is basically a point behind the vehicle's starting position. I am unable to figure out what I am doing wrong.", "type": "commented", "related_issue": null}, {"user_name": "brscholz", "datetime": "Feb 7, 2022", "body": "Correct me if I'm wrong, but I think that won't work anyway because you have zero clue as to where along its way the ray that is represented by the pixel met a surface? How do you want to extract that depth information from the image?", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "Apr 16, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/facebookresearch/fairseq/issues/4405", "issue_status": " Open\n", "issue_list": [{"user_name": "gutzcha", "datetime": "May 9, 2022", "body": "Hey,\nIs it possible to change the sampling rate from 16k to 250k?\nI am trying to use the wav2vec model and pretrain it on animal (mouse) vocalizations which are emitted at ~100 kHz, so my sampling rate is 250 kHz. I can't downsample it to 16kHz as was suggested to others with similar questions.\nEach audio segment is about 2-5 seconds long.", "type": "commented", "related_issue": null}, {"user_name": "gutzcha", "datetime": "May 9, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/voxel51/fiftyone/issues/113", "issue_status": " Open\n", "issue_list": [{"user_name": "tylerganter", "datetime": "May 29, 2020", "body": "Google Photos please!!!", "type": "commented", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jun 1, 2020", "body": "Spelling this out in a bit more detail:No selection mode:Image selection mode:Image selection mode (nice to have):", "type": "commented", "related_issue": null}, {"user_name": "tylerganter", "datetime": "Jun 1, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "tylerganter", "datetime": "Jun 1, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "tylerganter", "datetime": "Jun 1, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "tylerganter", "datetime": "Jun 1, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jun 1, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "brimoor", "datetime": "Jun 1, 2020", "body": [], "type": "issue", "related_issue": "#139"}, {"user_name": "lethosor", "datetime": "Aug 28, 2020", "body": [], "type": "issue", "related_issue": "#472"}, {"user_name": "brimoor", "datetime": "Aug 28, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "lethosor", "datetime": "Sep 8, 2020", "body": [], "type": "pull", "related_issue": "#509"}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4629", "issue_status": " Closed\n", "issue_list": [{"user_name": "withtimesgo1115", "datetime": "May 11, 2022", "body": "Hi, I tried to import the canvas2d module and then use its API to draw a  Rect shape instead of using cvat web labeling tools. However, after creating a canvas2d instance and mounting the canvas.html() to my root container element, I cannot draw a rectangle on the canvas. Actually, I can see the red lines to draw the rectangle shape but once I clicked the mouse, the shape cannot appear on the canvas anymore. I just call the API like this:what's wrong with this, could anyone give me some kind of guidance? Thanks in advance!", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 18, 2022", "body": "HiNeed more details to suggest, maybe code snippet, HTML code, gif demonstration, etc.", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Sep 19, 2022", "body": "Hi, , Is this issue still relevant for you?", "type": "commented", "related_issue": null}, {"user_name": "withtimesgo1115", "datetime": "Sep 19, 2022", "body": "Hey,  , nope, actually I have solved the problem. Thank you for your reminder :)", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Sep 19, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 19, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4184", "issue_status": " Open\n", "issue_list": [{"user_name": "aschernov", "datetime": "Jan 16, 2022", "body": "Due to our experience based on annotation tasks with large number of object classes which need to be annotated, we formed our proposal about segmentation and layer modes development. Briefly, we are suggesting to move work with layers to a separate segmentation mode. We tried to imagine how it can look like on the image below:\nBelow I will list what exactly we'd like to see implemented in the tool:We're suggesting to move the main actions which we can do with a polygon to its (object) separate menu where we could turn on/off different action for this polygon, for example, auto bordering.\nA couple of words of automatic bordering. The logic we want to see here is when we choose the first point of a polygon to which we want to make auto bordering, the rest points of other polygons will be hidden.\nThe example of a polygons menu implemented in one annotation tool:\nListed and demonstrated aboveTried to make some proposals aboveYou may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 2, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 2, 2022", "body": [], "type": "added this to", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3941", "issue_status": " Open\n", "issue_list": [{"user_name": "katab", "datetime": "Nov 23, 2021", "body": "When you have used the mouse to edit an attribute you can use a keyboard shortcut directly after, without clicking in the frame or pressing TAB first.If you have used the mouse to edit an attribute you then have to either click in the frame with the mouse or press TAB before you can use any other keyboard shortcut.The annotation process would be more efficient and reliable if the keyboard shortcut works directly as expected.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": " , thanks for the report. I will consider it as a bug because it can kill UX.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 23, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 2, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3438", "issue_status": " Open\n", "issue_list": [{"user_name": "nmanovic", "datetime": "Jul 19, 2021", "body": "When an user goes to a frame he/she should see the full scene. I will recommend to use \"bird eye view\" (top view) because it is easy to recognize the road, cars and other objects. Now users have to zoom out always (unnecessary action). The reference tool is initialized by a side view where all points are visible. It is also an acceptable solution.It was already a long story. Let me come back to the decision to show black and white points. The reference tool assigns different colors to a point depends on the distance from the ego-vehicle. Let me understand reasons why CVAT doesn't do that as well.When a 3D task is loaded, it doesn't allow users to manipulate with the loaded 3D frame using mouse. It isn't how the reference tool works and CVAT 2D works. It isn't convenient.\nFurthermore, if I choose \"move\" mode and go to another frame, \"select\" mode is activated and I  manipulate with the frame. Thus I can reproduce performance problems, which were reported previously.AR: please fix performance problems, but don't break/disable expected features and functionality.Projection of points on \"side\" and \"front\" views is excess. Please look at the reference tool and compare the current implementation in CVAT and expected implementation in the reference tool. On the same PCD file, in CVAT it is mostly impossible to see boundaries of objects on \"side\" and \"front\" views. Looks like points which are far away of the 3D bounding box should not be displayed at all.When I'm moving mouse under shapes, the selection works with a significant delay. I don't see the problem in the reference tool. Also, if I change \"opacity\" in the right side bar, UI is very unresponsive.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 4, 2021", "body": "Start drawing a cuboid using a standard way, double-click a perspective view. UI is broken, cuboid cannot be drawn anymore, even in a new drawing session. Need to reload the page.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 19, 2021", "body": [], "type": "pull", "related_issue": "#3234"}, {"user_name": "nmanovic", "datetime": "Jul 19, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Aug 2, 2021", "body": [], "type": "issue", "related_issue": "#1475"}, {"user_name": "bsekachev", "datetime": "Aug 4, 2021", "body": [], "type": "pull", "related_issue": "#3493"}, {"user_name": "nmanovic", "datetime": "Aug 6, 2021", "body": [], "type": "pull", "related_issue": "#3524"}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "added this to", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/3387", "issue_status": " Open\n", "issue_list": [{"user_name": "hnuzhy", "datetime": "Jul 4, 2021", "body": "I have read and searched the official docs and past issues for the solution. No one had the same problem with me.I want to annotate the head orientation of people in 2D image with a standard 3D cube. Here, the head is a rigid object. A standard cube is defined as follows: three sides of any vertex are perpendicular to each other, and all twelve sides are equal in length, or in unit length.After labeling, we could get the eight projected vertices of the cube in the two-dimensional coordinate system. If three Euler angles (pitch, yaw, roll) are used to represent the orientation of the head, these precise projection points can be converted into corresponding angles.I have three suggestions or roadmaps for adding unit  label in the new version of CVAT.Here are two examples of 3D model interaction. The first is the rotation interaction of a 3D head model in mayavi. The interactive operation needs to rely on both mouse and keyboard. The second is to use the 3D image editing tool in Windows 10 to place and operate 3D models on 2D images. All you need to do is use the mouse.\nExample 1\nExample 2Looking forward to your reply. I will be willing to do whatever I can to advance this functional part.", "type": "commented", "related_issue": null}, {"user_name": "chiehpower", "datetime": "Jul 7, 2021", "body": "this is so cool feature ...", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 7, 2021", "body": " , I agree that we need to improve the functionality. Your explanation is really helpful. Could you please describe your research area and organization? Unfortunately my team has huge amount of requests and we already have an approximate roadmap for Q3'21 and Q4'21. Thus I'm trying to clarify details which will help me to increase the priority of the feature.", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "Jul 8, 2021", "body": "Yes, it is a pretty cool function which is not easy to realize :-(", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "Jul 8, 2021", "body": " Hi, I'm glad you agree to my proposal. I am a PhD student in computer department from SJTU University. My research field is the intersection of AI and education. The detailed research direction is object detection and pose estimation in computer vision. I would like to talk about the motivation of this question from two aspects.Recently, I've been studying the methods of attention detection for students in the classroom. Among them, head orientation (head pose estimation) is one of the key factors. However, as far as I know, the head pose estimation algorithm of multi-person in 2D image is not well developed. At present, there are some SOTA algorithms for head pose estimation of a single well cropped head, including  and . But their effect is not ideal, and it is not easy to extend to the case of multiple people in a single image. Most importantly, the datasets used by these algorithms are obtained by 3D head projection (), or the 3D Euler collected by depth camera in the experimental scene ().\nPrediction example 1 of FSA-Net (The input can only be a single person's head with visible face.)\nPrediction example 2 of FSA-Net (First, the head bbox of each person is detected by MTCNN, and then the single head is estimated. Therefore, this is not an efficient or essential multi-person head pose estimation algorithm.)\nPrediction example of WHE-Net (The input can only be a single person's head with wide range pose. The predictable yaw angle of the head is omnidirectional.)Dataset has always been the cornerstone of deep learning algorithms, so is head pose estimation. Therefore, I want to try to annotate the 3D head orientation, or three Euler angles of the head directly in the 2D image. As mentioned for the first time in this issue, the most accurate annotation scheme focuses on how to use 3D cube to interact freely on 2D images. In my opinion, once such a dataset is constructed, it will help promote the great progress of the corresponding algorithm research. For example, a bottom-up method could be designed to directly predict the pose of all heads in the image at one time. At the same time, compared with a single captured head image, the complete scene and human body information in the original image can assist more accurate head pose estimation.:After investigation, I didn't find tools with real 3D cube annotation. Fortunately, close functional options were found in CVAT. The first is . However, the new builded cuboid lacks rotation freedom. The second is . By annotating three consecutive non coplanar edges of a 3D cube approaching the head orientation, we can deduce the approximate Euler angle. Unfortunately, there is a great subjectivity error in this annotation process. We can't see the actual pose of the generated cube directly, unless we use a real 3D cube to annotate interactively. If we use this method reluctantly, the credibility of the final annotation will be questioned.Here are three examples of rough annotation results with . Images are all from the public  dataset. The object we annotate is the head with any orientation in the image, including the visible, occluded and invisible face. In many cases, the current method of  annotation is difficult and inaccurate.In a word, it is very useful to add interactive annotation of rigid 3D graphics (which can only be rotated, translated and scaled) to 2D images. In addition to supporting the head orientation marking, the new function can also be extended to the annotation of other rigid objects. After the construction of similar datasets about general objects, we can try to develop a simple and direct 3D object pose estimation algorithm only based on 2D images. We expect that this method can be comparable to estimation algorithms based on RGB-D or 3D point cloud.Finally, I am not good at giving the overall improvement framework of CVAT about this enhancement from UI design or code addition, but I am willing to do what I can. I sincerely thank CVAT's main contributors for their work, and hope to carefully consider adding this task to roadmap.", "type": "commented", "related_issue": null}, {"user_name": "Kucev", "datetime": "Apr 7, 2022", "body": "I support the request. We also have a need for such functionality.", "type": "commented", "related_issue": null}, {"user_name": "schliffen", "datetime": "May 28, 2022", "body": "This is a growing request from automotive industry as well, we need cuboid annotations to be done on RGB images not points clouds.", "type": "commented", "related_issue": null}, {"user_name": "hnuzhy", "datetime": "May 28, 2022", "body": "Yes, you are right. Actually, I have written a simple 2D head pose annotation tool using  last year. As shown below, the annotator can label one head with adding a bounding box in the 2D image, and adjust the 3D head model through mouse or keyboard in the right area to make it have the similar orientation/pose with boxed head. The co-existed 3D cube will be projected in the 2D image. For every appearing head pose status, we will record the corresponding Euler angles.\n\nHowever, this tool can only run in desktop, and is not fully as what I expected originally (refer above question for details). Then, I was busy on other things until now. I did not update and perfect this tool for a long time. If possible, I still look forward to seeing this annotation function in .", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 7, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2550", "issue_status": " Open\n", "issue_list": [{"user_name": "aschernov", "datetime": "Dec 9, 2020", "body": "We have several suggestions how to improve the review mode:", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 9, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 9, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Dec 9, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 24, 2021", "body": [], "type": "removed this from the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/4642", "issue_status": " Closed\n", "issue_list": [{"user_name": "aldyhelnawan", "datetime": "May 18, 2022", "body": "Greetings,Hi, I've faced a problem when I try to deploy a model to docker followed the documentation from the .\nEverything is working fine until the   part while I try to run this command:I got an error with the message as shown:I want to try to use the automatic annotation on my Windows 10  WSL2 machine, but I'm stuck on that error. Am I missing something during the installation? Or is there an alternative way to use it?\nThank you in advance,Best regards,\nAldy", "type": "commented", "related_issue": null}, {"user_name": "aldyhelnawan", "datetime": "May 18, 2022", "body": "So I've managed to reinstall and continue to follow the steps that provided from the . When I tried to use the command:The process completed:And for using:I tried to deploy the YOLO v3 model, the process completed as message shown:Then I check the , the model is shown. But when I tried to use the AI Tool, the message shown:\nI checked , the model is shown and running:\nI'm actually getting confused here.\nIs the building process took that long? Or there's some error that I don't understand happen on my machine? I would appreciate any assistance.\nThank you in advance.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 18, 2022", "body": "Hi, I remember there was an issue that Deep extreme cut is not working in the latest version \nDid you pull CVAT from DockerHub or built it from sources (build with -f docker-compose.dev.yml file)?", "type": "commented", "related_issue": null}, {"user_name": "aldyhelnawan", "datetime": "May 18, 2022", "body": "Hi , thank you for your reply.I don't clearly understand the meaning of . But what I know is I've followed all the steps from  to installing the  for my .\nWhen installing the Automatic Annotation, I've also faced some issues similar to this  and managed to fix it.And also, I've tried to install it on my  from  to installing the  and it works fine with no error.I will try to follow from  and I will update the result.Thank you in advance.P.S.:\nSo, I notice from the  command that there are 2 , but one of them is . Is that normal?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "May 18, 2022", "body": "Sorry, I can't suggest anything about WSL, because I do not use it.If your built command does not include dev file (), installation receives the latest available image from DockerHub instead of making image using files from develop branch.", "type": "commented", "related_issue": null}, {"user_name": "aldyhelnawan", "datetime": "May 18, 2022", "body": "Hi .\nAs far as I follow the steps, I believe my built command doesn't include the dev file. Because there's no  on the command from the guide.", "type": "commented", "related_issue": null}, {"user_name": "aldyhelnawan", "datetime": "May 20, 2022", "body": "Hi , I've tried to follow these suggestions, but the error is still the same on my Windows 10 WSL2 machine. Even after reinstalling (again) from the beginning by following  to use the .I think it's ok for me now to use the CVAT auto annotation on my Linux Ubuntu 20.04 machine since there's no error or issue while I'm using it.\nThank you in advance.Best regards,\nAldy", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Sep 13, 2022", "body": "Hi , Is this issue still relevant for you?", "type": "commented", "related_issue": null}, {"user_name": "aldyhelnawan", "datetime": "Sep 14, 2022", "body": "Hi   , I think we can close this issue for now. I have used Ubuntu 20.04 to use all CVAT features on my machine, and no issues to date (other than the cityscapes-dataset format that doesn't provide the JSON polygon file while exported).\nThank you for all your support.", "type": "commented", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Sep 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "kirill-sizov", "datetime": "Sep 13, 2022", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Sep 14, 2022", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": "Sep 14, 2022", "body": [], "type": "moved this from", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2088", "issue_status": " Open\n", "issue_list": [{"user_name": "jinzishuai", "datetime": "Aug 26, 2020", "body": "I am training a model using YOLOv5 and the objects are labelled using the :I am having a bit of challengens creating a labeling task in CVAT. I am able to generate the 80 labels in JSON format but  it seems that CVAT will reorder them labels  therefore breaking the original order of COCO.Note that \"person\" is the first entry in the COCO list.It seems that we have a bug in the placement of the first label to the very end.It is my understanding that the order of the labels are important for machine learning trainings since in the end, each label is represented as a integer number such as  for the \"cup\" class. If the data is now labelled with a different number, then the model will be completely wrong.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "jinzishuai", "datetime": "Aug 26, 2020", "body": "Please note that I originally thought CVAT reorders the labels alphabetically but after further look at the resulting labels (screenshot above), they are actually not alphabetic either.", "type": "commented", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Dec 9, 2021", "body": ", can you please check if  would resolve the issue?", "type": "commented", "related_issue": null}, {"user_name": "jinzishuai", "datetime": "Aug 26, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "jinzishuai", "datetime": "Aug 26, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "snyk-bot", "datetime": "Oct 8, 2020", "body": [], "type": "pull", "related_issue": "#2278"}, {"user_name": "azhavoro", "datetime": "Nov 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Nov 30, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "azhavoro", "datetime": "Nov 30, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Dec 9, 2021", "body": [], "type": "self-assigned this", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/2051", "issue_status": " Open\n", "issue_list": [{"user_name": "manybodycpa", "datetime": "Aug 19, 2020", "body": "Would it be possible to have a shortcut to quickly navigate through the different tracks? For example, I have track 1 from frames 50-100, track 2 from 75-125 and track 3 from 150 to 200. With the shortcut, I would press the corresponding key once to go from frame 0 directly to 50, a second time to jump to frame 75, and a third time to jump to frame 150. For example, this would be very useful to loop through tracks and eliminate false positive detections or check immutable labels of entire tracks.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 26, 2020", "body": "I can suggest adding shortcuts to jump to the first/last frame of a track under mouse.\nGoing to the first frame of a next track looks at least not obvious (we can have a lot of tracks on different frames)", "type": "commented", "related_issue": null}, {"user_name": "manybodycpa", "datetime": "Aug 26, 2020", "body": "Good point. Could we jump to the next frame that contains the beginning (first box) of one or more tracks?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 26, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 26, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1979", "issue_status": " Open\n", "issue_list": [{"user_name": "yohayonyon", "datetime": "Aug 3, 2020", "body": "Right click on any shape should open the track shape menu and not just the delete shape optionRight clicking when a rectangle or a cuboid s chosen a track shape menu is opened.\nOn any other shape type, only a delete shape option is opened.Open the same menu for all shapes. It would shorten the annotation time.The expected behavior will fasten the annotation process.You may  channel for community support.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": "Here are some statements:For cuboids and rectangles you can open only shape menu (as on your screenshots it's like a copy of side panel item for convenience)\nFor polygons and polylines you can also open shape menu (right click a shape, not a point of the shape) and you can open points menu (by clicking a point of the shape)\nFor points you can open only point menuSo, according to your request all shapes except of a points set have the same right-click behaviour.", "type": "commented", "related_issue": null}, {"user_name": "yohayonyon", "datetime": "Aug 10, 2020", "body": "Thank you for the clarification regarding polygons and polylines.\nIt will be more efficient to have the same right-screen-side menu opened for a point as well. This way annotators will be able to set label and attributes faster.", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": "On the one hand I agree with you. On the other hand we should left ability to remove points from UI (without shortkeys).\nOne possible solution I see is to add a modifier key. So, if the key is pressed, we always open right-screen-side.\nMaybe you have other suggestions?", "type": "commented", "related_issue": null}, {"user_name": "yohayonyon", "datetime": "Aug 10, 2020", "body": "The points shape has no way to be marked as a whole, only a single point may be be marked at a time. I suggest to define such a way and when the entire shape is marked, the shape menu is opened.A modifier key sounds good to me.\nFor mouse users a 1st click on a point may mark only 1 point and a second may mark the entire points shape (and to generalize every following click may switch).", "type": "commented", "related_issue": null}, {"user_name": "yohayonyon", "datetime": "Aug 3, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 10, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 26, 2021", "body": [], "type": "removed this from the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/1215", "issue_status": " Open\n", "issue_list": [{"user_name": "ActiveChooN", "datetime": "Feb 28, 2020", "body": "Now we have only zoom control with scroll wheel movement, but it can be useful to add position control like in graphic editors. For example  for horizontal movement and  for vertical movement.", "type": "commented", "related_issue": null}, {"user_name": "ActiveChooN", "datetime": "Feb 28, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 29, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Feb 29, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "issue", "related_issue": null}]},
{"issue_url": "https://github.com/iNavFlight/inav/issues/1849", "issue_status": " Closed\n", "issue_list": [{"user_name": "digitalentity", "datetime": "Jul 1, 2017", "body": "Finding a compact board that would support all of required sensors may become tricky, also wiring may become a pain in the back.Suggestion - create a very simple board (or family of boards) that will talk to main FC over a new half-duplex protocol and act as a bridge between actual sensor and FC with INAV firmware.To keep things compatible with most boards out there and also to keep additional costs low I suggest we use common UART in half-duplex mode. Transfer will happen in TDMA fashion with all data transfer scheduled by the FC (master). For physical interface I see two options:Bill of materials price for the convertor board would be abour 4-5$, fairly large SMD components can be used to allow DIY-style.List of possible use-cases for the convertor board:", "type": "commented", "related_issue": null}, {"user_name": "giacomo892", "datetime": "Jul 1, 2017", "body": "Nice idea.I see also some others use-cases:", "type": "commented", "related_issue": null}, {"user_name": "moggiex", "datetime": "Jul 4, 2017", "body": " I think I understand what you're trying to do here, my suggestion would be to try & keep everything as simple as possible.With that in mind, could a secondary flight controller be used as a \"slave\" device to achieve this?Matt", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 4, 2017", "body": "Yes, a secondary flight controller could be used (in fact it's a great idea).\nHowever I'm thinking also about higher reliability for bigger machines.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 6, 2017", "body": "Sensor bus protocol proposal (using one-wire UART as an interface).Master initiates transmission by sending command+slot identifier packed into 1 byte:\n - where CC is command code and SSSSSS is slot identifier.DISCOVER command is followed by 1 byte of DevID. A device with matching DevID must respond with 2 bytes of desired poll interval and 2 bytes of CRC-16. A slot number must be remembered by the device and used in further transmissionThis command doesn't have any argumentrs. A device that was assigned a slot must respond  with 8 bytes of data and 2 bytes of CRC. Response delay shouldn't be longer than 2ms (to avoid mixing it up for guard interval).At 100kbps speed transmission of 1 byte command + 10 byts of response will take ~1.3ms. If device makes 0.5ms delay we still fit info <3ms total activity time to allow 2ms guard interval after transfer finishes.A maximum of 8 bytes of data is allowed for compatibility with CAN Bus so one message will fit into one CAN frame. A sensor may have more than one ID (i.e. GPS)Proposed assignment scheme: 4 bits data class + 4 bits data typeClass 0x0 - AHRS/MARG sensors\n0x00 - reserved for something really high priority\n0x08 - Magnetic field vector (3D)Class 0x1 - Altitude and climb rate data\n0x10 - GPS altitude and climb rate\n0x11 - Barometric altitude\n0x12 - Rangefinder altitudeClass 0x2 - 2D-position and velocity data\n0x20 - GPS coordinates (LAT/LON) + validity flag (LAT/LON fit into 31 bits, this leaves us 2 bits for flags)\n0x21 - GPS velocities (2D) and CoG + validity flag\n0x22 - Optical flow vector (2D) and qualityClass 0x8 - RC control\n0x80 - RC channels (4 channels with 10-bit resolution + 4 channels with 6-bit resolution)Class 0xF - sensor statistics and information (low priority, low rate)\n0xF0 - GPS sat count, HDOP etc", "type": "commented", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Jul 6, 2017", "body": "", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 6, 2017", "body": " that's just an idea of what sensor assignment may look like. Airspeed and surroundings - good catch :)WRITE messages could be implemented in a similar way as READ ones - only with different source of data on the wire.", "type": "commented", "related_issue": null}, {"user_name": "moggiex", "datetime": "Jul 6, 2017", "body": "Daft question gents, this line got me thinking:Thinking about FrSky kit whom has no \"satellite\" receivers support (there is the ability with the X8R I believe but the range is still low > 2.6Km) , could a second receiver be added to the child board, thus if the main one hits failsafe, the secondary one could be checked and if working, used instead?Thinking of your note around \"bigger machines\", on a bigger machine a secondary receiver could be easily placed in a different position to the main receiver.Just a thought!Matt", "type": "commented", "related_issue": null}, {"user_name": "Redshifft", "datetime": "Jul 6, 2017", "body": "RC Receivers are insignificant over Flight controller, sensors and code to control a Multirotor.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 7, 2017", "body": " good idea, it would be good to have multiple receiver support - we need to think about it.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 7, 2017", "body": " yes, sensor are more important, but reliable pilot input is also quite important thing to have. If we can add extra reliability - why shouldn't we?", "type": "commented", "related_issue": null}, {"user_name": "Redshifft", "datetime": "Jul 7, 2017", "body": " You don't need to be far away before any RC link is useless to try and guide a multirotor home - might be inconvenient to drop RC link the but the FC and sensors will get you home and backed up by a second sensor suite which on the side could even be logging waypoints to get back safely. You could even add to that Airplane style mag-less navigation to home.\nIt's more likely  any RX backup will be subject to the same cause of failure as the primary ( generally blanking or interference)", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 8, 2017", "body": " Any redundancy is better than no redundancy. Also, nobody is restricting you to use two receivers of the same type. One, for instance, could be DragonLink, the other - Crossfire.\nEven if all sensors fail, if you have a working RC link - you still may be able to switch to manual and bring the plane back.\nFor copters, however, things are different - here I totally agree with you that having a set of backup sensors is better than a reliable link.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 8, 2017", "body": "Other ideas for this \"UAVBus\" protocol:", "type": "commented", "related_issue": null}, {"user_name": "cabinw", "datetime": "Jul 10, 2017", "body": "  Got similar idea. It's really convenient for both multi-rotors and fixed-wings.\n Great idea! Using a secondary flight controller could be very helpful especially for long range fpv flights.I've designed a companion board for spracing f3 mainly for integrated OSD and separated BEC for servos. A companion board is not only for adding functions but also useful for deploying and wiring.Hope get more infos about \"UAVBus\" protocol.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 11, 2017", "body": "See this for first draft of  ", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 13, 2018", "body": "This issue / pull request has been automatically marked as stale because it has not had any activity in 60 days. The resources of the INAV team are limited,  and so we are asking for your help.\nThis issue / pull request will be closed if no further activity occurs within two weeks.", "type": "commented", "related_issue": null}, {"user_name": "stale", "datetime": "May 27, 2018", "body": "Automatically closing as inactive.", "type": "commented", "related_issue": null}, {"user_name": "dimon777", "datetime": "Jan 28, 2020", "body": "There is a big class problems which are not directly related to the flight aspect and require not only sensor boards but full companion computer. Deep sensor fusion (which require more powerful CPU). Inferring/classifying the land objects types during low pass flights or on the approach. It would be great if INAV would expose/allow such integration. I was able to connect RPI nano board to Pixhawk and expose FC to commands from RPI over Mavlink.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jan 29, 2020", "body": " what commands are you talking about specifically?", "type": "commented", "related_issue": null}, {"user_name": "dimon777", "datetime": "Jan 29, 2020", "body": "Disclaimer: I am not a professional drone expert :) yet! So for example, we want to implement an obstacle avoidance. The solution for this problem can't be generalized, because it is too complex and environmentally and situationally specific. I have a set of simple  sensors or in more complex use case set of matrix sensors outputting point cloud. This point cloud is fused with other sensors: gyro, acc, baro, cameras, perhaps with some static data like terrain heightmaps etc. to allow navigation in a complex environment and help drone become environment aware. What commands? Anything from roll/pitch/yaw/throttle corrections to help drone safely deliver that pizza to the customer avoiding trees, power lines, people, animals and what's not and make sure a parcel is not landed into the water from the overnight rainfall. This requires lot of computation and I feel ultimately not a job for a FC. If this makes sense.", "type": "commented", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 1, 2017", "body": [], "type": "added", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jul 15, 2017", "body": [], "type": "pull", "related_issue": "#1899"}, {"user_name": "stale", "datetime": "May 13, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "stale", "datetime": "May 27, 2018", "body": [], "type": "", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jan 29, 2020", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "stale", "datetime": "Jan 29, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "digitalentity", "datetime": "Jan 29, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "DzikuVx", "datetime": "Sep 3, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/772", "issue_status": " Open\n", "issue_list": [{"user_name": "rsnk96", "datetime": "Oct 14, 2019", "body": "It would be nice to have a feature to generate superpixels, and label superpixels in one mouse-click (sort of like a brush, but it labels the superpixel if the mouse has hovered over with the Mouse Button pressed down). An easy idea for this is to run K-means in RGB+(x,y) space, like the SLIC/ algorithm doesA sample implementation of SLIC in C++ is available , this can be ported to OpenCV.js", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 14, 2019", "body": " , did you try to annotate data using the approach? What is the annotation speed in the case? I like new features but I need some information how to prioritize them. Does the approach give better quality or better speed than traditional methods?", "type": "commented", "related_issue": null}, {"user_name": "rsnk96", "datetime": "Oct 14, 2019", "body": "This is a feature that is available in , sample usage:\nI believe this would give better speed in the following cases:", "type": "commented", "related_issue": null}, {"user_name": "jrreyes29", "datetime": "Dec 6, 2019", "body": "How do you can port this to opencvjs because opencvjs don't support this method createSuperpixelLSC()", "type": "commented", "related_issue": null}, {"user_name": "rsnk96", "datetime": "Dec 6, 2019", "body": "I'm not quite sure I followed you . Where is that method being called?", "type": "commented", "related_issue": null}, {"user_name": "jrreyes29", "datetime": "Dec 9, 2019", "body": "Hi  the method in opencv is this one ", "type": "commented", "related_issue": null}, {"user_name": "rsnk96", "datetime": "Dec 9, 2019", "body": "Ah, didn't know that it comes as a part of  now.I guess that leaves us with no option but to implement the superpixel method in JS from scratch...", "type": "commented", "related_issue": null}, {"user_name": "jrreyes29", "datetime": "Dec 9, 2019", "body": "but implement that in js from scratch take long time hahahaha that is the idea about opencvjs", "type": "commented", "related_issue": null}, {"user_name": "luiztauffer", "datetime": "May 10, 2020", "body": "superpixel annotation would be awesome!", "type": "commented", "related_issue": null}, {"user_name": "patzhang", "datetime": "Dec 17, 2020", "body": " is an implementation in javascript for Superpixels using SLIC. It has a nice  that you can try online.  Is the demo good enough to compare?Also, the  used superpixels (shown in the paper) in their .", "type": "commented", "related_issue": null}, {"user_name": "Hommus", "datetime": "May 3, 2022", "body": "\nAny ETA on this feature?How would one go about implementing a new tool / feature like this? I have created it in Python, I'm just struggling to re-implement it in javascript.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 14, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 14, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 14, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "juanmed", "datetime": "May 5, 2020", "body": [], "type": "issue", "related_issue": "#1492"}, {"user_name": "snyk-bot", "datetime": "Nov 20, 2020", "body": [], "type": "pull", "related_issue": "#2468"}, {"user_name": "nmanovic", "datetime": "Dec 17, 2020", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 19, 2021", "body": [], "type": "issue", "related_issue": "#2800"}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/747", "issue_status": " Open\n", "issue_list": [{"user_name": "4sfaloth", "datetime": "Oct 1, 2019", "body": "Hi all,this is just a feature suggestion I thought might be useful.Are there any plans on allowing users to customize the keyboard shortcuts? I feel like I it would be considerably easier to perform annotation tasks if one could adapt the shortcuts for each task", "type": "commented", "related_issue": null}, {"user_name": "mistermult", "datetime": "Jan 28, 2020", "body": " I'm interested in implementing custom shortcuts. However, I'm not sure what's best.I hope I can get your ideas or preferences on this topic.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 29, 2020", "body": " , great news! Sorry for delay with your previous PR. We definitely will integrate it in the future. Now we are busy with the new UI.I prefer the option . CVAT should have some settings per user (serialize/deserialize arbitrary JSON configuration into a DB table = (user_id, json_settings)). I don't think that it makes sense to have presets for a task. If a user has own settings they will be used otherwise default settings applied (shortcuts, options, theme, ...). What do you think?", "type": "commented", "related_issue": null}, {"user_name": "mistermult", "datetime": "Jan 30, 2020", "body": "  First,  mentioned task specific shortcuts. But I agree that shortcuts per user (or default vales) sound better for me.\nSaving an arbitrary JSON was my first idea. However, then migration, e.g. changing the names for shortcuts, might be little harder but still possible. So the alternative would be a table (user_id, shortcut_id, keys). If you don't have any objection, I'll go with JSON.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 30, 2020", "body": " , for tasks we can implement in the future. It can be a cascade scheme (default settings -> user settings -> task settings). Let's implement UserSettings = (user_id, settings) table where settings string is a json string. REST API could be  with GET, PATCH, DELETE, PUT methods. JSON settings should have a special key \"shortcuts\": { \"name1\": \"shortcut1\", ... }. Thus in the future we will use the approach as well to keep other settings. , could you please give us some hints how to implement shortcuts in the new UI?", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Jan 30, 2020", "body": "Hi, we are going to replace legacy annotation view with a React based single page application soon.\nBasic work with the new annotation view is complete. The latest version always can be found in the branch  and we merge it to develop regularly.Now the UI doesn't have any shortcuts and you would help us with it.\nHere are some ideas about client implementation:Something like:\nAction 1: [Ctrl + A]\nAction 2: [Ctrl + C]", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 25, 2020", "body": "Hi,\nAny news here?", "type": "commented", "related_issue": null}, {"user_name": "celhannah", "datetime": "Jul 5, 2021", "body": "Is there any update on user customizable shortcuts?", "type": "commented", "related_issue": null}, {"user_name": "jnothman", "datetime": "Aug 12, 2021", "body": "It seems to me that there is sense in allowing task-specific shortcuts, for instance, to allow a shortcut that creates a rectangle with a particular label.", "type": "commented", "related_issue": null}, {"user_name": "Meriipu", "datetime": "Aug 12, 2021", "body": "you can do ctrl+1 to at least ctrl+5 to change the default label rect (e.g for the next one)or you can do ctrl+1 to ctrl+5 while hovering a rect to change it", "type": "commented", "related_issue": null}, {"user_name": "Meriipu", "datetime": "Aug 12, 2021", "body": "personally I would benefit from shortcuts to  but none are configured atm", "type": "commented", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Aug 13, 2021", "body": "\nJust interested, why not to use mousewheel? Actually it has the same meaning (zoom in/out with keeping cursor position at the center)", "type": "commented", "related_issue": null}, {"user_name": "Meriipu", "datetime": "Aug 13, 2021", "body": "the scroller on my mousewheel is broken so I realize that is a bit unique", "type": "commented", "related_issue": null}, {"user_name": "Meriipu", "datetime": "Aug 13, 2021", "body": "Additionally I often use a graphics tablet combined with a keyboard key bound to left-click to annotate as 4-point annotation is a bit smoother that way, and I do not have a scroll-wheel on the tablet.", "type": "commented", "related_issue": null}, {"user_name": "doctorcolossus", "datetime": "Jan 28, 2022", "body": "I would also like this. I usually use my laptop's trackpad, rather than a mouse. If I only needed to draw one or two bounding boxes, it would be no biggy, but when it comes to annotating hundreds or thousands of images, CVAT causes me a lot of pain. Having used AutoCAD and GIMP extensively, it's very frustrating to me that CVAT offers no keyboard controls for panning or zooming.", "type": "commented", "related_issue": null}, {"user_name": "timurlenk07", "datetime": "Mar 26, 2022", "body": "I would also be interested in this feature. In our use case, certain attributes of tags should be updated on a hotkey press (for instance, tick/untick a checkbox), which would help us to be more performant.What is the current situation on this feature request? Does it have a planned feature launch date yet? ", "type": "commented", "related_issue": null}, {"user_name": "dudulasry", "datetime": "Jun 26, 2022", "body": "I'd also love to see shortcuts customization in CVAT. The current setup is not very convenient for me.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 2, 2019", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 2, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 2, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 20, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "bsekachev", "datetime": "Feb 27, 2020", "body": [], "type": "issue", "related_issue": "#1209"}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from the", "related_issue": null}]},
{"issue_url": "https://github.com/opencv/cvat/issues/425", "issue_status": " Open\n", "issue_list": [{"user_name": "nmanovic", "datetime": "Apr 25, 2019", "body": "For some tasks it is better to view 4, 9, or even more images on one screen and use mouse clicks to choose relevant images (e.g. classification face/non-face).The feature can be used to review interpolation results or tracking as well.", "type": "commented", "related_issue": null}, {"user_name": "rushtehrani", "datetime": "Jun 12, 2019", "body": " would this support classification scenarios like this?", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jun 13, 2019", "body": " , sure. On server side we already support classification. Only need to implement it in UI.", "type": "commented", "related_issue": null}, {"user_name": "verasativa", "datetime": "Jan 3, 2020", "body": "We have an UI component for this task just running in the developer machine. I'm currently working on moving it to our production machine, and then we would love to make a PR with it.", "type": "commented", "related_issue": null}, {"user_name": "danielrich", "datetime": "Jan 30, 2020", "body": " This would be super cool. I am curious if you have any ETA on that PR being available.", "type": "commented", "related_issue": null}, {"user_name": "Cittadini", "datetime": "Nov 13, 2020", "body": " can you give us any updates?", "type": "commented", "related_issue": null}, {"user_name": "verasativa", "datetime": "Nov 13, 2020", "body": "Actually I was emailed yesterday by Yoav from notraffic.tech asking about it. But I not longer work at Odd where we developed this feature, still I put him in contact with my old colleagues whom coded it and also would like to be integrated too. I'm now sending them a link to this tread, so they can comment on the issue.Good luck everybody!", "type": "commented", "related_issue": null}, {"user_name": "Cittadini", "datetime": "Nov 13, 2020", "body": "Ok, thank you very much!", "type": "commented", "related_issue": null}, {"user_name": "dotXem", "datetime": "Jan 3, 2022", "body": "Sorry for reopening this thread, but has this feature been added to CVAT? It would be very handy to have it!", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jan 4, 2022", "body": " , at the moment it isn't implemented and it isn't planned for the next release.", "type": "commented", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 25, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Apr 25, 2019", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Jul 22, 2019", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "nmanovic", "datetime": "Oct 21, 2019", "body": [], "type": "issue", "related_issue": "#786"}, {"user_name": "snyk-bot", "datetime": "Jul 13, 2021", "body": [], "type": "pull", "related_issue": "hixio-mh/cvat#33"}, {"user_name": "nmanovic", "datetime": "Nov 28, 2021", "body": [], "type": "removed this from the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1703", "issue_status": " Open\n", "issue_list": [{"user_name": "Janus-Xu", "datetime": "Nov 5, 2021", "body": "\ntimeseries labeling demo csv：\ndata with  rows no in 10000，goes fine；\n\ndata with  rows no in 20000\nbug1：\nbackground lines disappeared ，when mouse in，can still see data points：\n\nbut zoom out selection，background lines appeared；\nbug2：\nzoom out to a small selection，than scroll zooming to the end whole component crashed and page blank；\n\ndemo data here：\n\n", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Nov 5, 2021", "body": "Relative:\nMaybe too (last point NaN):\n", "type": "commented", "related_issue": null}, {"user_name": "InCaseOf", "datetime": "May 18, 2022", "body": "Any updates on this bug?", "type": "commented", "related_issue": null}, {"user_name": "Janus-Xu", "datetime": "Nov 5, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Nov 5, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Nov 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Feb 9, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1549", "issue_status": " Open\n", "issue_list": [{"user_name": "dsanders11", "datetime": "Sep 30, 2021", "body": "I'd like to be able to copy/duplicate a label. For example, I have a task where the different items to label are all the same size, so I'd like to draw the rectangle once, then duplicate it and change the label on the copy, ensuring that all of the rectangles are identical sizes. CVAT has copy functionality as described.A way to copy/duplicate a layer. In CVAT it's called copy, but I think duplicate might be a better word since copy might imply you'll then paste it, rather than immediately creating another label.None.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 1, 2021", "body": " Since LS 1.3 the selection of multiple regions and ctrl+c/ctrl+v hotkeys are available, have you tried them?", "type": "commented", "related_issue": null}, {"user_name": "dsanders11", "datetime": "Oct 1, 2021", "body": ", looks like Ctrl+D does exactly what I wanted, thanks! Using Ctrl+C and Ctrl+V was buggy for me, sometimes it worked, sometimes it didn't, and I couldn't figure out why, but Ctrl+D always worked. It seems like Ctrl+C/Ctrl+V did nothing for me when I first switch to an image and click an existing label.I see now that it's mentioned in the docs, but not in the list of hotkeys in Label Studio itself (at least under Settings), and there doesn't appear to be any way to do it without hotkeys. It would be nice if it was part of the UI so that it was better discoverable, and could be used with only a mouse, as I think a mouse-only workflow can be easier at times.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 1, 2021", "body": "Thank you for your points. So, if we add the duplicate button for regions, will your issue be solved?", "type": "commented", "related_issue": null}, {"user_name": "dsanders11", "datetime": "Oct 1, 2021", "body": "I think so, yea.", "type": "commented", "related_issue": null}, {"user_name": "dsanders11", "datetime": "Sep 30, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 7, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1326", "issue_status": " Open\n", "issue_list": [{"user_name": "alexandrnikitin", "datetime": "Aug 19, 2021", "body": "\nNavigating time series (moving forward) is pretty hard and frustrating. I need to move my mouse over the bottom window and move it slowly. With a long time series it's unusable.\nIt looks like scroll and/or hotkeys can ease the work, e.g. arrows or <,> or h,l (vim style)\n", "type": "commented", "related_issue": null}, {"user_name": "dflatow", "datetime": "Mar 1, 2022", "body": "This would be very useful.   for performance with longer time series.", "type": "commented", "related_issue": null}, {"user_name": "alexandrnikitin", "datetime": "Sep 4, 2022", "body": "Any updates on this? It would be great to have easier navigation.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Sep 9, 2022", "body": " unfortunately there are no updates :-(", "type": "commented", "related_issue": null}, {"user_name": "alexandrnikitin", "datetime": "Aug 19, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "smoreface", "datetime": "Sep 2, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 16, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 16, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "makseq", "datetime": "Mar 7, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dflatow", "datetime": "Mar 11, 2022", "body": [], "type": "issue", "related_issue": "#2118"}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1215", "issue_status": " Open\n", "issue_list": [{"user_name": "miwojc", "datetime": "Jul 24, 2021", "body": "\nRotating rectangles/polygons only works by using a mouse at the moment, there are no predefined rotations like horizontal, verical\nAdd buttons/keyboard shortcuts to quickly rotate rectangle/polygon by say 45deg and then fine tune it by 1deg\nThe only alternative for now is using mouse to rotate the rectangle/polygon manually.\nNone", "type": "commented", "related_issue": null}, {"user_name": "miwojc", "datetime": "Jul 24, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 28, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 12, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 1, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 9, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 9, 2021", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/1680", "issue_status": " Open\n", "issue_list": [{"user_name": "kaustuk", "datetime": "Oct 27, 2021", "body": "\nConsider the following scenario,An ASR system is giving a segment and we populate it in label studio. Now the person who is doing labeling needs to listen to the region and write text in textarea with is per region. To iterate over the region he would be using  to go to the next region. Doing this still it doesn't play the audio region automatically. So we need to click on the audio pane to actually play the audio region. Which slows down the labeling process.The current process step looks something like thisIdeally, steps should be this\n", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Oct 27, 2021", "body": "I would like to contribute to this feature. I actually explored this part. Here is the solution I would like to purposeOver here I would like to replace with the following if else", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 28, 2021", "body": " Nice idea! I would make it optional via LSF settings. Could you add a checkbox there like \"Autoplay next region\"?", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Nov 11, 2021", "body": "Hi , thanks for the reply and sorry for the delay in reply from my side. Yeah, that makes sense to have this feature configurable and it's the user's call if they want to enable it or not.Just to be clear  is this panel, right? And here we need to add a new checkbox for \"Autoplay next region\"?", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Nov 11, 2021", "body": " Yes, LSF Settings is what we can see on this screenshot. \"Autoplay next region\" makes sense.", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Nov 12, 2021", "body": "Hi , I have created the PR with this feature implemented. I tested it manually and it's working as expected.I haven't added any unit test case. Since I am not completely aware of the codebase and newbie in react, Can you provide some pointers around which test case I should refer to write testcase for this feature?", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Dec 5, 2021", "body": "Hi  , it's been a long time I have created PR with this feature, any approx timeline when PR would get reviewed or this feature would get added?", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 7, 2021", "body": " Sorry for this long waiting, I'll force our frontend team to merge it.", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Feb 12, 2022", "body": "Hi , sorry to bother you. Just wanted to check the status on this. Any thought by when this feature/PR would get reviewed?", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Feb 14, 2022", "body": " Thank you for pinging, I've asked to review your PR, then we will merge it.", "type": "commented", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Oct 27, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 28, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "kaustuk", "datetime": "Nov 12, 2021", "body": [], "type": "pull", "related_issue": "heartexlabs/label-studio-frontend#348"}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/812", "issue_status": " Open\n", "issue_list": [{"user_name": "NickShargan", "datetime": "Apr 15, 2021", "body": "\nAt the moment sliding window for signal can be moved only by mouse. Considering it is quite frequent action it feels unproductive during labeling.\nIt would be great to have hotkeys for sliding to right and left. It may save a lot of time.\nDon't see good alternatives.\n", "type": "commented", "related_issue": null}, {"user_name": "smoreface", "datetime": "Nov 11, 2021", "body": "Check out these hotkeys:  and let me know if they work for you in the latest version of Label Studio!", "type": "commented", "related_issue": null}, {"user_name": "reesehopkins", "datetime": "Nov 30, 2021", "body": "Unfortunately, these shortcuts do not work in the latest Docker build (v1.4). Also, you might consider alternative hotkeys for decreasing/increasing the region size --  is the keyboard shortcut to Go Back in Chrome.", "type": "commented", "related_issue": null}, {"user_name": "hlomzik", "datetime": "Dec 13, 2021", "body": "Hi,  ! We really don't have such hotkeys, but we are working on a good new shortcut system, and this will be there. I'll let you know when this happens!", "type": "commented", "related_issue": null}, {"user_name": "NickShargan", "datetime": "Apr 15, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "niklub", "datetime": "Apr 20, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "makseq", "datetime": "Dec 14, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/530", "issue_status": " Open\n", "issue_list": [{"user_name": "MNMaqsood", "datetime": "Jan 5, 2021", "body": "\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\nA clear and concise description of what you want to happen.\nA clear and concise description of any alternative solutions or features you've considered.\nAdd any other context or screenshots about the feature request here.", "type": "commented", "related_issue": null}, {"user_name": "MNMaqsood", "datetime": "Jan 5, 2021", "body": "Hey is it possible for you add an option to view the text of relevant labels after the addition of relations in the Relations panels? It only shows the \"A text\" now for every label of the relation. You can hover your mouse over that relation but it will be nice to also the text.\nP.S I have also requested it on Slack", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jan 15, 2021", "body": " This is not simple to build a good UX for this feature, because texts can be very long. Only tooltips are the fast solution.", "type": "commented", "related_issue": null}, {"user_name": "MNMaqsood", "datetime": "Jan 5, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "niklub", "datetime": "Jan 13, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "niklub", "datetime": "Mar 29, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 8, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/915", "issue_status": " Open\n", "issue_list": [{"user_name": "Joelvgent", "datetime": "Jul 13, 2022", "body": "When adjusting rectangles/boxes on objectss, it is quite time-inefficient to manually drag corners to the correct position. For example when 1/4 sides of the rectangle needs to be adjusted, it is required to drag the corner and possibly move the other side unintentionally, adding time to readjust sides that were not supposed to be moving in the first place. A possible solution that increases efficiency greatly would be the option to add any sort of WASD or arrow setup to move each side of a box individually by one tick. Adjusting boxes could then be done very accurately and swiftly, perhaps even a changing tick size feature depending on how far a photo is zoomed in on. Secondly, The option to switch ''target'' or jump from one box to another with the keyboard would eliminate the need for sketchy mouse adjustments. Combined with my previous Idea, it would lead to a huge jump in accuracy and speed when scanning a large number of photos.Imagine it like this: There is a photo with 8 objects, and some of them need readjustments. By using TAB for example it goes to the first box it can find, followed by pressing any WASD structure on the computer to adjust the sides in less than a few seconds. Press TAB to select the next box and repeat. The only use of the mouse would then be to create an entirely new box on an undentified item.I have a lot of experience readjusting object rectangles with the mouse, and am confident that adjustment efficiency could greatly be increased by implementing these features.Thank you for reading!", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/label-studio/issues/814", "issue_status": " Open\n", "issue_list": [{"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": "\nBeing able to annotate rewards is missing in this label studio. This reward annotation is useful for training RL agents.\nI'd like to have a way to create reward sketches like done here  on video data.\nI have not seen anything close to this in the repo.\nThese reward sketches are useful to have to train RL agents. This is the closest you can get to have a supervised signal for robotics.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 15, 2021", "body": " What kind of reward sketches do you want to have?\nLS has ratings, checkboxes and text areas, so you can have  for video.", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": " I'd like to draw a continuous function with X-axis being the frame number and Y-axis being the reward. To clarify what that means check the reward sketching section in  where you slide (and click) your mouse along and it draws the curves for you.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 15, 2021", "body": "Thanks! Now I've got it. It's a very cool feature!", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": "Do you think it is worth adding that in label-studio?I implemented it in python already (using matplotlib) but it's fairly simple and often requires more clicking.", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": "The code looks like this in case you're interested.`\nimport math\nimport numpy as npimport matplotlib.pyplot as plt\nfrom matplotlib.backend_bases import MouseEventclass DraggablePlotExample(object):\nu\"\"\" An example of plot with draggable markers \"\"\"if  == \"\":\nplot = DraggablePlotExample(hdf5File='images.hdf5')\n`", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": "some of this is inspired from ", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 15, 2021", "body": "Thank you very much! But we need to re-implement it in JS React for LS.", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": "Yes, sure. Let me know if you ever get to it. I'm happy to do testing for you. Thanks a lot for listening.", "type": "commented", "related_issue": null}, {"user_name": "makseq", "datetime": "Jul 19, 2022", "body": " - we introduced Number tag, it can help here.", "type": "commented", "related_issue": null}, {"user_name": "ankurhanda", "datetime": "Apr 15, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "makseq", "datetime": "Apr 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 6, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "makseq", "datetime": "Oct 8, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/885", "issue_status": " Open\n", "issue_list": [{"user_name": "vzeman", "datetime": "May 20, 2022", "body": "Python 3.10Traceback:Error occured, when I wanted to create rectangle and moved mouse over the image - application always crash", "type": "commented", "related_issue": null}, {"user_name": "vzeman", "datetime": "May 20, 2022", "body": "Update: with python 3.8 is application not crashing", "type": "commented", "related_issue": null}, {"user_name": "ropinheiro", "datetime": "May 26, 2022", "body": "I have this error with Python 3.10.4", "type": "commented", "related_issue": null}, {"user_name": "AjibolaPy", "datetime": "Jun 3, 2022", "body": "i also have this error python 3.10", "type": "commented", "related_issue": null}, {"user_name": "nine", "datetime": "Jun 9, 2022", "body": "Can not reproduce this bug on Debian GNU/Linux with following software versions:", "type": "commented", "related_issue": null}, {"user_name": "dvaupel", "datetime": "Jul 4, 2022", "body": "Same here. Ubuntu 22.04, installed with pip3.The manual installation from the start page works.", "type": "commented", "related_issue": null}, {"user_name": "Logiase", "datetime": "Jul 20, 2022", "body": "same error on Windows, installed with pip", "type": "commented", "related_issue": null}, {"user_name": "Tylermarques", "datetime": "Jul 22, 2022", "body": "Same issue with me. Solution was to use python3.8 for me.", "type": "commented", "related_issue": null}, {"user_name": "dhanashreemhatre", "datetime": "Aug 4, 2022", "body": "i am having same problem\npls someone help me to resolve it .\nQuestion", "type": "commented", "related_issue": null}, {"user_name": "windowshopr", "datetime": "Aug 21, 2022", "body": "same. Windows 10, python 3.10.5", "type": "commented", "related_issue": null}, {"user_name": "windowshopr", "datetime": "Sep 5, 2022", "body": "", "type": "commented", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#886"}, {"user_name": "matobodo", "datetime": "Jul 11, 2022", "body": [], "type": "issue", "related_issue": "#912"}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/862", "issue_status": " Open\n", "issue_list": [{"user_name": "wyfffffei", "datetime": "Mar 29, 2022", "body": "I suggest setting the shortcut key for adding the matrix（w） to a key near enter, which may be more convenient, otherwise it is tiring to press w and then press enter with one hand.", "type": "commented", "related_issue": null}, {"user_name": "Regenhardt", "datetime": "May 18, 2022", "body": "Why press enter?\nUsually it's (w) -> draw box with mouse -> (d) to get to next image -> (w) again", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/699", "issue_status": " Open\n", "issue_list": [{"user_name": "bonorico", "datetime": "Feb 3, 2021", "body": "[Feature request]: After closing rectangular box, a dialog with available classes pops up in the middle of the screen. It is extremely tiring for the eye to repeatedly wander here and there to get back to this dialog box to click the class label. It would be much more efficient to have this dialog box popping up exactly next to mouse pointer where the object is being annotated.A typical set-up is to have one hand on mouse and other one on W button. Additional to that, if classes dialog box could open there where image is being annotated, it would dramatically speed up things I believe.\nI tried to figure out how I could adapt this in present source code using qt, but I am not expert here... any suggestion ? Or would it be difficult for the developer to add this feat ?Thank you very much", "type": "commented", "related_issue": null}, {"user_name": "bonorico", "datetime": "Feb 9, 2021", "body": "SOLUTION: keyboard buttons can be customized by editing the source code (labelImg.py). For instance, in my case I change button \"w\" to \"+\", which is next to Enter key and allows me a much quicker combo open-window-draw-bow-confirm-class, not having to move my eyes away from annotation target too often. Also, holding mouse with left hand you can quickly switch to ctrl-+ to zoom in. In order to edit source code you must install by source:python3 -m venv \nsource /bin/activate\ncd \ngit clone  \npip install pyqt5==5.12.1 lxml\ncd labelImg\nmake qt5py3\npython3 labelImgNow custom-edit the key-strokes in labelImg.py and git-commit your changes.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/610", "issue_status": " Open\n", "issue_list": [{"user_name": "chrisrapson", "datetime": "Jun 25, 2020", "body": "First of all, thanks for developing an excellent tool and making it available! Our team has built our workflow around it and annotated about 1000 images already. While we were doing that, we came up with a few ideas for improvements. I have actually implemented them already, and could provide a PR for some or all of them, as you prefer.One more thing: I think, but am not completely sure, that you should subtract 1 when loading VOC bounding box coordinates. (And +1 when saving them.) That is because VOC was originally developed in matlab, and uses 1-based indices. We are using YOLO's annotation format, so this didn't affect us and I haven't modified anything.", "type": "commented", "related_issue": null}, {"user_name": "radek1991", "datetime": "Sep 7, 2020", "body": "Hey  at what price point supported and improved version of this tool is still interesting for you :) ?", "type": "commented", "related_issue": null}, {"user_name": "chrisrapson", "datetime": "Sep 8, 2020", "body": " I don't understand. The license on this repository is MIT, so it is free in all respects. Having access to the code allowed me to make modifications (I would say \"improvements\" but that's only subjective until I get input from others). I'm offering those modifications as a pull request - also free.", "type": "commented", "related_issue": null}, {"user_name": "chrisrapson", "datetime": "Oct 1, 2020", "body": " are you interested in a pull request for the other suggestions?", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 2, 2020", "body": "Sure. I am open to any pull request if it can be beneficial to users", "type": "commented", "related_issue": null}, {"user_name": "wch1446", "datetime": "Oct 23, 2020", "body": "allow panning by clicking and dragging  这个怎么实现呢", "type": "commented", "related_issue": null}, {"user_name": "chrisrapson", "datetime": "Oct 26, 2020", "body": "Hi  I'm sorry I didn't understand your message. Is that the mandarin translation for \"allow panning by clicking and dragging\"? Are you asking for it to be included in a menu or readme somewhere?", "type": "commented", "related_issue": null}, {"user_name": "wch1446", "datetime": "Oct 27, 2020", "body": "你的那个第三个要求是怎么实现呢？    我是中国人不好意思 英文不好 也不是程序员  只是现在有个项目用到了这个工具 发现放大图片后不能去拖动图片  去标注的时候效率很低  不知道你实现的是不是放大图片后能随意去拖动图片？", "type": "commented", "related_issue": null}, {"user_name": "chrisrapson", "datetime": "Oct 28, 2020", "body": " assuming the automatic translations are working correctly, I understand you're asking about how I implemented the panning by click and drag? You can see the changes in this commit:Basically, while the mouse button is held down, it uses the motion of the cursor to change the offset position of the frame. It works for me, and since tzutalin accepted the pull request, I assume it works for them. If you can provide any more details on why/when it's not working for you, I'd be happy to look into it.", "type": "commented", "related_issue": null}, {"user_name": "wch1446", "datetime": "Oct 29, 2020", "body": "I got it wrong. Your translation is to pull out the box to translate. What I want is that the marked picture can be translated after zooming in", "type": "commented", "related_issue": null}, {"user_name": "chrisrapson", "datetime": "Aug 24, 2020", "body": [], "type": "pull", "related_issue": "#636"}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/546", "issue_status": " Open\n", "issue_list": [{"user_name": "vik748", "datetime": "Jan 21, 2020", "body": "Hi,\nWe have been working with labeling on very high resolution photo mosaics. This package is great at being able to handle the resolutions.  The only annoying thing we found is that when we click the zoom out button in the toolbar, the image jumps around to a random new location.  It would be nice to get more consistent behavior with high res images.Thanks for the great package,\nVik", "type": "commented", "related_issue": null}, {"user_name": "teusbenschop", "datetime": "Jun 24, 2020", "body": "I am seeing the same behaviour when zooming in a lot to label very small objects. It will be similar to high resolution images in this issue. When zooming through the mouse and keyboard, the Control key is to be kept down for zooming. But at times the program loses track of the state of the Control key. Instead of zooming through the mouse, it scrolls. That causes undefined jumps in the experience of our labeller.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/458", "issue_status": " Open\n", "issue_list": [{"user_name": "yxchng", "datetime": "Apr 1, 2019", "body": "Using latest master code", "type": "commented", "related_issue": null}, {"user_name": "meatball-guy", "datetime": "Apr 4, 2019", "body": "I am also facing this issue. It is quite frustrating to use.\nIt happens randomly. Sometimes it is ok and sometimes it just does it\nEDIT 2: I also realised that the box also moves after moving to the next page, also by 1 pixel.", "type": "commented", "related_issue": null}, {"user_name": "yxchng", "datetime": "Apr 4, 2019", "body": " Any suggestion for better tools?", "type": "commented", "related_issue": null}, {"user_name": "duming", "datetime": "Apr 22, 2019", "body": "I also got this problem.\nWhen you want to label some small objects, the box will shift down or shift right by one pixel.\nSteps to reproduce:And I traced the bug a little bit.\nI think it may caused by different drawing functions labelimg using while/after creating the box.\nBefore you release your mouse left button, the shape is not add to canvas.shapes list yet. So canvas class draws the box using \nHowever after the the mouse is released, the shape is inserted to canvas.shapes. And canvas will call each shape's own paint() function to draw the box. In shape.paint(), it using QpainterPath API to draw the the vertexes and lines separately. These two functions may have different behaviors.I hope somebody familiar with QT can look into this issue.", "type": "commented", "related_issue": null}, {"user_name": "sunnyzhong812", "datetime": "Sep 25, 2020", "body": " ", "type": "commented", "related_issue": null}, {"user_name": "sunnyzhong812", "datetime": "Sep 30, 2020", "body": "I found that this happens when the viewing ratio ends at 1, 2, 6 and 7. But I don't know how can solve this problem.", "type": "commented", "related_issue": null}, {"user_name": "sunnyzhong812", "datetime": "Sep 30, 2020", "body": "I think it's ok.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/194", "issue_status": " Open\n", "issue_list": [{"user_name": "mrgloom", "datetime": "Nov 7, 2017", "body": "Is it possible to move through image via dragging by mouse?", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Nov 17, 2017", "body": "Can you elaborate more? Now, it can use the mouse to drag each bounding box.", "type": "commented", "related_issue": null}, {"user_name": "mrgloom", "datetime": "Nov 17, 2017", "body": "I talking not about dragging bboxes, but about behaviour of 'dragging image view' like we have on google maps, etc.", "type": "commented", "related_issue": null}, {"user_name": "luoyetx", "datetime": "Dec 7, 2017", "body": "I think this feature is helpful to label a big image.", "type": "commented", "related_issue": null}, {"user_name": "cooli7wa", "datetime": "Jul 30, 2018", "body": "\nI add dragging in , you can try.", "type": "commented", "related_issue": null}, {"user_name": "mrgloom", "datetime": "Nov 17, 2017", "body": [], "type": "issue", "related_issue": "wkentaro/labelme#37"}, {"user_name": "vdalv", "datetime": "May 24, 2018", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "rjdbcm", "datetime": "Apr 25, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/186", "issue_status": " Open\n", "issue_list": [{"user_name": "andrew-veresov", "datetime": "Oct 29, 2017", "body": "Please add an ability to use the keyboard to move bounding box borders.\nLet each border has a key combination assigned to it. For example for the left border, ctrl+shift+w for the top, ctrl+shift+x for the bottom border and ctrl+shift+d for the right one.\nBy pressing key combination and left/right or top/bottom arrows user can move the border in the left/right or top/bottom direction.", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Nov 6, 2017", "body": "Hi  ,\nSo far, it supports using arrow keyboard to move the selected bounding box. Does it not work in your environment?", "type": "commented", "related_issue": null}, {"user_name": "andrew-veresov", "datetime": "Nov 6, 2017", "body": "Hi  !\nYes it works. But it will be really great if LabelImg get an ability to resize a box via keyboard. May be an ability to simply increase/decrease height or width of a rectangle using a keyboard.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 27, 2018", "body": " \nI'm thinking that WASD would  the bounding box size in the up/left/down/right direction, respectively... What should be the keyboard input for  the box size? SHIFT + WASD?", "type": "commented", "related_issue": null}, {"user_name": "lorenzob", "datetime": "May 27, 2018", "body": "My idea was this:I think directly using WSAD is confusing. Usually W goes in one direction, S does the opposite. Here S moves a different element. If I moved a border too much I cannot simply \"go back\" as I'm used to do with the arrows keys.\nSHIFT+WSAD also complicates things because you probably have one hand on the mouse and it is not so easy to use SHIFT+WSAD with one hand only. Also is not very natural to press SHIFT+D to go left.Selecting the element with the mouse pointer takes a little time but I'm already using the mouse, probably I'm still very close to the box and by selecting one corner I can move two borders.In general, I think that arrow keys should also act like WSAD for left-handed people.", "type": "commented", "related_issue": null}, {"user_name": "andrew-veresov", "datetime": "Jun 3, 2018", "body": "I think it's better to focus on moving edges not the corners. Usually when the need to shift an edge  appears you'are looking at the middle of an edge, not at the corner. This is cause most of the real life objects are not rectangular.\nSo, I think it will be easy if WSAD will move a corresponding edge. Yep it will affect two corners at a time. But it's exactly what I needed most of the time. In this case Shift+WSAD might move an edge in the opposite direction. And the mouse still could be used to select a rectangle.", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Oct 30, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 27, 2018", "body": [], "type": "issue", "related_issue": "#171"}, {"user_name": "rjdbcm", "datetime": "Apr 25, 2019", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/335", "issue_status": " Open\n", "issue_list": [{"user_name": "acscas", "datetime": "Jul 23, 2018", "body": "This is not an issue per-se, but suggests added functionality that is basic and would improve the App. App works great (Py 2.7, CentOS) -- thanks so much for contributing!I'm wondering if it's possible to add a panning ability for labelling large images? It's currently difficult to do so since you have to use the scroll wheel at bottom.I don't have experience in PyQt, but if someone does could they suggest a way to enable this? Or would it require a large time investment?", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "Jul 26, 2018", "body": "Can you clarify the experience you're looking for?For example, if you move the mouse off to the right, the focus should shift towards the right?", "type": "commented", "related_issue": null}, {"user_name": "acscas", "datetime": "Jul 26, 2018", "body": "Sure thing -- When I zoom into an image it is no longer completely displayed in the window. Is there a way to pan to the left or right? For example, in some GUIs you can shift+click and then when you move the mouse the image moves instead. In this way, if it's hidden out of view you can slide it back into view without going to the horizontal scroll bar at the bottom of the window.When I click to make a new bounding box and then drag off screen (after first zooming in first so the image flows beyond the window), the window does not slide to show the endiing position of the bounding box (for example). This is shown in the attached image.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "Jul 28, 2018", "body": "Thanks for the clarification. These are definitely worthwhile features, but unfortunately tzutalin and I are a bit strapped for time, so not sure when we could have this implemented.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "Jul 26, 2018", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/243", "issue_status": " Open\n", "issue_list": [{"user_name": "eypros", "datetime": "Feb 23, 2018", "body": "I am using labelImg to annotate object for object annotation task. I found it quite useful but I noticed that when an annotation is modified and I switch to another image the annotation is lost. This is quite strange for a program feature.The more intuitive approach would be to ask if changes should be saved and then proceed to opening the new image. Another alternative would be to automatically save changes if file already exist (annotation file is being edited and not created from scratch for example).", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 17, 2018", "body": "I'm not experiencing this w/ the latest code, so I believe this issue has since been dealt w/. Please pull the latest code or build.", "type": "commented", "related_issue": null}, {"user_name": "eypros", "datetime": "May 24, 2018", "body": "I have now a better understanding of the program. So, on version 1.6 the problem is still present but in a specific situation:\nIf I move between images using the / or their keyboard shortcut the warning exists (as was expected) BUT if I choose to pick an image by mouse it does not.\nSo, please take a look into that also.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 24, 2018", "body": "Interesting, thanks for the update.", "type": "commented", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 23, 2018", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 24, 2018", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "vdalv", "datetime": "May 24, 2018", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/426", "issue_status": " Open\n", "issue_list": [{"user_name": "keides2", "datetime": "Jan 5, 2019", "body": "I use xrdp on Ubuntu 16.04 and I remotely connect from Windows 7 remote desktop.\nI can key in the terminal on Ubuntu, but I can not key in labelImg. Mouse input is available.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/56", "issue_status": " Open\n", "issue_list": [{"user_name": "alexdominguez09", "datetime": "Feb 20, 2017", "body": "Hi,\nI am using this tools to distribute it to students so they can annotate images. We have been trying it for a while, in UBUNTU 16.04 machines and it is a very simple and straight forward tool. Perfect for the job.However, our students claim that they need too many mouse clicks per image, and after evaluating the user interface and usability, we have come to the conclusion that the label image tool would need an improvement in the usability.At the moment, a whole annotation requires:Next Image button (1 click) -> Create Box button (1 click)->  actual box creation (2 clicks) -> Select label dialogue (2 fast clicks) -> Save button (1 click) -> Enter (1 touch key) ->Next Image ... -> ...That is 7 clicks plus 1 enter key.Our request (as we dont have the knowledge or resources to accomplish it) would be to reduce it to 2 clicks.That would be, once the image to work with is on the screen:That way, we reduce the work form 8 interactions to 2. Obviously, that would be implemented with a mode that user could select (turbo mode), so the user can always go back to the normal mode, to change the label, location..or any other setting.Well, that is our result of a studding after using it for a while.Thanks for the good work.Alex", "type": "commented", "related_issue": null}, {"user_name": "alexdominguez09", "datetime": "Feb 20, 2017", "body": "I forgot to comment, that our proposition would be such, as it is faster to annotate just one label per operator, than going to several labels per image. Eg. One operator labels only dog, other operator label cats, other does cars, ..and so on.", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "Feb 21, 2017", "body": "Hi  ,\nThanks for your suggestion. I will take your suggestion into account. If adding a turbo mode, it will be better. I even thought I should create a tool to help people label automatically for the first time. I will try to think about it and do it when I have free time.", "type": "commented", "related_issue": null}, {"user_name": "zuphilip", "datetime": "Feb 21, 2017", "body": "Usually, it can also be that there are more object of the same label you have to annotate. Thus, I am not sure how often we can automatically continue after the first box is entered. Moreover, it might be that I want to edit the box a little after the first sketch, but then with your turbo mode I would already be on the next image.I like the option that one does not have to choose a label, if there is a default label which is used all over. This would be similar to  .I envision more, that creating a new rectangle and the next page will be entered with the keyboard shortcut. Thus my left hand is on the keyboard pressing , ,  and my right hand entering the rectangles with the mouse. The annotations are IMO automatically saved when I click on \"next image\".", "type": "commented", "related_issue": null}, {"user_name": "cooli7wa", "datetime": "Jul 30, 2018", "body": "I also think two click is nice, one click one corner.But I think dialogue box is necessary, because:So I modify this in , you can try it.", "type": "commented", "related_issue": null}, {"user_name": "tzutalin", "datetime": "May 18, 2017", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/heartexlabs/labelImg/issues/499", "issue_status": " Open\n", "issue_list": [{"user_name": "DamonDBT", "datetime": "Sep 4, 2019", "body": "\nNow when you are making a box, click the left button, hold down the mouse, and release the mouse to form a box.\nBut if you do a lot of label, it is very painful.\nClick the left button, release the left button, move the mouse, the box displays in real time, click the left button again to confirm the box.\nAvoid moving the mouse with the left button for a long time, reducing labor intensity.\nCan the software default to a rectangular box instead of a square box? Save the switch operation, thank you.", "type": "commented", "related_issue": null}, {"user_name": "jyxjjj", "datetime": "Oct 30, 2019", "body": "The Author is form Canada.\nEnglish plz.", "type": "commented", "related_issue": null}, {"user_name": "DamonDBT", "datetime": "Oct 31, 2019", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/590", "issue_status": " Open\n", "issue_list": [{"user_name": "Brigadirk", "datetime": "Aug 18, 2017", "body": "When I start up the Minecraft server, I'm able to move and adjust the screen as I please.However, after starting a mission, any attempt to move the Minecraft window will send it spiralling out of control across the screen, often sliding out of vision, necessitating a restart of the server.Additionally, adding a videoproducer to the XML centers the Minecraft window. Combined with the screen playing ice hockey with the Minecraft window, this can make it very annoying to run tests on a single laptop screen.According to David Bignell, this only happens on Mac, hence the title.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 18, 2017", "body": "Yes, after some digging it seems it's due to to the following code in EntityRenderer:This is also the reason why the OSX Minecraft window is so reluctant to relinquish the mouse capture (Malmo contains code to ensure Minecraft doesn't keep hold of the mouse, which works fine on Windows and Linux). Any time the window gains focus and the mouse is outside of the window, the mouse is automatically grabbed, and moved to within the window... Unfortunately, if you are dragging the window, the window gets moved with the mouse (leaving the mouse outside the window client area again, thus re-triggering the move, shifting the window again, etc.etc.)", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 21, 2017", "body": "As for the automatic centring of the window after resize...\nIn theory this should be an easy fix - just grab the x/y coords of the window before resizing, and reset them afterwards:Unfortunately there are a few problems with this:\nFirstly, lwjgl (which Minecraft uses) has mismatched position getters/setters - the getters seem to take into account the window furniture (title bar etc), whereas the setters don't - so callingwill result in the window gradually creeping up and to the left.Secondly, this is lwjgl's helpful OSX implementation of the  method (which is called by ):So calling  on OSX doesn't actually move the window at all... though it does cache the values for x and y, which will then be used the next time the window is recreated (eg when the dimensions change), so, somewhat counter-intuitively, this almost works:Why is the  necessary at all, in this case?\nBecause Display's cached x and y values (which are used when recreating the window) have nothing to do with the window's  position. They are not updated if the window is moved by the user. (This is what causes the centring in the first place - by default they are set to -1, which is a magic value used to indicate that the window should be centred, and moving the window manually doesn't change them, so each resize re-centres the window.)Fortunately  and  get the  window positions (hence the mismatch between getters and setters - the  and  methods which return the cached values are private), and then these values can be written back to the cached values using , so that the window will appear in  the right place after recreation.Ugh.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Aug 22, 2017", "body": "To make matters worse, according to the lwjgl documentation for , \"The window is clamped to remain entirely on the screen.\"\nAs far as I can tell, this is just a lie. There doesn't appear to be any code anywhere that actually implements this. So the result of creeping up and to the left, eventually, is that the Minecraft window disappears off screen entirely.Admittedly, it takes a while for this to happen, and under normal working conditions the Minecraft window won't be changing size often, so it shouldn't present a problem in most cases.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Sep 28, 2017", "body": "This has been \"fixed\" (allowing for the above ugliness) in 0.31.0.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Oct 13, 2017", "body": "Am reopening this because the fix (making sure  is false for the check in ) breaks the continuous attack command.", "type": "commented", "related_issue": null}, {"user_name": "Brigadirk", "datetime": "Aug 18, 2017", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Sep 28, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Oct 13, 2017", "body": [], "type": "reopened this", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/577", "issue_status": " Open\n", "issue_list": [{"user_name": "DaveyBiggers", "datetime": "Jul 13, 2017", "body": "This is a regression as of version 0.30.0 - Minecraft sometimes steals focus (mouse and keyboard). This seems to happen at the end of each mission.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Jul 20, 2017", "body": "Can't seem to reproduce this now, but have definitely seen it recently.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2401", "issue_status": " Open\n", "issue_list": [{"user_name": "Dulanic", "datetime": "Jun 4, 2022", "body": "**Is your feature request related to a problem? Please describe. **As a user, I would like to be able to easily remove faces without having to move my mouse all over. As part of this, I would like to be able to keep my mouse in place and just keep clicking to remove faces.As x is pressed to remove faces, that face should roll to the bottom of the webui and move the next face to the top. The other option I've thought of just now and maybe is a better idea, allow bulk select on the people tab?  A select all button could be a option?Other option I thought of too, maybe set keys (1,2,3,4) toggle the the remove face on the people tab for each face (1 for 1st photo, 2 for 2nd photo and so on?) Though that would likely be seperate coverage under enhanced keyboard uses. So Ill focus on 1st thing i said.Moving my mosue from face to fce over and over. Not a huge problem, but when it is say a group photo, it can have 10+ faces to click x on.See how the X'ed faces stay at the top.", "type": "commented", "related_issue": null}, {"user_name": "pabsi", "datetime": "Jun 9, 2022", "body": "In the meantime, I'd appreciate a query on MySQL that could just remove all faces from a photo (or the equivalent of clicking \"X\" on each one of them as discarded faces).\n", "type": "commented", "related_issue": null}, {"user_name": "centralhardware", "datetime": "Jul 5, 2022", "body": "it would be grate, if i can batch select photos and have button for remove all faces from all image", "type": "commented", "related_issue": null}, {"user_name": "Dulanic", "datetime": "Jun 4, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/2103", "issue_status": " Open\n", "issue_list": [{"user_name": "Alestrix", "datetime": "Mar 3, 2022", "body": "On \"small\" screens (in my case Amazon Silk browser on 4k screen, but apparently rendered for better readability at far distance), the navigation menu on the left side is not fully displayed with no way to scroll down, as there is no scroll bar displayed.I found a similar topic  but this is not the same issue.You can reproduce the behavior on a regular browser if you increase browser zoom to a value that causes the navigation pane to become longer than the screen's height. In this case there is no way to reach the hidden areas of the navigation unless you have a scroll wheel on your mouse, which is not the case for all devices.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 3, 2022", "body": "You have no mouse wheel, touch screen, or touch pad available? Firefox would display a scrollbar.We should eventually provide a special UI for TVs. Also other issues with those browsers.", "type": "commented", "related_issue": null}, {"user_name": "Alestrix", "datetime": "Mar 3, 2022", "body": "Thanks, I'll see whether I can get Firefox working on that device.PS: yes, no wheel or touch available on that device\nPPS: Is there a GitHub issue for the TV interface? As there's another problem I stumbled across when using Photoprism on the TV which I would add as a comment there, which is: no RETURN key on TV, so after a search term is entered, there is no way to start that search.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 6, 2022", "body": "I thought we have an issue for a TV-friendly UI already, but found only these:Turned this issue into an idea that we can put it on our roadmap as soon as we have the resources.", "type": "commented", "related_issue": null}, {"user_name": "bm55", "datetime": "Jun 30, 2022", "body": "How about casting to another browser running PhotoPrism similar to how Jellyfin casting works. For example - if you have a laptop or another low powered device with browser connected to TV and PhotoPrism is opened then use your phone to cast and control the browser connected to TV?", "type": "commented", "related_issue": null}, {"user_name": "Alestrix", "datetime": "Mar 3, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 6, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 6, 2022", "body": [], "type": "added", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1409", "issue_status": " Open\n", "issue_list": [{"user_name": "rosscullen", "datetime": "Jul 4, 2021", "body": "Just installed today and really impressed so far. I'm a traditional user with a 'folders and files' style workflow.Would be great to see a back button on the menu bar (I've mocked up an example below)Could also be used in other areas. I got the idea off the Home Assistant project where it works very well on it. Hopefully other users feel similarly and the idea hasn't already been suggested. Thanks and keep up the great work :-)", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 5, 2021", "body": "Any reason you don't use the standard browser back button?", "type": "commented", "related_issue": null}, {"user_name": "rosscullen", "datetime": "Jul 5, 2021", "body": "Thats a fair and valid question...\nPersonally, I think over time I have lost trust with the browser back button. eCommerce forms... eBanking... a lot of the time you loose your search results or page takes longer to re-render etc.\n*\nI've noticed on the readme that one of the \"Key Features\" of Photoprism is that the system \"is built as a progressive web app\"... If someone is using it as a PWA, where's the browser back button?I'm no UX expert/researcher but I have generally found with web applications, the navigation is contained within the application area (particularly if you're using a browser in kiosk mode or similar). And it is quicker than dragging your mouse all the way up to the top left hand corner of the screen. By having your own back button, there may also be an opportunity to used a cached version of the previous page... but I don't know enough to understand the 'behind the scenes' of the core similarities/differences.Which also brings up another question... would you consider using breadcrumbs to navigate folders/sub-folders? Similar to $ynology Photos.Lastly, when you open a photo, would you consider centering the buttons that currently are in the top right (and a back button like $ynology Photos).Appreciate you taking the time to consider my suggestions, really impressed what you have achieved to date... well done", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 6, 2021", "body": "That's a valid point. Using touch gestures could be an option as well.We already have breadcrumbs in Originals (see  ) That's the only section with subfolders.We plan to build our own optimized photo viewer: .", "type": "commented", "related_issue": null}, {"user_name": "rosscullen", "datetime": "Jul 7, 2021", "body": "Thanks for updates 👍🏻", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jul 5, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 2, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 2, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 16, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/microsoft/malmo/issues/869", "issue_status": " Open\n", "issue_list": [{"user_name": "DarthMalloc", "datetime": "Feb 29, 2020", "body": "I have built Malmo on Linux Mint 19.3 with OpenJDK8, and when I attempt to run Minecraft with the Malmo mod using./launchClient.sh -port 10234 -envI get the following output, as shown in the screenshot below:As the picture shows, the second last line is[15:13:59] [Realms Notification Availability checker /INFO]: Could not authorize you against Realms server: Invalid session idI think this is preventing my from running missions, because when I try to run tutorial_1.py, nothing happens. There is no error message, but the terminal gets stuck on the commandpython3.7 tutorial_1.py.\nI would greatly appreciate help in resolving this issue.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Mar 2, 2020", "body": "I believe the Realms server error message is a red-herring - if memory serves, this is always displayed, and is nothing to worry about. From the log, it looks as though Minecraft is running and is ready for use (the line \"CLIENT enter state: DORMANT\" means we're ready to receive a mission).I suspect there are two problems here:a) Malmo's default communication port is 10000 - if that's undesirable for some reason, you can override it - as you are doing, by passing the \"-port 10234\"cmdline argument. But the client code (eg tutorial_1.py) needs to know what to connect to. The tutorials all assume the default port, so you either need to run Minecraft on the default port, or change your client code to pass in 10234. For details of how to do that, see (for example)  - look for the ClientPool stuff around line 70.b) The other problem might be your use of the -env flag - this runs Malmo in the \"MalmoEnv\" mode, where it can present as a gym environment, but the tutorials (and all the python samples) were written before this feature became available, and communicate with Minecraft using a non-gym protocol (which is much more flexible but potentially less convenient, depending on your needs). I suspect none of the samples will work if the -env flag is specified.Let us know if this helps - if not, we'll do some more digging.", "type": "commented", "related_issue": null}, {"user_name": "DarthMalloc", "datetime": "Mar 3, 2020", "body": "Thank you very much for getting back to me. I tried the approach that you suggested, but I am still seeing the same problem. Here is the entire output that I am seeing when I run the command./launchClient.sh -port 10000This mapping 'snapshot_20161220' was designed for MC 1.11! Use at your own peril.\n#################################################\nForgeGradle 2.2-SNAPSHOT-3966cea\n\n#################################################\nPowered by MCP unknown\n\nby: Searge, ProfMobius, Fesh0r,\nR4wk, ZeuX, IngisKahn, bspkrs\n#################################################\n:deobfCompileDummyTask\n:deobfProvidedDummyTask\n:getVersionJson\n:extractUserdev UP-TO-DATE\n:extractDependencyATs SKIPPED\n:extractMcpData SKIPPED\n:extractMcpMappings SKIPPED\n:genSrgs SKIPPED\n:downloadClient SKIPPED\n:downloadServer SKIPPED\n:splitServerJar SKIPPED\n:mergeJars SKIPPED\n:deobfMcSRG SKIPPED\n:decompileMc SKIPPED\n:fixMcSources SKIPPED\n:applySourcePatches\n:remapMcSources SKIPPED\n:recompileMc SKIPPED\n:extractNatives SKIPPED\n:getAssetIndex UP-TO-DATE\n:getAssets\n:makeStart SKIPPED\n:setupDecompWorkspaceBUILD SUCCESSFULTotal time: 13.407 secsThis build could be faster, please consider using the Gradle Daemon: \nThis mapping 'snapshot_20161220' was designed for MC 1.11! Use at your own peril.\n#################################################\nForgeGradle 2.2-SNAPSHOT-3966cea\n\n#################################################\nPowered by MCP unknown\n\nby: Searge, ProfMobius, Fesh0r,\nR4wk, ZeuX, IngisKahn, bspkrs\n#################################################\n:deobfCompileDummyTask\n:deobfProvidedDummyTask\n:sourceApiJava UP-TO-DATE\n:compileApiJava UP-TO-DATE\n:processApiResources UP-TO-DATE\n:apiClasses UP-TO-DATE\n:copyModToClient\n:copyModToServer\n:deleteSchemas\n:copySchemas\n:jaxb UP-TO-DATE\n:sourceMainJava UP-TO-DATE\n:compileJava\nwarning: [options] bootstrap class path not set in conjunction with -source 1.6\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n1 warning\n:processResources UP-TO-DATE\n:classes\n:jar UP-TO-DATE\n:sourceTestJava UP-TO-DATE\n:compileTestJava UP-TO-DATE\n:processTestResources UP-TO-DATE\n:testClasses UP-TO-DATE\n:test UP-TO-DATE\n:extractMcpData SKIPPED\n:extractMcpMappings SKIPPED\n:getVersionJson\n:extractUserdev UP-TO-DATE\n:genSrgs SKIPPED\n:reobfJar\n:assemble\n:check UP-TO-DATE\n:buildBUILD SUCCESSFULTotal time: 7.514 secsThis build could be faster, please consider using the Gradle Daemon: \nThis mapping 'snapshot_20161220' was designed for MC 1.11! Use at your own peril.\n#################################################\nForgeGradle 2.2-SNAPSHOT-3966cea\n\n#################################################\nPowered by MCP unknown\n\nby: Searge, ProfMobius, Fesh0r,\nR4wk, ZeuX, IngisKahn, bspkrs\n#################################################\n:deobfCompileDummyTask\n:deobfProvidedDummyTask\n:sourceApiJava UP-TO-DATE\n:compileApiJava UP-TO-DATE\n:processApiResources UP-TO-DATE\n:apiClasses UP-TO-DATE\n:copyModToClient\n:copyModToServer\n:deleteSchemas\n:copySchemas\n:jaxb UP-TO-DATE\n:sourceMainJava UP-TO-DATE\n:compileJava\nwarning: [options] bootstrap class path not set in conjunction with -source 1.6\nNote: Some input files use or override a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\nNote: Some input files use unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n1 warning\n:processResources UP-TO-DATE\n:classes\n:jar\n:getVersionJson\n:extractNatives SKIPPED\n:extractUserdev UP-TO-DATE\n:getAssetIndex UP-TO-DATE\n:getAssets\nCurrent status: 1048/1196   87%\n:makeStart SKIPPED\n:runClient\n[20:17:53] [main/INFO] [GradleStart]: Extra: []\n[20:17:53] [main/INFO] [GradleStart]: Found and added coremod: com.microsoft.Malmo.OverclockingPlugin\n[20:17:53] [main/INFO] [GradleStart]: Running with arguments: [--userProperties, {}, --assetsDir, /home/justin/.gradle/caches/minecraft/assets, --assetIndex, 1.11, --accessToken{REDACTED}, --version, 1.11.2, --tweakClass, net.minecraftforge.fml.common.launcher.FMLTweaker, --tweakClass, net.minecraftforge.gradle.tweakers.CoremodTweaker]\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Using primary tweak class name net.minecraftforge.fml.common.launcher.FMLTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.gradle.tweakers.CoremodTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLTweaker\n[20:17:53] [main/INFO] [FML]: Forge Mod Loader version 13.20.0.2228 for Minecraft 1.11.2 loading\n[20:17:53] [main/INFO] [FML]: Java is Java HotSpot(TM) 64-Bit Server VM, version 1.8.0_221, running on Linux:amd64:4.15.0-76-generic, installed at /usr/lib/jvm/jdk1.8.0_221/jre\n[20:17:53] [main/INFO] [FML]: Managed to load a deobfuscated Minecraft name- we are in a deobfuscated environment. Skipping runtime deobfuscation\n[20:17:53] [main/INFO] [FML]: Found a command line coremod : com.microsoft.Malmo.OverclockingPlugin\n[20:17:53] [main/WARN] [FML]: The coremod com.microsoft.Malmo.OverclockingPlugin does not have a MCVersion annotation, it may cause issues with this version of Minecraft\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.gradle.tweakers.CoremodTweaker\n[20:17:53] [main/INFO] [GradleStart]: Injecting location in coremod net.minecraftforge.fml.relauncher.FMLCorePlugin\n[20:17:53] [main/INFO] [GradleStart]: Injecting location in coremod net.minecraftforge.classloading.FMLForgePlugin\n[20:17:53] [main/INFO] [GradleStart]: Injecting location in coremod com.microsoft.Malmo.OverclockingPlugin\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.gradle.tweakers.AccessTransformerTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLInjectionAndSortingTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n[20:17:53] [main/ERROR] [FML]: The binary patch set is missing. Either you are in a development environment, or things are not going to work!\n[20:17:53] [main/ERROR] [FML]: FML appears to be missing any signature data. This is not a good thing\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.relauncher.CoreModManager$FMLPluginWrapper\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.FMLDeobfTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.gradle.tweakers.AccessTransformerTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Loading tweak class name net.minecraftforge.fml.common.launcher.TerminalTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Calling tweak class net.minecraftforge.fml.common.launcher.TerminalTweaker\n[20:17:53] [main/INFO] [LaunchWrapper]: Launching wrapped minecraft {net.minecraft.client.main.Main}\n[20:17:53] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:58]: MALMO: Attempting to transform MinecraftServer\n[20:17:53] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:187]: MALMO: Found Minecraft, attempting to transform it\n[20:17:53] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:193]: MALMO: Found Minecraft.runGameLoop() method, attempting to transform it\n[20:17:53] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockRenderer:208]: MALMO: Hooked into call to Minecraft.updateDisplay()\n[20:17:54] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:42]: Transformed Name: net.minecraft.client.entity.EntityPlayerSP\n[20:17:54] [main/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:42]: Transformed Name: net.minecraft.client.entity.AbstractClientPlayer\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:transform:58]: MALMO: Attempting to transform MinecraftServer\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:123]: MALMO: Found MinecraftServer, attempting to transform it\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:129]: MALMO: Found MinecraftServer.run() method, attempting to transform it\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:137]: MALMO: Transforming LDC\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:137]: MALMO: Transforming LDC\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:137]: MALMO: Transforming LDC\n[20:17:54] [Client thread/INFO] [STDOUT]: [com.microsoft.Malmo.OverclockingClassTransformer:overclockServer:137]: MALMO: Transforming LDC\n[20:17:54] [Client thread/INFO]: Setting user: Player243\n[20:17:56] [Client thread/WARN]: Skipping bad option: lastServer:\n[20:17:56] [Client thread/INFO]: LWJGL Version: 2.9.4\n[20:17:57] [Client thread/INFO]: [STDOUT]: MALMO: Attempting to transform MinecraftServer\n[20:17:57] [Client thread/INFO]: [STDOUT]: MALMO: Found GlStateManager, attempting to transform it\n[20:17:57] [Client thread/INFO]: [STDOUT]: MALMO: Found GlStateManager.bindTexture() method, attempting to transform it\n[20:17:57] [Client thread/INFO]: [STDOUT]: MALMO: Hooked into call to GlStateManager.bindTexture()\n[20:17:57] [Client thread/INFO]: [STDOUT]: ---- Minecraft Crash Report ----\n// Everything's going to plan. No, really, that was supposed to happen.Time: 3/2/20 8:17 PM\nDescription: Loading screen debug infoThis is just a prompt for computer specs to be printed. THIS IS NOT A ERROR-- System Details --\nDetails:\nMinecraft Version: 1.11.2\nOperating System: Linux (amd64) version 4.15.0-76-generic\nJava Version: 1.8.0_221, Oracle Corporation\nJava VM Version: Java HotSpot(TM) 64-Bit Server VM (mixed mode), Oracle Corporation\nMemory: 149972144 bytes (143 MB) / 590348288 bytes (563 MB) up to 1908932608 bytes (1820 MB)\nJVM Flags: 1 total; -Xmx2G\nIntCache: cache: 0, tcache: 0, allocated: 0, tallocated: 0\nFML:\nLoaded coremods (and transformers):\nOverclockingPlugin (MalmoMod-0.37.0.jar)\ncom.microsoft.Malmo.OverclockingClassTransformer\nGL info: ' Vendor: 'nouveau' Version: '4.3 (Compatibility Profile) Mesa 19.2.8' Renderer: 'NV117'\n[20:17:57] [Client thread/INFO] [FML]: MinecraftForge v13.20.0.2228 Initialized\n[20:17:57] [Client thread/INFO] [FML]: Replaced 232 ore recipes\n[20:17:57] [Client thread/INFO] [FML]: Found 0 mods from the command line. Injecting into mod discoverer\n[20:17:57] [Client thread/INFO] [FML]: Searching /home/justin/MalmoPlatform/build/install/Minecraft/run/mods for mods\n[20:17:58] [Client thread/INFO] [FML]: Forge Mod Loader has identified 5 mods to load\n[20:17:58] [Client thread/INFO] [FML]: Attempting connection with missing mods [minecraft, mcp, FML, forge, malmomod] at CLIENT\n[20:17:58] [Client thread/INFO] [FML]: Attempting connection with missing mods [minecraft, mcp, FML, forge, malmomod] at SERVER\n[20:17:58] [Client thread/INFO]: Reloading ResourceManager: Default, FMLFileResourcePack:Forge Mod Loader, FMLFileResourcePack:Minecraft Forge, FMLFileResourcePack:Microsoft Malmo Mod\n[20:17:58] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:17:58] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:17:58] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:17:58] [Client thread/INFO] [FML]: Processing ObjectHolder annotations\n[20:17:58] [Client thread/INFO] [FML]: Found 444 ObjectHolder annotations\n[20:17:58] [Client thread/INFO] [FML]: Identifying ItemStackHolder annotations\n[20:17:58] [Client thread/INFO] [FML]: Found 0 ItemStackHolder annotations\n[20:17:58] [Client thread/INFO] [FML]: Applying holder lookups\n[20:17:58] [Client thread/INFO] [FML]: Holder lookups applied\n[20:17:58] [Client thread/INFO] [FML]: Applying holder lookups\n[20:17:58] [Client thread/INFO] [FML]: Holder lookups applied\n[20:17:58] [Client thread/INFO] [FML]: Applying holder lookups\n[20:17:58] [Client thread/INFO] [FML]: Holder lookups applied\n[20:17:58] [Client thread/INFO] [FML]: Configured a dormant chunk cache size of 0\n[20:17:58] [Client thread/INFO]: [STDOUT]: Testing schemas against internal version number: 0.37\n[20:17:58] [Forge Version Check/INFO] [ForgeVersionCheck]: [forge] Starting version check at \n[20:17:59] [Forge Version Check/INFO] [ForgeVersionCheck]: [forge] Found status: OUTDATED Target: 13.20.1.2386\n[20:17:59] [Client thread/INFO] [FML]: Applying holder lookups\n[20:17:59] [Client thread/INFO] [FML]: Holder lookups applied\n[20:17:59] [Client thread/INFO] [FML]: Injecting itemstacks\n[20:17:59] [Client thread/INFO] [FML]: Itemstack injection complete\n[20:17:59] [Sound Library Loader/INFO]: Starting up SoundSystem...\n[20:18:00] [Thread-7/INFO]: Initializing LWJGL OpenAL\n[20:18:00] [Thread-7/INFO]: (The LWJGL binding of OpenAL.  For more information, see )\n[20:18:00] [Thread-7/INFO]: OpenAL initialized.\n[20:18:00] [Sound Library Loader/INFO]: Sound engine started\n[20:18:00] [Client thread/INFO] [FML]: Max texture size: 16384\n[20:18:00] [Client thread/INFO]: Created: 16x16 textures-atlas\n[20:18:01] [Client thread/INFO]: [STDOUT]: CLIENT request state: WAITING_FOR_MOD_READY\n[20:18:01] [Client thread/INFO] [FML]: Injecting itemstacks\n[20:18:01] [Client thread/INFO] [FML]: Itemstack injection complete\n[20:18:01] [Client thread/INFO] [FML]: Forge Mod Loader has successfully loaded 5 mods\n[20:18:01] [Client thread/INFO]: Reloading ResourceManager: Default, FMLFileResourcePack:Forge Mod Loader, FMLFileResourcePack:Minecraft Forge, FMLFileResourcePack:Microsoft Malmo Mod\n[20:18:01] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:18:01] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:18:01] [Client thread/WARN]: ResourcePack: ignored non-lowercase namespace: MalmoMod in /home/justin/MalmoPlatform/build/install/Minecraft/build/libs/MalmoMod-0.37.0.jar\n[20:18:01] [Client thread/INFO]: SoundSystem shutting down...\n[20:18:02] [Client thread/WARN]: Author: Paul Lamb, \n[20:18:02] [Sound Library Loader/INFO]: Starting up SoundSystem...\n[20:18:02] [Thread-9/INFO]: Initializing LWJGL OpenAL\n[20:18:02] [Thread-9/INFO]: (The LWJGL binding of OpenAL.  For more information, see )\n[20:18:02] [Thread-9/INFO]: OpenAL initialized.\n[20:18:02] [Sound Library Loader/INFO]: Sound engine started\n[20:18:02] [Client thread/INFO] [FML]: Max texture size: 16384\n[20:18:02] [Client thread/INFO]: Created: 512x512 textures-atlas\n[20:18:03] [Client thread/WARN]: Skipping bad option: lastServer:\n[20:18:04] [Client thread/INFO]: [STDOUT]: CLIENT enter state: WAITING_FOR_MOD_READY\n[20:18:04] [Thread-11/INFO]: [STDOUT]: INFO: ->mcp(10000) Listening for messages on port 10000\n[20:18:04] [Client thread/INFO]: [STDOUT]: CLIENT request state: DORMANT\n[20:18:04] [Client thread/INFO]: [STDOUT]: CLIENT enter state: DORMANT\n[20:18:04] [Realms Notification Availability checker /INFO]: Could not authorize you against Realms server: Invalid session idI am wondering if this provides any useful information as to what the problem might be. One detail that might be important is that there are two instances of ./launchClient.sh. One is in the MalmoPlatform/Minecraft directory, and the other is in the MalmoPlatform/build/install/Minecraft directory. The one that I have been running is the latter of the two, and I am curious about whether that might have something to do with the problem.", "type": "commented", "related_issue": null}, {"user_name": "DaveyBiggers", "datetime": "Mar 3, 2020", "body": "Thanks for the trace. Okay, so what happens when you try to run the samples? You say the terminal just gets stuck on the command ? Can you step through the python code and see exactly where it gets stuck?", "type": "commented", "related_issue": null}, {"user_name": "DarthMalloc", "datetime": "Mar 3, 2020", "body": "Thank you very much for getting back to me again. I put in some print commands to mark checkpoints in tutorial_1.py. The code runs normally until it gets to the line\n\nOnce it hits this point, it just gets stuck, but does not show any error messages. I ran the ctest command in verbose mode inside the MalmoPlatform/build directory, and I got the following output:UpdateCTestConfiguration  from :/home/justin/MalmoPlatform/build/DartConfiguration.tcl\nParse Config file:/home/justin/MalmoPlatform/build/DartConfiguration.tcl\nUpdateCTestConfiguration  from :/home/justin/MalmoPlatform/build/DartConfiguration.tcl\nParse Config file:/home/justin/MalmoPlatform/build/DartConfiguration.tcl\nTest project /home/justin/MalmoPlatform/build\nConstructing a list of tests\nDone constructing a list of tests\nUpdating test list for fixtures\nAdded 0 tests to meet fixture requirements\nChecking test dependency graph...\nChecking test dependency graph end\ntest 1\nStart   1: CppTests_create_tcp_server1: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_create_tcp_server\n1: Environment variables:\n1:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n1: Test timeout computed to be: 1500\n1: Server listening on port: 10062\n1/113 Test   : CppTests_create_tcp_server .....................................   Passed    0.07 sec\ntest 2\nStart   2: CppTests_test_agent_host2: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_agent_host\n2: Environment variables:\n2:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n2: Test timeout computed to be: 1500\n2: Malmo version: 0.37.0\n2:\n2: Allowed options:\n2:   -h [ --help ]         show description of allowed options\n2:   --test                run this as an integration test\n2:\n2:\n2/113 Test   : CppTests_test_agent_host .......................................   Passed    0.03 sec\ntest 3\nStart   3: CppTests_test_argument_parser3: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_argument_parser\n3: Environment variables:\n3:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n3: Test timeout computed to be: 1500\n3/113 Test   : CppTests_test_argument_parser ..................................   Passed    0.01 sec\ntest 4\nStart   4: CppTests_test_client_server4: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_client_server\n4: Environment variables:\n4:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n4: Test timeout computed to be: 1500\n4: Starting server..\n4: Sending messages..\n4: Exiting..\n4/113 Test   : CppTests_test_client_server ....................................   Passed    0.21 sec\ntest 5\nStart   5: CppTests_test_mission5: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_mission\n5: Environment variables:\n5:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n5: Test timeout computed to be: 1500\n5/113 Test   : CppTests_test_mission ..........................................   Passed    0.01 sec\ntest 6\nStart   6: CppTests_test_parameter_set6: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_parameter_set\n6: Environment variables:\n6:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n6: Test timeout computed to be: 1500\n6/113 Test   : CppTests_test_parameter_set ....................................   Passed    0.00 sec\ntest 7\nStart   7: CppTests_test_persistence7: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_persistence\n7: Environment variables:\n7:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n7: Test timeout computed to be: 1500\n7: ERROR: Cannot write to /home/justin/MalmoPlatform/build/Malmo/test/CppTests/c://path//that//probably//does//not//exist//output.tgz - check the path exists and you have permission to write there.\n7: Error starting mission: Failed to find an available client for this mission - tried all the clients in the supplied client pool.\n7:\n7/113 Test   : CppTests_test_persistence ......................................***Failed   60.64 sec\ntest 8\nStart   8: CppTests_test_string_server8: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_string_server\n8: Environment variables:\n8:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n8: Test timeout computed to be: 1500\n8: Starting server..\n8: Sending messages..\n8: Exiting..\n8: Starting server..\n8: Sending messages..\n8: terminate called after throwing an instance of 'std::runtime_error'\n8:   what():  Response read failed Operation canceled\n8/113 Test   : CppTests_test_string_server ....................................***Exception: Child aborted 61.24 sec\ntest 9\nStart   9: CppTests_test_video_server9: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_video_server\n9: Environment variables:\n9:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n9: Test timeout computed to be: 1500\n9/113 Test   : CppTests_test_video_server .....................................   Passed    5.19 sec\ntest 10\nStart  10: CppTests_test_video_writer10: Test command: /home/justin/MalmoPlatform/build/Malmo/test/CppTests/CppTests_test_video_writer\n10: Environment variables:\n10:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n10: Test timeout computed to be: 1500\n10: beginning video writer test...complete.\n10: test complete.\n10/113 Test  : CppTests_test_video_writer .....................................   Passed    0.54 sec\ntest 11\nStart  11: JavaTests_test_agent_host11: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/test/JavaTests/test_agent_host.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"test_agent_host\"\n11: Environment variables:\n11:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n11: Test timeout computed to be: 1500\n11: Error: A JNI error has occurred, please check your installation and try again\n11: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: test_agent_host has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n11: \tat java.lang.ClassLoader.defineClass1(Native Method)\n11: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n11: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n11: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n11: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n11: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n11: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n11: \tat java.security.AccessController.doPrivileged(Native Method)\n11: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n11: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n11: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n11: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n11: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n11/113 Test  : JavaTests_test_agent_host ......................................***Failed    0.47 sec\ntest 12\nStart  12: JavaTests_test_argument_parser12: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/test/JavaTests/test_argument_parser.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"test_argument_parser\"\n12: Environment variables:\n12:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n12: Test timeout computed to be: 1500\n12: Error: A JNI error has occurred, please check your installation and try again\n12: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: test_argument_parser has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n12: \tat java.lang.ClassLoader.defineClass1(Native Method)\n12: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n12: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n12: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n12: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n12: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n12: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n12: \tat java.security.AccessController.doPrivileged(Native Method)\n12: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n12: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n12: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n12: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n12: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n12/113 Test  : JavaTests_test_argument_parser .................................***Failed    0.06 sec\ntest 13\nStart  13: JavaTests_test_mission13: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/test/JavaTests/test_mission.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"test_mission\"\n13: Environment variables:\n13:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n13: Test timeout computed to be: 1500\n13: Error: A JNI error has occurred, please check your installation and try again\n13: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: test_mission has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n13: \tat java.lang.ClassLoader.defineClass1(Native Method)\n13: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n13: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n13: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n13: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n13: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n13: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n13: \tat java.security.AccessController.doPrivileged(Native Method)\n13: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n13: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n13: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n13: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n13: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n13/113 Test  : JavaTests_test_mission .........................................***Failed    0.06 sec\ntest 14\nStart  14: JavaTests_test_parameter_set14: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/test/JavaTests/test_parameter_set.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"test_parameter_set\"\n14: Environment variables:\n14:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n14: Test timeout computed to be: 1500\n14: Error: A JNI error has occurred, please check your installation and try again\n14: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: test_parameter_set has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n14: \tat java.lang.ClassLoader.defineClass1(Native Method)\n14: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n14: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n14: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n14: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n14: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n14: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n14: \tat java.security.AccessController.doPrivileged(Native Method)\n14: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n14: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n14: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n14: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n14: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n14/113 Test  : JavaTests_test_parameter_set ...................................***Failed    0.05 sec\ntest 15\nStart  15: JavaTests_test_wrapping15: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/test/JavaTests/test_wrapping.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"test_wrapping\"\n15: Environment variables:\n15:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n15: Test timeout computed to be: 1500\n15: Error: A JNI error has occurred, please check your installation and try again\n15: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: test_wrapping has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n15: \tat java.lang.ClassLoader.defineClass1(Native Method)\n15: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n15: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n15: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n15: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n15: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n15: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n15: \tat java.security.AccessController.doPrivileged(Native Method)\n15: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n15: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n15: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n15: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n15: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n15/113 Test  : JavaTests_test_wrapping ........................................***Failed    0.07 sec\ntest 16\nStart  16: PythonTests_test_wrapping16: Test command: /usr/bin/python3.6 \"test_wrapping.py\"\n16: Environment variables:\n16:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n16:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n16:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n16: Test timeout computed to be: 1500\n16/113 Test  : PythonTests_test_wrapping ......................................***Exception: SegFault  0.58 sec\ntest 17\nStart  17: PythonTests_test_argument_parser17: Test command: /usr/bin/python3.6 \"test_argument_parser.py\"\n17: Environment variables:\n17:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n17:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n17:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n17: Test timeout computed to be: 1500\n17/113 Test  : PythonTests_test_argument_parser ...............................***Exception: SegFault  0.24 sec\ntest 18\nStart  18: PythonTests_test_agent_host18: Test command: /usr/bin/python3.6 \"test_agent_host.py\"\n18: Environment variables:\n18:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n18:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n18:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n18: Test timeout computed to be: 1500\n18/113 Test  : PythonTests_test_agent_host ....................................***Exception: SegFault  0.20 sec\ntest 19\nStart  19: PythonTests_test_mission19: Test command: /usr/bin/python3.6 \"test_mission.py\"\n19: Environment variables:\n19:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n19:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n19:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n19: Test timeout computed to be: 1500\n19/113 Test  : PythonTests_test_mission .......................................***Exception: SegFault  0.20 sec\ntest 20\nStart  20: PythonTests_test_parameter_set20: Test command: /usr/bin/python3.6 \"test_parameter_set.py\"\n20: Environment variables:\n20:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n20:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n20:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n20: Test timeout computed to be: 1500\n20/113 Test  : PythonTests_test_parameter_set .................................***Exception: SegFault  0.20 sec\ntest 21\nStart  21: PythonTests_test_malmoutils21: Test command: /usr/bin/python3.6 \"test_malmoutils.py\"\n21: Environment variables:\n21:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n21:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n21:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n21: Test timeout computed to be: 1500\n21/113 Test  : PythonTests_test_malmoutils ....................................***Exception: SegFault  0.20 sec\ntest 22\nStart  22: PreIntegrationTests22: Test command: /usr/bin/python3.6 \"launch_minecraft_in_background.py\"\n22: Environment variables:\n22:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n22: Test timeout computed to be: 1500\n22: Something is listening on port 10000 - will assume Minecraft is running.\n22/113 Test  : PreIntegrationTests ............................................   Passed    0.09 sec\ntest 23\nStart  23: CppIntegrationTests_run_mission_help23: Test command: /home/justin/MalmoPlatform/build/Malmo/samples/Cpp_examples/run_mission \"--help\"\n23: Environment variables:\n23:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n23: Test timeout computed to be: 1500\n23: Malmo version: 0.37.0\n23:\n23: Allowed options:\n23:   -h [ --help ]         show description of allowed options\n23:   --test                run this as an integration test\n23:\n23:\n23/113 Test  : CppIntegrationTests_run_mission_help ...........................   Passed    0.03 sec\ntest 24\nStart  24: CppIntegrationTests_run_mission24: Test command: /home/justin/MalmoPlatform/build/Malmo/samples/Cpp_examples/run_mission \"--test\"\n24: Environment variables:\n24:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n24: Test timeout computed to be: 1500\n24: Waiting for the mission to start...\n24: video,observations,rewards received: 22,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 29,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 31,10,0\n24: video,observations,rewards received: 29,10,0\n24: video,observations,rewards received: 31,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 29,10,0\n24: video,observations,rewards received: 31,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 29,10,0\n24: video,observations,rewards received: 31,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 30,10,0\n24: video,observations,rewards received: 2,1,0\n24: Mission has stopped.\n24/113 Test  : CppIntegrationTests_run_mission ................................   Passed   11.02 sec\ntest 25\nStart  25: JavaIntegrationTests_JavaExamples_run_mission_help25: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/samples/Java_examples/JavaExamples_run_mission.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"JavaExamples_run_mission\" \"--help\"\n25: Environment variables:\n25:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n25: Test timeout computed to be: 1500\n25: Error: A JNI error has occurred, please check your installation and try again\n25: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: JavaExamples_run_mission has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n25: \tat java.lang.ClassLoader.defineClass1(Native Method)\n25: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n25: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n25: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n25: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n25: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n25: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n25: \tat java.security.AccessController.doPrivileged(Native Method)\n25: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n25: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n25: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n25: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n25: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n25/113 Test  : JavaIntegrationTests_JavaExamples_run_mission_help .............***Failed    0.10 sec\ntest 26\nStart  26: JavaIntegrationTests_JavaExamples_run_mission26: Test command: /usr/bin/java \"-cp\" \"/home/justin/MalmoPlatform/build/Malmo/samples/Java_examples/JavaExamples_run_mission.jar:/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper/MalmoJavaJar.jar\" \"-Djava.library.path=/home/justin/MalmoPlatform/build/Malmo/src/JavaWrapper\" \"JavaExamples_run_mission\" \"--test\"\n26: Environment variables:\n26:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n26: Test timeout computed to be: 1500\n26: Error: A JNI error has occurred, please check your installation and try again\n26: Exception in thread \"main\" java.lang.UnsupportedClassVersionError: JavaExamples_run_mission has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n26: \tat java.lang.ClassLoader.defineClass1(Native Method)\n26: \tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n26: \tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n26: \tat java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\n26: \tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n26: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n26: \tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n26: \tat java.security.AccessController.doPrivileged(Native Method)\n26: \tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n26: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n26: \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n26: \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n26: \tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)\n26/113 Test  : JavaIntegrationTests_JavaExamples_run_mission ..................***Failed    0.08 sec\ntest 27\nStart  27: PythonIntegrationTests_animation_test_help27: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/animation_test.py\" \"--help\"\n27: Environment variables:\n27:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n27:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n27: Test timeout computed to be: 1500\n27/113 Test  : PythonIntegrationTests_animation_test_help .....................***Exception: SegFault  0.22 sec\ntest 28\nStart  28: PythonIntegrationTests_animation_test28: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/animation_test.py\" \"--test\" \"--recording_dir\" \"animation_test\" \"--record_video\"\n28: Environment variables:\n28:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n28:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n28: Test timeout computed to be: 1500\n28/113 Test  : PythonIntegrationTests_animation_test ..........................***Exception: SegFault  0.19 sec\ntest 29\nStart  29: PythonIntegrationTests_braitenberg_simulation_help29: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/braitenberg_simulation.py\" \"--help\"\n29: Environment variables:\n29:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n29:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n29: Test timeout computed to be: 1500\n29/113 Test  : PythonIntegrationTests_braitenberg_simulation_help .............***Exception: SegFault  0.19 sec\ntest 30\nStart  30: PythonIntegrationTests_braitenberg_simulation30: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/braitenberg_simulation.py\" \"--test\" \"--recording_dir\" \"braitenberg_simulation\" \"--record_video\"\n30: Environment variables:\n30:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n30:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n30: Test timeout computed to be: 1500\n30/113 Test  : PythonIntegrationTests_braitenberg_simulation ..................***Exception: SegFault  0.20 sec\ntest 31\nStart  31: PythonIntegrationTests_build_test_help31: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/build_test.py\" \"--help\"\n31: Environment variables:\n31:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n31:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n31: Test timeout computed to be: 1500\n31: Traceback (most recent call last):\n31:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/build_test.py\", line 42, in \n31:     from past.utils import old_div\n31: ModuleNotFoundError: No module named 'past'\n31/113 Test  : PythonIntegrationTests_build_test_help .........................***Failed    0.10 sec\ntest 32\nStart  32: PythonIntegrationTests_build_test32: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/build_test.py\" \"--test\" \"--recording_dir\" \"build_test\" \"--record_video\"\n32: Environment variables:\n32:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n32:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n32: Test timeout computed to be: 1500\n32: Traceback (most recent call last):\n32:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/build_test.py\", line 42, in \n32:     from past.utils import old_div\n32: ModuleNotFoundError: No module named 'past'\n32/113 Test  : PythonIntegrationTests_build_test ..............................***Failed    0.10 sec\ntest 33\nStart  33: PythonIntegrationTests_chat_reward_help33: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chat_reward.py\" \"--help\"\n33: Environment variables:\n33:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n33:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n33: Test timeout computed to be: 1500\n33/113 Test  : PythonIntegrationTests_chat_reward_help ........................***Exception: SegFault  0.19 sec\ntest 34\nStart  34: PythonIntegrationTests_chat_reward34: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chat_reward.py\" \"--test\" \"--recording_dir\" \"chat_reward\" \"--record_video\"\n34: Environment variables:\n34:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n34:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n34: Test timeout computed to be: 1500\n34/113 Test  : PythonIntegrationTests_chat_reward .............................***Exception: SegFault  0.19 sec\ntest 35\nStart  35: PythonIntegrationTests_chunk_test_help35: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chunk_test.py\" \"--help\"\n35: Environment variables:\n35:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n35:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n35: Test timeout computed to be: 1500\n35: Traceback (most recent call last):\n35:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chunk_test.py\", line 37, in \n35:     from past.utils import old_div\n35: ModuleNotFoundError: No module named 'past'\n35/113 Test  : PythonIntegrationTests_chunk_test_help .........................***Failed    0.10 sec\ntest 36\nStart  36: PythonIntegrationTests_chunk_test36: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chunk_test.py\" \"--test\" \"--recording_dir\" \"chunk_test\" \"--record_video\"\n36: Environment variables:\n36:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n36:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n36: Test timeout computed to be: 1500\n36: Traceback (most recent call last):\n36:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/chunk_test.py\", line 37, in \n36:     from past.utils import old_div\n36: ModuleNotFoundError: No module named 'past'\n36/113 Test  : PythonIntegrationTests_chunk_test ..............................***Failed    0.10 sec\ntest 37\nStart  37: PythonIntegrationTests_craft_work_help37: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/craft_work.py\" \"--help\"\n37: Environment variables:\n37:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n37:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n37: Test timeout computed to be: 1500\n37/113 Test  : PythonIntegrationTests_craft_work_help .........................***Exception: SegFault  0.19 sec\ntest 38\nStart  38: PythonIntegrationTests_craft_work38: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/craft_work.py\" \"--test\" \"--recording_dir\" \"craft_work\" \"--record_video\"\n38: Environment variables:\n38:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n38:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n38: Test timeout computed to be: 1500\n38/113 Test  : PythonIntegrationTests_craft_work ..............................***Exception: SegFault  0.19 sec\ntest 39\nStart  39: PythonIntegrationTests_decision_tree_test_help39: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/decision_tree_test.py\" \"--help\"\n39: Environment variables:\n39:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n39:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n39: Test timeout computed to be: 1500\n39: Traceback (most recent call last):\n39:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/decision_tree_test.py\", line 23, in \n39:     from past.utils import old_div\n39: ModuleNotFoundError: No module named 'past'\n39/113 Test  : PythonIntegrationTests_decision_tree_test_help .................***Failed    0.10 sec\ntest 40\nStart  40: PythonIntegrationTests_decision_tree_test40: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/decision_tree_test.py\" \"--test\" \"--recording_dir\" \"decision_tree_test\" \"--record_video\"\n40: Environment variables:\n40:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n40:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n40: Test timeout computed to be: 1500\n40: Traceback (most recent call last):\n40:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/decision_tree_test.py\", line 23, in \n40:     from past.utils import old_div\n40: ModuleNotFoundError: No module named 'past'\n40/113 Test  : PythonIntegrationTests_decision_tree_test ......................***Failed    0.10 sec\ntest 41\nStart  41: PythonIntegrationTests_default_world_test_help41: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/default_world_test.py\" \"--help\"\n41: Environment variables:\n41:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n41:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n41: Test timeout computed to be: 1500\n41/113 Test  : PythonIntegrationTests_default_world_test_help .................***Exception: SegFault  0.19 sec\ntest 42\nStart  42: PythonIntegrationTests_default_world_test42: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/default_world_test.py\" \"--test\" \"--recording_dir\" \"default_world_test\" \"--record_video\"\n42: Environment variables:\n42:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n42:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n42: Test timeout computed to be: 1500\n42/113 Test  : PythonIntegrationTests_default_world_test ......................***Exception: SegFault  0.19 sec\ntest 43\nStart  43: PythonIntegrationTests_depth_map_runner_help43: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/depth_map_runner.py\" \"--help\"\n43: Environment variables:\n43:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n43:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n43: Test timeout computed to be: 1500\n43: Traceback (most recent call last):\n43:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/depth_map_runner.py\", line 23, in \n43:     from past.utils import old_div\n43: ModuleNotFoundError: No module named 'past'\n43/113 Test  : PythonIntegrationTests_depth_map_runner_help ...................***Failed    0.10 sec\ntest 44\nStart  44: PythonIntegrationTests_depth_map_runner44: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/depth_map_runner.py\" \"--test\" \"--recording_dir\" \"depth_map_runner\" \"--record_video\"\n44: Environment variables:\n44:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n44:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n44: Test timeout computed to be: 1500\n44: Traceback (most recent call last):\n44:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/depth_map_runner.py\", line 23, in \n44:     from past.utils import old_div\n44: ModuleNotFoundError: No module named 'past'\n44/113 Test  : PythonIntegrationTests_depth_map_runner ........................***Failed    0.10 sec\ntest 45\nStart  45: PythonIntegrationTests_discrete_3d_test_help45: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/discrete_3d_test.py\" \"--help\"\n45: Environment variables:\n45:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n45:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n45: Test timeout computed to be: 1500\n45: Traceback (most recent call last):\n45:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/discrete_3d_test.py\", line 39, in \n45:     from past.utils import old_div\n45: ModuleNotFoundError: No module named 'past'\n45/113 Test  : PythonIntegrationTests_discrete_3d_test_help ...................***Failed    0.10 sec\ntest 46\nStart  46: PythonIntegrationTests_discrete_3d_test46: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/discrete_3d_test.py\" \"--test\" \"--recording_dir\" \"discrete_3d_test\" \"--record_video\"\n46: Environment variables:\n46:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n46:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n46: Test timeout computed to be: 1500\n46: Traceback (most recent call last):\n46:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/discrete_3d_test.py\", line 39, in \n46:     from past.utils import old_div\n46: ModuleNotFoundError: No module named 'past'\n46/113 Test  : PythonIntegrationTests_discrete_3d_test ........................***Failed    0.10 sec\ntest 47\nStart  47: PythonIntegrationTests_drawing_test_help47: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/drawing_test.py\" \"--help\"\n47: Environment variables:\n47:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n47:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n47: Test timeout computed to be: 1500\n47: Traceback (most recent call last):\n47:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/drawing_test.py\", line 23, in \n47:     from past.utils import old_div\n47: ModuleNotFoundError: No module named 'past'\n47/113 Test  : PythonIntegrationTests_drawing_test_help .......................***Failed    0.10 sec\ntest 48\nStart  48: PythonIntegrationTests_drawing_test48: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/drawing_test.py\" \"--test\" \"--recording_dir\" \"drawing_test\" \"--record_video\"\n48: Environment variables:\n48:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n48:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n48: Test timeout computed to be: 1500\n48: Traceback (most recent call last):\n48:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/drawing_test.py\", line 23, in \n48:     from past.utils import old_div\n48: ModuleNotFoundError: No module named 'past'\n48/113 Test  : PythonIntegrationTests_drawing_test ............................***Failed    0.10 sec\ntest 49\nStart  49: PythonIntegrationTests_file_test_help49: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/file_test.py\" \"--help\"\n49: Environment variables:\n49:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n49:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n49: Test timeout computed to be: 1500\n49/113 Test  : PythonIntegrationTests_file_test_help ..........................***Exception: SegFault  0.19 sec\ntest 50\nStart  50: PythonIntegrationTests_file_test50: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/file_test.py\" \"--test\" \"--recording_dir\" \"file_test\" \"--record_video\"\n50: Environment variables:\n50:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n50:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n50: Test timeout computed to be: 1500\n50/113 Test  : PythonIntegrationTests_file_test ...............................***Exception: SegFault  0.19 sec\ntest 51\nStart  51: PythonIntegrationTests_hit_test_help51: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/hit_test.py\" \"--help\"\n51: Environment variables:\n51:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n51:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n51: Test timeout computed to be: 1500\n51: Traceback (most recent call last):\n51:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/hit_test.py\", line 29, in \n51:     from past.utils import old_div\n51: ModuleNotFoundError: No module named 'past'\n51/113 Test  : PythonIntegrationTests_hit_test_help ...........................***Failed    0.10 sec\ntest 52\nStart  52: PythonIntegrationTests_hit_test52: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/hit_test.py\" \"--test\" \"--recording_dir\" \"hit_test\" \"--record_video\"\n52: Environment variables:\n52:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n52:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n52: Test timeout computed to be: 1500\n52: Traceback (most recent call last):\n52:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/hit_test.py\", line 29, in \n52:     from past.utils import old_div\n52: ModuleNotFoundError: No module named 'past'\n52/113 Test  : PythonIntegrationTests_hit_test ................................***Failed    0.10 sec\ntest 53\nStart  53: PythonIntegrationTests_inventory_test_help53: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/inventory_test.py\" \"--help\"\n53: Environment variables:\n53:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n53:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n53: Test timeout computed to be: 1500\n53: Traceback (most recent call last):\n53:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/inventory_test.py\", line 39, in \n53:     from past.utils import old_div\n53: ModuleNotFoundError: No module named 'past'\n53/113 Test  : PythonIntegrationTests_inventory_test_help .....................***Failed    0.10 sec\ntest 54\nStart  54: PythonIntegrationTests_inventory_test54: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/inventory_test.py\" \"--test\" \"--recording_dir\" \"inventory_test\" \"--record_video\"\n54: Environment variables:\n54:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n54:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n54: Test timeout computed to be: 1500\n54: Traceback (most recent call last):\n54:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/inventory_test.py\", line 39, in \n54:     from past.utils import old_div\n54: ModuleNotFoundError: No module named 'past'\n54/113 Test  : PythonIntegrationTests_inventory_test ..........................***Failed    0.10 sec\ntest 55\nStart  55: PythonIntegrationTests_MazeRunner_help55: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MazeRunner.py\" \"--help\"\n55: Environment variables:\n55:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n55:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n55: Test timeout computed to be: 1500\n55/113 Test  : PythonIntegrationTests_MazeRunner_help .........................***Exception: SegFault  0.21 sec\ntest 56\nStart  56: PythonIntegrationTests_MazeRunner56: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MazeRunner.py\" \"--test\" \"--recording_dir\" \"MazeRunner\" \"--record_video\"\n56: Environment variables:\n56:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n56:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n56: Test timeout computed to be: 1500\n56/113 Test  : PythonIntegrationTests_MazeRunner ..............................***Exception: SegFault  0.19 sec\ntest 57\nStart  57: PythonIntegrationTests_mission_quit_command_example_help57: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mission_quit_command_example.py\" \"--help\"\n57: Environment variables:\n57:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n57:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n57: Test timeout computed to be: 1500\n57/113 Test  : PythonIntegrationTests_mission_quit_command_example_help .......***Exception: SegFault  0.19 sec\ntest 58\nStart  58: PythonIntegrationTests_mission_quit_command_example58: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mission_quit_command_example.py\" \"--test\" \"--recording_dir\" \"mission_quit_command_example\" \"--record_video\"\n58: Environment variables:\n58:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n58:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n58: Test timeout computed to be: 1500\n58/113 Test  : PythonIntegrationTests_mission_quit_command_example ............***Exception: SegFault  0.19 sec\ntest 59\nStart  59: PythonIntegrationTests_mob_fun_help59: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mob_fun.py\" \"--help\"\n59: Environment variables:\n59:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n59:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n59: Test timeout computed to be: 1500\n59: Traceback (most recent call last):\n59:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mob_fun.py\", line 25, in \n59:     from future import standard_library\n59: ModuleNotFoundError: No module named 'future'\n59/113 Test  : PythonIntegrationTests_mob_fun_help ............................***Failed    0.10 sec\ntest 60\nStart  60: PythonIntegrationTests_mob_fun60: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mob_fun.py\" \"--test\" \"--recording_dir\" \"mob_fun\" \"--record_video\"\n60: Environment variables:\n60:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n60:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n60: Test timeout computed to be: 1500\n60: Traceback (most recent call last):\n60:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mob_fun.py\", line 25, in \n60:     from future import standard_library\n60: ModuleNotFoundError: No module named 'future'\n60/113 Test  : PythonIntegrationTests_mob_fun .................................***Failed    0.10 sec\ntest 61\nStart  61: PythonIntegrationTests_mouse_steering_test_help61: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mouse_steering_test.py\" \"--help\"\n61: Environment variables:\n61:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n61:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n61: Test timeout computed to be: 1500\n61/113 Test  : PythonIntegrationTests_mouse_steering_test_help ................***Exception: SegFault  0.19 sec\ntest 62\nStart  62: PythonIntegrationTests_mouse_steering_test62: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/mouse_steering_test.py\" \"--test\" \"--recording_dir\" \"mouse_steering_test\" \"--record_video\"\n62: Environment variables:\n62:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n62:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n62: Test timeout computed to be: 1500\n62/113 Test  : PythonIntegrationTests_mouse_steering_test .....................***Exception: SegFault  0.20 sec\ntest 63\nStart  63: PythonIntegrationTests_moving_target_test_help63: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/moving_target_test.py\" \"--help\"\n63: Environment variables:\n63:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n63:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n63: Test timeout computed to be: 1500\n63/113 Test  : PythonIntegrationTests_moving_target_test_help .................***Exception: SegFault  0.19 sec\ntest 64\nStart  64: PythonIntegrationTests_moving_target_test64: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/moving_target_test.py\" \"--test\" \"--recording_dir\" \"moving_target_test\" \"--record_video\"\n64: Environment variables:\n64:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n64:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n64: Test timeout computed to be: 1500\n64/113 Test  : PythonIntegrationTests_moving_target_test ......................***Exception: SegFault  0.19 sec\ntest 65\nStart  65: PythonIntegrationTests_overclock_test_help65: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/overclock_test.py\" \"--help\"\n65: Environment variables:\n65:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n65:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n65: Test timeout computed to be: 1500\n65: Traceback (most recent call last):\n65:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/overclock_test.py\", line 29, in \n65:     from past.utils import old_div\n65: ModuleNotFoundError: No module named 'past'\n65/113 Test  : PythonIntegrationTests_overclock_test_help .....................***Failed    0.10 sec\ntest 66\nStart  66: PythonIntegrationTests_overclock_test66: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/overclock_test.py\" \"--test\" \"--recording_dir\" \"overclock_test\" \"--record_video\"\n66: Environment variables:\n66:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n66:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n66: Test timeout computed to be: 1500\n66: Traceback (most recent call last):\n66:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/overclock_test.py\", line 29, in \n66:     from past.utils import old_div\n66: ModuleNotFoundError: No module named 'past'\n66/113 Test  : PythonIntegrationTests_overclock_test ..........................***Failed    0.10 sec\ntest 67\nStart  67: PythonIntegrationTests_patchwork_quilt_help67: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/patchwork_quilt.py\" \"--help\"\n67: Environment variables:\n67:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n67:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n67: Test timeout computed to be: 1500\n67: Traceback (most recent call last):\n67:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/patchwork_quilt.py\", line 26, in \n67:     from past.utils import old_div\n67: ModuleNotFoundError: No module named 'past'\n67/113 Test  : PythonIntegrationTests_patchwork_quilt_help ....................***Failed    0.10 sec\ntest 68\nStart  68: PythonIntegrationTests_patchwork_quilt68: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/patchwork_quilt.py\" \"--test\" \"--recording_dir\" \"patchwork_quilt\" \"--record_video\"\n68: Environment variables:\n68:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n68:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n68: Test timeout computed to be: 1500\n68: Traceback (most recent call last):\n68:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/patchwork_quilt.py\", line 26, in \n68:     from past.utils import old_div\n68: ModuleNotFoundError: No module named 'past'\n68/113 Test  : PythonIntegrationTests_patchwork_quilt .........................***Failed    0.10 sec\ntest 69\nStart  69: PythonIntegrationTests_quit_from_reaching_position_test_help69: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/quit_from_reaching_position_test.py\" \"--help\"\n69: Environment variables:\n69:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n69:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n69: Test timeout computed to be: 1500\n69/113 Test  : PythonIntegrationTests_quit_from_reaching_position_test_help ...***Exception: SegFault  0.19 sec\ntest 70\nStart  70: PythonIntegrationTests_quit_from_reaching_position_test70: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/quit_from_reaching_position_test.py\" \"--test\" \"--recording_dir\" \"quit_from_reaching_position_test\" \"--record_video\"\n70: Environment variables:\n70:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n70:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n70: Test timeout computed to be: 1500\n70/113 Test  : PythonIntegrationTests_quit_from_reaching_position_test ........***Exception: SegFault  0.19 sec\ntest 71\nStart  71: PythonIntegrationTests_radar_test_help71: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/radar_test.py\" \"--help\"\n71: Environment variables:\n71:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n71:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n71: Test timeout computed to be: 1500\n71: Traceback (most recent call last):\n71:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/radar_test.py\", line 28, in \n71:     from future import standard_library\n71: ModuleNotFoundError: No module named 'future'\n71/113 Test  : PythonIntegrationTests_radar_test_help .........................***Failed    0.10 sec\ntest 72\nStart  72: PythonIntegrationTests_radar_test72: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/radar_test.py\" \"--test\" \"--recording_dir\" \"radar_test\" \"--record_video\"\n72: Environment variables:\n72:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n72:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n72: Test timeout computed to be: 1500\n72: Traceback (most recent call last):\n72:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/radar_test.py\", line 28, in \n72:     from future import standard_library\n72: ModuleNotFoundError: No module named 'future'\n72/113 Test  : PythonIntegrationTests_radar_test ..............................***Failed    0.10 sec\ntest 73\nStart  73: PythonIntegrationTests_render_speed_test_help73: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/render_speed_test.py\" \"--help\"\n73: Environment variables:\n73:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n73:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n73: Test timeout computed to be: 1500\n73: Traceback (most recent call last):\n73:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/render_speed_test.py\", line 25, in \n73:     from past.utils import old_div\n73: ModuleNotFoundError: No module named 'past'\n73/113 Test  : PythonIntegrationTests_render_speed_test_help ..................***Failed    0.10 sec\ntest 74\nStart  74: PythonIntegrationTests_render_speed_test74: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/render_speed_test.py\" \"--test\" \"--recording_dir\" \"render_speed_test\" \"--record_video\"\n74: Environment variables:\n74:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n74:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n74: Test timeout computed to be: 1500\n74: Traceback (most recent call last):\n74:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/render_speed_test.py\", line 25, in \n74:     from past.utils import old_div\n74: ModuleNotFoundError: No module named 'past'\n74/113 Test  : PythonIntegrationTests_render_speed_test .......................***Failed    0.10 sec\ntest 75\nStart  75: PythonIntegrationTests_reward_for_discarding_items_test_help75: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_discarding_items_test.py\" \"--help\"\n75: Environment variables:\n75:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n75:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n75: Test timeout computed to be: 1500\n75/113 Test  : PythonIntegrationTests_reward_for_discarding_items_test_help ...***Exception: SegFault  0.19 sec\ntest 76\nStart  76: PythonIntegrationTests_reward_for_discarding_items_test76: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_discarding_items_test.py\" \"--test\" \"--recording_dir\" \"reward_for_discarding_items_test\" \"--record_video\"\n76: Environment variables:\n76:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n76:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n76: Test timeout computed to be: 1500\n76/113 Test  : PythonIntegrationTests_reward_for_discarding_items_test ........***Exception: SegFault  0.19 sec\ntest 77\nStart  77: PythonIntegrationTests_reward_for_items_test_help77: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_items_test.py\" \"--help\"\n77: Environment variables:\n77:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n77:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n77: Test timeout computed to be: 1500\n77/113 Test  : PythonIntegrationTests_reward_for_items_test_help ..............***Exception: SegFault  0.19 sec\ntest 78\nStart  78: PythonIntegrationTests_reward_for_items_test78: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_items_test.py\" \"--test\" \"--recording_dir\" \"reward_for_items_test\" \"--record_video\"\n78: Environment variables:\n78:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n78:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n78: Test timeout computed to be: 1500\n78/113 Test  : PythonIntegrationTests_reward_for_items_test ...................***Exception: SegFault  0.19 sec\ntest 79\nStart  79: PythonIntegrationTests_reward_for_mission_end_test_help79: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_mission_end_test.py\" \"--help\"\n79: Environment variables:\n79:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n79:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n79: Test timeout computed to be: 1500\n79/113 Test  : PythonIntegrationTests_reward_for_mission_end_test_help ........***Exception: SegFault  0.19 sec\ntest 80\nStart  80: PythonIntegrationTests_reward_for_mission_end_test80: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/reward_for_mission_end_test.py\" \"--test\" \"--recording_dir\" \"reward_for_mission_end_test\" \"--record_video\"\n80: Environment variables:\n80:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n80:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n80: Test timeout computed to be: 1500\n80/113 Test  : PythonIntegrationTests_reward_for_mission_end_test .............***Exception: SegFault  0.19 sec\ntest 81\nStart  81: PythonIntegrationTests_robust_frames_help81: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/robust_frames.py\" \"--help\"\n81: Environment variables:\n81:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n81:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n81: Test timeout computed to be: 1500\n81: Traceback (most recent call last):\n81:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/robust_frames.py\", line 29, in \n81:     from future import standard_library\n81: ModuleNotFoundError: No module named 'future'\n81/113 Test  : PythonIntegrationTests_robust_frames_help ......................***Failed    0.10 sec\ntest 82\nStart  82: PythonIntegrationTests_robust_frames82: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/robust_frames.py\" \"--test\" \"--recording_dir\" \"robust_frames\" \"--record_video\"\n82: Environment variables:\n82:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n82:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n82: Test timeout computed to be: 1500\n82: Traceback (most recent call last):\n82:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/robust_frames.py\", line 29, in \n82:     from future import standard_library\n82: ModuleNotFoundError: No module named 'future'\n82/113 Test  : PythonIntegrationTests_robust_frames ...........................***Failed    0.10 sec\ntest 83\nStart  83: PythonIntegrationTests_run_mission_help83: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/run_mission.py\" \"--help\"\n83: Environment variables:\n83:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n83:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n83: Test timeout computed to be: 1500\n83/113 Test  : PythonIntegrationTests_run_mission_help ........................***Exception: SegFault  0.45 sec\ntest 84\nStart  84: PythonIntegrationTests_run_mission84: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/run_mission.py\" \"--test\" \"--recording_dir\" \"run_mission\" \"--record_video\"\n84: Environment variables:\n84:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n84:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n84: Test timeout computed to be: 1500\n84/113 Test  : PythonIntegrationTests_run_mission .............................***Exception: SegFault  0.20 sec\ntest 85\nStart  85: PythonIntegrationTests_tabular_q_learning_help85: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/tabular_q_learning.py\" \"--help\"\n85: Environment variables:\n85:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n85:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n85: Test timeout computed to be: 1500\n85: Traceback (most recent call last):\n85:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/tabular_q_learning.py\", line 27, in \n85:     from future import standard_library\n85: ModuleNotFoundError: No module named 'future'\n85/113 Test  : PythonIntegrationTests_tabular_q_learning_help .................***Failed    0.10 sec\ntest 86\nStart  86: PythonIntegrationTests_tabular_q_learning86: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/tabular_q_learning.py\" \"--test\" \"--recording_dir\" \"tabular_q_learning\" \"--record_video\"\n86: Environment variables:\n86:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n86:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n86: Test timeout computed to be: 1500\n86: Traceback (most recent call last):\n86:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/tabular_q_learning.py\", line 27, in \n86:     from future import standard_library\n86: ModuleNotFoundError: No module named 'future'\n86/113 Test  : PythonIntegrationTests_tabular_q_learning ......................***Failed    0.10 sec\ntest 87\nStart  87: PythonIntegrationTests_teleport_test_help87: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/teleport_test.py\" \"--help\"\n87: Environment variables:\n87:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n87:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n87: Test timeout computed to be: 1500\n87/113 Test  : PythonIntegrationTests_teleport_test_help ......................***Exception: SegFault  0.19 sec\ntest 88\nStart  88: PythonIntegrationTests_teleport_test88: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/teleport_test.py\" \"--test\" \"--recording_dir\" \"teleport_test\" \"--record_video\"\n88: Environment variables:\n88:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n88:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n88: Test timeout computed to be: 1500\n88/113 Test  : PythonIntegrationTests_teleport_test ...........................***Exception: SegFault  0.19 sec\ntest 89\nStart  89: PythonIntegrationTests_to_string_test_help89: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/to_string_test.py\" \"--help\"\n89: Environment variables:\n89:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n89:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n89: Test timeout computed to be: 1500\n89/113 Test  : PythonIntegrationTests_to_string_test_help .....................***Exception: SegFault  0.21 sec\ntest 90\nStart  90: PythonIntegrationTests_to_string_test90: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/to_string_test.py\" \"--test\" \"--recording_dir\" \"to_string_test\" \"--record_video\"\n90: Environment variables:\n90:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n90:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n90: Test timeout computed to be: 1500\n90/113 Test  : PythonIntegrationTests_to_string_test ..........................***Exception: SegFault  0.19 sec\ntest 91\nStart  91: PreMultiAgentIntegrationTests91: Test command: /usr/bin/python3.6 \"launch_minecraft_in_background.py\" \"10001\"\n91: Environment variables:\n91:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n91: Test timeout computed to be: 1500\n91: Something is listening on port 10001 - will assume Minecraft is running.\n91/113 Test  : PreMultiAgentIntegrationTests ..................................   Passed    0.04 sec\ntest 92\nStart  92: PythonIntegrationTests_two_diggers_help92: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/two_diggers.py\" \"--help\"\n92: Environment variables:\n92:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n92:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n92: Test timeout computed to be: 1500\n92/113 Test  : PythonIntegrationTests_two_diggers_help ........................***Exception: SegFault  0.19 sec\ntest 93\nStart  93: PythonIntegrationTests_two_diggers93: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/two_diggers.py\" \"--test\" \"--recording_dir\" \"two_diggers\" \"--record_video\"\n93: Environment variables:\n93:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n93:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n93: Test timeout computed to be: 1500\n93/113 Test  : PythonIntegrationTests_two_diggers .............................***Exception: SegFault  0.19 sec\ntest 94\nStart  94: PythonIntegrationTests_team_reward_test_help94: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/team_reward_test.py\" \"--help\"\n94: Environment variables:\n94:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n94:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n94: Test timeout computed to be: 1500\n94/113 Test  : PythonIntegrationTests_team_reward_test_help ...................***Exception: SegFault  0.19 sec\ntest 95\nStart  95: PythonIntegrationTests_team_reward_test95: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/team_reward_test.py\" \"--test\" \"--recording_dir\" \"team_reward_test\" \"--record_video\"\n95: Environment variables:\n95:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n95:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n95: Test timeout computed to be: 1500\n95/113 Test  : PythonIntegrationTests_team_reward_test ........................***Exception: SegFault  0.19 sec\ntest 96\nStart  96: PythonIntegrationTests_MultiMaze_help96: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MultiMaze.py\" \"--help\"\n96: Environment variables:\n96:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n96:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n96: Test timeout computed to be: 1500\n96: Traceback (most recent call last):\n96:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MultiMaze.py\", line 30, in \n96:     from past.utils import old_div\n96: ModuleNotFoundError: No module named 'past'\n96/113 Test  : PythonIntegrationTests_MultiMaze_help ..........................***Failed    0.10 sec\ntest 97\nStart  97: PythonIntegrationTests_MultiMaze97: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MultiMaze.py\" \"--test\" \"--recording_dir\" \"MultiMaze\" \"--record_video\"\n97: Environment variables:\n97:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n97:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n97: Test timeout computed to be: 1500\n97: Traceback (most recent call last):\n97:   File \"/home/justin/MalmoPlatform/Malmo/samples/Python_examples/MultiMaze.py\", line 30, in \n97:     from past.utils import old_div\n97: ModuleNotFoundError: No module named 'past'\n97/113 Test  : PythonIntegrationTests_MultiMaze ...............................***Failed    0.10 sec\ntest 98\nStart  98: ValidationTests_default_world_198: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/default_world_1.xml\"\n98: Environment variables:\n98:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n98:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n98:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n98: Test timeout computed to be: 1500\n98:\n98: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n98:\n98/113 Test  : ValidationTests_default_world_1 ................................***Failed    0.02 sec\ntest 99\nStart  99: ValidationTests_default_flat_199: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/default_flat_1.xml\"\n99: Environment variables:\n99:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n99:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n99:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n99: Test timeout computed to be: 1500\n99:\n99: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n99:\n99/113 Test  : ValidationTests_default_flat_1 .................................***Failed    0.02 sec\ntest 100\nStart 100: ValidationTests_tricky_arena_1100: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/tricky_arena_1.xml\"\n100: Environment variables:\n100:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n100:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n100:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n100: Test timeout computed to be: 1500\n100:\n100: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n100:\n100/113 Test : ValidationTests_tricky_arena_1 .................................***Failed    0.02 sec\ntest 101\nStart 101: ValidationTests_eating_1101: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/eating_1.xml\"\n101: Environment variables:\n101:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n101:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n101:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n101: Test timeout computed to be: 1500\n101:\n101: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n101:\n101/113 Test : ValidationTests_eating_1 .......................................***Failed    0.03 sec\ntest 102\nStart 102: ValidationTests_cliff_walking_1102: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/cliff_walking_1.xml\"\n102: Environment variables:\n102:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n102:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n102:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n102: Test timeout computed to be: 1500\n102:\n102: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n102:\n102/113 Test : ValidationTests_cliff_walking_1 ................................***Failed    0.02 sec\ntest 103\nStart 103: ValidationTests_maze_1103: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/mazes/maze_1.xml\"\n103: Environment variables:\n103:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n103:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n103:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n103: Test timeout computed to be: 1500\n103:\n103: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n103:\n103/113 Test : ValidationTests_maze_1 .........................................***Failed    0.03 sec\ntest 104\nStart 104: ValidationTests_maze_2104: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/mazes/maze_2.xml\"\n104: Environment variables:\n104:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n104:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n104:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n104: Test timeout computed to be: 1500\n104:\n104: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n104:\n104/113 Test : ValidationTests_maze_2 .........................................***Failed    0.02 sec\ntest 105\nStart 105: ValidationTests_basic105: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/basic.xml\"\n105: Environment variables:\n105:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n105:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n105:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n105: Test timeout computed to be: 1500\n105:\n105: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n105:\n105/113 Test : ValidationTests_basic ..........................................***Failed    0.02 sec\ntest 106\nStart 106: ValidationTests_obstacles106: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/obstacles.xml\"\n106: Environment variables:\n106:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n106:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n106:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n106: Test timeout computed to be: 1500\n106:\n106: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n106:\n106/113 Test : ValidationTests_obstacles ......................................***Failed    0.02 sec\ntest 107\nStart 107: ValidationTests_simpleRoomMaze107: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/simpleRoomMaze.xml\"\n107: Environment variables:\n107:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n107:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n107:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n107: Test timeout computed to be: 1500\n107:\n107: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n107:\n107/113 Test : ValidationTests_simpleRoomMaze .................................***Failed    0.02 sec\ntest 108\nStart 108: ValidationTests_attic108: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/attic.xml\"\n108: Environment variables:\n108:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n108:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n108:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n108: Test timeout computed to be: 1500\n108:\n108: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n108:\n108/113 Test : ValidationTests_attic ..........................................***Failed    0.02 sec\ntest 109\nStart 109: ValidationTests_vertical109: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/vertical.xml\"\n109: Environment variables:\n109:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n109:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n109:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n109: Test timeout computed to be: 1500\n109:\n109: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n109:\n109/113 Test : ValidationTests_vertical .......................................***Failed    0.03 sec\ntest 110\nStart 110: ValidationTests_complexity_usage110: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/complexity_usage.xml\"\n110: Environment variables:\n110:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n110:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n110:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n110: Test timeout computed to be: 1500\n110:\n110: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n110:\n110/113 Test : ValidationTests_complexity_usage ...............................***Failed    0.03 sec\ntest 111\nStart 111: ValidationTests_medium111: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/medium.xml\"\n111: Environment variables:\n111:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n111:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n111:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n111: Test timeout computed to be: 1500\n111:\n111: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n111:\n111/113 Test : ValidationTests_medium .........................................***Failed    0.02 sec\ntest 112\nStart 112: ValidationTests_hard112: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/classroom/hard.xml\"\n112: Environment variables:\n112:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n112:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n112:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n112: Test timeout computed to be: 1500\n112:\n112: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n112:\n112/113 Test : ValidationTests_hard ...........................................***Failed    0.03 sec\ntest 113\nStart 113: ValidationTests_tutorial_6113: Test command: /usr/bin/python3.6 \"/home/justin/MalmoPlatform/sample_missions/validate.py\" \"/home/justin/MalmoPlatform/sample_missions/../Malmo/samples/Python_examples/tutorial_6.xml\"\n113: Environment variables:\n113:  PYTHONPATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n113:  LD_LIBRARY_PATH=/home/justin/MalmoPlatform/build/Malmo/src/PythonWrapper\n113:  MALMO_XSD_PATH=/home/justin/MalmoPlatform/Schemas\n113: Test timeout computed to be: 1500\n113:\n113: Error: Requires the MalmoPython module to be present in the python path or the current directory.\n113:\n113/113 Test : ValidationTests_tutorial_6 .....................................***Failed    0.02 sec11% tests passed, 101 tests failed out of 113Total Test time (real) = 153.06 secThe following tests FAILED:\n7 - CppTests_test_persistence (Failed)\n8 - CppTests_test_string_server (Child aborted)\n11 - JavaTests_test_agent_host (Failed)\n12 - JavaTests_test_argument_parser (Failed)\n13 - JavaTests_test_mission (Failed)\n14 - JavaTests_test_parameter_set (Failed)\n15 - JavaTests_test_wrapping (Failed)\n16 - PythonTests_test_wrapping (SEGFAULT)\n17 - PythonTests_test_argument_parser (SEGFAULT)\n18 - PythonTests_test_agent_host (SEGFAULT)\n19 - PythonTests_test_mission (SEGFAULT)\n20 - PythonTests_test_parameter_set (SEGFAULT)\n21 - PythonTests_test_malmoutils (SEGFAULT)\n25 - JavaIntegrationTests_JavaExamples_run_mission_help (Failed)\n26 - JavaIntegrationTests_JavaExamples_run_mission (Failed)\n27 - PythonIntegrationTests_animation_test_help (SEGFAULT)\n28 - PythonIntegrationTests_animation_test (SEGFAULT)\n29 - PythonIntegrationTests_braitenberg_simulation_help (SEGFAULT)\n30 - PythonIntegrationTests_braitenberg_simulation (SEGFAULT)\n31 - PythonIntegrationTests_build_test_help (Failed)\n32 - PythonIntegrationTests_build_test (Failed)\n33 - PythonIntegrationTests_chat_reward_help (SEGFAULT)\n34 - PythonIntegrationTests_chat_reward (SEGFAULT)\n35 - PythonIntegrationTests_chunk_test_help (Failed)\n36 - PythonIntegrationTests_chunk_test (Failed)\n37 - PythonIntegrationTests_craft_work_help (SEGFAULT)\n38 - PythonIntegrationTests_craft_work (SEGFAULT)\n39 - PythonIntegrationTests_decision_tree_test_help (Failed)\n40 - PythonIntegrationTests_decision_tree_test (Failed)\n41 - PythonIntegrationTests_default_world_test_help (SEGFAULT)\n42 - PythonIntegrationTests_default_world_test (SEGFAULT)\n43 - PythonIntegrationTests_depth_map_runner_help (Failed)\n44 - PythonIntegrationTests_depth_map_runner (Failed)\n45 - PythonIntegrationTests_discrete_3d_test_help (Failed)\n46 - PythonIntegrationTests_discrete_3d_test (Failed)\n47 - PythonIntegrationTests_drawing_test_help (Failed)\n48 - PythonIntegrationTests_drawing_test (Failed)\n49 - PythonIntegrationTests_file_test_help (SEGFAULT)\n50 - PythonIntegrationTests_file_test (SEGFAULT)\n51 - PythonIntegrationTests_hit_test_help (Failed)\n52 - PythonIntegrationTests_hit_test (Failed)\n53 - PythonIntegrationTests_inventory_test_help (Failed)\n54 - PythonIntegrationTests_inventory_test (Failed)\n55 - PythonIntegrationTests_MazeRunner_help (SEGFAULT)\n56 - PythonIntegrationTests_MazeRunner (SEGFAULT)\n57 - PythonIntegrationTests_mission_quit_command_example_help (SEGFAULT)\n58 - PythonIntegrationTests_mission_quit_command_example (SEGFAULT)\n59 - PythonIntegrationTests_mob_fun_help (Failed)\n60 - PythonIntegrationTests_mob_fun (Failed)\n61 - PythonIntegrationTests_mouse_steering_test_help (SEGFAULT)\n62 - PythonIntegrationTests_mouse_steering_test (SEGFAULT)\n63 - PythonIntegrationTests_moving_target_test_help (SEGFAULT)\n64 - PythonIntegrationTests_moving_target_test (SEGFAULT)\n65 - PythonIntegrationTests_overclock_test_help (Failed)\n66 - PythonIntegrationTests_overclock_test (Failed)\n67 - PythonIntegrationTests_patchwork_quilt_help (Failed)\n68 - PythonIntegrationTests_patchwork_quilt (Failed)\n69 - PythonIntegrationTests_quit_from_reaching_position_test_help (SEGFAULT)\n70 - PythonIntegrationTests_quit_from_reaching_position_test (SEGFAULT)\n71 - PythonIntegrationTests_radar_test_help (Failed)\n72 - PythonIntegrationTests_radar_test (Failed)\n73 - PythonIntegrationTests_render_speed_test_help (Failed)\n74 - PythonIntegrationTests_render_speed_test (Failed)\n75 - PythonIntegrationTests_reward_for_discarding_items_test_help (SEGFAULT)\n76 - PythonIntegrationTests_reward_for_discarding_items_test (SEGFAULT)\n77 - PythonIntegrationTests_reward_for_items_test_help (SEGFAULT)\n78 - PythonIntegrationTests_reward_for_items_test (SEGFAULT)\n79 - PythonIntegrationTests_reward_for_mission_end_test_help (SEGFAULT)\n80 - PythonIntegrationTests_reward_for_mission_end_test (SEGFAULT)\n81 - PythonIntegrationTests_robust_frames_help (Failed)\n82 - PythonIntegrationTests_robust_frames (Failed)\n83 - PythonIntegrationTests_run_mission_help (SEGFAULT)\n84 - PythonIntegrationTests_run_mission (SEGFAULT)\n85 - PythonIntegrationTests_tabular_q_learning_help (Failed)\n86 - PythonIntegrationTests_tabular_q_learning (Failed)\n87 - PythonIntegrationTests_teleport_test_help (SEGFAULT)\n88 - PythonIntegrationTests_teleport_test (SEGFAULT)\n89 - PythonIntegrationTests_to_string_test_help (SEGFAULT)\n90 - PythonIntegrationTests_to_string_test (SEGFAULT)\n92 - PythonIntegrationTests_two_diggers_help (SEGFAULT)\n93 - PythonIntegrationTests_two_diggers (SEGFAULT)\n94 - PythonIntegrationTests_team_reward_test_help (SEGFAULT)\n95 - PythonIntegrationTests_team_reward_test (SEGFAULT)\n96 - PythonIntegrationTests_MultiMaze_help (Failed)\n97 - PythonIntegrationTests_MultiMaze (Failed)\n98 - ValidationTests_default_world_1 (Failed)\n99 - ValidationTests_default_flat_1 (Failed)\n100 - ValidationTests_tricky_arena_1 (Failed)\n101 - ValidationTests_eating_1 (Failed)\n102 - ValidationTests_cliff_walking_1 (Failed)\n103 - ValidationTests_maze_1 (Failed)\n104 - ValidationTests_maze_2 (Failed)\n105 - ValidationTests_basic (Failed)\n106 - ValidationTests_obstacles (Failed)\n107 - ValidationTests_simpleRoomMaze (Failed)\n108 - ValidationTests_attic (Failed)\n109 - ValidationTests_vertical (Failed)\n110 - ValidationTests_complexity_usage (Failed)\n111 - ValidationTests_medium (Failed)\n112 - ValidationTests_hard (Failed)\n113 - ValidationTests_tutorial_6 (Failed)\nErrors while running CTestI am not sure if this output reveals anything useful, but it seems to show a problem with the current PYTHONPATH. If that is the issue, I am wondering what I should set the PYTHONPATH to be.", "type": "commented", "related_issue": null}, {"user_name": "DarthMalloc", "datetime": "Mar 9, 2020", "body": "Could someone please respond to my latest post? I really need to get this resolved.", "type": "commented", "related_issue": null}, {"user_name": "DarthMalloc", "datetime": "Mar 10, 2020", "body": "I have an update from yesterday. I was successful in running the compiled C++ implementation of the run_mission example, but I am still unable to run any of the Python examples. Based on the results of the Ctest command, it looks like the problem has something to do with Python integration. I am comfortable proceeding in C++ if necessary, although Python would be preferable, so I would therefore really appreciate if someone would be willing to guide me in addressing the Python integration problem.", "type": "commented", "related_issue": null}, {"user_name": "satyamedh", "datetime": "Jun 8, 2020", "body": " , Forge provides a development version on Minecraft. Which does not include any multiplayer code. Because If you can do that, there will be no reason to buy the game. Thus I suggest you to build the mod using  and copy it into your Minecraft mods dir. then launch Minecraft 1.11.2 with forge from Minecraft launcher", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1865", "issue_status": " Open\n", "issue_list": [{"user_name": "joachimtingvold", "datetime": "Dec 31, 2021", "body": "When searching for people to assign a face, the list could maybe be sorted/prioritized based on different metadata. Right now, the list is long if you have many persons, and it's tedious to write the whole name, or scroll through the list with mouse/keyboard.Not sure if all of these would be sensible in terms of performance. Also not sure of the prioritization of these.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 1, 2022", "body": "Tried that, but turned out to be confusing.", "type": "commented", "related_issue": null}, {"user_name": "joachimtingvold", "datetime": "Jan 1, 2022", "body": "Maybe make it configureable/user preference, then? I guess it's somewhat personal taste to some degree. And I guess some of them makes more sense than others.You usually refer to people by given name or surname, not something \"in the middle\". Alas, searching from the start of given name and/or surname would make more sense than the current search.As an example, search for \"foo\" should sort like this (where first part is given name, and last part is surname):... which is currently not the case (regardless of all the other metadata). Not sure if that's just my opinion, though.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jan 1, 2022", "body": "I agree sorting can be improved but we need to do more research to find a solution that is not confusing.We are currently focusing on multi-user support.Let's keep this issue open to hear more opinions :)", "type": "commented", "related_issue": null}, {"user_name": "srett", "datetime": "Jan 1, 2022", "body": "DigiKam bumps the last 10 (or so) people you assigned to a marker to the top of the list and prints them in bold, and likewise, when typing part of a name, those same 10 people, if matching, are displayed at the top, again in bold. It's rather primitive but still effective, and probably less confusing than adding a bunch of magic and heuristics.", "type": "commented", "related_issue": null}, {"user_name": "joachimtingvold", "datetime": "Jan 1, 2022", "body": "I have not used DigiKam, but I guess it somewhat depends on the context you are assigning them from? (cluster, album, image, etc).If you are browsing an album, people assigned to other photos in the same album would be most likely to reappear. If you are going through photos already assigned to a person (to verify and/or assign other people in those photos), you can suddenly span albums/events/years/whatnot, where \"last used\" might not make much sense?Maybe I'm overthinking this :D", "type": "commented", "related_issue": null}, {"user_name": "srett", "datetime": "Jan 1, 2022", "body": "I'd optimize for the common case, which is (assuming you finished your initial import) adding new photos every now and then, which will be mostly from one or a few events, with recurring people.\nBut even in the scenario you describe, you will most likely go through the pictures of that person in chronological order, so if you fix assignments, it's probably the same few people again, e.g. Bob got confused with Bill a couple times at Jane's wedding, then the next time Bob appears in your collection is some trip in the Alps, where he got tagged as John a few times, and so on.\nFrom my experience I can say it works well enough to be convenient and not get in your way.I don't think it's worth it adding 5 different heuristics for suggestions plus a config dialog, or even trying to pick the right one automatically (and getting it wrong sometimes for maximum confusion).", "type": "commented", "related_issue": null}, {"user_name": "joachimtingvold", "datetime": "Jan 1, 2022", "body": "I don't disagree, but the \"initial import\" can be a big pain depending on the size. I'm migrating to PP from a self-made image system, and I have about 70k+ pictures that are sorted into albums, and probably equally many unsorted. I'm a few days into importing them into PP now. and my wrists are not happy with me at all. Mostly due to the bad/non-optimal workflow of assigning persons to faces/photos (but also some other workflows).I'm not complaining, and I know we're very early in the whole facial recognition implementation, but yeah.I don't necessarily mean that we should implement all 5 heuristics (and there might be other ones that is more suitable, like the one you suggested), but we should at least improve the current search significantly. I have only scratched the surface of my import, and I already have 300+ people (and only about 30-40k of my 120k+ images imported). Maybe I'm an outlier, but that shouldn't be an excuse to improve on things.", "type": "commented", "related_issue": null}, {"user_name": "IeuanK", "datetime": "May 7, 2022", "body": "As per my now-closed issue , I think it would be ideal to sort primarily by AI confidence.\nGiven that this issue has other ideas, it would be neat to make it configurable; have an option to store confidence levels for all people/faces and some options for which factors you want to use when sorting the dropdown?I really hope this gets added at some point because it would make fixing faces a lot less of a pain.", "type": "commented", "related_issue": null}, {"user_name": "Sohalt", "datetime": "Jul 30, 2022", "body": "I very much agree that complicated heuristics can be too confusing, but the suggestion in  is very simple and a huge improvement over the current state imho (highest priority for exact match, then prefix match, then prefix of subword (e.g. surname), then any other substring match).\nIf I try to search for \"Maria\", it's extremely annoying to have \"Alma\" show up first when typing \"Ma\".", "type": "commented", "related_issue": null}, {"user_name": "joachimtingvold", "datetime": "Dec 31, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#1552"}, {"user_name": "graciousgrey", "datetime": "Jan 1, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jan 1, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 7, 2022", "body": [], "type": "issue", "related_issue": "#2305"}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1633", "issue_status": " Open\n", "issue_list": [{"user_name": "Pheggas", "datetime": "Oct 17, 2021", "body": "Hello. So i found this bug/weird behaviour. As user i'd like to copy the photo (usually by right clicking on PC) and send it to 3rd party service like Facebook Messenger to share some of my photos with other people. But when i right click, the photo UI just closes and throws me back to the gallery.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 17, 2021", "body": "Works for me. How did you do that? What operating system & browser version?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 25, 2021", "body": "We need more information to reproduce this:", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 25, 2021", "body": "Works on  using Google Chrome on macOS:", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Oct 27, 2021", "body": "Good to knowIt may be caused by it's version tho. As i didn't update photoprism in a while, i have version 210523-b1856b9d-Linux-x86_64", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 27, 2021", "body": "Wouldn't know why, but testing the same version wouldn't hurt either Can up update the image to see if it's fixed?", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Oct 27, 2021", "body": "No prob. I'll do it as soon as i'll have more time", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 13, 2021", "body": "Update: So i get into it, re-pulled image and re-build it. Now i have latest  image and it still does the same thing. Even if i switch browsers. Basically all the points i write above applies to the latest version.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 13, 2021", "body": "Strange, still working for me. Any special input devices or drivers in place that might change the event that is fired? From what I remember having read the code years ago, it won't fire if a mobile device is detected to avoid issues with touch gestures.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 13, 2021", "body": "Think it's because either iOS or Android use contextmenu to signal \"long touch\". Click is fired for a \"short touch\". The click duration may also play a role on desktop computers, it's worth testing at least. I know we have long click implemented in our search result views (list, mosaic, cards).", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 13, 2021", "body": "Yeah. I mean, on my main PC. I tried same thing on older PC with no additional input devices attached. Same thing.", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 13, 2021", "body": "Might be. Definitely worth a try of deleting that chunk of code for the test build.What i actually found out is, this doesn't happen on the demo site of PhotoPrism. Or, that I thought. Then I realized this issue only happens on 16:9 aspect ratio photos in landscape mode. On the other hand, in portrait mode with 9:16 aspect ratio, right click only zooms in the photo which is the expected result i think.So the next step would be upload test photo on demo site of PhotoPrism with variables that I described above and see what happen.\nIn case it wouldn't show this issue, I would upload one of my photos from my personal gallery where I'm 100% sure it have this issue.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 14, 2021", "body": "Could it depend on the original or thumbnail pixel size? Context menu may be somehow coupled with the zoom function. Would be great if you can check the PhotoSwipe issue tracker for related problems Does your server use default thumb settings or did you change anything?", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 14, 2021", "body": "According to my settings, I'm pretty sure those are default ones.\n", "type": "commented", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 15, 2021", "body": "I was able to replicate the issue inside your demo instance of Photoprism. In , you can see two photos. One of which is from my personal gallery and the second one (with the lake thing), is from net. Both of these are 16:9 aspect ratio images and each have different resolutin.This fact can approve my theory.If the aspect ratio equals 16:9, you  face this issue.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Oct 18, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Oct 25, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 13, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "Pheggas", "datetime": "Nov 15, 2021", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/306", "issue_status": " Open\n", "issue_list": [{"user_name": "michelePap", "datetime": "Mar 11, 2018", "body": "Visiting any webpage with a google vr frame with chrome on android, moving the smartphone the scene does not change visual as usual. Things remain the same in the cardboard view.\nWith firefox and samsung browser there are no problem", "type": "commented", "related_issue": null}, {"user_name": "ddorwin", "datetime": "Mar 12, 2018", "body": "Which version of Chrome exactly? There was a bug in Chrome 65 () that should be fixed soon. See also the discussion in , including changes in Chrome 66. Updating the polyfill should handle the latter fine, but an upcoming Chrome update is required to fix the Chrome 65 issue. /cc ", "type": "commented", "related_issue": null}, {"user_name": "michelePap", "datetime": "Mar 13, 2018", "body": "On chrome stable version 65.0.3325.109", "type": "commented", "related_issue": null}, {"user_name": "neilloispy", "datetime": "Mar 13, 2018", "body": "Yes this is the same issue which came up with Android Chrome version 62.0.3202.84 last year and was fixed in subsequent versions.", "type": "commented", "related_issue": null}, {"user_name": "ddorwin", "datetime": "Mar 16, 2018", "body": "A new version of the polyfill containing a workaround for the Chrome 65 issue has been released: ", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Mar 16, 2018", "body": "Looks like vrview is using 0.9.x, and uses WebVRConfig in unexpected ways; need to confirm if the YAW_ONLY option actually works (it's set after polyfill is brought in so looks like it wouldn't do anything), and how necessary mouse and keyboard controls (although it looks like it's using a different click/drag mechanism?)", "type": "commented", "related_issue": null}, {"user_name": "dkudrin", "datetime": "Mar 17, 2018", "body": " polyfill doesn't helps. Have yot tried it?", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Mar 19, 2018", "body": " do you have a test page using the latest polyfill release? what doesn't work?", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "May 14, 2018", "body": [], "type": "issue", "related_issue": "#317"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/289", "issue_status": " Open\n", "issue_list": [{"user_name": "uberstudio", "datetime": "Dec 14, 2017", "body": "Confirmed on Edge, Firefox 55+ on Windows 10,\nI've seen more reports of this issue popping up lately.\nThis can also be recreated in Chrome easily by enabling the WebVR experimental flag, so this is definitely coming to Chrome as well.I've noticed requiring the legacy polyfill and enabling the deprecated API has been suggested as a fix in several forums, but I believe this is only a temporary hack.", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "Dec 14, 2017", "body": "This is the same issue reported in , which was not resolved but is currently closed.It's probably the same as  as well.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Dec 14, 2017", "body": "We closed it because builds on windows are flawed but if you use the embed that we are hosting here:   it should work. Is that not true?", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "Dec 14, 2017", "body": " I'm not able to test it right now, but I don't think so;  is not a build issue.As I understand it, the problem is that the native WebVR implementations don't provide mouse and keyboard devices in  the way the polyfill does. As far as I know, the solution discussed in these two comments has not been implemented: by ssh-esoteric\n by ", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Dec 15, 2017", "body": " your understanding sounds correct to me -- on native implementations, those displays will be provided instead. If you don't have a Vive/Oculus or something hooked up, you won't be able to view it. If we used that flag in the polyfill mentioned in the above comment, then that would add the MouseKeyboardVRDisplay in this scenario", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Dec 15, 2017", "body": "Reopening , related to Edge (now with native WebVR) not windows builds", "type": "commented", "related_issue": null}, {"user_name": "uberstudio", "datetime": "Dec 18, 2017", "body": "I tried using this build: But it's a totally different package than the official VRView library, looks like this one doesn't include Three.js, three-vrcontrols, possibly more (I didn't check, I'd rather have a complete build)", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "May 14, 2018", "body": [], "type": "issue", "related_issue": "#317"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/288", "issue_status": " Open\n", "issue_list": [{"user_name": "cwilso", "datetime": "Dec 11, 2017", "body": "OSX, Chrome 62, clicking and dragging with mouse does not move view around as it should.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/317", "issue_status": " Open\n", "issue_list": [{"user_name": "davidh64", "datetime": "May 14, 2018", "body": "The latest  seems to include fixes for  introduced by newer versions of Chrome, but webvr-polyfill v0.10.0 removed MouseKeyboardVRDisplay which provided 3DOF controls for desktop when no native displays are connected. Is anyone actively working on updating to the newer version of the polyfill? It seems that this would resolve , but would require a replacement for the mouse and keyboard controls on desktop that were previously provided by the polyfill.", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "May 14, 2018", "body": "In addition to , this should also fix .I assume the replacement for  would also resolve , , and .", "type": "commented", "related_issue": null}, {"user_name": "davidshttintin", "datetime": "Aug 25, 2018", "body": "how do u update to the latest version of webvr-polyfill? do u replace codes in vrview.js?", "type": "commented", "related_issue": null}, {"user_name": "transpirman", "datetime": "Sep 23, 2018", "body": "it seems that by changing this unique line in embed.(min.)js, it works on newer chrome mobile as well :\nif(this.isIOS||this.isFirefoxAndroid){ this.gyroscope.multiplyScalar(Math.PI/180); }\nremove the if statement to keep only the \"this.gyroscope.multiplyScalar(Math.PI/180); \"", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1241", "issue_status": " Open\n", "issue_list": [{"user_name": "electricpollution", "datetime": "Apr 28, 2021", "body": "Video thumbnails are currently created from the first frame hardcoded with the code  Any video that has a black first frame or anything not meaningful is not useful for previews.Suggest defaulting to the first frame or having a parameter to customize how many seconds  into the video a preview is created.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 28, 2021", "body": "Related discussion: ", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Apr 28, 2021", "body": " has funded $5.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Jun 3, 2021", "body": "Get video full length:\nGet video black bars and remove them. Useful if someone has.. black bars encoded into the video. (python running a bash script 10 times and averaging it, so in Go you'd just do the same thing):This \"croplist\" should work directly into ffmpeg as  (including the quotes)\nNote my 00:\" + times + \":00 uses a random int 3-59 (cuz I use this for stripping out black bars in legal videos I acquire) but for you guys, you'd need to use the time you get in the previous FFPROBE command (first one I posted)\nYou can also run this for a set amount of time:\n\nHowever it does not work well for dark movies / videos. Some scenes are very dark and some are bright. SO I found randomly skipping around the video and looking at a frame is far better.\nI've also been doing this for over a decade so it is well tested.Last all that is needed is to pick a random time from 0 to the end of the original \"duration\" of the video.Looks like what  guy linked they were already looking at a thumbnailer program though. That is a good project it's used in almost all of the linux file managers. I know he said he was trying to not use C++ stuff though so my above implementations you can cut out black bars + get full duration + pick a random timestamp to make a thumbnail out of while only calling FFMPEG. you can replace AWK with something in Go to parse the output instead.", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Jun 3, 2021", "body": "Actually ignore that crap above you can extract ACTION frames (frames that have pixel movement, thus not black!) purely in ffmpeg\nThis is all you need for every video:\nthe magic is here:  this is generous here, a video file that does not have a lot of pixel shifts is going to get a thumbnail here. However the thumbnail may be in the beginning of the video and not a very good thumbnail.\nIf you want a great thumbnail, maybe make this a setting so people can say \"yes I want better thumbnails\" or whatever.\nBut you check less generously and if it fails, have a fallback to a more generous check, like this:or something like that, this is what I'd personally do. However that is the only command you need. You can copy paste the above command and replace your thumbnail right now and it'll already work better!", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 3, 2021", "body": " Any idea what the performance impact might be, if any?", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Jun 3, 2021", "body": " how fast is it now? I tested on a \"hollywood movie\" the 0.1 and 0.2 are almost instant\nAll of my videos are h264 as well so it's very fast decode. and it is ryzen 3700X processor.\nVideo is 1280x720 resolution too so pretty fast.From my test I think yours (for me) would be around \"902.35 millis\" and vs the 0.2 which is \"1.02 secs\" I think that's well worth it.\n(assuming millis in the linux time command is milliseconds so 902 is almost 1 second, so they basically take the same time)For me, personally, 0.8 is worth it taking 4 seconds to process the frame it finds is only like 20 seconds into the video.. So if you have like a long video and it never finds one with a lot of pixels changing then it's gonna take a LONG time since it'd go through the whole video and fail.. and then try a lower number.", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Jun 3, 2021", "body": "Technically there's no guarantee that even a 0.1 threshold would work if your video is extremely dark / black almost the entire thing. Somewhere I have videos of us exploring a cave that are terrible videos, if I can find them I'll test it but I guess we need a fallback to just grab the first frame as you do now, in the very worst case?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 4, 2021", "body": "Bottom line is that a fallback must be implemented and tested, so it's a bit more complicated than just replacing the current ffmpeg command?", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Jun 7, 2021", "body": "Since someone can upload a video that is just all 1 color and never changes, yes. If we guaranteed people would upload actual videos then we can guarantee 0.1 would get a thumbnail. But we can't guarantee what people upload.", "type": "commented", "related_issue": null}, {"user_name": "G2G2G2G", "datetime": "Aug 8, 2021", "body": "What needs to be done to add this?   I feel bad for  and   lolI can write this in like 15 minutes. Where is a video thumb created which function?  I cannot find itI've tested scene,0.8 on thousands of random videos on imgur.com and none failed. My fear is a lightning video like:  will only work on 0.6:\n\nHowever terrible videos (like the one I uploaded that is entirely white the entire duration) won't find any movement.. obviouslyVideos like this also don't really trigger any thumbnails due to how slow they move\n\nsome do, some don't. It's based on if a bar jumps in 1 frame or not. Sometimes on ones like the \"international exports\" video, when the USSR split into russia etc, russia exported a ton of extra crap to other countries driving their bar from non existent to in the top 5 for 5 frames of the video, which triggers  to detect it, but normally nothing will get any thumbnail thus needing a \"first frame\" fallback (which is easy to do, but just an example of why it's needed)", "type": "commented", "related_issue": null}, {"user_name": "Jarvid", "datetime": "Feb 4, 2022", "body": "Hello,my 2c as I just stumbled accross the black thumbnail (black screen in 1st second) problem: of the thumbnail  is hardly worth the effort.\nEvery setting will work one some, but fail on other videos.Either the thumbnail is custom, where the user can select a specific frame of the video in the GUI or upload a specific picture,\nor\nsimple create a collage thumbnail.Did it with a 2x2 layout to have big enough pictures even on a mobile.\nTook the duration(video playback time) from the Mediafile metadata to get 4 frames evenly distributed on the source videos.\nOnly 4 lines in covert.go.\nBut Indexing is slower now and scales with the playback time of the video. Took 25 seconds for a 60 minutes video with 2giga bytes file size while the original approach was only 1 second.I'm happy with the result though. Unless you happen to have a lot of collage pictures in your library, this change makes videos also more distinguishable from pictures aside from the little arrow icon.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Feb 4, 2022", "body": "We need the full, first frame so it can be used as still image for . Otherwise, the UI would have to load the entire video just to display the first frame. Note that Live Photos will only play when you hover over them with the mouse, for example in search results.", "type": "commented", "related_issue": null}, {"user_name": "Jarvid", "datetime": "Feb 5, 2022", "body": "Excuse my way to write the same content in a new reply again, but I as orignally answered via E-Mail github does not allow to use markdown in reply.I see, is this still limited to videos up to 3 seconds or intended to be used for all kind of videos in the Future?-edit-\nOk, isLive() is always false im my 2 seconds hvc test example, even tough it is correctly typed as live by photoprism.\nIs this data populated later?\nBut aside from this thing:\nI don't see any network traffic difference indicating it is loading the entire video for the first frame.\nIt behaves just as in an unmodified photoprism instance. Well with the current difference, the thumbnail is a collage and once you mouseover the clip starts just normal.\nI've attached a video to show the current way it looks.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Feb 5, 2022", "body": "The type is determined automatically based on the duration (and possibly other properties like the codec, see public source code), but it is possible to change it manually. So to avoid unnecessary complexity, it would be best to always have the same behavior - at least until we can pay an armada of developers to implement bells and whistles.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 28, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 28, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Apr 28, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "Observe-1", "datetime": "May 3, 2021", "body": [], "type": "issue", "related_issue": "#1260"}, {"user_name": "graciousgrey", "datetime": "Aug 4, 2021", "body": [], "type": "issue", "related_issue": "#648"}, {"user_name": "lastzero", "datetime": "Nov 1, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 2, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 2, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 7, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1307", "issue_status": " Open\n", "issue_list": [{"user_name": "graciousgrey", "datetime": "May 20, 2021", "body": "At the moment we use photoswipe as photo viewer. As there are some limitations we plan to implement one on our own.The new photo viewer should solve the following issues:", "type": "commented", "related_issue": null}, {"user_name": "jucor", "datetime": "Jun 16, 2021", "body": "Is there any worth to \"just\" build an interface on top of Photoswipe rather than rewriting the whole viewer?", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 20, 2021", "body": "Well, it's architecture is not really extensible. It would all be ugly workarounds. Already did that in large parts, but need to make a cut somewhere.", "type": "commented", "related_issue": null}, {"user_name": "jucor", "datetime": "Jun 20, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "tribut", "datetime": "Aug 12, 2021", "body": "When this gets implemented, I would really appreciate some sidebar/overlay for media information. Having to go to the edit view just to look up the file name or assigned tags is cumbersome.Screenshots from librephotos and lychee, to make it clear what I'm looking for:\n", "type": "commented", "related_issue": null}, {"user_name": "RandomHacks-Git", "datetime": "Aug 14, 2021", "body": "I was going to open an issue but I guess the solution would be implemented as part of this new photo viewer.\nAfter briefly testing photoprism I'm currently using another solution for the sole reason that there is no easy way to skip to the next video (or go back to the previous) inside the lightbox, always having to exit the lightbox and open the next video is quite cumbersome and not very intuitive.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Aug 15, 2021", "body": " Yes, that is one of the main reasons why we need to build our own photo viewer :)", "type": "commented", "related_issue": null}, {"user_name": "aravindhsampath", "datetime": "Aug 20, 2021", "body": "I am here from the context of  work. It would be wonderful if the new photo viewer considers to implement an UI wherein the photo shows tags/labels associated with it visually like how Facebook photos with people. It may be triggered upon mouse hover in the area, or a button on the top that says\"labels\" or \"tags\" may be clicked to show this info.When manual tagging of people gets implemented in Photoprism, it would be very useful to see \"who all appear in this photo\". Perhaps, also adding the ability to manually tag people right there while viewing the photo.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Sep 26, 2021", "body": " has funded $10.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Oct 5, 2021", "body": "An anonymous user has funded $20.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Oct 19, 2021", "body": " has funded $10.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "IssueHuntBot", "datetime": "Dec 27, 2021", "body": "An anonymous user has funded $50.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 28, 2021", "body": "Please don't add funds to IssueHunt anymore! While we like IssueHunt and are grateful for the donations we've received so far, it hasn't proven to be a sustainable funding option for us as we spend much of our time maintaining existing features and providing support.If we don't have enough resources to provide support and bugfixes, we can't start working on new features.", "type": "commented", "related_issue": null}, {"user_name": "theseraphim", "datetime": "Jan 2, 2022", "body": "Deep zoom on touchscreens is ok but theres a limit to how far it will zoom before it gets... \"bouncy\" not sure if this is related to dynamic preview size or static preview size (i did raise an issue with having static image size too high causing thumbnail duplication) or some other feature, but id like to be able to zoom to at least 400% of full resolution.I wonder if a small interim fix would be a toggle for \"full resolution\" where instead of loading the fit_xxxx resolution it just forces the original to load within photoswipe?should be possible if you use pwsp.currItem.src (which i think is what is powering the download button) but that might just require the image be opened in a new tab and use the browsers built in image viewer... im not sure if thats better or not.", "type": "commented", "related_issue": null}, {"user_name": "joachimtingvold", "datetime": "Jan 4, 2022", "body": "One other aspect, which I'm not sure is directly related to the current constraints with photo viewer, would be to scroll down to current picture when exiting full sized view.As an example; if you search for something, click on the first picture, and start viewing in full sized view. You look at 147 photos. When exiting full sized view, it'd be nice if you got back to the thumbnail overview being scrolled down to show the 147th photo (so you have the context).Usecase would for example be if you wanted to add certain photos to an album, you would search, browse until you find the correct context, exit viewer, and select the photos before/after, and add to album.Not sure if this warrants it's own issue, as it's most likely not strictly related to the restrictions of the current photo viewer?", "type": "commented", "related_issue": null}, {"user_name": "GlassedSilver", "datetime": "Apr 9, 2022", "body": "Is a revamp of how Live Photos (and similar media kinds by the other OEMs) are presented on the table with this rework as well?Having to click a button to play the Live Photo, for it to load a video player frame and it not being the exact same size as the photo is cumbersome and not very elegant.You'll ask me how to implement it then, and I say: the UX/UI can be referenced in Apple Photos.The UI of it for this media kind is spot on. (not to anyone's surprise I guess, but any deviation from it really feels odd)", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "May 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "issue", "related_issue": "#508"}, {"user_name": "graciousgrey", "datetime": "Jun 16, 2021", "body": [], "type": "issue", "related_issue": "#1379"}, {"user_name": "graciousgrey", "datetime": "Jul 6, 2021", "body": [], "type": "issue", "related_issue": "#1409"}, {"user_name": "graciousgrey", "datetime": "Aug 19, 2021", "body": [], "type": "issue", "related_issue": "#22"}, {"user_name": "graciousgrey", "datetime": "Sep 24, 2021", "body": [], "type": "issue", "related_issue": "#1548"}, {"user_name": "graciousgrey", "datetime": "Sep 26, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Sep 26, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "aravindhsampath", "datetime": "Sep 30, 2021", "body": [], "type": "issue", "related_issue": "#1573"}, {"user_name": "graciousgrey", "datetime": "Oct 2, 2021", "body": [], "type": "issue", "related_issue": "#1581"}, {"user_name": "graciousgrey", "datetime": "Oct 18, 2021", "body": [], "type": "issue", "related_issue": "#1498"}, {"user_name": "lastzero", "datetime": "Oct 30, 2021", "body": [], "type": "issue", "related_issue": "#1687"}, {"user_name": "lastzero", "datetime": "Nov 1, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#1050"}, {"user_name": "graciousgrey", "datetime": "Nov 3, 2021", "body": [], "type": "issue", "related_issue": "#247"}, {"user_name": "lastzero", "datetime": "Nov 16, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "isleshocky77", "datetime": "Jan 4, 2022", "body": [], "type": "issue", "related_issue": "#373"}, {"user_name": "graciousgrey", "datetime": "Feb 7, 2022", "body": [], "type": "issue", "related_issue": "#2016"}, {"user_name": "lastzero", "datetime": "Mar 31, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Mar 31, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 5, 2022", "body": [], "type": "issue", "related_issue": "#2223"}, {"user_name": "graciousgrey", "datetime": "May 21, 2022", "body": [], "type": "issue", "related_issue": "#2325"}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#2292"}, {"user_name": "graciousgrey", "datetime": "Jul 22, 2022", "body": [], "type": "issue", "related_issue": "#2539"}, {"user_name": "ark-", "datetime": "Aug 2, 2022", "body": [], "type": "issue", "related_issue": "#2571"}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/1187", "issue_status": " Open\n", "issue_list": [{"user_name": "red-avtovo", "datetime": "Apr 11, 2021", "body": "Thank you very much for the project! I really enjoy using it.I would like to notice, that it would be useful to group photos on a small scale which are very close to each other or let to open cluster view (similar as a moments view) and show the subset of photos from the place, clicked on. It is not very easy to navigate through the subset of the photos or share a group of them from a specific place\n", "type": "commented", "related_issue": null}, {"user_name": "shawnbarton", "datetime": "Apr 13, 2021", "body": "I came here to make this exact suggestion. This is an important feature to me since without it the \"Places\" feature doesn't have any real use in a large library. Keep up the great work.Edit: Attached is a more extreme example (low image quality intentional). In this example \"places\" still shows individual bubbles for thousands of images at a \"street-level\" zoom. As described above, I would like to, at some (maybe configurable) zoom level, be able to see an overview of all the images in that place.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Apr 13, 2021", "body": "Thank you very much! It's true that we need to optimize this further. As you might have noticed, the maps already cluster automatically - but only up to a certain zoom level as you otherwise might not be able to see individual photos. It's more work to handle situations as this, e.g. by showing a \"stack\" that then opens in the photo viewer like stacks in regular search results.", "type": "commented", "related_issue": null}, {"user_name": "magnuznilzzon", "datetime": "May 3, 2021", "body": "I suspect it is pretty common to have the situation that  has, especially people with families will have a ton of pictures centered on their home. I know I do, that is how I found this thread.", "type": "commented", "related_issue": null}, {"user_name": "el-tiuri", "datetime": "May 24, 2021", "body": "Great idea! I like the way it’s implemented in iOS’ Photos app, and this would be pretty similar. The way it works on iOS is that it dynamically creates stacks of photos that are close together relative to the zoom level, and if you tap on a stack you get a grid of all the photos in that stack.I especially like this because it enables you to easily see all the photos you took in a particular place (regardless of scale - could be a city, could be a continent) regardless of when you took it.I think limiting the automatic clustering functionality to low zoom levels would be a bit limiting compared to the use it could have, and I don’t really see the benefit of it compared to enabling it on all zoom levels.Let me know if you want me to share some screenshots of the iOS photos app, in case that’d be helpful.", "type": "commented", "related_issue": null}, {"user_name": "ohthehugemanatee", "datetime": "Jul 11, 2021", "body": "I have the same issue. We in fact already have the dynamic stacks. Just when you click/tap on a circle with a number of photos, that should take you immediately to a pre-filled search results page. That would be plenty.  Users can zoom in closer with the plus/minus keys and get more precise, smaller stacks.", "type": "commented", "related_issue": null}, {"user_name": "fivestones", "datetime": "Dec 23, 2021", "body": "I agree with . Right now the circles with the number inside just zooms in when clicked. Instead, clicking on these circles should open a search results page with all those photos shown. Zooming in on the map is easily done other ways and doesn't need to be done by clicking on the circles that have numbers inside.Without the ability to view a search result for a group of photos in a particular location, the maps view is not very useful. For example, it would be really great to be looking at a photo, and easily see all my other photos from the same location or that were taken nearby as a search result. Right now I can go to the location of a photo on the map, but can't just see other photos in the same location without zooming all the way in, and then the other photos are only visible as little circles until I click on each one individually. Another example use would be to just show all the photos taken in, for example, Africa. Or the Grand Canyon. Or Paris. Or my home street. Again, I can find these photos if I look on the map, but only by zooming all the way in to see individual photos and then clicking on each one. If I have a small circle over Africa that says \"27\", I should be able to just click it and see all those 27 photos taken in Africa in a search result.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 23, 2021", "body": "To see the photos in regular search results, you can also type the name of the location in the regular search field, for example \"Africa\". Changing the view type between map and cards / list view causes a lot of load in the front and backend. For this reason, you typically don't want to jump back and forth all the time.", "type": "commented", "related_issue": null}, {"user_name": "fivestones", "datetime": "Jan 13, 2022", "body": "Being able to search for a location and see photos taken there is great. Thanks! I didn't know about that.But as for the map view, what's the point if you can't click on a location and see all the photos in that location? I do this all the time on my iPhone--I'm looking for a particular photo, don't know quite when it was but I know where I was. Go to the map view in the photos app, zoom in until I'm only seeing photos in the area I know the photo was taken, tap on that indicator for a bunch of photos, and there I have all those photos to scroll through, just from that location. It's great, I can do it on a location as big as a continent or as small as my house, and it's very quick--takes make 10 seconds to find a specific photo this way.It makes sense that a map view in a photos app would work this way, and then whatever load in the front and backend that it causes could be worked on.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jan 13, 2022", "body": " We will find a nice solution when we find time to work on this :)", "type": "commented", "related_issue": null}, {"user_name": "alber70g", "datetime": "Feb 28, 2022", "body": "I have an idea on how to do this:We could split the circle in a upper and bottom half.\nUpper half: shows a  that the user can tap/click and it'll zoom (current behaviour)\nLower half: shows the number of pictures in that area that the user can tap/click that takes the user to a album view of those pictures", "type": "commented", "related_issue": null}, {"user_name": "ttimasdf", "datetime": "Jul 29, 2022", "body": "As for zooming, on desktop we can use mouse scrolls, on mobile device we can use two-finger gestures. Clicking on the circle could be used to browse all the photo inside the whole group. It's rather intuitive.Here's a screen record from Huawei Gallery app, may be a good reference i think?", "type": "commented", "related_issue": null}, {"user_name": "fivestones", "datetime": "Aug 1, 2022", "body": "I agree, there is no need for any clicking/tapping on the numbers to be a zoom option. We can just zoom with two fingers or mouse scroll.\nI'd love to see it look like in the above Huawei Gallery app.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Aug 1, 2022", "body": "Huawei uses local images in a native app. We use thumbnails with a responsive Web UI that must be fast and work on all browsers and devices. So it's a tiny bit more complicated.", "type": "commented", "related_issue": null}, {"user_name": "fivestones", "datetime": "Aug 3, 2022", "body": "I suppose it is. But to change from the current, not very useful behavior (zooming in when clicking on a number) to a very useful behavior (showing a search result of all the photos in that area when clicking on the number) should be a super easy change to make. The photos in that area have already been found in the database, which is how we know how many there are. We just need the link to go to a display of all of those particular photos.If we want to zoom, we can already do that without clicking on the numbers.To show a photo, just pick the first one out of all those represented by a number, and show it there. Already when you zoom in until only a single photo is on a particular place on the map, that photo is shown. So it shouldn't be hard to show that same number to represent a bunch of photos in a more zoomed out view.", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Aug 3, 2022", "body": "No promises, but maybe I'll try creating a POC for this feature on the weekend", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Aug 6, 2022", "body": "Had a couple hours today. This is the current progress:Still a lot to do (its rough around the edges, has no tests etc), but it proves that it's doable.See ", "type": "commented", "related_issue": null}, {"user_name": "svengreb", "datetime": "Aug 7, 2022", "body": " That already looks great for a POC, well done!\nI'd like to throw in a small improvement suggestion: Instead of showing only numbers it would be nice to use a photo of this cluster instead as \"background\". This way it would be easier to quickly find what you are looking for instead of having to click on each cluster in an area when you are not quite sure if it was really taken at this specific location.\nThe number could be shown either as overlay or maybe we can introduce a new variant implementation of the cluster circle component that can have a small \"badge\" outside of the circle that contains the number. The second option would be great to prevent contrast issues of the number when the \"background\" photo has the same color like the font.To allow users to change this behavior we can make this customizable through new setting keys, e.g. to make it possible to select a specific criteria which image will be used as \"background\" (latest photo, most viewed etc.) and, of course, disable the feature to only show numbers instead like already possible in your POC.", "type": "commented", "related_issue": null}, {"user_name": "ttimasdf", "datetime": "Aug 8, 2022", "body": " This idea has already been considered by project developer. So, no hurry ", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Aug 8, 2022", "body": "All photo IDs of a cluster are known when the cluster is rendered.Showing  photo of a cluster is not a problem I think. Showing a photo based on some criteria would be more difficult.The map already renders custom elements for single images.\nRendering a different custom element, for example with a counter attached in a little bubble, should be no problem as far as I can tell.", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Aug 10, 2022", "body": "I hope no one waiting for this feature is in a hurry.\nI'd really like to spend another night or two developing this, but time is tight in the coming days/weeks.\nWith some luck I'll have some free time at the end of next week", "type": "commented", "related_issue": null}, {"user_name": "Istria1704", "datetime": "Sep 3, 2022", "body": "I found this thread just before I almost opened a new one about the same thing. Very nice it is being worked on!I also agree that there should be a way to \"show all photos in cluster\".\nIf the zooming function is simple replaced by it, I'd be fine with that. But maybe something like double-tapping is possible to keep both functions? That could work universally on both desktop and mobile, no?", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Sep 7, 2022", "body": "I'm slowly but surely making progress whenever i find some free time.\nIn it's current stateI tried making the dialog fullscreen on screens with  of width, but that somehow breaks closing it?\nFor now, having it not-fullscreen isn't to bad, even on mobile devices.a known issue is, that opening a cluster currently resets the search if a search-term was entered.\nOther than that, and a lot of testing and a little cleaning up, the only missing thing afaik is implementing better custom-elements to be rendered on the map (something like maybe the first 4 images in a cluster with an outside bubble displaying the image count)", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Sep 17, 2022", "body": "current progress. What do you think about the squared rectangles instead of circles (to see more of the images), and borders and counter bubbles?", "type": "commented", "related_issue": null}, {"user_name": "svengreb", "datetime": "Sep 17, 2022", "body": "The count badge/bubble is nice and definitely better that just a one-colored circle without an image preview!\nShowing multiple image previews is also a nice idea, but it should be opt-in since the images are really small and maybe a single image might be preferred. Adding this as an option to the settings would be really great.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2022", "body": "The new thumbnails on the map look great! As for the overlay, I haven't had enough time to think about it and didn't want to start a discussion based on unfinished work. An alternative could be to display the images at the bottom with horizontal scrolling, similar to Google Maps and other services. Of course, not so many images would be visible there at the same time and it would require developing yet another view.", "type": "commented", "related_issue": null}, {"user_name": "heikomat", "datetime": "Sep 27, 2022", "body": "i'll try to fix the \"opening a cluster resets the search\" tomorrow or in two days (will have some free time then).\n do you consider that Bug a show-stopper?More important though: Are there any missing must-haves for that feature i should work on before you consider merging it?", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Sep 28, 2022", "body": " The bug itself would not be a blocker.\nBut since we just changed 10 000 lines of code, we want to test these changes thoroughly before merging more changes.", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 12, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 12, 2021", "body": [], "type": "added this to", "related_issue": null}, {"user_name": "lrq3000", "datetime": "Sep 17, 2021", "body": [], "type": "issue", "related_issue": "#1533"}, {"user_name": "graciousgrey", "datetime": "Nov 11, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "heikomat", "datetime": "Aug 6, 2022", "body": [], "type": "pull", "related_issue": "#2596"}, {"user_name": "heikomat", "datetime": "Aug 27, 2022", "body": [], "type": "issue", "related_issue": "#2653"}, {"user_name": "lastzero", "datetime": "Sep 17, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2022", "body": [], "type": "moved this from", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "lastzero", "datetime": "Sep 17, 2022", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/255", "issue_status": " Open\n", "issue_list": [{"user_name": "Olexyy", "datetime": "Sep 29, 2017", "body": "I've implemented mouse data ( but for normal centering i had to add 100px for 'y' and 200px for 'x'). So now, on 'hover' hotspot, div can de put to that place, describing details of it. On blur it vanishes. Does it make sense to commit to master?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/230", "issue_status": " Open\n", "issue_list": [{"user_name": "graymouser", "datetime": "Jul 14, 2017", "body": "Similar to  , trying to implement a 'click to add hotspot' interface and getting mouse yaw/pitch on click would be super helpful, whether via modified getPosition or a separate function.Does anyone know a three.js command sequence that would work this out from worldRenderer.hotspotRenderer.pointer or similar? Have been fiddling with it but so far no luck, any help much appreciated.", "type": "commented", "related_issue": null}, {"user_name": "WojciechJasinski", "datetime": "Oct 8, 2017", "body": "As far as I can see onGetPosition returns somethig called camera rotation. The problem is that when I new hotspot whit Yaw and Pitch values returned by this method I get new hotspot rendered in very wrong position (90 degrees to the left).\nFurthermore loading pano I suspect that camera looks at Vector3(0, 0, 0) but in fact it looks at point 90 degrees to the right.", "type": "commented", "related_issue": null}, {"user_name": "WojciechJasinski", "datetime": "Oct 8, 2017", "body": "I've also tried to send position using  on mouse up but still no effect:", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/203", "issue_status": " Open\n", "issue_list": [{"user_name": "theunreal", "datetime": "Jun 25, 2017", "body": "Currently it seem like the getPosition() method returns  .This is not really useful, a better way to implement it is to return the yaw and pitch position of a mouse click in the viewer inside the  event.", "type": "commented", "related_issue": null}, {"user_name": "graymouser", "datetime": "Jul 14, 2017", "body": [], "type": "issue", "related_issue": "#230"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/274", "issue_status": " Open\n", "issue_list": [{"user_name": "computerjazz", "datetime": "Nov 13, 2017", "body": "Steps to reproduce: update chrome on android device and go to \nExample does not respond to phone orientation changes.Hoping that upcoming Chromium release fixes the issue: ", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Nov 13, 2017", "body": "Is this Chrome 62.0? Related: ", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Nov 13, 2017", "body": "And is this specifically for webview?", "type": "commented", "related_issue": null}, {"user_name": "computerjazz", "datetime": "Nov 13, 2017", "body": "It is definitely an issue for webview, but I also see the issue when navigating to the above link in android chrome 62.0.3202.84.When I open up the remote device Chrome developer tools for that link I see:Here's what I see -- image does not move with phone movement:\nI have been able to successfully view some 360 content in chrome, so I can't pinpoint the exact issue.", "type": "commented", "related_issue": null}, {"user_name": "aerialglasgow", "datetime": "Nov 14, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "ademarre", "datetime": "Nov 16, 2017", "body": "I see the same issues on Android after updating to Chrome 62. It is only a problem when running VR View through a cross-domain iframe, and it is solved by adding  Feature Policy attribute to the .In addition to the accelerometer, these issues present as well:Perhaps coincidentally, Firefox on Android is broken with the same symptoms, but  does not resolve it.", "type": "commented", "related_issue": null}, {"user_name": "aerialglasgow", "datetime": "Nov 17, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "ademarre", "datetime": "Nov 17, 2017", "body": ", I don't know about that line number, but I was putting it in  after this line:\nNonetheless, you are correct that it does not solve the problem. I was mistaken and had a false positive when I tested it. Although I am confident that it is only an issue with cross-domain iframes.Since the  Feature Policy doesn't fix it for Chrome, I want to revise my previous statement to say that The  change is probably still good though, because of this: ", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Nov 18, 2017", "body": " do you mind putting up a PR with this change?", "type": "commented", "related_issue": null}, {"user_name": "adamweld", "datetime": "Nov 30, 2017", "body": "This issue is still present for me even after adding allow='vr' to the iframe.For example view my site in Chrome on Android:\nThe viewpoint appears locked to the ground. Tapping and dragging to change yaw still works. Here's a code snippet:Note I've also tried using a self hosted version of VRView with the following src line:But in that case the image fails to load.", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "Dec 14, 2017", "body": "It seems to be fixed with Chrome 64. Verified with Chrome Dev 64.0.3282.12 on Android 8.1.0.Firefox on Android is still unresolved as of the latest nightly 59.0a1 (2017-12-13).", "type": "commented", "related_issue": null}, {"user_name": "neilloispy", "datetime": "Mar 13, 2018", "body": "Just to let you know exactly the same issue has appeared again in latest release of Chrome 65.0.3325.109, however not an issue in other browsers, using sample view here:\n", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Mar 13, 2018", "body": "Does this also occur in Chrome 65 in the webview?", "type": "commented", "related_issue": null}, {"user_name": "neilloispy", "datetime": "Mar 14, 2018", "body": "", "type": "", "related_issue": null}, {"user_name": "thomclae33", "datetime": "Mar 16, 2018", "body": "Accelerometer tracking seems indeed broken on Chrome mobile", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Mar 16, 2018", "body": "There are a few regressions in Chrome m65 detailed in the , vrview needs to be updated with the latest polyfill to mitigate these changes, although there still could be low frequency of devicemotion events, which there's not much we can do until m66", "type": "commented", "related_issue": null}, {"user_name": "McGern", "datetime": "Mar 22, 2018", "body": "Can confirm it was working for me this morning using an aframe demo (don't know what version of Chrome sorry). I have set updates to manual on my phone. When I updated to 65.0.3325.109 the gyroscope is not working anymore. Using a oneplus one.", "type": "commented", "related_issue": null}, {"user_name": "tksharpless", "datetime": "Apr 17, 2018", "body": "On my Samsung Galaxy S6, several recent versions of Chrome, Chrome Dev and Chrome Beta show incorrect gyro behavior at multiple VR/360 websites. Either the gyro response is wildly unstable & hypersensitive, or it works very slowly and only on the vertical axis. The current Samsung Internet app is usable, though both jumpy & sluggish.Rolling back the factory installed Chrome app to the original version, which is several years old, restored correct gyro behavior. I was also able able to install version 63.0.3239.71 of Chrome Beta, which works right. So this is definitely a case of \"better is the assasin of good\".I  hope there is a fix real soon, as this is a major problem for  the VR industry.", "type": "commented", "related_issue": null}, {"user_name": "tinywolf3", "datetime": "May 8, 2018", "body": "I tested with .\nMobile Chrome(v66) and Opera Mini(v33) have the problem.\nMobile Firefox(v60) and Samsung Internet(v6.4) are fine.The other 360 web viewer, , is fine in all browsers.I hope fix this soon.", "type": "commented", "related_issue": null}, {"user_name": "steindelek", "datetime": "May 10, 2018", "body": "To fix  issue in latest Chrome or webview, just force vrview to calculate the rotation the same way as for Firefox and IOS. In embed.js in FusionPoseSensor.prototype.updateDeviceMotion function take this.gyroscope.multiplyScalar(Math.PI / 180) out of \"isIOS or isFirefoxAndroid\" condition.\nHope that helps :)", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "May 10, 2018", "body": "Chrome m65 has deviceorientation platform issues that the polyfill cannot work around. In Chrome m66, the deviceorientation events are now calculated similarly to iOS/Firefox -- the latest webvr-polyfill version addresses this, which this library needs to update to", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "May 10, 2018", "body": " is anyone actively working on updating this library to the new polyfill?I understand there will be some work involved to provide new keyboard/mouse controls since  was removed from the new polyfill. I'm not sure of the best way to solve that in VRView.", "type": "commented", "related_issue": null}, {"user_name": "tinywolf3", "datetime": "May 14, 2018", "body": "  Thank you for your help.\nI change the function, isFirefoxAndroid to isAndroid.\n\nNow, all browsers are OK.", "type": "commented", "related_issue": null}, {"user_name": "Donosor", "datetime": "May 23, 2018", "body": " Also on Huawei?\nCan you help me please? if I modify mi code don't work nothing. I see the white screen on all browser!", "type": "commented", "related_issue": null}, {"user_name": "kadupenido", "datetime": "May 24, 2018", "body": "  Thank you. I made the changes and it worked out here.\nI tested on android 7.1 and chrome 66 and it worked fine", "type": "commented", "related_issue": null}, {"user_name": "jedaan", "datetime": "Jul 11, 2018", "body": "\ni tested on android 7.0 and chrome 67 and it ", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Nov 20, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ademarre", "datetime": "Nov 20, 2017", "body": [], "type": "pull", "related_issue": "#277"}, {"user_name": "ademarre-ca", "datetime": "Nov 22, 2017", "body": [], "type": "pull", "related_issue": "#278"}, {"user_name": "davidh64", "datetime": "May 14, 2018", "body": [], "type": "issue", "related_issue": "#317"}]},
{"issue_url": "https://github.com/photoprism/photoprism/issues/152", "issue_status": " Open\n", "issue_list": [{"user_name": "ghost", "datetime": "Dec 7, 2019", "body": "I think a timeline view or a timeline scrollbar, like in Google Photos or Synology Moment, would be a cool feature to have.", "type": "commented", "related_issue": null}, {"user_name": "dennorske", "datetime": "Dec 11, 2019", "body": "I agree, this also sounds like what  wants. I support it ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 14, 2019", "body": "I personally don't find the timeline scrollbar in Google Photos helpful in practice. We thought more about a punch card view and automatic clustering by date & location.Can you post some screenshots of Synology Moment?", "type": "commented", "related_issue": null}, {"user_name": "0liu", "datetime": "Dec 16, 2019", "body": "I think Synology Moment tried to imtate Apple Photos moments function. Essentially it uses the combination of time and location to group photos into \"moments\", instead of time only as in a timeline.", "type": "commented", "related_issue": null}, {"user_name": "dennorske", "datetime": "Dec 17, 2019", "body": "Moments sound cool!\nBut considering the current self hosting market, there's not many services supporting a timeline feature. One of the main reasons is that I want it to easily scroll through pictures sorted by date. I have been looking for over 2 weeks and can't seem to find something useful except for maybe plex and the currently abandoned ownphotos project.\nElse I would survive with a standard gallery/year/month folder structure, like many others.I am more than happy to donate to this feature specifically, if it hasn't been considered added to the project. I do think it will cover a big gap in the projects out there.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 17, 2019", "body": " I guess it will take a few donations to really pay a developer to do this, but we listen to community feedback and have this on our list once the basics are done :)If we do a \"pro\" version with additional features, our sponsors will certainly get a free license!", "type": "commented", "related_issue": null}, {"user_name": "0liu", "datetime": "Dec 18, 2019", "body": "As far as I know  has implemented timeline and place view separately, but not moments.", "type": "commented", "related_issue": null}, {"user_name": "dennorske", "datetime": "Dec 28, 2019", "body": "Could it be similar to what Nextcloud maps have? There's an app in the store called maps, and it allows you to save locations, as well as visualise GPS tagged photos. It scans through and shows up like this:\nThe nice thing is, when you zoom out, the pictures groups up. If they are taken in a small area, and you click the icon with a number under, it spreads out all the thumbnails around and you can have a glimpse at them:Reason I am showing it, is because it looks very fluent and nice. Will this be similar to what is wanted above? There is also a time-period slider on the bottom where you can define from-to dates, and the map will show only the relevant pictures.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 29, 2019", "body": " Thank you very much for the screenshots! We spent the last couple of days (Merry Christmas!) preparing our database for efficient clustering (country/year/month) and location search using S2 cell IDs (more efficient than lat/long). See also .Next, we need to provide our own location service to actually be able to index many photos in acceptable time. When this is done, we can finally focus on the UI, certainly room for improvement. One of our ideas is to show a path on the map where you've been and then you can travel along.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 23, 2020", "body": "Looks like you get this feature in our  first!", "type": "commented", "related_issue": null}, {"user_name": "leopoldsedev", "datetime": "Jan 26, 2020", "body": "Here's some more inspiration for the UI. It is taken from plex. I think it's nice how photos are separated by day and dynamically tiled based on the image size. The scrollbar on the right is overlayed by the years and the smaller ticks indicate a high density of photos there. Overall it's nice to scroll through memories like this.\n", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 26, 2020", "body": "Google Photos is similar, personally never found this very useful or pleasing to my eye TBH. Lots of space wasted when you only have one or two photos per day.", "type": "commented", "related_issue": null}, {"user_name": "dennorske", "datetime": "Feb 2, 2020", "body": "If not a timeline, it could be a scrollbar that shows month + year close to it when you scroll?\nSome sort of seeking like that is very handy, at least if you're an ancient google photos user like me :)\nIt is extremely handy to do. But I would not say this is the most important feature, it is more a nice-to-have if  is implemented.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Feb 2, 2020", "body": "Maybe I can't properly use it because my mouse is too fast Let's focus on the essentials to get a stable release, there will be enough time to experiment afterwards.Clustering by year, month and country is easy with our current database schema. We'll do sharing next as this has higher impact on our architecture. Worked a lot on events and validation recently for a smooth user experience.", "type": "commented", "related_issue": null}, {"user_name": "dennorske", "datetime": "Feb 2, 2020", "body": "That is great, thank you for all the efforts in the project!\nI am excited to see the sharing come along, that is clearly something I am missing. Good luck", "type": "commented", "related_issue": null}, {"user_name": "mpodshivalin", "datetime": "Jun 12, 2020", "body": "\nI'm off-topic, but I've seen a lot of mentions of a possible Pro version. Are there any plans on how it will be licensed? E.g. a free-as-a-beer but proprietary license is not something I'd potentially want", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 13, 2020", "body": "Fact is, we want to be as free and open as possible and think this provides real value to our users, like when you look at how we develop together with our community.On the other hand, we've been working full-time for about ~2,5 years now and there are still a lot of requests waiting to be implemented. GitHub sponsors doesn't even cover the monthly costs of our servers. So the idea is to release a special version with additional features for our sponsors, contributors and eventually paying customers. See \"Funding\" in our docs. Obviously you shouldn't be allowed to take this and sell it on your own. Suggestions welcome.", "type": "commented", "related_issue": null}, {"user_name": "mpodshivalin", "datetime": "Jun 14, 2020", "body": "\nOf course selling/getting funding for free software is a difficult issue.\nI've checked the Funding page and the first thing I see is \"This project is about freedom and privacy but not necessarily about free beer.\". So, it has to be free software, or else there won't be much of a difference with other proprietary projects.According to the funding page, there won't be Enterprise customers (as opposed to Nextcloud for example). So the project should be funded by regular users, which value privacy so they won't accept the proprietary solution. And by \"Regular users\" I mean not a super tech-savvy users (it's not a big market), but real regular people. And the most important thing for regular people is convenience.Make the source code available for both versions with the same license, but do not distribute a \"Pro\" version for free. This is how e.g. OpenProject works - both versions are GPL'ed. So yes, it should be possible to obtain the source code for the \"Pro\" version without paying, but it shouldn't make sense because it won't be convenient. And the most important thing for regular people is convenience. So if a user obtained the source code for the \"Pro\" version - he must build everything from source, he won't have upgrades and will have a generally semi-working setup in a long term. For example - iOS user can't have this app even if he has a source code, unless he has an up-to-date Mac, general knowledge on how to install apps to and iPhone and he must have a developer account (which won't solve all of the problems). This is very inconvenient and it's better and easier to buy an app, even if it's GPL'ed. This is my opinion :) And it's just one example - servers can be more convenient, too.And, actually, getting back to the topic, from my experience, regular people, when they say they need a \"Cloud\", mainly mean photos, and they certainly mean Timeline. This view is the default view in Apple+Google photos, and it is mainly used because people (including me) don't have much time to organize their photos, they don't want to add photos to albums unless it was an event, something significant, like a trip. And they don't want to filter photos by year, month, etc. they don't want to think. The Timeline interface, though seems simple, lets people look for their photos in a convenient way, with the ability to go to a specific date and grouping by date is letting them find events quickly without organizing anything", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 15, 2020", "body": "Shouldn't be difficult. I'm part of the free software community since the 90s and it was clear from early on, that free doesn't mean free beer. I see it as an additional feature that provides value to our users. Unlike back in the 90s, software as a service is very popular today, so the GPL only provides limited protection to us and other startups that invest a lot of time and money into their products.It's not clear to me why the right to commercially distribute every line of code we release should be important to private end-users.If there were more sponsors, we could release everything under a MIT or BSD style license. As a matter of fact, we only have very few sponsors and no other funding whatsoever. So we need to find a different solution unless you want us to stop working on this.For now, you have the new Calendar view plus the year and month search filter in our main view. You can also sort by date (added, newest first, oldest first), relevance, similarity and file name. Plus we automatically create moments based on country and year for you. It feels like this is enough for a first release and of course we're using your feedback to continuously improve our product.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 15, 2020", "body": " Pro doesn't mean you won't be able to get or modify the source code. Take a look at how GitLab handles dual-licensing, they also distribute the code.Everyone can use our software under the terms of the AGPL from now on: This is far from being proprietary. Those who need additional rights or advanced features like multi-user support MAY pay for it at some point in the future. We pay for development and take all the risk. Let us know if you get a better deal somewhere :)", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Jun 24, 2021", "body": " has funded $75.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "jimboolio", "datetime": "Jun 24, 2021", "body": "To my understanding this may fix the slow scrolling issue mentioned in issue . I am running photoprism on 10 core server with 60gb ram and storage is from high performance san over smb (on linux) and the scrolling is painfully slow even on local network. I have 200 000 photos on my photoprism instance which I moved from Goolge photos and I miss the ability to quicly scroll all of my photos at once as I used to do there.I tried Nextcloud too, but it is just unusably slow/crashes with my library. Photoprism can handle my photos without crashing (great job photoprism devs!!!), I just cannot view/scroll them very quicly because after scrolling too far on the grid view, my client device heats up and slows down. I see the potential in this project and if the ux was a bit faster on my probably quite unique and heavy use case this could soon become my main photo management software. Even if the timeline feture cannot solve my weird use case I would be happy to see the timeline feature.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 30, 2021", "body": "Yep, we'll get to this. The current photo search wasn't designed to scroll through your entire library - instead it may show higher resolution thumbs and more metadata than Google Photos, which only provides very limited metadata with search results, basically just the preview. So there's pros and cons. Working hard to grow our team so that we get more done in less time.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jul 24, 2021", "body": "We are working on facial recognition right now, and also need a short break after working nonstop for more than 3 years. This has nothing to do with backend coder thinking.", "type": "commented", "related_issue": null}, {"user_name": "neonsoftware", "datetime": "Jul 28, 2021", "body": " thanks for updating on current items on the table. Have an outstanding holiday 👍🏼", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Sep 16, 2021", "body": " has funded $5.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Sep 26, 2021", "body": "An anonymous user has funded $15.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Sep 29, 2021", "body": "An anonymous user has funded $50.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "issuehunt-app", "datetime": "Oct 10, 2021", "body": " has funded $2.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "IssueHuntBot", "datetime": "Dec 23, 2021", "body": "An anonymous user has funded $30.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "IssueHuntBot", "datetime": "Dec 23, 2021", "body": "An anonymous user has cancelled funding for this issue. (Cancelled amount: $30.00) ", "type": "commented", "related_issue": null}, {"user_name": "IssueHuntBot", "datetime": "Dec 23, 2021", "body": "An anonymous user has funded $10.00 to this issue.", "type": "commented", "related_issue": null}, {"user_name": "IssueHuntBot", "datetime": "Dec 23, 2021", "body": "An anonymous user has cancelled funding for this issue. (Cancelled amount: $10.00) ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 23, 2021", "body": "Please don't add funds to IssueHunt anymore! While we like IssueHunt and are grateful for the donations we've received so far, it hasn't proven to be a sustainable funding option for us as we spend much of our time maintaining existing features and providing support.If we don't have enough resources to provide support and bugfixes, we can't start working on new features.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 23, 2021", "body": "See  for sponsorship options ", "type": "commented", "related_issue": null}, {"user_name": "tolicodes", "datetime": "Dec 23, 2021", "body": "Actually....I would be open doing some work on this in the near future (need it for a project).  Maybe if someone on the team is wiling to pair with me to get started.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 23, 2021", "body": "We already know how to implement it, but must focus on multi user 100% in January and also should look into upgrading Vuetify first. Help with providing support and answering questions is much appreciated.", "type": "commented", "related_issue": null}, {"user_name": "tolicodes", "datetime": "Dec 23, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 24, 2021", "body": "We need a proper API for it first, which is what native app developers are waiting for as well. It's not complicated, we just have to limit , as our current donations don't fund a large team where everyone can focus on something else.", "type": "commented", "related_issue": null}, {"user_name": "laterwet", "datetime": "Jan 17, 2022", "body": "Currently, photoprism is using vuetify 1.5 (released 3 years ago). Looking at their , I see they added  component in version 2.3.Using this component would solve multiple of the listed issues, such as:Someone already upgraded in this PR: ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 17, 2022", "body": "I wish we had a team of TypeScript developers, but no. Anyway, we will focus on the missing timeline server API first... the JS part is no magic, it just needs to be done... we develop , meaning before we implement a UI, we work on the API.", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 17, 2022", "body": "Take a look at , this is (by far) not the only issue on our ToDo list. Otherwise, it would have been done long ago. We can't work on everything at the same time, especially not with our .", "type": "commented", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Jun 29, 2022", "body": "No timeline view yet, but the  comes with a much faster scrolling experience ", "type": "commented", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 8, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 23, 2019", "body": [], "type": "issue", "related_issue": "#21"}, {"user_name": "lastzero", "datetime": "Dec 30, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "lastzero", "datetime": "Dec 30, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jan 22, 2020", "body": [], "type": "issue", "related_issue": "#214"}, {"user_name": "lastzero", "datetime": "Jan 23, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 15, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 16, 2020", "body": [], "type": "", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 23, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "lastzero", "datetime": "Jun 23, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lastzero", "datetime": "Nov 1, 2021", "body": [], "type": "removed  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Nov 3, 2021", "body": [], "type": "issue", "related_issue": "#786"}, {"user_name": "lastzero", "datetime": "Nov 16, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "graciousgrey", "datetime": "Apr 14, 2022", "body": [], "type": "issue", "related_issue": "#2253"}, {"user_name": "lastzero", "datetime": "Apr 16, 2022", "body": [], "type": "issue", "related_issue": "#2258"}, {"user_name": "lastzero", "datetime": "Jun 17, 2022", "body": [], "type": "pull", "related_issue": "#2292"}, {"user_name": "lastzero", "datetime": "Jun 17, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/150", "issue_status": " Open\n", "issue_list": [{"user_name": "markwoodward23", "datetime": "Apr 27, 2017", "body": "I'm creating a slide based website based on Dragdealer that uses the arrow keys or dragging horizontally to navigate between slides. As soon as the Vrview loads on one of the slides, it hijacks the keyboard and mouse movements so I can't change slide anymore. I've tried commenting out the event listeners in embed.js, which stops the commands, but instead of allowing me to use the normal dragging and or arrow keys, it doesn't do anything.Is it possible to change the arrows keys or remove the key binds?", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "May 8, 2017", "body": "You might want to take this question to stack exchange... (since its not really a vrview issue)The keyboard events happen in the WebVR Polyfill.\ncommenting out line 38 should work. Or commenting out that code directly in embed.js\n(which you may have already tried)", "type": "commented", "related_issue": null}, {"user_name": "tommytee", "datetime": "May 11, 2017", "body": "I apologize for suggesting stack exchange / stack overflow.  I recently tried to use stack overflow and discovered that it doesn't work very well.\nI created a google group for questions and discussion.\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/192", "issue_status": " Open\n", "issue_list": [{"user_name": "rsacksteder", "datetime": "Jun 20, 2017", "body": "VR view is broken in the following version of Microsoft Edge (which came with the Windows Creator Update):Microsoft Edge 40.15063.0.0\nMicrosoft EdgeHTML 15.15063Before updating, I was unable to reproduce the issue in the following version of Microsoft Edge:Microsoft Edge 38.14393.1066.0\nMicrosoft EdgeHTML 14.14393To reproduce this issue:Interestingly I noticed that the viewer at  works okay, even with the affected version of Edge. I believe the reason for this is that an older version of VR view is being used on that page.After noticing this, I used git bisect and found that commit  (\"Move to latest polyfill, but so far using the provided distortion correction\") appears to have introduced this issue in combination with whatever changes were made recently to Microsoft Edge. Prior to that commit, the viewer appears to work even in the affected version of Edge. But any versions of the viewer from that commit and newer are broken. I suspect that the mentioned commit made a change to the Edge polyfill that is not playing nicely with the latest release version of Edge.There don't appear to be any console errors to go off of and I set up a test with everything on the same origin, which didn't appear to resolve the issue, so it doesn't appear to be a cross-origin issue either.", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "Jun 20, 2017", "body": "The panorama loads but viewport interaction with the mouse does not work.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Jun 20, 2017", "body": "", "type": "", "related_issue": null}, {"user_name": "ademarre", "datetime": "Aug 11, 2017", "body": "In Firefox 55, VR View is breaking identically to Edge 15. Interestingly, Edge 15 and Firefox 55 both introduced WebVR support. VR View works correctly when  is turned off in Firefox's advanced config.In both browsers, the older version of VR View at this URL works correctly: ", "type": "commented", "related_issue": null}, {"user_name": "ademarre", "datetime": "Sep 14, 2017", "body": "It also breaks on Chrome if you enable WebVR in .Verified with Chrome  on Windows 8.1.", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Sep 14, 2017", "body": "I think this is fixed by upgrading the webvr-polyfill to 0.9.38 that includes  that doesn't engage the polyfill when we have full native support ", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Sep 15, 2017", "body": "Upgraded to webvr-polyfill 0.9.38", "type": "commented", "related_issue": null}, {"user_name": "sportflier", "datetime": "Sep 19, 2017", "body": "On FF 32-bit 55.0.3 with webvr-polyfill 0.9.38, doesn't work.\nJavaScript errors in build/embed.js: display is undefined (var display = this.controls.getVRDisplay();)", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Sep 19, 2017", "body": " on desktop or mobile? Works for me on FF 55 (both with and without  on) ", "type": "commented", "related_issue": null}, {"user_name": "ssh-esoteric", "datetime": "Sep 20, 2017", "body": "I think I found the root of this issue:Is it possible for VRView (or the polyfill) to register an instance of MouseKeyboardVRDisplay with the browser so that it's natively returned from getVRDisplays()? (Possibly by emitting a vrdisplayconnect event? - I'm not too familiar with the WebVR spec)A temporary workaround is to forcibly enable the polyfill:", "type": "commented", "related_issue": null}, {"user_name": "jsantell", "datetime": "Sep 20, 2017", "body": "In that case, there's an option  that will need to be added -- reopening // ", "type": "commented", "related_issue": null}, {"user_name": "sportflier", "datetime": "Sep 20, 2017", "body": " I've seen the issue on desktop (Windows), but haven't tried yet on mobile or investigated setting .", "type": "commented", "related_issue": null}, {"user_name": "benhamelin", "datetime": "Oct 12, 2017", "body": "Just cloned and built this morning, on Windows 10 (thanks to  ) .I was getting errors in both FF and Edge from the embed.js script. I compared it to  and found them to be different, so used the copy from this URL and that solved the issue. Can't tell when the build was published.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Oct 12, 2017", "body": "We just added some information to the README advising windows users to download the builds from the github pages link you posted. I am closing this for now.", "type": "commented", "related_issue": null}, {"user_name": "btb", "datetime": "Feb 27, 2018", "body": "Still broken for me, on both Firefox 58.0.2 (64-bit) and Microsoft Edge 41.16299.248.0, EdgeHTML 16.16299Forcibly enabling polyfill does make it work.", "type": "commented", "related_issue": null}, {"user_name": "walkthelot", "datetime": "Apr 18, 2018", "body": "Any luck on this? This is crazy. :(", "type": "commented", "related_issue": null}, {"user_name": "gctommy", "datetime": "May 4, 2018", "body": "Still doesn't load on the latest Firefox.But if anyone's looking for an alternative, A-Frame works great on both the latest Chrome and Firefox (not tested on Edge): ", "type": "commented", "related_issue": null}, {"user_name": "mdbarr73", "datetime": "Oct 11, 2018", "body": "Firefox is still broken on version 62.0.3, image loads but pan does not work.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Jun 20, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "gol4er2219", "datetime": "Aug 29, 2017", "body": [], "type": "issue", "related_issue": "#246"}, {"user_name": "lincolnfrog", "datetime": "Sep 15, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "jsantell", "datetime": "Sep 20, 2017", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Oct 12, 2017", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "ademarre", "datetime": "Dec 14, 2017", "body": [], "type": "issue", "related_issue": "#289"}, {"user_name": "jsantell", "datetime": "Dec 15, 2017", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "ademarre", "datetime": "May 14, 2018", "body": [], "type": "issue", "related_issue": "#317"}]},
{"issue_url": "https://github.com/googlearchive/vrview/issues/78", "issue_status": " Open\n", "issue_list": [{"user_name": "pszafer", "datetime": "Nov 16, 2016", "body": "Hello,\nI wanted to use your library to put panorama/360 photos on my website, but first thing I see, that with my touchscreen laptop I cannot move sample photo on your website Is it a bug or vrview library is not touch friendly on PC devices?", "type": "commented", "related_issue": null}, {"user_name": "paulstone", "datetime": "Nov 21, 2016", "body": "Hi I had this problem today too on a kiosk we've installed. Did you fix the issue/find a workaround? From what I could tell, it wasn't working because the touchscreen doesn't have (or Chrome wasn't recognising) the click-and-drag event.Thanks,\nPaul.", "type": "commented", "related_issue": null}, {"user_name": "pszafer", "datetime": "Nov 22, 2016", "body": "No success so far.\nAccording to this file  line 54:I imagine that they implemented touchevents into library.", "type": "commented", "related_issue": null}, {"user_name": "paulstone", "datetime": "Nov 22, 2016", "body": "Hi,I have ended up using the A-frame library instead for the kiosk I'm working on (). It works when testing it in Chrome simulating a touch PC. Just waiting to update it on the actual kiosk though to REALLY test it works though.This is basically what I have replicated on our app: On desktop/mobile you can pan around in all directions. However on touch desktop it appears to allow you to pan around left/right but NOT up/down.  Luckily in our case, there is nothing interesting to see on the floor or the ceiling so this is acceptable for us :)Hopefully this fallback/feature can/will be implemented in vrview.Thanks,\nPaul.", "type": "commented", "related_issue": null}, {"user_name": "paulstone", "datetime": "Nov 22, 2016", "body": "Just an update from me for completeness. The Aframe library worked on the actual touchscreen PC kiosk in that it allows us to pan left-right fully. No panning up or down, but that was ok for our purposes (nothing interesting to see up or down, thankfully).", "type": "commented", "related_issue": null}, {"user_name": "borismus", "datetime": "Nov 22, 2016", "body": "The if (!Util.isMobile()) { check is incorrect. Instead, touch events\nshould event.preventDefault() to stop the mouse event from being emitted.On Tue, Nov 22, 2016 at 6:48 AM Paul Stone  wrote:", "type": "commented", "related_issue": null}, {"user_name": "pszafer", "datetime": "Dec 6, 2016", "body": "Do you have plans to correct it in nearest future?", "type": "commented", "related_issue": null}, {"user_name": "RolandMac", "datetime": "Mar 12, 2017", "body": " Do you mind providing a bit more detail as to how to correct this issue.", "type": "commented", "related_issue": null}, {"user_name": "phoenix5999", "datetime": "Mar 15, 2017", "body": "Seem like it still not work until now.", "type": "commented", "related_issue": null}, {"user_name": "tgunz", "datetime": "Jun 14, 2017", "body": "While A-Frame is great, it's also complete overkill for those of us looking to incorporate this kind of feature into a kiosk. I'd love to help figure out how to enable touchscreen capabilities with anyone who's got the time.", "type": "commented", "related_issue": null}, {"user_name": "lincolnfrog", "datetime": "Jun 20, 2017", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/704", "issue_status": " Open\n", "issue_list": [{"user_name": "edckt", "datetime": "Mar 26, 2019", "body": "I can open the webcam when first the Java file. But after doing the first mouseClicked, the second trigger of captureImage shows 'device error' on the WebcamPanel.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/64", "issue_status": " Closed\n", "issue_list": [{"user_name": "sriramsv", "datetime": "Oct 3, 2021", "body": "I'm currently working on modifying this demo app to make it a Tasker plugin. I can send the messages the app hears now to Tasker, but I don't see any way to get the message back to the app and update the message in the UI fragment.I'm not very familiar with kotlin, so any help is appreciated!", "type": "commented", "related_issue": null}, {"user_name": "sriramsv", "datetime": "Oct 4, 2021", "body": "I want to achieve this flow where in the response coming from the assistant could either be initiated by an event or the request-response flow is not being called synchronously by the DialogAPI.\n  ", "type": "commented", "related_issue": null}, {"user_name": "sriramsv", "datetime": "Oct 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/sarxos/webcam-capture/issues/601", "issue_status": " Open\n", "issue_list": [{"user_name": "reginavaleria96", "datetime": "Dec 18, 2017", "body": "Is it possible to access and take picture from a phone connected through  using this project?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 18, 2017", "body": "Hi ,It seems you can, but I don't know the exact URL to be used with . This is the claim from their web page:When you already know what is the URL to access MJPEG stream from the camera you can take a look at the IP camera examples of Webcam Capture to learn more on how to integrate it into your solution:Please let me know if you have any further questions.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 19, 2017", "body": ", FYI, I found information on MJPEG here:Check this section: .", "type": "commented", "related_issue": null}, {"user_name": "neilyoung", "datetime": "Dec 19, 2017", "body": " Just out of curiosity: Would it be possible to  from such a cam?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 20, 2017", "body": ", yes, but the performance will be much below what you can get with UVC (USB). This is due to:In addition to 2, you can skip this and get RAW bytes, but these won't be RGB, but a JPEG frame, which you will have to convert to RGB to have compatibility with  (thus doing the same what  is doing underneath).", "type": "commented", "related_issue": null}, {"user_name": "neilyoung", "datetime": "Dec 20, 2017", "body": " Clear and understandable. Thanks for the explanations. Merry Christmas to you and thanks too for all your efforts.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 10, 2018", "body": "I have included webcam-capture-0.3.12-20171103.095135-4.jar in my library but I'm getting  and \nWhere can I find IpCamDeviceRegistry and IpCamMode?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": ", use these JARs: ", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": "And remove webcam-capture-0.3.12-20171103.095135-4.jar beforehand. The zip I posted above, already contains newest webcam-capture dependency.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 10, 2018", "body": ", and one more missing JAR file: ", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": "Thanks for the JARs!\nDo you mind explaining why is it necessary to have\n\n( line 57 onward)?and do webcam.open() and webcam.close() work the same way for IPcam?", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "Hi ,In regards to your questions:This is the code which is executed when action is performed on a \"Snapshot\" button. When you click this button the  will iterate over all  instances in  list, get image and save it to file with . The file names will be , , , etc (as many as you have webcams). Without this code you would not have ability to save images into files.And in regards to:No. The  and  works in a different for IP cameras when you compare to  webcam.In case of classic webcam (the ) when you invoke  then hardware device is turned on (LED is powered) and USB capacity is allocated to stream image frames. After you invoke  the USB capacity is released and UVC device is turned off. There is a direct connection between UVC being turned on/off and the webcam state (unless something is broken).The  from the other hand is always turned on, unless you turn it off with a manual switch, a power cord or administration panel. In this case when you invoke  a persistent HTTP connection is made which is used to stream MJPEG frames from camera to your computer, but this is done with the IP/TCP instead of USB (as in case of UVC). Then, when you invoke , the webcam is not turned off, but the HTTP connection is closed instead and IP camera device remains active.This is a difference. I hope I was able to describe it in a understandable way.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "Just for your information, if you would like to avoid managing JARs manually, I suggest you to take a look at .Maven is a tool to build Java projects and manage JAR dependencies. It may be a little hard to learn at the beginning, but there a dozens of tutorials on the internet. The ZIP bundles I sent you are also a list of files prepared by Maven.The more JARs you have in the project the higher chance you will have to use some dependency manager (Maven, Gradle or Ivy). When Java project grows in time it comes into the point where managing JARs manually is impossible (due to massive amount of incompatible libraries).My Webcam Capture API project is also managed and build with Maven.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": " Thank you for your explanation, it was clear and understandable. I was confused about the webcam iteration, that was why I had to ask. I hope you don't mind.If I have the default onboard webcam on my laptop and an IP camera connected via droidCam, how do I tell the program to specifically take one picture from my DroidCam camera?And thank you for sharing about Maven, I have heard of it and had considered using it. My mistake was thinking that my small project won't have so many libraries and deciding to manage them manually, but now I know that I was wrong :')", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "In regards to:Yes, sorry for this. The code I initially wrote was to support multiple cameras, but finally I left only one IP camera in the code.You have to use composite driver to \"compose\" two drivers into one, example:You set it in the same way as you would do it with a different drivers:You can access cameras by index so the ones from first driver will be first one the list (e.g. index 0), and the ones from second driver will be added later (e.g. index 1, 2, 3, etc).The other option is to use camera names, but this may be tricky on Windows since it may work well on your computer, but different computers may have different camera names. In Linux this is simple because on different computers every all cameras are simply , , etc, and on Windows this can be , , etc. depending on the webcam hardware.", "type": "commented", "related_issue": null}, {"user_name": "reginavaleria96", "datetime": "Jan 18, 2018", "body": "Thanks, that is one helpful explanation! I am going to try it now. I hope you don't mind if I still have more questions in the future.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "If you link your carrier with Java then Maven is a tool you have to, at least, be familiar with. It has thousands of plugins and configuration options which may be overwhelming at the beginning because different people on the internet may suggest different solution, but after you understand how it works it will become one of the fundamental tools in your toolbox.When you want to manage you project with Maven it's enough to put  file in the project main directory.The simplest example for your project can be:Instead of providing many JARs I only had to add one dependency in the XML, that is:And when I  into the project and run  I can see what JARs are used:Which will display all JARs required by your project:And e.g. if I want to export all required JARs together with your own application JAR I can do the following:And all JARs will be magically be placed in a  directory:I spent many years working in a terminal environments and I'm used to do things from command line, but the same can be done from a decent IDEs, e.g. Eclipse or IntelliJ.For example when you are using Eclipse IDE for Java EE Developers (basic Eclipse installation does not have Maven plugin installed) you can create Maven project from the mouse menu and manage dependencies with a very nice POM editor.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 18, 2018", "body": "And one last thing - almost all (probably 99.99%) of all decent Java frameworks and libraries are deployed into Maven Central repository which is used as a source from where all dependencies are downloaded.Jersey, JBoss, Hibernate, JSP, all Apache Commons, SL4J, Logback, Jetty, Netty, Vavr, Akka, and of course Webcam Capture API, all are available from Maven.", "type": "commented", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 29, 2018", "body": "Hello Sir,\nI am trying to use  as a Webcam. The program is identifying the device but it not showing the panel. I referred  example. Please help me.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 29, 2018", "body": "", "type": "", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 29, 2018", "body": "Yes, I am able to view the Image in browser. I didn't make any changes to the code. I just copied the code from example program. Here is the code.", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 30, 2018", "body": "Hi ,If you can view it in browser and you can't in a Java code, means that your code is broken. The URL you provided to IP cam driver:Points to a web page, not MJPEG stream! A static HTML, nothing to stream video feed from. Please take a look at the DridCam web page and ! You will find this information:Therefore, your code should be fixed to use proper URL:I installed DroidCam and easily found it myself by checking what is the image source (just press F12 in Firefox):This is my code:And it works perfectly well:After reading this far please also take a look at the problem I described below.I noticed that you can connect  to the DroidCam. Therefore if you have your DroidCam web page open in a browser you  to view it from code and vice versa - when you have your DroidCam streaming open from the Java code, you won't be able to view it in a browser. This is DroidCam limitation, not the Webcam Capture issue.", "type": "commented", "related_issue": null}, {"user_name": "avatar31", "datetime": "Jan 31, 2018", "body": "Thank You very much Sir. Now it is working", "type": "commented", "related_issue": null}, {"user_name": "sarxos", "datetime": "Dec 19, 2017", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "sarxos", "datetime": "Jan 30, 2018", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/58", "issue_status": " Closed\n", "issue_list": [{"user_name": "krauzee", "datetime": "Nov 2, 2020", "body": "When I click on mic button my app is crashing with error:\nInflateException: Binary XML file line : Binary XML file line : You must supply a layout_width attribute.\nCaused by: java.lang.UnsupportedOperationException: Bin\nary XML file line : You must supply a layout_width attribute.", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Nov 2, 2020", "body": "Full stacktrace:android.view.InflateException: Binary XML file line : Binary XML file line : You must supply a layout_width attribute.\nCaused by: java.lang.UnsupportedOperationException: Binary XML file line : You must supply a layout_width attribute.\nat android.content.res.TypedArray.getLayoutDimension(TypedArray.java:767)\nat android.view.ViewGroup$LayoutParams.setBaseAttributes(ViewGroup.java:7036)\nat android.view.ViewGroup$MarginLayoutParams.(ViewGroup.java:7218)\nat androidx.recyclerview.widget.RecyclerView$LayoutParams.(RecyclerView.java:11905)\nat androidx.recyclerview.widget.RecyclerView$LayoutManager.generateLayoutParams(RecyclerView.java:8591)\nat androidx.recyclerview.widget.RecyclerView.generateLayoutParams(RecyclerView.java:4611)\nat android.view.LayoutInflater.inflate(LayoutInflater.java:509)\nat android.view.LayoutInflater.inflate(LayoutInflater.java:430)\nat com.justai.aimybox.components.extensions.ViewKt.inflate(View.kt:22)\nat com.justai.aimybox.components.extensions.ViewKt.inflate$default(View.kt:21)\nat com.justai.aimybox.components.adapter.ResponseDelegate.createViewHolder(ResponseDelegate.kt:15)\nat com.justai.aimybox.components.adapter.ResponseDelegate.createViewHolder(ResponseDelegate.kt:11)\nat com.justai.aimybox.components.base.DelegatedAdapter.onCreateViewHolder(DelegatedAdapter.kt:37)\nat com.justai.aimybox.components.base.DelegatedAdapter.onCreateViewHolder(DelegatedAdapter.kt:8)\nat androidx.recyclerview.widget.RecyclerView$Adapter.createViewHolder(RecyclerView.java:7216)\nat androidx.recyclerview.widget.RecyclerView$Recycler.tryGetViewHolderForPositionByDeadline(RecyclerView.java:6347)\nat androidx.recyclerview.widget.RecyclerView$Recycler.getViewForPosition(RecyclerView.java:6231)\nat androidx.recyclerview.widget.RecyclerView$Recycler.getViewForPosition(RecyclerView.java:6227)\nat androidx.recyclerview.widget.LinearLayoutManager$LayoutState.next(LinearLayoutManager.java:2330)\nat androidx.recyclerview.widget.LinearLayoutManager.layoutChunk(LinearLayoutManager.java:1631)\nat androidx.recyclerview.widget.LinearLayoutManager.fill(LinearLayoutManager.java:1591)\nat androidx.recyclerview.widget.LinearLayoutManager.onLayoutChildren(LinearLayoutManager.java:668)\nat androidx.recyclerview.widget.RecyclerView.dispatchLayoutStep2(RecyclerView.java:4230)\nat androidx.recyclerview.widget.RecyclerView.dispatchLayout(RecyclerView.java:3941)\nat androidx.recyclerview.widget.RecyclerView.onLayout(RecyclerView.java:4499)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat com.justai.aimybox.components.view.AimyboxButton.onLayout(AimyboxButton.kt:367)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.LinearLayout.setChildFrame(LinearLayout.java:1741)\n2020-11-02 13:30:09.558 1383-1383/ru.akbarslife.akbars E/AndroidRuntime:     at android.widget.LinearLayout.layoutVertical(LinearLayout.java:1585)\nat android.widget.LinearLayout.onLayout(LinearLayout.java:1494)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.LinearLayout.setChildFrame(LinearLayout.java:1741)\nat android.widget.LinearLayout.layoutVertical(LinearLayout.java:1585)\nat android.widget.LinearLayout.onLayout(LinearLayout.java:1494)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.widget.FrameLayout.layoutChildren(FrameLayout.java:323)\nat android.widget.FrameLayout.onLayout(FrameLayout.java:261)\nat com.android.internal.policy.DecorView.onLayout(DecorView.java:730)\nat android.view.View.layout(View.java:17666)\nat android.view.ViewGroup.layout(ViewGroup.java:5577)\nat android.view.ViewRootImpl.performLayout(ViewRootImpl.java:2390)\nat android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2112)\nat android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1298)\nat android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6437)\nat android.view.Choreographer$CallbackRecord.run(Choreographer.java:876)\nat android.view.Choreographer.doCallbacks(Choreographer.java:688)\nat android.view.Choreographer.doFrame(Choreographer.java:623)\nat android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:862)\nat android.os.Handler.handleCallback(Handler.java:754)\nat android.os.Handler.dispatchMessage(Handler.java:95)\nat android.os.Looper.loop(Looper.java:163)\nat android.app.ActivityThread.main(ActivityThread.java:6238)\nat java.lang.reflect.Method.invoke(Native Method)\nat com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:933)\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:823)my build.gradle:android {\ncompileSdkVersion 29\nbuildToolsVersion \"29.0.3\"}dependencies {//Aimylogic integration\nimplementation(\"com.justai.aimybox:google-platform-speechkit:0.14.0\")\nimplementation(\"com.justai.aimybox:core:0.14.0\")\nimplementation(\"com.justai.aimybox:components:0.1.9\")\n}In the app and activity class, I did everything exactly according to the instructions.", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Nov 3, 2020", "body": "Please provide your Android version", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Nov 3, 2020", "body": "MIUI 11.0.2 Android 7.1.2", "type": "commented", "related_issue": null}, {"user_name": "Neerav006", "datetime": "Nov 4, 2020", "body": "Just put these styles in theme.xml file it should work", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Nov 19, 2020", "body": "I added this to my styles.xml\nbut I still get the error. What am I doing wrong?\n", "type": "commented", "related_issue": null}, {"user_name": "Billthebest1", "datetime": "Nov 19, 2020", "body": "youre in better shape than me im compleatly new to this have android studio and dont have a clue on how to get a assisstant app to work lol ive tryied them all  this one seems the closest to the one i had but google got rid of  it even works with dialogue flo  but i cant get it to work im missing something in the instructions on how to do it and every time i try i end up downloading more and more software it went from android studio to github desktop to some thing that starts with z i cant think of the name atm  i just want a assisstant app like the one i paid for the one by speaktoit that one was perfect it was eficiant ran everything so useful af plus customizable and funny and its like im going to have to take 8 years of college in order to get it back", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Nov 22, 2020", "body": " are you sure you've applied this style in your AndroidManifest?", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 1, 2020", "body": "I solved this problem, thanks", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 2, 2020", "body": " did my suggest help you or you've found another solution?", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 2, 2020", "body": "your suggestion helped me, thanks!", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 1, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/53", "issue_status": " Closed\n", "issue_list": [{"user_name": "mellahysf", "datetime": "Aug 26, 2020", "body": "Hi,When i run the app demo in my device, i send my first speech (\"what time is it\"), but no reply !!\nit gives me the error below (in andoird studio) :com.justai.aimybox.core.ApiRequestTimeoutException: Request timeout: AimyboxRequest(query=what time is it, apiKey=Ldf0j7WZi3KwNah2aNeXVIACz0lb9qMH, unitId=8d456677-94d9-4a9a-afd4-fecb599b4545, data={}). Server didn't respond within 10000 ms.\nat com.justai.aimybox.api.DialogApi$send$3.invokeSuspend(DialogApi.kt:69)\nat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\nat kotlinx.coroutines.ResumeModeKt.resumeUninterceptedWithExceptionMode(ResumeMode.kt:56)\nat kotlinx.coroutines.TimeoutCoroutine.afterCompletionInternal(Timeout.kt:98)\nat kotlinx.coroutines.JobSupport.completeStateFinalization(JobSupport.kt:310)\nat kotlinx.coroutines.JobSupport.tryFinalizeFinishingState(JobSupport.kt:236)\nat kotlinx.coroutines.JobSupport.tryMakeCompletingSlowPath(JobSupport.kt:849)\nat kotlinx.coroutines.JobSupport.tryMakeCompleting(JobSupport.kt:811)\nat kotlinx.coroutines.JobSupport.makeCompletingOnce$kotlinx_coroutines_core(JobSupport.kt:787)\nat kotlinx.coroutines.AbstractCoroutine.resumeWith(AbstractCoroutine.kt:111)\nat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:46)\nat kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:334)\nat kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594)\nat kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60)\nat kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740)\nD/OkHttp: <-- HTTP FAILED: java.net.SocketTimeoutException: failed to connect to api.aimybox.com/63.34.12.130 (port 443) from /10.0.8.1 (port 40158) after 10000ms", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Aug 26, 2020", "body": "There is a timeout of the server side. Please ensure your project is alive once you've done the training (in the case you're using aimybox console)", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 27, 2020", "body": "I try the project in action in aimybox console () and it works.\nit works also in postman (by accessing  with a POST request).\nBut when I try to test it in my device (with android studio), it still gives me the same error above !! :(", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 27, 2020", "body": "I found the problem.\nit was some configurations to do in the device to allow the app to use network and mic.\nNow It works both in my device and in the emulator :D\nthank you.", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/52", "issue_status": " Closed\n", "issue_list": [{"user_name": "mellahysf", "datetime": "Aug 24, 2020", "body": "Hi,\nI want to run the demo, but both in a device and emulator (in android studio),  when I click on the mic button, it doesn't work !!\nWhen I click on the mic button, it cancels quickly and I can't send the speech.Thank you for your help !Below the trace given to me in the android studio :I/Aimybox(Aimybox-Components): [main] STANDBY\nI/Aimybox(STT): [DefaultDispatcher-worker-4] Begin recognition\nI/Aimybox(Aimybox-Components): [main] LISTENING\nE/Aimybox(STT): [DefaultDispatcher-worker-3] Failed to get recognition result\ncom.justai.aimybox.speechkit.google.platform.GooglePlatformSpeechToTextException: Exception [3]: Audio recording error.\nat com.justai.aimybox.speechkit.google.platform.GooglePlatformSpeechToText$createRecognitionListener$1.onError(GooglePlatformSpeechToText.kt:77)\nat android.speech.SpeechRecognizer$InternalListener$1.handleMessage(SpeechRecognizer.java:450)\nat android.os.Handler.dispatchMessage(Handler.java:107)\nat android.os.Looper.loop(Looper.java:214)\nat android.app.ActivityThread.main(ActivityThread.java:7356)\nat java.lang.reflect.Method.invoke(Native Method)\nat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\nE/Aimybox(Aimybox-Components): [main]\ncom.justai.aimybox.speechkit.google.platform.GooglePlatformSpeechToTextException: Exception [3]: Audio recording error.\nat com.justai.aimybox.speechkit.google.platform.GooglePlatformSpeechToText$createRecognitionListener$1.onError(GooglePlatformSpeechToText.kt:77)\nat android.speech.SpeechRecognizer$InternalListener$1.handleMessage(SpeechRecognizer.java:450)\nat android.os.Handler.dispatchMessage(Handler.java:107)\nat android.os.Looper.loop(Looper.java:214)\nat android.app.ActivityThread.main(ActivityThread.java:7356)\nat java.lang.reflect.Method.invoke(Native Method)\nat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\nI/Aimybox(Aimybox-Components): [main] STANDBY", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 26, 2020", "body": "now It works fine in my device but doesn't in emulator android studio", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 27, 2020", "body": "And now it works also in the emulator.\nSome configurations were required at the emulator to allow access to the network and microphone.", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Aug 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/42", "issue_status": " Closed\n", "issue_list": [{"user_name": "morfeusys", "datetime": "Nov 12, 2019", "body": "Aimybox  could be invoked from everywhere of the application. Not only by clicking on the mic button, but also through the voice trigger event or any other trigger (camera, sensors, etc). Thus  should appear once a recognition event is fired.", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 28, 2019", "body": [], "type": "issue", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 28, 2019", "body": [], "type": "", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Jan 20, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/14", "issue_status": " Closed\n", "issue_list": [{"user_name": "morfeusys", "datetime": "Aug 15, 2019", "body": "Make it possible to configure any labels of Aimybox fragment through  for example as well as styles.", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Oct 2, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/48", "issue_status": " Closed\n", "issue_list": [{"user_name": "kvbulusu", "datetime": "Mar 11, 2020", "body": "I'm new to android and a newbie qn. Along with voice, how can I add a text so users can type too?  In your example/demo code what should I change?  I want to have both voice and text... I'm using rasa nlu stack....", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Mar 11, 2020", "body": "You have to implement text input UI on your side and add it to your application. Once the user inputs some text, it should be passed to send() method of Aimybox. All other work will be done under the hood.", "type": "commented", "related_issue": null}, {"user_name": "kvbulusu", "datetime": "Mar 11, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "kvbulusu", "datetime": "Mar 11, 2020", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Mar 11, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/9", "issue_status": " Closed\n", "issue_list": [{"user_name": "morfeusys", "datetime": "Aug 8, 2019", "body": "Now there are some required components with versions from . This requires one more module to start using Aimybox SDK. Is it possible to simplify build.gradle to get rid of additional module?", "type": "commented", "related_issue": null}, {"user_name": "lambdatamer", "datetime": "Aug 12, 2019", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "lambdatamer", "datetime": "Aug 15, 2019", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/64", "issue_status": " Closed\n", "issue_list": [{"user_name": "sriramsv", "datetime": "Oct 3, 2021", "body": "I'm currently working on modifying this demo app to make it a Tasker plugin. I can send the messages the app hears now to Tasker, but I don't see any way to get the message back to the app and update the message in the UI fragment.I'm not very familiar with kotlin, so any help is appreciated!", "type": "commented", "related_issue": null}, {"user_name": "sriramsv", "datetime": "Oct 4, 2021", "body": "I want to achieve this flow where in the response coming from the assistant could either be initiated by an event or the request-response flow is not being called synchronously by the DialogAPI.\n  ", "type": "commented", "related_issue": null}, {"user_name": "sriramsv", "datetime": "Oct 7, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/70", "issue_status": " Open\n", "issue_list": [{"user_name": "Di-Zayn", "datetime": "May 15, 2022", "body": "Hello, I'm running the code in master branch but it failed. Here is the error in logs:\n\nIs it because of the region that I can't use Google's services?(I'm in China)", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/65", "issue_status": " Open\n", "issue_list": [{"user_name": "chinaandylee", "datetime": "Oct 29, 2021", "body": "when i run the this app on mobile phone, the memory continues to grow and is not released. I don't know why. Here ask for help.\n", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/63", "issue_status": " Open\n", "issue_list": [{"user_name": "wahyouwebid", "datetime": "Apr 16, 2021", "body": "I have used Kaldi to trigger the speech but it doesn't work, I still have to click the button first\n\nplease for the solution, thank you", "type": "commented", "related_issue": null}, {"user_name": "rubycho", "datetime": "Apr 16, 2021", "body": "Though this comment is not the answer for your question, I suggest you to hide your API KEY on your screenshot.", "type": "commented", "related_issue": null}, {"user_name": "wahyouwebid", "datetime": "Apr 16, 2021", "body": "on the github there is also API KEY, this is proof\n", "type": "commented", "related_issue": null}, {"user_name": "rubycho", "datetime": "Apr 16, 2021", "body": "Oh it was the key from the repo. I thought it was your own key. Sorry.", "type": "commented", "related_issue": null}, {"user_name": "cdhiraj40", "datetime": "Mar 18, 2022", "body": "hey,  did you find a solution for this?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/61", "issue_status": " Open\n", "issue_list": [{"user_name": "Tedcas", "datetime": "Feb 11, 2021", "body": "Hi,I'm working in a new project where I need a custom voice assistant in my android app. I tried this example  and everything works perfectly, however, if I go from MainActivity to another activity and I try to load the assistant on this new activity, tts, stt and wake word doesn't work, I press the floating button and the assitant does its animation, but thats all, my question is, How can I implement something like the example project but extending the assistant in all the activities?Thank you very much in advance.", "type": "commented", "related_issue": null}, {"user_name": "bgubanov", "datetime": "Feb 12, 2021", "body": "Hello!\nAre you initializing the Aimybox object from the Application instance, like in the example?\nCan you send code snippet with the second activity?\nI added second activity to example project, copied code of first activity and everything started correctly on each of activities.", "type": "commented", "related_issue": null}, {"user_name": "Billthebest1", "datetime": "Feb 12, 2021", "body": "", "type": "", "related_issue": null}, {"user_name": "Tedcas", "datetime": "Feb 14, 2021", "body": "Hi, first of all many thanks for your reply ¡, I'm sorry I couldn't reply you earlier, answering your question, yes I'm initializing the Aimybox object from Application instance like the example, here is my code for that I've modified it a little bit to adjust to my project:class AimyboxApplication : Application(), AimyboxProvider {And this is the class I use to create the Assistant in every class of my project:class VoiceAssistant constructor(context: Context, container: Int) : Serializable, ActivitiesFather() {So in Activity 1 I call the following methods on onCreate():And then in the next activity I do the same (I simplified this step, calling those two statements in the father of all the Activities of my project)I'm not sure what I'm doing wrong, I must admit I'm not used to Kotlin (nowadays I'm learning the language) and I could make a misstake there.Again many thanks for your help in advance.", "type": "commented", "related_issue": null}, {"user_name": "Tedcas", "datetime": "Apr 23, 2021", "body": "Hi, it's been a while since my last posr, after doing some tests I've realized that the problem only happens when I use KaldiVoiceTrigger with the TTS, STT and DialogApi together, if I disable the KaldiVoiceTrigger system, all works perfectly. I've try updating all the dependencies to the last version, but nothing change.", "type": "commented", "related_issue": null}, {"user_name": "cdhiraj40", "datetime": "Jul 5, 2022", "body": "does KaldiVoiceTrigger work perfectly for you folks?  \nFor me, it just triggers every time I say anything! Let me know thanks.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/59", "issue_status": " Open\n", "issue_list": [{"user_name": "krauzee", "datetime": "Dec 1, 2020", "body": "I wanna custom ui. I wanna see messages from user on right and I ovveride \"gravity\" attribute, but its not works:(\nAny help is appreciated.\n1", "type": "commented", "related_issue": null}, {"user_name": "bgubanov", "datetime": "Dec 2, 2020", "body": "Hello, please try using android:layout_gravity instead of android:gravity", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 2, 2020", "body": "Hello! I tried using \"android:layout_gravity\" but it also doesn't work:(", "type": "commented", "related_issue": null}, {"user_name": "bgubanov", "datetime": "Dec 2, 2020", "body": "Have you overrided android:gravity attribute of parent layout? If yes, try to delete it. I tried your case with your params on default layout and it worked", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 2, 2020", "body": "do you mean by parent_layout is \"aimybox_container\"? Can you show me your code please?", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 3, 2020", "body": "Please, try to set attr \"layout_width\" to wrap_content, after this attr gravity and layout_gravity was no work", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 3, 2020", "body": " we have published a new version of components. Please try to use  and then update your styles like:", "type": "commented", "related_issue": null}, {"user_name": "krauzee", "datetime": "Dec 3, 2020", "body": "Gradle can't download components-0.1.10\nEndless loading occurs:(so already 20 minutes\n", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 3, 2020", "body": "You can check that it's available via jcenter normally.", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Dec 3, 2020", "body": [], "type": "pull", "related_issue": "#60"}, {"user_name": "just-ai", "datetime": "Dec 4, 2020", "body": [], "type": "deleted a comment from", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/55", "issue_status": " Open\n", "issue_list": [{"user_name": "mellahysf", "datetime": "Sep 4, 2020", "body": "Hi,I made a chatbot with english language and i integrated it with aimybox and its works.\nNow i want to use the same chatbot but with french language (maybe with using translator API !! but how to do that ?)Thanks for help !", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Sep 9, 2020", "body": "Aimybox console supports only English and Russian languages for now. You could create another chatbot using any other supported tools like Dialogflow or Rasa and connect your Aimybox based application directly to it using an appropriate library from Aimybox repostory.", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Sep 9, 2020", "body": "Thanks  for your reply.\nYes im created 2 chatbots in Rasa; One in english and another in french language. But how to distinguish between them in the aimAbox based application? In term of speech to text and text to speech methods, we specify only one local language !!", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Sep 9, 2020", "body": " you can create two separate Rasa webhooks and determine language on the application starting and initialise Aimybox with an appropriate webhook URL.", "type": "commented", "related_issue": null}, {"user_name": "mellahysf", "datetime": "Sep 22, 2020", "body": " can you tell me please how can I add 2 buttons in running Aimybox, and according to the clicked button call an appropriate URL (treatment should be in the method createAimybox of class AimyboxApplication of file AimyboxApplication.kt) ?", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/54", "issue_status": " Open\n", "issue_list": [{"user_name": "mellahysf", "datetime": "Sep 4, 2020", "body": "Hi,I'm looking for Arabic speech recognition.\nGooglePlatformTextToSpeech(context, Locale.ENGLISH) does not support arabic text to speech.Thanks", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/47", "issue_status": " Open\n", "issue_list": [{"user_name": "lewisccx", "datetime": "Jan 31, 2020", "body": "Hi When I click the button link from dialogflow API, it auto dismiss itself instead of opening the link in browser, how do I solve that?", "type": "commented", "related_issue": null}, {"user_name": "Neerav006", "datetime": "Nov 4, 2020", "body": "Same issue here", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Jan 31, 2020", "body": [], "type": "transferred this issue from just-ai/aimybox-android-sdk", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Jan 31, 2020", "body": [], "type": "changed the title", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/46", "issue_status": " Open\n", "issue_list": [{"user_name": "babat00nday", "datetime": "Dec 28, 2019", "body": "The demo app works fine when I push to my android device. However, when I try to test on my Android Studio device, the speech recognition functionality does not work. It cancels as soon as I click on the microphone button.", "type": "commented", "related_issue": null}, {"user_name": "achernin", "datetime": "Jul 14, 2020", "body": [], "type": "pull", "related_issue": "#49"}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/37", "issue_status": " Open\n", "issue_list": [{"user_name": "lambdatamer", "datetime": "Oct 14, 2019", "body": "Some modules (like Snowboy VT & Google Cloud) requires access to external storage. We need to find a way to check/request the mic and storage permissions before VT start.", "type": "commented", "related_issue": null}]},
{"issue_url": "https://github.com/just-ai/aimybox-android-assistant/issues/36", "issue_status": " Open\n", "issue_list": [{"user_name": "morfeusys", "datetime": "Oct 2, 2019", "body": "Need to have a way to add custom widgets that could be rendered by the Aimybox's fragment.\nWould be great to have a single place where the user can add their map of  ->  (or something like that) during the  initialisation.", "type": "commented", "related_issue": null}, {"user_name": "chinaandylee", "datetime": "Oct 29, 2021", "body": "The same needs are in front of me. Is there any update?", "type": "commented", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Oct 2, 2019", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Oct 2, 2019", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "morfeusys", "datetime": "Jan 21, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "just-ai", "datetime": "Sep 4, 2020", "body": [], "type": "deleted a comment from", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5700", "issue_status": " Closed\n", "issue_list": [{"user_name": "tianliuxin", "datetime": "Aug 11, 2022", "body": "allennlp.common.params.Params only implentment , I think implenmenting  will be better.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 25, 2022", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "tianliuxin", "datetime": "Aug 11, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 25, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 25, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5637", "issue_status": " Closed\n", "issue_list": [{"user_name": "eraldoluis", "datetime": "May 21, 2022", "body": "\n metric () returns a dictionary with three keys: ,  and . Under each key, a list comprises the corresponding values for each class/label. This is problematic for some logging plugins (TensorBoard and Weight&Bias, for instance) because these plugins assume that each metric key comprises one unique value. In fact, W&B can work with lists, but it is usually less convenient (it is harder to choose a specific metric to plot, for instance).Another problem is that you need to choose between having the individual values for each class or the average, but not both. If you choose to have the average, the per-class values are not returned.\nI have implemented a class called  that solve this by returning a dictionary with keys:where  is the index (or the label) of the class and  is the (optional) requested average (micro, macro or weighted). You can even request more than one average.This implementation just overrides the (...) and get_metric(...) methods. The (...) method is the same because it provides all the necessary counts. It is just the output of get_metric(...) that is not convenient in some cases.\nNone.\nThis problem occurred to me when I was implementing a solution for issue ", "type": "commented", "related_issue": null}, {"user_name": "eraldoluis", "datetime": "May 21, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "eraldoluis", "datetime": "May 21, 2022", "body": [], "type": "pull", "related_issue": "#5638"}, {"user_name": "dirkgr", "datetime": "Jun 2, 2022", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5620", "issue_status": " Closed\n", "issue_list": [{"user_name": "jacoby149", "datetime": "Apr 13, 2022", "body": "\nI would like to fine tune the following model : \nI would like to fine tune it on squad data.\nHowever there is no documentation that explains how to fine tune allen ai models from huggingface.\nI think there is an opportunity here to make allenNLP much more easy to use and convenient.\nI would like a documentation page that has clear instructions on how to fine tune any of the allenAI hugging face models.\nI tried using the allenAI command line, but I struggle without a documentation example.", "type": "commented", "related_issue": null}, {"user_name": "AkshitaB", "datetime": "Apr 18, 2022", "body": " Thank you for the request. Here's our config for training the transformer qa model: This will finetune the roberta-large model for the task. If you wish to start with the trained transformer_qa model itself, you can modify the config as follows:We are no longer focused on adding new features to the allennlp library/docs, but will be happy to review contributions and/or provide further assistance in debugging.Also checkout the allennlp guide here: ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Apr 27, 2022", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "jacoby149", "datetime": "Apr 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "github-actions", "datetime": "Apr 27, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Apr 27, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5597", "issue_status": " Closed\n", "issue_list": [{"user_name": "tarohi24", "datetime": "Mar 13, 2022", "body": "Hello, is called inside  () . If I understand correctly, the method does some sanity checks by comparing a pair of token ids that should be different from each other.The current implementation compares  or , but both of the pairs don't work for some Japanese pre-trained tokenizers. For example, the vocabulary of the  contains none of  because it only accepts full-width characters.So I'd like to pass a custom pair to  by adding a parameter to . I'm ready to make a PR, but I opened this issue to make sure that it's a good approach.Thanks,", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Mar 22, 2022", "body": "Hi , yes please go ahead with that PR and feel free to tag me for a review when you open it ", "type": "commented", "related_issue": null}, {"user_name": "tarohi24", "datetime": "Mar 26, 2022", "body": "Sure I'll do that soon!", "type": "commented", "related_issue": null}, {"user_name": "tarohi24", "datetime": "Mar 31, 2022", "body": "HI  , I made a PR for this. It looks like I cannot assign a reviewer for some reasons. Could you check that? Thank you!", "type": "commented", "related_issue": null}, {"user_name": "tarohi24", "datetime": "Mar 13, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Mar 22, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "tarohi24", "datetime": "Mar 26, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "tarohi24", "datetime": "Mar 26, 2022", "body": [], "type": "pull", "related_issue": "#5608"}, {"user_name": "dirkgr", "datetime": "Apr 8, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5614", "issue_status": " Closed\n", "issue_list": [{"user_name": "vikigenius", "datetime": "Apr 5, 2022", "body": "Currently looking at the discussion here  and the code It seems like you have to manually wrap each individual unit of partition.Looking at the tutorial for fairscale: \nThere is an  function that automatically wraps each submodule for you. This is incredibly convenient if you would just like to wrap a huge pretrained transformer embedder yourself.Is there a possibility of providing an option to auto_wrap modules?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Apr 6, 2022", "body": "We could definitely add support for that. I'm happy to review a PR.In the meantime you could just  the whole module. There might not be much of a performance difference between -ing it and -ing it.", "type": "commented", "related_issue": null}, {"user_name": "vikigenius", "datetime": "Apr 6, 2022", "body": "Sure, I will work on a PR.I am curious behind your statement that there is not much of a difference between wrap and auto-wrap. Can you elaborate?From my understanding when you wrap a whole module, it will overlap the all gather step only in the final step and all the parameters needed by the whole module should be present in each GPU.However, if you use auto_wrap you will be wrapping each layer/submodule and only the parameters for a particular layer need to be in the GPU at any given time. This seems like it will be slower but a lot more memory efficient ?Am I missing something here, or is my understanding wrong?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Apr 6, 2022", "body": "I think you're correct. But the FairScale docs actually say that this will \"improve training speed by overlapping the all-gather step across the forward pass.\" I'm not entirely sure what that means / how sharding individual layers would speed things up. But it does make sense that it would save a lot of memory.So, ignore my previous comment.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Apr 25, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 9, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "May 24, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 8, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 22, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 6, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 21, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 1, 2022", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "vikigenius", "datetime": "Apr 5, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Apr 8, 2022", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jul 21, 2022", "body": [], "type": "removed their assignment", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 1, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 1, 2022", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5576", "issue_status": " Closed\n", "issue_list": [{"user_name": "zhaowei-wang98", "datetime": "Feb 23, 2022", "body": "When I try to load a model with the above code, a deluge of log info takes my terminal and disrupts my personal log info.\nCould it be silent?\nNo log info about loading a model appears on my terminal.\nkeep it silent or output only one or two lines to show that the loading process is finished.\nno.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Feb 24, 2022", "body": "All of the messages are using the Python logging facilities. You can limit it to only show warning messages from AllenNLP by doing this:If you want to only see warnings from all packages (not just AllenNLP), you can even do this:", "type": "commented", "related_issue": null}, {"user_name": "zhaowei-wang98", "datetime": "Feb 23, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Feb 24, 2022", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Feb 25, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5521", "issue_status": " Closed\n", "issue_list": [{"user_name": "himkt", "datetime": "Dec 21, 2021", "body": " raises an error that fails to load .\nAfter executing , the problem does not happen anymore.OS:Python version: 3.9.9", "type": "commented", "related_issue": null}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": "I found the error doesn't occurs when running  and .\nSorry for raising the fixed problem. It's enough to wait the next release of AllenNLP.", "type": "commented", "related_issue": null}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": "After removing , the error occurs again and I re-opened the issue.\nPlease close if you think here is not good place to discuss.", "type": "commented", "related_issue": null}, {"user_name": "idiomaticrefactoring", "datetime": "Dec 22, 2021", "body": "I also find the problem when I run test suite.\npython3.7 -m pytest -v tests/core/policies/test_unexpected_intent_policy.py", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 23, 2021", "body": "See also .  should fix our CI and Docker image issues. But I'm not sure what else we can do about this other than downgrading the necessary dependencies.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jan 4, 2022", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": [], "type": "pull", "related_issue": "optuna/optuna#3200"}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "himkt", "datetime": "Dec 21, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "HideakiImamura", "datetime": "Dec 22, 2021", "body": [], "type": "pull", "related_issue": "optuna/optuna-examples#75"}, {"user_name": "epwalsh", "datetime": "Dec 23, 2021", "body": [], "type": "pull", "related_issue": "#5529"}, {"user_name": "epwalsh", "datetime": "Dec 23, 2021", "body": [], "type": "pull", "related_issue": "#5529"}, {"user_name": "github-actions", "datetime": "Jan 4, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jan 4, 2022", "body": [], "type": "", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jan 4, 2022", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jan 4, 2022", "body": [], "type": "added", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jan 12, 2022", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jan 12, 2022", "body": [], "type": "pull", "related_issue": "#5540"}, {"user_name": "dirkgr", "datetime": "Jan 13, 2022", "body": [], "type": "pull", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Feb 5, 2022", "body": [], "type": "issue", "related_issue": "#5559"}, {"user_name": "toshihikoyanase", "datetime": "Apr 18, 2022", "body": [], "type": "pull", "related_issue": "optuna/optuna#3353"}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5478", "issue_status": " Closed\n", "issue_list": [{"user_name": "HarshTrivedi", "datetime": "Nov 20, 2021", "body": "I’m using T5 model implemented in allennlp, and need to add extra special tokens to its vocabulary.I’ve added extra tokens in the tokenizer in my reader with . But I also need to extend model’s token embeddings. Usually, when the transformer is loaded with , it’s taken care of by itself because of . I can also do it by invoking HF’s  manually, but, T5 object here doesn’t have this method on it. The T5 object  is allennlp's module, so doesn't have HF methods like .My current solution is to manually extend T5 embeddings and lm_head, but it'd be good to have native support for this in allennlp. Assigning you based on the discussion on slack.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 6, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 20, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jan 3, 2022", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jan 6, 2022", "body": "Fixed in .", "type": "commented", "related_issue": null}, {"user_name": "HarshTrivedi", "datetime": "Nov 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "HarshTrivedi", "datetime": "Nov 20, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jan 6, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5491", "issue_status": " Closed\n", "issue_list": [{"user_name": "JohnGiorgi", "datetime": "Nov 30, 2021", "body": "The paper  (published at ICLR 2021) demonstrated that re-initializing the last few layers of a pretrained transformer before fine-tuning can reduce the variance between re-runs, speed up convergence and improve final task performance, nicely summarized in their figures:\nThe intuition is that some of the final layers may be over-specified to the pretraining objective(s) and therefore the pretrained weights can provide a bad initialization for downstream tasks.Ideally, you could easily specify which layers to re-initialize in a , something like:The  of  would take care of correctly re-initializing the specified layers for the given .You could achieve this right now with the , but this would require:I've drafted a solution that works (but requires more testing). Essentially, we add a new parameter to , , which can be an integer or list of integers. In , we re-initialize as follows:I sanity-checked it by testing that the weights of the specified layers are indeed re-initialized. I also trained a model with re-initialized layers on my own task and got a non-negligible performance boost.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 3, 2021", "body": "Hey , I do think this would be a good addition. Feel free to ping me when you start the PR!", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 15, 2021", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Dec 15, 2021", "body": "Oops, still working on  so I think it makes sense to keep this open!", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 15, 2021", "body": "Unfortunately there's no easy way to check if an issue has an open linked pull request from the GitHub API, which should be a sufficient condition to keep the issue open ", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Nov 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Dec 10, 2021", "body": [], "type": "pull", "related_issue": "#5505"}, {"user_name": "github-actions", "datetime": "Dec 15, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Dec 15, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 15, 2021", "body": [], "type": "reopened this", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 15, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 23, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5558", "issue_status": " Closed\n", "issue_list": [{"user_name": "h-vetinari", "datetime": "Feb 5, 2022", "body": "In ,  :Since that issue is closed, it's exceedingly easy to overlook this, and so I thought I'd open a new issue.The migration will be kicked off as soon as  is merged, and will then be observable under . It might be a while until all dependencies get published, but I'll try to keep an eye on the PRs that the migrator opens (you can too! yes, you! you can ping me if you find something has stalled, which is always possible in a volunteer-only organisation).", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Feb 7, 2022", "body": "With the merging of , allennlp plus all extras mentioned in the readme are now available for osx-arm in conda-forge - closing.", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Feb 5, 2022", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Feb 5, 2022", "body": [], "type": "pull", "related_issue": "conda-forge/conda-forge-pinning-feedstock#2485"}, {"user_name": "h-vetinari", "datetime": "Feb 7, 2022", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5355", "issue_status": " Closed\n", "issue_list": [{"user_name": "amitkparekh", "datetime": "Aug 11, 2021", "body": "\nI like removing boilerplate from code and using the training configs as much as possible.The vocab created from instances with  and  do not update the vocab with the model's entire vocab. I understand that this might not be desired behaviour, which is why  exists.But this doesn't help if I want to create a vocab from both instances and the pretrained transformer. Building a multitask model, I've found that I would like this feature.\nAn additional constructor for creating the vocab from both instances and the pretrained transformer. The construction should support multiple model names for various namespaces, which are likely used in multitask models.In my training config, I would like it to just be this (where the keys are the namespace, and values are the model name).Users could  extend the vocabulary during the  of their classes, but this would result in repeated code for each namespace they want to extend.For example, extending a single namespace with one model would result in:but if there were multiple models, you'd probably end up seeing:I tried implementing my solution  and it works as I described above with the additional tokens being added to the vocab from the transformer.I realised I came across this feature because I was trying to do things before the  of module within my model. I now see that the vocab is indexed after the model is initialised, so this is not  necessary. But _ shows that there is some demand.But that brings up a different problem when I need the vocab size during model initialisation but it's not accurate until after the model is fully constructed.I thought about closing but I feel like this has some use if this functionality is desired? Feel free to just close this if it's of no use to anyone!", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2021", "body": "I think that would be a fine additional  constructor. Can you make it into a PR, with a small test? I'd be happy to review it.", "type": "commented", "related_issue": null}, {"user_name": "amitkparekh", "datetime": "Aug 20, 2021", "body": " — Just submitted a PR!", "type": "commented", "related_issue": null}, {"user_name": "amitkparekh", "datetime": "Aug 11, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "amitkparekh", "datetime": "Aug 11, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "amitkparekh", "datetime": "Aug 19, 2021", "body": [], "type": "pull", "related_issue": "#5368"}, {"user_name": "dirkgr", "datetime": "Aug 24, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5358", "issue_status": " Closed\n", "issue_list": [{"user_name": "seyeeet", "datetime": "Aug 15, 2021", "body": "Hello\nI found that examples are missing from the provided documents. would it be apoosible to provide a simple examples for the models so we can use them while using the modules?\nfor example, I am interested in learning NER with , but I am not sure how I can do it and I cannot find any exmples in your website that shows me the size/structure of inputs.\nThanks", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2021", "body": "It takes a little bit of work, but you can start from the demo, and look at the parts from there. We have a NER demo here: It includes instructions of how to run the model, and how to train it. You can also check out how the components work together. The training config for this model is here: You can see that it uses the  reader, which lives at  and is documented at .For the model, it uses , which lives at  and is documented at .Similarly, you can find the code and documentation for the other components.If that is too detailed, I recommend the AllenNLP guide at . It won't talk specifically about NER, but it will introduce many important AllenNLP concepts.", "type": "commented", "related_issue": null}, {"user_name": "seyeeet", "datetime": "Aug 15, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "seyeeet", "datetime": "Aug 15, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5258", "issue_status": " Closed\n", "issue_list": [{"user_name": "codeananda", "datetime": "Jun 13, 2021", "body": "There is a  install for Linux. Could you please also add one for Mac?", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 21, 2021", "body": ", do you know what it would take to get this done?", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Jun 21, 2021", "body": "Hey allThanks for the ping, I've had a long-standing , but it was then still missing osx-builds for pytorch/torchvision. Those have since arrived, but there are still some errors in the test suite on OSX. Any help fixing those is appreciated. :)", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 21, 2021", "body": "Errors in the AllenNLP test suite, or in the torchvision test suite?", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Jun 22, 2021", "body": "The AllenNLP test suite. For details, see the  from ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 23, 2021", "body": "It looks like the problem is thatI don't know why that is, but as long as that's the case, the AllenNLP tests won't run.It's surprising though, because I'm running it on Mac right now, wit PyTorch installed from Conda, and it all works fine, including the distributed setup. Why doesn't it use this version when it runs the tests?", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 8, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 22, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 2, 2021", "body": "Have not forgotten. Am waiting for the resolution on .", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Aug 2, 2021", "body": "Which Mac's are we talking about? The processor architecture changed with the M1, so building for pre-M1 is different. I'm currently looking to get the osx-x86_64 build running (equals pre-M1), but since conda-forge does not have M1 CI, we can only cross-compile, and I would need someone to test-run a candidate artefact before we can merge.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 3, 2021", "body": "I am currently not concerned about the M1 Macs. As far as I know, AllenNLP works fine on those with the official PyTorch conda package, so people running that can use that. Of course it would be better if the conda version of PyTorch also worked, but let's get that to work on x86 first before we worry about M1.", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Aug 4, 2021", "body": "osx-x86_64 builds are available now. :)", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 4, 2021", "body": "Does that mean we can close this issue?", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Aug 4, 2021", "body": "We can keep it for the M1 packages if you want.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 4, 2021", "body": "Too many open issues. We'll wait until someone requests them. By that point, the fine folks at Apple and Facebook might have solved some of the problems we'd otherwise have already.", "type": "commented", "related_issue": null}, {"user_name": "Vladimir-Chan", "datetime": "Nov 27, 2021", "body": " Hi, there is a  install of version 2.6.0. Could you please upgrade it to the stable version 2.8.0? If it is difficult to build a install for mac, could you please build it for linux?", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Nov 30, 2021", "body": "There's an effort  to do that, but due to the new dependencies added in 2.7 & 2.8, this is taking longer than expected. Also, if you have an M1, conda-forge is not building allennlp for osx-arm yet, but that could be done in principle.PS. Posting on long-closed issues makes it very likely that your comment gets overlooked.", "type": "commented", "related_issue": null}, {"user_name": "h-vetinari", "datetime": "Dec 1, 2021", "body": "Allennlp 2.8 is now in conda-forge.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 1, 2021", "body": "Sweet, thank you!", "type": "commented", "related_issue": null}, {"user_name": "RoyLiberman", "datetime": "Jan 17, 2022", "body": "can you please add support for mac M1 systems on conda-forge?", "type": "commented", "related_issue": null}, {"user_name": "codeananda", "datetime": "Jun 13, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 18, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 4, 2021", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "conda-forge/conda-forge-pinning-feedstock#2485"}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5155", "issue_status": " Closed\n", "issue_list": [{"user_name": "nelson-liu", "datetime": "Apr 26, 2021", "body": "Right now, allennlp currently evaluates / saves checkpoints at the end of every epoch. However, it'd be nice if there was some way of evaluating within an epoch.  For example, say an epoch is 10,000 steps, and I want to evaluate / save a checkpoint every 500 steps.This is especially important when fine-tuning pre-trained language models. Often, the optimal checkpoint does not occur at the end of an epoch, and models can quickly overfit. For example, the huggingface default scripts save checkpoints every 500 steps. it'd be nice to have some sort of similar option for allennlp.\nperhaps the checkpointing side of this could be controlled by the checkpointer, similar to how  exists. Perhaps . However, maybe there needs to be analogous argument in the  for evaluation.", "type": "commented", "related_issue": null}, {"user_name": "nelson-liu", "datetime": "Apr 26, 2021", "body": "To be concrete: I'm trying to train BERT on the IMDb sentiment dataset (25K examples). It peaks after epoch 2 (and starts overfitting from epoch 3) but i'm sure that there are better checkpoints between the end of epoch 2 and the end of epoch 3. In general, these pre-trained language models aren't trained for very many epochs (e.g., 3-4), so only evaluating and saving 3 or 4 checkpoints seems like you'd leave a lot of performance on the table.", "type": "commented", "related_issue": null}, {"user_name": "ArjunSubramonian", "datetime": "Apr 27, 2021", "body": "This would indeed be a great feature to have. I've made it so that Contributions are Welcome. I also labeled it as Easy from a first glance (and the fact that you provide suggestions and a reference above), but let me know if you think this could be a more difficult change.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Apr 28, 2021", "body": "In practice I always set  to a suitable number for this exact reason. Is there a reason that approach is sub-optimal for you use-case, ?See also ", "type": "commented", "related_issue": null}, {"user_name": "nelson-liu", "datetime": "Apr 28, 2021", "body": "The reason is that i either did not know about it, and/or forgot that this was a thing :) This seems to fit the bill perfectly, thanks  .", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Apr 28, 2021", "body": "Great, no problem!", "type": "commented", "related_issue": null}, {"user_name": "nelson-liu", "datetime": "Apr 26, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ArjunSubramonian", "datetime": "Apr 27, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "nelson-liu", "datetime": "Apr 28, 2021", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4823", "issue_status": " Closed\n", "issue_list": [{"user_name": "tomsherborne", "datetime": "Nov 26, 2020", "body": "The  activation is currently not a possible option from the set of registered Activations. Since this class just directly called the PyTorch classes - adding this in is a 1 line addition. Motivation is that models like BART/BERT use this activation in many places and elegant consistency of activation function across models that are \"something pretrained\" + \"more weights trained on AllenNLP\" would be nice.\nAdd the following snippet to the end of the  class\nManually hardcoding the activation. This isn't very robust and modules such as FeedForward complain since Gelu isnt a registered activation to insert between layers (as far as I can tell).Thanks - happy to submit a tiny PR for this", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Nov 30, 2020", "body": "Hi , yes please go ahead a submit a PR for this. Thanks!", "type": "commented", "related_issue": null}, {"user_name": "tomsherborne", "datetime": "Nov 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": null, "datetime": [], "body": [], "type": "", "related_issue": "#4827"}, {"user_name": "epwalsh", "datetime": "Nov 30, 2020", "body": [], "type": "pull", "related_issue": "#4828"}, {"user_name": "epwalsh", "datetime": "Dec 2, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4856", "issue_status": " Closed\n", "issue_list": [{"user_name": "epwalsh", "datetime": "Dec 9, 2020", "body": "In particular, it would be great if you do something like this:", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 10, 2020", "body": "I think we'd use one LR scheduler that calls into others to make this as flexible as possible.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 9, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 9, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 18, 2020", "body": [], "type": "pull", "related_issue": "#4871"}, {"user_name": "epwalsh", "datetime": "Dec 19, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4877", "issue_status": " Closed\n", "issue_list": [{"user_name": "schmmd", "datetime": "Dec 21, 2020", "body": " is going to investigate whether we need this and get back to .", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 22, 2020", "body": "The model we want is ResNext-152 trained on Visual Genome. Neither torchvision nor detectron has this, so we'll hold off for now. We might still need detectron if it turns out to be the easiest way to get that model.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Dec 21, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "schmmd", "datetime": "Dec 21, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 22, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4541", "issue_status": " Closed\n", "issue_list": [{"user_name": "PrettyMeng", "datetime": "Aug 6, 2020", "body": "It seems that in current AllenNLP, the model we save and create cannot interact with apis in transformers. (because we have different keys in model.state_dict()) I think it will be highly helpful if we can convert a trained AllenNLP model to a transformers model. This will provide more flexibility in applications and researchs.", "type": "commented", "related_issue": null}, {"user_name": "wj-Mcat", "datetime": "Aug 6, 2020", "body": "Waiting for this feature.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Aug 6, 2020", "body": "In general, this isn't possible, because, e.g., they don't have coref or SRL models.  We do want to make it easy to do this when it is possible, though.  For instance, you could take the base transformer weights and save them.  This isn't very hard right now, just a few lines in a simple script, but we could document it / make it one line instead.  Can you give more detail on what exactly you want to do?  Which models, and which weights, are you hoping to have be compatible?For more info on a simple script to do what you want, see .", "type": "commented", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 6, 2020", "body": "Thanks for your quick reply! For example, if I want to run some example scripts from transformers on a trained AllenNLP Roberta model, it seems to be troublesome in my case. It would also be great if there is more example scripts in AllenNLP.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Aug 6, 2020", "body": "That's still not enough detail.  What exactly do you want to do?  Which scripts?  \"Roberta\" isn't a single model.", "type": "commented", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 6, 2020", "body": "For example, I have finetuned Roberta to train a text classifier. Based on this, I want to run language modeling on this trained model, using this . It would be great if we can directly apply an AllenNLP model to scripts like this.", "type": "commented", "related_issue": null}, {"user_name": "AkshitaB", "datetime": "Aug 7, 2020", "body": "Closing as a duplicate of ", "type": "commented", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 6, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "AkshitaB", "datetime": "Aug 7, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4528", "issue_status": " Closed\n", "issue_list": [{"user_name": "PrettyMeng", "datetime": "Aug 3, 2020", "body": "For the cluster that I'm using, it does not allow specifying cuda device in my code. I think it's pretty common in many different clusters. I have not found how to do it for current allennlp library. Does there exist one that I have not noticed or can you add a feature like this? For example, by setting some config parameters, we can use model.cuda() instead of model.to(device). Thanks in advance!", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 3, 2020", "body": "Hey @PrettyPrettyMeng, we changed the behavior of the  command recently to automatically detect and use a GPU when it can if you don't explicitly set the \"cuda_device\" parameter in the trainer. Have you tried that?", "type": "commented", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 4, 2020", "body": "That's great! Thanks. Maybe I'm using some older version and it automatically use CPU when I don't explicitly set the \"cuda_device\" parameter in the trainer.", "type": "commented", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 3, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "PrettyMeng", "datetime": "Aug 4, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5228", "issue_status": " Closed\n", "issue_list": [{"user_name": "offendo", "datetime": "May 27, 2021", "body": "Hello! Thanks for the fantastic work on this project.In the  class, the  flag for DDP is set to , which causes the following warning when running distributed code.(Issue  and PR  are related)The warning states it can adversely affect performance, but I'm not clear on how much exactly. Simply traversing the graph sounds like a minimal compute expenditure, yeah?If that's the case, maybe we can disable the warning? If it's not controllable by the end user anyway (since it's not a parameter to ), is it useful to print out?On the other hand, if it is significantly detrimental to performance, maybe it should be a parameter. Although, the constructor signature is already rather imposing...Thanks for your time!", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 27, 2021", "body": "I think I just want to turn that parameter off completely. It's not reliable with multitask models, it prints a warning, and it's weird that we get this check only in the distributed setting. If we want to provide this functionality, I'd rather do it in a . , , any concerns?", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 28, 2021", "body": " says he has this fixed in his work on T5.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 28, 2021", "body": "Update: There is no PR yet for Pete's work, but we'll link the issue when there is one.", "type": "commented", "related_issue": null}, {"user_name": "offendo", "datetime": "May 29, 2021", "body": "Thanks, appreciate it!", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 14, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jun 28, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 28, 2021", "body": "Have not forgotten.  is still working on his PR.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Jul 13, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "offendo", "datetime": "May 27, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 27, 2021", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jul 13, 2021", "body": [], "type": "pull", "related_issue": "#5242"}, {"user_name": "epwalsh", "datetime": "Jul 19, 2021", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4513", "issue_status": " Closed\n", "issue_list": [{"user_name": "Rajwrita", "datetime": "Jul 26, 2020", "body": "A sticky navbar would probably be helpful to access the various pages on the website with ease.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jul 27, 2020", "body": "Hi , I agree, I think that would be nice. I'm not sure if that's an easy change, maybe  would know?", "type": "commented", "related_issue": null}, {"user_name": "jonborchardt", "datetime": "Jul 27, 2020", "body": "the demo has a sticky top bar...\nwhat exactly are you wanting to see?\ncan you provide a screen shot of existing and let me know what you would like estimated?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jul 27, 2020", "body": " sorry, I think  was talking about  and I saw you had some commits recently, so I thought you might be the best person to ping", "type": "commented", "related_issue": null}, {"user_name": "jonborchardt", "datetime": "Jul 27, 2020", "body": "", "type": "", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jul 27, 2020", "body": "Oh, awesome. Thanks Jon!Closing this then.  let me know if you were talking about something other than allennlp.org and I can re-open.", "type": "commented", "related_issue": null}, {"user_name": "Rajwrita", "datetime": "Jul 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jul 27, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4486", "issue_status": " Closed\n", "issue_list": [{"user_name": "dirkgr", "datetime": "Jul 16, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Mar 3, 2021", "body": "This issue is being closed due to lack of activity. If you think it still needs to be addressed, please comment on this thread ", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Mar 3, 2021", "body": "We're still interested in this, and some of these are part of our , but we don't have active work going on with these datasets that isn't being tracked elsewhere already.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jul 16, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jul 16, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 9, 2020", "body": [], "type": "modified the milestones:", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Feb 22, 2021", "body": [], "type": "changed the title", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Feb 22, 2021", "body": [], "type": "removed this from the", "related_issue": null}, {"user_name": "github-actions", "datetime": "Mar 3, 2021", "body": [], "type": "", "related_issue": null}, {"user_name": "github-actions", "datetime": "Mar 3, 2021", "body": [], "type": "", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4422", "issue_status": " Closed\n", "issue_list": [{"user_name": "dirkgr", "datetime": "Jun 30, 2020", "body": "We’ll write a  that takes a  as one of its  arguments. This way we define the different datasets we want to read. Much like the , this reader’s  method takes a , so that we can pass the right path to the right inner reader. Accordingly, the  keys in the config need to be dictionaries.’s  method will return a .  will act like  (chaining the inner datasets into one large one), with two differences:We’ll write a  that takes a  as a  argument that defines the different data loaders for each task. It also takes a  that determines which task gets batched next. The  method batches instances from the next task and returns them as a .When it does this, it makes one change to the  that come from the inner data loaders: The instances will have a “task” field identifying the task, so the batch with those instances will contain a  with the task in it. Since we know that one batch always comes from a single task, we replace that field with a single  containing the task name.Note that this design makes it possible to define different batch sizes and shuffling options per task.Multitask schedulers are the only type of object that is genuinely new. As usual, we should have a base class with a a few implementations:We’ll write a  base class. Derivations of this class hold some parameters within themselves (the “backbone”), and also support one or more “heads”, which perform the actual tasks. Every head is a  in its own right, i.e., it returns a loss from the  method.The ’s  method must take a  that describes the task-specific heads, each with a name. Optionally, it should take a , containing scaling factors for the loss for each task.The  method should take the following arguments:The heads return a  of results, including a loss. The  renames all of the results such that they are prefixed by the head name. For example, if the  head returns a tensor under the name , the  returns a tensor under the name . In addition, it computes a loss based on the loss from the head, and the loss weight for the task.Similarly, the ’s  call will call  on all of the heads, and prefix the results with the head’s names.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 30, 2020", "body": "My first pass at how I would design something for this:You have a  or a , which goes from inputs to an encoded representation that's ready to have various task heads applied.  (Though, really, the  already does what the  would do, so we may not need anything new there, just for text+images.)  Then you have a  that takes as  arguments a  and a list of prediction heads.   is a bit tricky, you probably have to have it accept  and make assumptions about the names of things that it gets.  But assuming you can work that out, you then have the model apply whatever heads are required based on the inputs it receives, compute a joint loss, and that's it.  You can configure this pretty easily to add another head just by adding another prediction head to the list, and an appropriate dataset inside a multi-task dataset reader.", "type": "commented", "related_issue": null}, {"user_name": "jiasenlu", "datetime": "Jul 1, 2020", "body": "I think what  suggested about model forward pass makes sense. For multi-task dataloader, we can borrow the existing code from M3Transofrmers. What we need is to call  before each iteration, and it will output the batched data for different tasks.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2020", "body": "I updated this issue's description to be basically a spec, ready to implement. , does it address your concerns?", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Aug 17, 2020", "body": "I put the description of Stop&Go into , since we can do that separately from this.", "type": "commented", "related_issue": null}, {"user_name": "wlhgtc", "datetime": "Aug 18, 2020", "body": "Yes.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Nov 16, 2020", "body": "  is going to check if there's anything left to do.", "type": "commented", "related_issue": null}, {"user_name": "mateuszpieniak", "datetime": "Nov 5, 2021", "body": "How can we distribute a single instance to two or more heads?", "type": "commented", "related_issue": null}, {"user_name": "jbrry", "datetime": "Nov 5, 2021", "body": " A quick workaround I ended up doing was introducing a boolean value  for the  model, where if specified, skips some of the dataset reader to task-head mapping logic.It's not properly tested but served me for my needs. You can see some of the control flow .", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 18, 2021", "body": "To support this properly I would write another version of  that supports this.  is not that big a class anyways. But if you have a workaround, that's good too.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 30, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 30, 2020", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "wlhgtc", "datetime": "Aug 12, 2020", "body": [], "type": "issue", "related_issue": "#4551"}, {"user_name": "dirkgr", "datetime": "Aug 17, 2020", "body": [], "type": "issue", "related_issue": "#4566"}, {"user_name": "matt-gardner", "datetime": "Aug 26, 2020", "body": [], "type": "pull", "related_issue": "#4601"}, {"user_name": "schmmd", "datetime": "Nov 16, 2020", "body": [], "type": "unassigned", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Nov 23, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4362", "issue_status": " Closed\n", "issue_list": [{"user_name": "mateuszpieniak", "datetime": "Jun 16, 2020", "body": "\nI noticed that BERT-like architecture is usually fully fine-tuned, but very shortly (1-2 epochs) with some warm-up (to avoid catastrophic forgetting) - for example in . I believe that such a resolution is not enough for good validation score estimation i.e. you don't really know the curve's shape. I could lower down learning rate and increase the number of epochs, but it will take a lot of time to train.A slightly different example of calculating the validation score more frequently is , which was implemented in Caffe (no epoch concept). In the paper, the LR test was performed on the whole validation set after N batches.\nProvide an additional argument to  that denotes the frequency of validation evaluation.\nDecrease the learning rate and increase the number of epochs.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 16, 2020", "body": "I was actually just thinking about this as well. We already  have a setting for this: .But this isn't perfect because if  is less than the total number of batches in the dataset, then you're not guaranteed to see every unique instance:\nThat said, I don't think it would too hard to tweak it to ensure that every instance is seen (provided you're running for enough epochs). The  would just have to keep some internal state about where to pick up again next time  is called.", "type": "commented", "related_issue": null}, {"user_name": "mateuszpieniak", "datetime": "Jun 17, 2020", "body": " Super cool! In my case, I don't need such guarantees. I am not sure though whether  scheduler reads the proper number of batches, but I guess I will find out tomorrow.Btw, out of curiosity, do you think it can be considered a regularization? In theory the bigger the dataset, the better generalization properties. On the other hand sampling data from the training, dataset restricts you from fitting perfectly into your training dataset. This can be considered a regularization technique, but the \"effective dataset size\" is smaller due to sampling.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 17, 2020", "body": "Hmm yea I suppose you could think of it that way. Kind of like dropout, except applied to training instances instead of individual weights.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Jun 19, 2020", "body": " is the right answer here. If that does not guarantee that you'll see all instances, we have to fix it. Is that the case?", "type": "commented", "related_issue": null}, {"user_name": "mateuszpieniak", "datetime": "Jun 19, 2020", "body": " Yes, that's the case. I am satisfied with the current implementation of  for my purpose, but I think that such a guarantee should be more intuitive for a user.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 22, 2020", "body": "Ok, sounds like this one can be closed, but I've open up a separate issue to address the behavior with : .", "type": "commented", "related_issue": null}, {"user_name": "mateuszpieniak", "datetime": "Jun 16, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 22, 2020", "body": [], "type": "issue", "related_issue": "#4393"}, {"user_name": "epwalsh", "datetime": "Jun 22, 2020", "body": [], "type": "closed this as", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Apr 28, 2021", "body": [], "type": "issue", "related_issue": "#5155"}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4351", "issue_status": " Closed\n", "issue_list": [{"user_name": "JohnGiorgi", "datetime": "Jun 11, 2020", "body": "As per , I think it would be very useful if there were instructions for uploading the weights of a transformer-based language model trained with AllenNLP to .A mechanism (if it doesn't already exist) and instructions for uploading the weights of a pretrained language model trained with AllenNLP to .N/A.I first asked if this was possible  and  asked me to open a feature request here.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Jun 11, 2020", "body": " is this something you did with one of your models?  Apologies if I'm misremembering.", "type": "commented", "related_issue": null}, {"user_name": "ZhaofengWu", "datetime": "Jun 11, 2020", "body": "I uploaded and only uploaded SpanBERT (, ). It's not generated by AllenNLP and was already packaged in a way that huggingface recognizes, so it's probably not super relevant here.", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "Jun 12, 2020", "body": " if you have a huggingface model in memory is that sufficient?  If so, we can give you some pointers on how to do that.  If not, we need to figure something bigger out.", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Jun 15, 2020", "body": " Do you mean did I train a huggingface model with allennlp? If so then yes, all my trainable weights are in a ", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 15, 2020", "body": "The question is, if you can get the in-memory  from , is that sufficient to follow their instructions to upload weights, or is something else needed?  I haven't done this before, but it looks like there's a  method that you can call on the in-memory , which you can pull out of our  object.", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Jun 15, 2020", "body": "Oh sorry, I understand now. Yes that seems obvious in hindsight, I guess it is just a matter of figuring out where/when to call  but I am sure I can figure that out. Thanks for the guidance!", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Jun 24, 2020", "body": "Okay, for anyone who lands on this issue, I have figured out how to do this with an  object:Of course, this implementation relied on knowing that for my model,  exists and that is is a .I confirmed that a model saved this way can be uploaded (and downloaded from) .", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 12, 2020", "body": " thanks for that example.And for everyone in this thread, where do you think would be the best place to document this? A couple of ideas (not necessarily mutually exclusive):", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Aug 12, 2020", "body": " No problem, there's just a couple of issues I ran into:My updated example looked like:There also appears to be something strange happening if you call this on a distributed model. I never really got to the bottom of it, but when I ran this code on a CPU, uploading/downloading to  worked fine. When I actually went to train my model with AllenNLP in a distributed setup and run this code, uploading works but downloading throws a weird error. This callback might need to make a copy of the model/tokenizer on the CPU before calling  to avoid this.In the end, I actually found it was easier to write a script that loads a predictor and from there calls  (see ).", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 12, 2020", "body": "Thanks , that's helpful!", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Aug 12, 2020", "body": "Put a section in the guide instead of this?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 12, 2020", "body": " either way is fine with me, I just wasn't sure if this would be within the scope of the guide.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 13, 2020", "body": "After talking with  and  we decided that it would beneficial (and very little extra work) to publish mini tutorials like these on both Medium and the guide.We want the guide to be the centralized source of truth for AllenNLP documentation, but cross-posting on Medium would make these tutorials easier to find and could ultimately help drive traffic to the guide as well.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 14, 2020", "body": "Medium post on this here: .I'll hold off on closing this issue until I've added it to the guide as well so I don't forget.Thanks  for bringing this up!", "type": "commented", "related_issue": null}, {"user_name": "JohnGiorgi", "datetime": "Jun 11, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Aug 6, 2020", "body": [], "type": "issue", "related_issue": "#4541"}, {"user_name": "AkshitaB", "datetime": "Aug 7, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 12, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Aug 18, 2020", "body": [], "type": "pull", "related_issue": "allenai/allennlp-guide#139"}, {"user_name": "epwalsh", "datetime": "Aug 18, 2020", "body": [], "type": "pull", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4405", "issue_status": " Closed\n", "issue_list": [{"user_name": "liyucheng09", "datetime": "Jun 26, 2020", "body": "", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 26, 2020", "body": "I  all you need to do is .  Then you can use the predictor in your script however you want.  Does that work for you?", "type": "commented", "related_issue": null}, {"user_name": "liyucheng09", "datetime": "Jun 27, 2020", "body": "Thanks for your immediate replay. I successfully build a predictor by your codes:But when I use the predictor by  there is an NotInplementedError, this is because the predictor is a default Predictor and the predict function is not defined yet.Then I use the  Predictor which has the defined  function:But it return a default Predictor also and the there is no  function obviously.So I still have no idea that how to use the archiev file to initialize a custom Predictor and use it to predict.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 29, 2020", "body": "If you look at , you'll see that there's a  argument to .  The predictor name for  is .But I'm a bit confused about your second line of code - that looks to me like it should return an instance of , not .  Are you sure it's returning just a ?", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Aug 18, 2020", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "liyucheng09", "datetime": "Jun 26, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 26, 2020", "body": [], "type": "added", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 26, 2020", "body": [], "type": "self-assigned this", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Aug 18, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5409", "issue_status": " Open\n", "issue_list": [{"user_name": "martin-kirilov", "datetime": "Sep 14, 2021", "body": "Spacy 3 introduced new transformer-based models that can be run on GPU. I think it's a good idea to add support for these types of language models in AllenNLP, since now it doesn't work out-of-the-box.My suggestion is to add an option to specify whether to load the model on CPU or GPU, and also use it accordingly.\nThis would require changes to both allennlp and allennlp-models, as in some models (e.g. SRL), there should be made some changes to the .", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Sep 20, 2021", "body": "Hey , this sounds like a good feature to have. Feel free to submit a PR if you get a chance.", "type": "commented", "related_issue": null}, {"user_name": "github-actions", "datetime": "Oct 5, 2021", "body": " this is just a friendly ping to make sure you haven't forgotten about this issue ", "type": "commented", "related_issue": null}, {"user_name": "martin-kirilov", "datetime": "Sep 14, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "AkshitaB", "datetime": "Sep 17, 2021", "body": [], "type": "assigned", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Sep 20, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Oct 7, 2021", "body": [], "type": "removed their assignment", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5488", "issue_status": " Open\n", "issue_list": [{"user_name": "unikcc", "datetime": "Nov 30, 2021", "body": "\nI' a newbie for AllenNLP but I've fall in love with it to handle my own NLP task.\nBut each time I start a new project, I have to copy the previous code of other AllenNLP project since I can't remember the file architecture.\nThen there will me much old code that is redundant for the new project and I have to delete so much lines that are task-specific.\nSo I need a tool or just a command(e.g. ) to init a golden template project of AllenNLP, which contains some basic moduel for runing a AllenNLP code, like model,  configuration, data processor and demo data folder, etc.\nI think this feature will make researcher much cheerful to work with AllenNLP.\nAdd a command to init a simplest AllenNLP project, just like spring-boot-start for java web developer.\nOr provide a demo code project of AllenNLP and I can easily access for it.\nNo", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 3, 2021", "body": "Hi , we have a template repo for this purpose: . You could also make your own template repo if that one is not exactly what you're looking for.", "type": "commented", "related_issue": null}, {"user_name": "lgessler", "datetime": "Dec 6, 2021", "body": "I'd just like to add that I agree with  that this kind of a thing is a great boon for beginners. CLI templater tools ( and  are the ones I'm most familiar with), when done well, can get a user straight into the \"real code\" they want to write without requiring them to handle small but cognitively non-negligible things like, for example, renaming packages or deleting files they don't intend to use. (Templaters avoid those two problems by parameterizing package name and parameterizing modules so you can choose whether you want CI, tests, flake8/black/tox support, JSON-, Jsonnet-, or code-based execution, etc.)Maybe it turns out this problem is better handled as a 3rd party lib than as an AllenNLP-owned functionality, but regardless I think this would be an awesome addition to the AllenNLP ecosystem. Perhaps I and anyone else who's interested could work towards implementing something like this?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Dec 6, 2021", "body": "I think that's true. At least it would be easier to get up and running and faster to develop since you wouldn't have to wait for feedback from us to merge PRs. I'm happy to provide feedback / advice if you want it though.", "type": "commented", "related_issue": null}, {"user_name": "unikcc", "datetime": "Nov 30, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Dec 10, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4397", "issue_status": " Open\n", "issue_list": [{"user_name": "dhruvdcoder", "datetime": "Jun 24, 2020", "body": "I wish to use a pre-trained embedding layer in my model with possible vocabulary extension before finetuning. Currently, there is no way to do this using a config file unless I write my own train command. Such a feature would be really useful while finetuning a general Transformer in a  layer for a specific domain.Ideally, this approach should be available with any  with the details about how to extend the vocabulary left to the concrete class. Depending on the embedder, we might also want to pass the extra words to the tokenizer. As far as  goes, the Huggingface APIs offer a way to extend the vocabulary of the tokenizer as well as the model (ref:  ).\nThe easiest way I can think of achieving this would be to add an extra parameter in the constructor of  and  which points to an extra vocab file (which can optionally also contain weights to initialize the extra vocabulary tokens for the embedder).\nI have not really considered an alternative approach. I can come up with it if the proposed approach does not seem reasonable.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 26, 2020", "body": "We should try to figure this out when / after we rethink our vocab. See .But in the meantime if there's a quick fix for this, we are open to that.", "type": "commented", "related_issue": null}, {"user_name": "dhruvdcoder", "datetime": "Jun 24, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 26, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4295", "issue_status": " Open\n", "issue_list": [{"user_name": "xdwang0726", "datetime": "May 27, 2020", "body": "For now, Embedding only take .txt and .hdf5 for pre-trained embedding format. Would it be possible to add .bin format as .bin is the most commonly used for pre-trained format. Thank you!", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "May 27, 2020", "body": "It's certainly possible and shouldn't be too difficult. Contributions welcome!", "type": "commented", "related_issue": null}, {"user_name": "schmmd", "datetime": "May 29, 2020", "body": " can you give a clear example of the type of file you are proposing adding support for?  We're not sure what  format is, and it'd be good for us to understand the format before anyone begins implementation on adding support.", "type": "commented", "related_issue": null}, {"user_name": "gabeorlanski", "datetime": "Feb 15, 2021", "body": "I have encountered issues ( in AllenNLP but python in general) where  files would not load in python because they were made on a different os than the one I was using. Mostly it was Windows vs. Linux, but I even had issues in WSL vs. pure Linux. So any implementation may have to deal with this.Also, , I would assume the  he refers to is just pickled data from python's  module. It could be completely wrong, though.", "type": "commented", "related_issue": null}, {"user_name": "xdwang0726", "datetime": "Feb 21, 2021", "body": "For example, the google pretrained word2vec is in .bin file (GoogleNews-vectors-negative300.bin)", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 30, 2021", "body": "\nI see that the .bin you have pointed to  is from \nI remembered trying word2vec from gensim in 2016 I think.I can see that gensim comment to load the bin file indicates that it is in a Word2Vec only C-based format.  and specifically Can you point to any other case of such .bin vectors or/and somewhere where it can be figured out what exactly the bin is formatted as. Is it pickle like  indicated (which with the above word2vec case is not)?One can always write one-time script to convert from word2vec bin to desired hdf5 or/and text.I was thinking of picking this up in the coming week and hence the question.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Apr 5, 2021", "body": "I am also interested in getting this done, but without a clear indication of what exactly the format is, I don't see how we can do it.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "May 27, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "May 27, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4524", "issue_status": " Open\n", "issue_list": [{"user_name": "bratao", "datetime": "Jul 30, 2020", "body": "\nI want to train a model that maximize f-1, but when two epochs have the same f-1, check if loss decreased.Some models that I'm training achieve a f-1 of 1.0. but there is still potential for a decrease in loss.\nThat the metric tracker allows to use a secondary metric to track.Would AllenNLP accept a PR with this feature or is out of scope?", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jul 30, 2020", "body": "Yes, this is something we'd accept a PR for.  I would recommend changing the type of  to .", "type": "commented", "related_issue": null}, {"user_name": "ghost", "datetime": "Mar 30, 2021", "body": "We were checking to see if we could implement this but,\n you seem to have implemented it in .\nPlease check.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "Mar 30, 2021", "body": "It works a little bit different now. The way I changed it, it sums up several metrics and checks whether the sum has improved. Does that solve the problem you were trying to solve?", "type": "commented", "related_issue": null}, {"user_name": "bratao", "datetime": "Mar 30, 2021", "body": "For me, yes!", "type": "commented", "related_issue": null}, {"user_name": "bratao", "datetime": "Jul 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jul 30, 2020", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/4304", "issue_status": " Closed\n", "issue_list": [{"user_name": "dirkgr", "datetime": "May 30, 2020", "body": "People using the library () are ending up with lots of model directories that are full of checkpoints, wasting many TB of space. We could have an  command that finds these and deletes them, leaving only the final model.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "May 30, 2020", "body": "Finding arbitrary directories that were used seems really challenging.  We could walk a directory looking for weights files, but how do we know which ones to keep...?An alternative solution is to just change the number of serialized models that we keep around, which we did already: ", "type": "commented", "related_issue": null}, {"user_name": "ZhaofengWu", "datetime": "May 30, 2020", "body": "+1 to Matt's comment. I was having the same problem in 0.9 but with this change this shouldn't be a problem anymore.Regardless, if we want to do this, it might be better to come up with a name other than  to avoid confusion with model pruning.", "type": "commented", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 5, 2020", "body": "In issue review we thought we should just close this.  Please reopen  if you want to discuss further.", "type": "commented", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 30, 2020", "body": [], "type": "added this to the", "related_issue": null}, {"user_name": "dirkgr", "datetime": "May 30, 2020", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "matt-gardner", "datetime": "Jun 5, 2020", "body": [], "type": "closed this as", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5450", "issue_status": " Open\n", "issue_list": [{"user_name": "HOZHENWAI", "datetime": "Oct 27, 2021", "body": "\nOn the current version of 2.7.0 of allennlp and versions 4.11.3 of transformers, layoutlmv2 is not supported :Error occurs since they added an argument  as second argument of the fast layoutlm_v2 tokenizer which breaks the reverse engineer of the special tokens of allennlp pretrained_transformer_tokenizer.\nIdeally, naming the arguments in  of pretrained_transformer_tokenizer should do the work but I'm afraid of repercussions on other tokenizer that have different argument name (those not based of Bert maybe?)\nMoreover since layoutlm_v2 added a few input to the model (images and boxes), modifications should be made to _unfold_long_sequences, _fold_long_sequences and forward of the pretrained_transformer_embedder and pretrained_transformer_mismatched_embedder to account for additional inputs.If it's okay with you, I'd like to work in it.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Oct 29, 2021", "body": "Hi , yes this would be a good fix to have. Feel free to open a PR when you're ready.", "type": "commented", "related_issue": null}, {"user_name": "HOZHENWAI", "datetime": "Oct 27, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Oct 29, 2021", "body": [], "type": "added  the", "related_issue": null}]},
{"issue_url": "https://github.com/allenai/allennlp/issues/5237", "issue_status": " Open\n", "issue_list": [{"user_name": "danieldeutsch", "datetime": "Jun 2, 2021", "body": "\nI typically used compressed datasets (e.g. gzipped) to save disk space. This works fine with AllenNLP during training because I can write my dataset reader to load the compressed data. However, the  command opens the file and reads lines for the . This fails when it tries to load data from my compressed files.\nEither automatically detect the file is compressed or add a flag to  that indicates that the file is compressed. One method that I have used to detect if a file is gzipped is , although it isn't 100% accurate. I have an implementation . Otherwise a flag like  to mark how the file is compressed should be sufficient. Passing the type of compression would allow support for gzip, bz2, or any other method.", "type": "commented", "related_issue": null}, {"user_name": "ArjunSubramonian", "datetime": "Jun 3, 2021", "body": "I think this feature is a great idea! The latter design (passing a flag) seems better to me.I am adding  here to get his input as well.", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 3, 2021", "body": "Yeup, this seems reasonable. I think we should try to automatically detect the compression type, but also have the flag so that users can override it when the automatic detection fails.You may find this helper function useful: ", "type": "commented", "related_issue": null}, {"user_name": "Dbhasin1", "datetime": "Jun 29, 2021", "body": "Hi, I'd like to try working on this. I'm relatively a noobie so are there any pointers I should keep in mind before raising a pull request?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Jun 29, 2021", "body": "Hi , check out ", "type": "commented", "related_issue": null}, {"user_name": "spranjal25", "datetime": "Oct 14, 2021", "body": "Hi ! is this issue still not resolved? I'm looking for issues to start contributing to AllenNLP, can I take this up if not resolved already?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Oct 25, 2021", "body": "Hi , we haven't heard from  for a while on their PR, so it's probably okay for you take over at this point.", "type": "commented", "related_issue": null}, {"user_name": "Dbhasin1", "datetime": "Oct 25, 2021", "body": "hey, sorry I'd been engaged elsewhere for a while. I'd like to give it one more shot!", "type": "commented", "related_issue": null}, {"user_name": "aterzgar", "datetime": "Jul 21, 2022", "body": "is the issue still open ?", "type": "commented", "related_issue": null}, {"user_name": "Akshat977", "datetime": "Sep 10, 2022", "body": "Hi  ,  ,\nThis issue seems to be a good initiation of my journey towards contribution to FOSS projects.\nCan you please assign this to me?", "type": "commented", "related_issue": null}, {"user_name": "epwalsh", "datetime": "Sep 12, 2022", "body": "Hi , feel free to open a PR when you're ready", "type": "commented", "related_issue": null}, {"user_name": "danieldeutsch", "datetime": "Jun 2, 2021", "body": [], "type": "added  the", "related_issue": null}, {"user_name": "ArjunSubramonian", "datetime": "Jun 3, 2021", "body": [], "type": "added", "related_issue": null}, {"user_name": "Dbhasin1", "datetime": "Jul 2, 2021", "body": [], "type": "pull", "related_issue": "#5299"}]}
]
}